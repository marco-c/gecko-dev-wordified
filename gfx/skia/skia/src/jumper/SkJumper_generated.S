#
Copyright
2017
Google
Inc
.
#
#
Use
of
this
source
code
is
governed
by
a
BSD
-
style
license
that
can
be
#
found
in
the
LICENSE
file
.
#
This
file
is
generated
semi
-
automatically
with
this
command
:
#
src
/
jumper
/
build_stages
.
py
#
if
defined
(
__MACH__
)
#
define
HIDDEN
.
private_extern
#
define
FUNCTION
(
name
)
#
define
BALIGN4
.
align
2
#
define
BALIGN8
.
align
3
#
define
BALIGN16
.
align
4
#
define
BALIGN32
.
align
5
#
else
.
section
.
note
.
GNU
-
stack
"
"
%
progbits
#
define
HIDDEN
.
hidden
#
define
FUNCTION
(
name
)
.
type
name
%
function
#
define
BALIGN4
.
balign
4
#
define
BALIGN8
.
balign
8
#
define
BALIGN16
.
balign
16
#
define
BALIGN32
.
balign
32
#
endif
.
text
#
if
defined
(
__x86_64__
)
BALIGN32
HIDDEN
_sk_start_pipeline_skx
.
globl
_sk_start_pipeline_skx
FUNCTION
(
_sk_start_pipeline_skx
)
_sk_start_pipeline_skx
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
152
0
0
0
/
/
jae
cb
<
_sk_start_pipeline_skx
+
0xcb
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
8
/
/
lea
0x8
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
119
67
/
/
ja
8c
<
_sk_start_pipeline_skx
+
0x8c
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
8
/
/
lea
0x8
(
%
r12
)
%
rdx
.
byte
73
131
196
16
/
/
add
0x10
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
193
/
/
jbe
4d
<
_sk_start_pipeline_skx
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
41
/
/
je
bd
<
_sk_start_pipeline_skx
+
0xbd
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
131
195
1
/
/
add
0x1
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
15
133
116
255
255
255
/
/
jne
3f
<
_sk_start_pipeline_skx
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
197
248
119
/
/
vzeroupper
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_skx
.
globl
_sk_just_return_skx
FUNCTION
(
_sk_just_return_skx
)
_sk_just_return_skx
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_skx
.
globl
_sk_seed_shader_skx
FUNCTION
(
_sk_seed_shader_skx
)
_sk_seed_shader_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
242
125
40
124
194
/
/
vpbroadcastd
%
edx
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
98
242
125
40
124
201
/
/
vpbroadcastd
%
ecx
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
241
116
56
88
13
246
195
3
0
/
/
vaddps
0x3c3f6
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
21
239
195
3
0
/
/
vbroadcastss
0x3c3ef
(
%
rip
)
%
ymm2
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dither_skx
.
globl
_sk_dither_skx
FUNCTION
(
_sk_dither_skx
)
_sk_dither_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
114
125
40
124
194
/
/
vpbroadcastd
%
edx
%
ymm8
.
byte
197
61
254
5
141
197
3
0
/
/
vpaddd
0x3c58d
(
%
rip
)
%
ymm8
%
ymm8
#
3c6c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x474
>
.
byte
98
114
125
40
124
201
/
/
vpbroadcastd
%
ecx
%
ymm9
.
byte
196
65
53
239
200
/
/
vpxor
%
ymm8
%
ymm9
%
ymm9
.
byte
196
98
125
88
21
185
195
3
0
/
/
vpbroadcastd
0x3c3b9
(
%
rip
)
%
ymm10
#
3c500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b4
>
.
byte
196
65
53
219
218
/
/
vpand
%
ymm10
%
ymm9
%
ymm11
.
byte
196
193
37
114
243
5
/
/
vpslld
0x5
%
ymm11
%
ymm11
.
byte
196
65
61
219
210
/
/
vpand
%
ymm10
%
ymm8
%
ymm10
.
byte
196
193
45
114
242
4
/
/
vpslld
0x4
%
ymm10
%
ymm10
.
byte
196
98
125
88
37
158
195
3
0
/
/
vpbroadcastd
0x3c39e
(
%
rip
)
%
ymm12
#
3c504
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b8
>
.
byte
196
98
125
88
45
153
195
3
0
/
/
vpbroadcastd
0x3c399
(
%
rip
)
%
ymm13
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
196
65
53
219
245
/
/
vpand
%
ymm13
%
ymm9
%
ymm14
.
byte
196
193
13
114
246
2
/
/
vpslld
0x2
%
ymm14
%
ymm14
.
byte
196
65
37
235
222
/
/
vpor
%
ymm14
%
ymm11
%
ymm11
.
byte
196
65
61
219
237
/
/
vpand
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
21
254
237
/
/
vpaddd
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
21
235
210
/
/
vpor
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
53
219
204
/
/
vpand
%
ymm12
%
ymm9
%
ymm9
.
byte
196
193
53
114
209
1
/
/
vpsrld
0x1
%
ymm9
%
ymm9
.
byte
196
65
61
219
196
/
/
vpand
%
ymm12
%
ymm8
%
ymm8
.
byte
196
193
61
114
208
2
/
/
vpsrld
0x2
%
ymm8
%
ymm8
.
byte
196
65
45
235
192
/
/
vpor
%
ymm8
%
ymm10
%
ymm8
.
byte
196
65
61
235
195
/
/
vpor
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
75
195
3
0
/
/
vbroadcastss
0x3c34b
(
%
rip
)
%
ymm9
#
3c50c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c0
>
.
byte
98
114
61
56
168
13
69
195
3
0
/
/
vfmadd213ps
0x3c345
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c510
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c4
>
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
65
52
89
192
/
/
vmulps
%
ymm8
%
ymm9
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
88
201
/
/
vaddps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
88
210
/
/
vaddps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
252
93
195
/
/
vminps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
244
93
203
/
/
vminps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
236
93
211
/
/
vminps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_skx
.
globl
_sk_uniform_color_skx
FUNCTION
(
_sk_uniform_color_skx
)
_sk_uniform_color_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm0
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_skx
.
globl
_sk_black_color_skx
FUNCTION
(
_sk_black_color_skx
)
_sk_black_color_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
210
194
3
0
/
/
vbroadcastss
0x3c2d2
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_skx
.
globl
_sk_white_color_skx
FUNCTION
(
_sk_white_color_skx
)
_sk_white_color_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
185
194
3
0
/
/
vbroadcastss
0x3c2b9
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
252
40
216
/
/
vmovaps
%
ymm0
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_rgba_skx
.
globl
_sk_load_rgba_skx
FUNCTION
(
_sk_load_rgba_skx
)
_sk_load_rgba_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
197
252
16
72
32
/
/
vmovups
0x20
(
%
rax
)
%
ymm1
.
byte
197
252
16
80
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm2
.
byte
197
252
16
88
96
/
/
vmovups
0x60
(
%
rax
)
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_rgba_skx
.
globl
_sk_store_rgba_skx
FUNCTION
(
_sk_store_rgba_skx
)
_sk_store_rgba_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
197
252
17
72
32
/
/
vmovups
%
ymm1
0x20
(
%
rax
)
.
byte
197
252
17
80
64
/
/
vmovups
%
ymm2
0x40
(
%
rax
)
.
byte
197
252
17
88
96
/
/
vmovups
%
ymm3
0x60
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_skx
.
globl
_sk_clear_skx
FUNCTION
(
_sk_clear_skx
)
_sk_clear_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_skx
.
globl
_sk_srcatop_skx
FUNCTION
(
_sk_srcatop_skx
)
_sk_srcatop_skx
:
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
88
194
3
0
/
/
vbroadcastss
0x3c258
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
226
61
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
196
226
61
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
196
226
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
196
194
69
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_skx
.
globl
_sk_dstatop_skx
FUNCTION
(
_sk_dstatop_skx
)
_sk_dstatop_skx
:
.
byte
196
98
125
24
5
39
194
3
0
/
/
vbroadcastss
0x3c227
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
226
101
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
226
101
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
226
101
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
60
89
195
/
/
vmulps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_skx
.
globl
_sk_srcin_skx
FUNCTION
(
_sk_srcin_skx
)
_sk_srcin_skx
:
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_skx
.
globl
_sk_dstin_skx
FUNCTION
(
_sk_dstin_skx
)
_sk_dstin_skx
:
.
byte
197
228
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
228
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
228
89
214
/
/
vmulps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_skx
.
globl
_sk_srcout_skx
FUNCTION
(
_sk_srcout_skx
)
_sk_srcout_skx
:
.
byte
196
98
125
24
5
202
193
3
0
/
/
vbroadcastss
0x3c1ca
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_skx
.
globl
_sk_dstout_skx
FUNCTION
(
_sk_dstout_skx
)
_sk_dstout_skx
:
.
byte
196
226
125
24
5
169
193
3
0
/
/
vbroadcastss
0x3c1a9
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
92
219
/
/
vsubps
%
ymm3
%
ymm0
%
ymm3
.
byte
197
228
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
228
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
228
89
214
/
/
vmulps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_skx
.
globl
_sk_srcover_skx
FUNCTION
(
_sk_srcover_skx
)
_sk_srcover_skx
:
.
byte
196
98
125
24
5
136
193
3
0
/
/
vbroadcastss
0x3c188
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
93
184
192
/
/
vfmadd231ps
%
ymm8
%
ymm4
%
ymm0
.
byte
196
194
85
184
200
/
/
vfmadd231ps
%
ymm8
%
ymm5
%
ymm1
.
byte
196
194
77
184
208
/
/
vfmadd231ps
%
ymm8
%
ymm6
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_skx
.
globl
_sk_dstover_skx
FUNCTION
(
_sk_dstover_skx
)
_sk_dstover_skx
:
.
byte
196
98
125
24
5
99
193
3
0
/
/
vbroadcastss
0x3c163
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
196
226
61
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm8
%
ymm0
.
byte
196
226
61
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm8
%
ymm1
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
196
226
61
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_skx
.
globl
_sk_modulate_skx
FUNCTION
(
_sk_modulate_skx
)
_sk_modulate_skx
:
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_skx
.
globl
_sk_multiply_skx
FUNCTION
(
_sk_multiply_skx
)
_sk_multiply_skx
:
.
byte
196
98
125
24
5
42
193
3
0
/
/
vbroadcastss
0x3c12a
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
52
89
208
/
/
vmulps
%
ymm0
%
ymm9
%
ymm10
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
61
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm10
.
byte
196
194
93
168
194
/
/
vfmadd213ps
%
ymm10
%
ymm4
%
ymm0
.
byte
197
52
89
209
/
/
vmulps
%
ymm1
%
ymm9
%
ymm10
.
byte
196
98
61
184
213
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm10
.
byte
196
194
85
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm5
%
ymm1
.
byte
197
52
89
210
/
/
vmulps
%
ymm2
%
ymm9
%
ymm10
.
byte
196
98
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm10
.
byte
196
194
77
168
210
/
/
vfmadd213ps
%
ymm10
%
ymm6
%
ymm2
.
byte
197
52
89
203
/
/
vmulps
%
ymm3
%
ymm9
%
ymm9
.
byte
196
66
69
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm7
%
ymm8
.
byte
196
194
69
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__skx
.
globl
_sk_plus__skx
FUNCTION
(
_sk_plus__skx
)
_sk_plus__skx
:
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
217
192
3
0
/
/
vbroadcastss
0x3c0d9
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
244
88
205
/
/
vaddps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
236
88
214
/
/
vaddps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_skx
.
globl
_sk_screen_skx
FUNCTION
(
_sk_screen_skx
)
_sk_screen_skx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
196
194
93
172
192
/
/
vfnmadd213ps
%
ymm8
%
ymm4
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
196
194
85
172
200
/
/
vfnmadd213ps
%
ymm8
%
ymm5
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
196
194
77
172
208
/
/
vfnmadd213ps
%
ymm8
%
ymm6
%
ymm2
.
byte
197
100
88
199
/
/
vaddps
%
ymm7
%
ymm3
%
ymm8
.
byte
196
194
69
172
216
/
/
vfnmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__skx
.
globl
_sk_xor__skx
FUNCTION
(
_sk_xor__skx
)
_sk_xor__skx
:
.
byte
196
98
125
24
5
132
192
3
0
/
/
vbroadcastss
0x3c084
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
226
61
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
180
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
226
61
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
180
89
210
/
/
vmulps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
226
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
180
89
219
/
/
vmulps
%
ymm3
%
ymm9
%
ymm3
.
byte
196
98
69
168
195
/
/
vfmadd213ps
%
ymm3
%
ymm7
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
195
/
/
vmovaps
%
ymm8
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_skx
.
globl
_sk_darken_skx
FUNCTION
(
_sk_darken_skx
)
_sk_darken_skx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
95
193
/
/
vmaxps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
95
201
/
/
vmaxps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
95
209
/
/
vmaxps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
8
192
3
0
/
/
vbroadcastss
0x3c008
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_skx
.
globl
_sk_lighten_skx
FUNCTION
(
_sk_lighten_skx
)
_sk_lighten_skx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
93
193
/
/
vminps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
93
201
/
/
vminps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
93
209
/
/
vminps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
179
191
3
0
/
/
vbroadcastss
0x3bfb3
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_skx
.
globl
_sk_difference_skx
FUNCTION
(
_sk_difference_skx
)
_sk_difference_skx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
93
193
/
/
vminps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
93
201
/
/
vminps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
93
209
/
/
vminps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
82
191
3
0
/
/
vbroadcastss
0x3bf52
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_skx
.
globl
_sk_exclusion_skx
FUNCTION
(
_sk_exclusion_skx
)
_sk_exclusion_skx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
12
191
3
0
/
/
vbroadcastss
0x3bf0c
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colorburn_skx
.
globl
_sk_colorburn_skx
FUNCTION
(
_sk_colorburn_skx
)
_sk_colorburn_skx
:
.
byte
98
241
92
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm4
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
98
125
24
13
233
190
3
0
/
/
vbroadcastss
0x3bee9
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
215
/
/
vsubps
%
ymm7
%
ymm9
%
ymm10
.
byte
197
44
89
216
/
/
vmulps
%
ymm0
%
ymm10
%
ymm11
.
byte
197
36
88
228
/
/
vaddps
%
ymm4
%
ymm11
%
ymm12
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
98
209
124
40
194
197
0
/
/
vcmpeqps
%
ymm13
%
ymm0
%
k0
.
byte
98
114
126
40
56
240
/
/
vpmovm2d
%
k0
%
ymm14
.
byte
197
52
92
203
/
/
vsubps
%
ymm3
%
ymm9
%
ymm9
.
byte
197
52
89
252
/
/
vmulps
%
ymm4
%
ymm9
%
ymm15
.
byte
98
225
68
40
92
196
/
/
vsubps
%
ymm4
%
ymm7
%
ymm16
.
byte
98
225
124
32
89
195
/
/
vmulps
%
ymm3
%
ymm16
%
ymm16
.
byte
98
242
125
40
76
192
/
/
vrcp14ps
%
ymm0
%
ymm0
.
byte
98
241
124
32
89
192
/
/
vmulps
%
ymm0
%
ymm16
%
ymm0
.
byte
197
196
93
192
/
/
vminps
%
ymm0
%
ymm7
%
ymm0
.
byte
197
196
92
192
/
/
vsubps
%
ymm0
%
ymm7
%
ymm0
.
byte
196
194
101
168
195
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm0
.
byte
197
132
88
192
/
/
vaddps
%
ymm0
%
ymm15
%
ymm0
.
byte
196
195
125
74
199
224
/
/
vblendvps
%
ymm14
%
ymm15
%
ymm0
%
ymm0
.
byte
196
195
125
74
196
128
/
/
vblendvps
%
ymm8
%
ymm12
%
ymm0
%
ymm0
.
byte
98
241
84
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm5
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
197
44
89
217
/
/
vmulps
%
ymm1
%
ymm10
%
ymm11
.
byte
197
36
88
229
/
/
vaddps
%
ymm5
%
ymm11
%
ymm12
.
byte
98
209
116
40
194
197
0
/
/
vcmpeqps
%
ymm13
%
ymm1
%
k0
.
byte
98
114
126
40
56
240
/
/
vpmovm2d
%
k0
%
ymm14
.
byte
197
52
89
253
/
/
vmulps
%
ymm5
%
ymm9
%
ymm15
.
byte
98
225
68
40
92
197
/
/
vsubps
%
ymm5
%
ymm7
%
ymm16
.
byte
98
225
124
32
89
195
/
/
vmulps
%
ymm3
%
ymm16
%
ymm16
.
byte
98
242
125
40
76
201
/
/
vrcp14ps
%
ymm1
%
ymm1
.
byte
98
241
124
32
89
201
/
/
vmulps
%
ymm1
%
ymm16
%
ymm1
.
byte
197
196
93
201
/
/
vminps
%
ymm1
%
ymm7
%
ymm1
.
byte
197
196
92
201
/
/
vsubps
%
ymm1
%
ymm7
%
ymm1
.
byte
196
194
101
168
203
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm1
.
byte
197
132
88
201
/
/
vaddps
%
ymm1
%
ymm15
%
ymm1
.
byte
196
195
117
74
207
224
/
/
vblendvps
%
ymm14
%
ymm15
%
ymm1
%
ymm1
.
byte
196
195
117
74
204
128
/
/
vblendvps
%
ymm8
%
ymm12
%
ymm1
%
ymm1
.
byte
98
241
76
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm6
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
197
44
88
222
/
/
vaddps
%
ymm6
%
ymm10
%
ymm11
.
byte
98
209
108
40
194
197
0
/
/
vcmpeqps
%
ymm13
%
ymm2
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
197
52
89
238
/
/
vmulps
%
ymm6
%
ymm9
%
ymm13
.
byte
197
68
92
246
/
/
vsubps
%
ymm6
%
ymm7
%
ymm14
.
byte
197
12
89
243
/
/
vmulps
%
ymm3
%
ymm14
%
ymm14
.
byte
98
242
125
40
76
210
/
/
vrcp14ps
%
ymm2
%
ymm2
.
byte
197
140
89
210
/
/
vmulps
%
ymm2
%
ymm14
%
ymm2
.
byte
197
196
93
210
/
/
vminps
%
ymm2
%
ymm7
%
ymm2
.
byte
197
196
92
210
/
/
vsubps
%
ymm2
%
ymm7
%
ymm2
.
byte
196
194
101
168
210
/
/
vfmadd213ps
%
ymm10
%
ymm3
%
ymm2
.
byte
197
148
88
210
/
/
vaddps
%
ymm2
%
ymm13
%
ymm2
.
byte
196
195
109
74
213
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm2
%
ymm2
.
byte
196
195
109
74
211
128
/
/
vblendvps
%
ymm8
%
ymm11
%
ymm2
%
ymm2
.
byte
196
194
69
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colordodge_skx
.
globl
_sk_colordodge_skx
FUNCTION
(
_sk_colordodge_skx
)
_sk_colordodge_skx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
98
209
92
40
194
192
0
/
/
vcmpeqps
%
ymm8
%
ymm4
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
98
125
24
21
186
189
3
0
/
/
vbroadcastss
0x3bdba
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
44
92
223
/
/
vsubps
%
ymm7
%
ymm10
%
ymm11
.
byte
197
36
89
224
/
/
vmulps
%
ymm0
%
ymm11
%
ymm12
.
byte
98
241
124
40
194
195
0
/
/
vcmpeqps
%
ymm3
%
ymm0
%
k0
.
byte
98
114
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm13
.
byte
197
44
92
211
/
/
vsubps
%
ymm3
%
ymm10
%
ymm10
.
byte
197
44
89
244
/
/
vmulps
%
ymm4
%
ymm10
%
ymm14
.
byte
197
12
88
248
/
/
vaddps
%
ymm0
%
ymm14
%
ymm15
.
byte
98
225
100
40
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm16
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
98
242
125
40
76
192
/
/
vrcp14ps
%
ymm0
%
ymm0
.
byte
98
241
124
32
89
192
/
/
vmulps
%
ymm0
%
ymm16
%
ymm0
.
byte
197
196
93
192
/
/
vminps
%
ymm0
%
ymm7
%
ymm0
.
byte
196
194
101
168
196
/
/
vfmadd213ps
%
ymm12
%
ymm3
%
ymm0
.
byte
197
140
88
192
/
/
vaddps
%
ymm0
%
ymm14
%
ymm0
.
byte
196
195
125
74
199
208
/
/
vblendvps
%
ymm13
%
ymm15
%
ymm0
%
ymm0
.
byte
196
195
125
74
196
144
/
/
vblendvps
%
ymm9
%
ymm12
%
ymm0
%
ymm0
.
byte
98
209
84
40
194
192
0
/
/
vcmpeqps
%
ymm8
%
ymm5
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
197
36
89
225
/
/
vmulps
%
ymm1
%
ymm11
%
ymm12
.
byte
98
241
116
40
194
195
0
/
/
vcmpeqps
%
ymm3
%
ymm1
%
k0
.
byte
98
114
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm13
.
byte
197
44
89
245
/
/
vmulps
%
ymm5
%
ymm10
%
ymm14
.
byte
197
12
88
249
/
/
vaddps
%
ymm1
%
ymm14
%
ymm15
.
byte
98
225
100
40
89
197
/
/
vmulps
%
ymm5
%
ymm3
%
ymm16
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
98
242
125
40
76
201
/
/
vrcp14ps
%
ymm1
%
ymm1
.
byte
98
241
124
32
89
201
/
/
vmulps
%
ymm1
%
ymm16
%
ymm1
.
byte
197
196
93
201
/
/
vminps
%
ymm1
%
ymm7
%
ymm1
.
byte
196
194
101
168
204
/
/
vfmadd213ps
%
ymm12
%
ymm3
%
ymm1
.
byte
197
140
88
201
/
/
vaddps
%
ymm1
%
ymm14
%
ymm1
.
byte
196
195
117
74
207
208
/
/
vblendvps
%
ymm13
%
ymm15
%
ymm1
%
ymm1
.
byte
196
195
117
74
204
144
/
/
vblendvps
%
ymm9
%
ymm12
%
ymm1
%
ymm1
.
byte
98
209
76
40
194
192
0
/
/
vcmpeqps
%
ymm8
%
ymm6
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
197
36
89
202
/
/
vmulps
%
ymm2
%
ymm11
%
ymm9
.
byte
98
241
108
40
194
195
0
/
/
vcmpeqps
%
ymm3
%
ymm2
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
197
44
89
230
/
/
vmulps
%
ymm6
%
ymm10
%
ymm12
.
byte
197
28
88
234
/
/
vaddps
%
ymm2
%
ymm12
%
ymm13
.
byte
197
100
89
246
/
/
vmulps
%
ymm6
%
ymm3
%
ymm14
.
byte
197
228
92
210
/
/
vsubps
%
ymm2
%
ymm3
%
ymm2
.
byte
98
242
125
40
76
210
/
/
vrcp14ps
%
ymm2
%
ymm2
.
byte
197
140
89
210
/
/
vmulps
%
ymm2
%
ymm14
%
ymm2
.
byte
197
196
93
210
/
/
vminps
%
ymm2
%
ymm7
%
ymm2
.
byte
196
194
101
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm3
%
ymm2
.
byte
197
156
88
210
/
/
vaddps
%
ymm2
%
ymm12
%
ymm2
.
byte
196
195
109
74
213
176
/
/
vblendvps
%
ymm11
%
ymm13
%
ymm2
%
ymm2
.
byte
196
195
109
74
209
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm2
%
ymm2
.
byte
196
194
69
184
218
/
/
vfmadd231ps
%
ymm10
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_skx
.
globl
_sk_hardlight_skx
FUNCTION
(
_sk_hardlight_skx
)
_sk_hardlight_skx
:
.
byte
196
98
125
24
5
178
188
3
0
/
/
vbroadcastss
0x3bcb2
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
52
89
208
/
/
vmulps
%
ymm0
%
ymm9
%
ymm10
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
61
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm10
.
byte
197
124
88
216
/
/
vaddps
%
ymm0
%
ymm0
%
ymm11
.
byte
98
241
36
40
194
195
2
/
/
vcmpleps
%
ymm3
%
ymm11
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
197
124
89
228
/
/
vmulps
%
ymm4
%
ymm0
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
100
89
239
/
/
vmulps
%
ymm7
%
ymm3
%
ymm13
.
byte
197
68
92
244
/
/
vsubps
%
ymm4
%
ymm7
%
ymm14
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
148
92
192
/
/
vsubps
%
ymm0
%
ymm13
%
ymm0
.
byte
196
195
125
74
196
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm0
%
ymm0
.
byte
197
172
88
192
/
/
vaddps
%
ymm0
%
ymm10
%
ymm0
.
byte
197
52
89
209
/
/
vmulps
%
ymm1
%
ymm9
%
ymm10
.
byte
196
98
61
184
213
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm10
.
byte
197
116
88
217
/
/
vaddps
%
ymm1
%
ymm1
%
ymm11
.
byte
98
241
36
40
194
195
2
/
/
vcmpleps
%
ymm3
%
ymm11
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
197
116
89
229
/
/
vmulps
%
ymm5
%
ymm1
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
68
92
245
/
/
vsubps
%
ymm5
%
ymm7
%
ymm14
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
116
89
206
/
/
vmulps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
148
92
201
/
/
vsubps
%
ymm1
%
ymm13
%
ymm1
.
byte
196
195
117
74
204
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm1
%
ymm1
.
byte
197
172
88
201
/
/
vaddps
%
ymm1
%
ymm10
%
ymm1
.
byte
197
52
89
202
/
/
vmulps
%
ymm2
%
ymm9
%
ymm9
.
byte
196
98
61
184
206
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm9
.
byte
197
108
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm10
.
byte
98
241
44
40
194
195
2
/
/
vcmpleps
%
ymm3
%
ymm10
%
k0
.
byte
98
114
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm10
.
byte
197
108
89
222
/
/
vmulps
%
ymm6
%
ymm2
%
ymm11
.
byte
196
65
36
88
219
/
/
vaddps
%
ymm11
%
ymm11
%
ymm11
.
byte
197
68
92
230
/
/
vsubps
%
ymm6
%
ymm7
%
ymm12
.
byte
197
228
92
210
/
/
vsubps
%
ymm2
%
ymm3
%
ymm2
.
byte
196
193
108
89
212
/
/
vmulps
%
ymm12
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
148
92
210
/
/
vsubps
%
ymm2
%
ymm13
%
ymm2
.
byte
196
195
109
74
211
160
/
/
vblendvps
%
ymm10
%
ymm11
%
ymm2
%
ymm2
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_skx
.
globl
_sk_overlay_skx
FUNCTION
(
_sk_overlay_skx
)
_sk_overlay_skx
:
.
byte
196
98
125
24
5
206
187
3
0
/
/
vbroadcastss
0x3bbce
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
52
89
208
/
/
vmulps
%
ymm0
%
ymm9
%
ymm10
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
61
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm10
.
byte
197
92
88
220
/
/
vaddps
%
ymm4
%
ymm4
%
ymm11
.
byte
98
241
36
40
194
199
2
/
/
vcmpleps
%
ymm7
%
ymm11
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
197
124
89
228
/
/
vmulps
%
ymm4
%
ymm0
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
100
89
239
/
/
vmulps
%
ymm7
%
ymm3
%
ymm13
.
byte
197
68
92
244
/
/
vsubps
%
ymm4
%
ymm7
%
ymm14
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
148
92
192
/
/
vsubps
%
ymm0
%
ymm13
%
ymm0
.
byte
196
195
125
74
196
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm0
%
ymm0
.
byte
197
172
88
192
/
/
vaddps
%
ymm0
%
ymm10
%
ymm0
.
byte
197
52
89
209
/
/
vmulps
%
ymm1
%
ymm9
%
ymm10
.
byte
196
98
61
184
213
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm10
.
byte
197
84
88
221
/
/
vaddps
%
ymm5
%
ymm5
%
ymm11
.
byte
98
241
36
40
194
199
2
/
/
vcmpleps
%
ymm7
%
ymm11
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
197
116
89
229
/
/
vmulps
%
ymm5
%
ymm1
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
68
92
245
/
/
vsubps
%
ymm5
%
ymm7
%
ymm14
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
116
89
206
/
/
vmulps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
148
92
201
/
/
vsubps
%
ymm1
%
ymm13
%
ymm1
.
byte
196
195
117
74
204
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm1
%
ymm1
.
byte
197
172
88
201
/
/
vaddps
%
ymm1
%
ymm10
%
ymm1
.
byte
197
52
89
202
/
/
vmulps
%
ymm2
%
ymm9
%
ymm9
.
byte
196
98
61
184
206
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm9
.
byte
197
76
88
214
/
/
vaddps
%
ymm6
%
ymm6
%
ymm10
.
byte
98
241
44
40
194
199
2
/
/
vcmpleps
%
ymm7
%
ymm10
%
k0
.
byte
98
114
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm10
.
byte
197
108
89
222
/
/
vmulps
%
ymm6
%
ymm2
%
ymm11
.
byte
196
65
36
88
219
/
/
vaddps
%
ymm11
%
ymm11
%
ymm11
.
byte
197
68
92
230
/
/
vsubps
%
ymm6
%
ymm7
%
ymm12
.
byte
197
228
92
210
/
/
vsubps
%
ymm2
%
ymm3
%
ymm2
.
byte
196
193
108
89
212
/
/
vmulps
%
ymm12
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
148
92
210
/
/
vsubps
%
ymm2
%
ymm13
%
ymm2
.
byte
196
195
109
74
211
160
/
/
vblendvps
%
ymm10
%
ymm11
%
ymm2
%
ymm2
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_softlight_skx
.
globl
_sk_softlight_skx
FUNCTION
(
_sk_softlight_skx
)
_sk_softlight_skx
:
.
byte
98
225
124
40
40
194
/
/
vmovaps
%
ymm2
%
ymm16
.
byte
98
225
124
40
40
225
/
/
vmovaps
%
ymm1
%
ymm20
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
98
241
108
40
194
199
1
/
/
vcmpltps
%
ymm7
%
ymm2
%
k0
.
byte
98
242
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm1
.
byte
197
92
94
207
/
/
vdivps
%
ymm7
%
ymm4
%
ymm9
.
byte
196
67
109
74
201
16
/
/
vblendvps
%
ymm1
%
ymm9
%
ymm2
%
ymm9
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
124
88
248
/
/
vaddps
%
ymm0
%
ymm0
%
ymm15
.
byte
196
65
52
88
217
/
/
vaddps
%
ymm9
%
ymm9
%
ymm11
.
byte
196
65
36
88
235
/
/
vaddps
%
ymm11
%
ymm11
%
ymm13
.
byte
197
4
92
243
/
/
vsubps
%
ymm3
%
ymm15
%
ymm14
.
byte
196
98
125
24
29
172
186
3
0
/
/
vbroadcastss
0x3baac
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
36
92
225
/
/
vsubps
%
ymm9
%
ymm11
%
ymm12
.
byte
196
98
13
168
227
/
/
vfmadd213ps
%
ymm3
%
ymm14
%
ymm12
.
byte
197
156
89
212
/
/
vmulps
%
ymm4
%
ymm12
%
ymm2
.
byte
196
66
21
168
237
/
/
vfmadd213ps
%
ymm13
%
ymm13
%
ymm13
.
byte
98
226
125
40
24
21
167
186
3
0
/
/
vbroadcastss
0x3baa7
(
%
rip
)
%
ymm18
#
3c514
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c8
>
.
byte
98
161
52
40
88
202
/
/
vaddps
%
ymm18
%
ymm9
%
ymm17
.
byte
98
81
116
32
89
229
/
/
vmulps
%
ymm13
%
ymm17
%
ymm12
.
byte
98
226
125
40
24
29
149
186
3
0
/
/
vbroadcastss
0x3ba95
(
%
rip
)
%
ymm19
#
3c518
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2cc
>
.
byte
98
50
53
40
184
227
/
/
vfmadd231ps
%
ymm19
%
ymm9
%
ymm12
.
byte
98
194
125
40
78
201
/
/
vrsqrt14ps
%
ymm9
%
ymm17
.
byte
98
162
125
40
76
201
/
/
vrcp14ps
%
ymm17
%
ymm17
.
byte
98
81
116
32
92
201
/
/
vsubps
%
ymm9
%
ymm17
%
ymm9
.
byte
197
12
89
247
/
/
vmulps
%
ymm7
%
ymm14
%
ymm14
.
byte
98
225
92
40
88
204
/
/
vaddps
%
ymm4
%
ymm4
%
ymm17
.
byte
98
161
116
32
88
201
/
/
vaddps
%
ymm17
%
ymm17
%
ymm17
.
byte
98
241
116
32
194
199
2
/
/
vcmpleps
%
ymm7
%
ymm17
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
67
53
74
196
128
/
/
vblendvps
%
ymm8
%
ymm12
%
ymm9
%
ymm8
.
byte
196
65
12
89
192
/
/
vmulps
%
ymm8
%
ymm14
%
ymm8
.
byte
196
98
101
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm3
%
ymm8
.
byte
197
36
92
247
/
/
vsubps
%
ymm7
%
ymm11
%
ymm14
.
byte
197
140
89
192
/
/
vmulps
%
ymm0
%
ymm14
%
ymm0
.
byte
197
36
92
203
/
/
vsubps
%
ymm3
%
ymm11
%
ymm9
.
byte
196
226
53
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm9
%
ymm0
.
byte
98
241
4
40
194
195
2
/
/
vcmpleps
%
ymm3
%
ymm15
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
196
227
61
74
210
192
/
/
vblendvps
%
ymm12
%
ymm2
%
ymm8
%
ymm2
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
212
94
215
/
/
vdivps
%
ymm7
%
ymm5
%
ymm2
.
byte
196
227
45
74
210
16
/
/
vblendvps
%
ymm1
%
ymm2
%
ymm10
%
ymm2
.
byte
98
49
92
32
88
196
/
/
vaddps
%
ymm20
%
ymm20
%
ymm8
.
byte
197
108
88
226
/
/
vaddps
%
ymm2
%
ymm2
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
60
92
251
/
/
vsubps
%
ymm3
%
ymm8
%
ymm15
.
byte
98
225
36
40
92
202
/
/
vsubps
%
ymm2
%
ymm11
%
ymm17
.
byte
98
226
5
40
168
203
/
/
vfmadd213ps
%
ymm3
%
ymm15
%
ymm17
.
byte
98
113
116
32
89
237
/
/
vmulps
%
ymm5
%
ymm17
%
ymm13
.
byte
196
66
29
168
228
/
/
vfmadd213ps
%
ymm12
%
ymm12
%
ymm12
.
byte
98
161
108
40
88
202
/
/
vaddps
%
ymm18
%
ymm2
%
ymm17
.
byte
98
81
116
32
89
228
/
/
vmulps
%
ymm12
%
ymm17
%
ymm12
.
byte
98
50
109
40
184
227
/
/
vfmadd231ps
%
ymm19
%
ymm2
%
ymm12
.
byte
98
226
125
40
78
202
/
/
vrsqrt14ps
%
ymm2
%
ymm17
.
byte
98
162
125
40
76
201
/
/
vrcp14ps
%
ymm17
%
ymm17
.
byte
98
241
116
32
92
210
/
/
vsubps
%
ymm2
%
ymm17
%
ymm2
.
byte
197
4
89
255
/
/
vmulps
%
ymm7
%
ymm15
%
ymm15
.
byte
98
225
84
40
88
205
/
/
vaddps
%
ymm5
%
ymm5
%
ymm17
.
byte
98
161
116
32
88
201
/
/
vaddps
%
ymm17
%
ymm17
%
ymm17
.
byte
98
241
116
32
194
199
2
/
/
vcmpleps
%
ymm7
%
ymm17
%
k0
.
byte
98
114
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm10
.
byte
196
195
109
74
212
160
/
/
vblendvps
%
ymm10
%
ymm12
%
ymm2
%
ymm2
.
byte
197
132
89
210
/
/
vmulps
%
ymm2
%
ymm15
%
ymm2
.
byte
196
226
101
184
213
/
/
vfmadd231ps
%
ymm5
%
ymm3
%
ymm2
.
byte
98
49
12
40
89
212
/
/
vmulps
%
ymm20
%
ymm14
%
ymm10
.
byte
196
98
53
184
213
/
/
vfmadd231ps
%
ymm5
%
ymm9
%
ymm10
.
byte
98
241
60
40
194
195
2
/
/
vcmpleps
%
ymm3
%
ymm8
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
195
109
74
213
128
/
/
vblendvps
%
ymm8
%
ymm13
%
ymm2
%
ymm2
.
byte
197
44
88
194
/
/
vaddps
%
ymm2
%
ymm10
%
ymm8
.
byte
197
204
94
215
/
/
vdivps
%
ymm7
%
ymm6
%
ymm2
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
227
45
74
202
16
/
/
vblendvps
%
ymm1
%
ymm2
%
ymm10
%
ymm1
.
byte
98
177
124
32
88
208
/
/
vaddps
%
ymm16
%
ymm16
%
ymm2
.
byte
197
116
88
209
/
/
vaddps
%
ymm1
%
ymm1
%
ymm10
.
byte
196
65
44
88
210
/
/
vaddps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
108
92
227
/
/
vsubps
%
ymm3
%
ymm2
%
ymm12
.
byte
197
36
92
217
/
/
vsubps
%
ymm1
%
ymm11
%
ymm11
.
byte
196
98
29
168
219
/
/
vfmadd213ps
%
ymm3
%
ymm12
%
ymm11
.
byte
197
36
89
222
/
/
vmulps
%
ymm6
%
ymm11
%
ymm11
.
byte
196
66
45
168
210
/
/
vfmadd213ps
%
ymm10
%
ymm10
%
ymm10
.
byte
98
49
116
40
88
234
/
/
vaddps
%
ymm18
%
ymm1
%
ymm13
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
98
50
117
40
184
211
/
/
vfmadd231ps
%
ymm19
%
ymm1
%
ymm10
.
byte
98
114
125
40
78
233
/
/
vrsqrt14ps
%
ymm1
%
ymm13
.
byte
98
82
125
40
76
237
/
/
vrcp14ps
%
ymm13
%
ymm13
.
byte
197
148
92
201
/
/
vsubps
%
ymm1
%
ymm13
%
ymm1
.
byte
197
28
89
231
/
/
vmulps
%
ymm7
%
ymm12
%
ymm12
.
byte
197
76
88
238
/
/
vaddps
%
ymm6
%
ymm6
%
ymm13
.
byte
196
65
20
88
237
/
/
vaddps
%
ymm13
%
ymm13
%
ymm13
.
byte
98
241
20
40
194
199
2
/
/
vcmpleps
%
ymm7
%
ymm13
%
k0
.
byte
98
114
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm13
.
byte
196
195
117
74
202
208
/
/
vblendvps
%
ymm13
%
ymm10
%
ymm1
%
ymm1
.
byte
197
156
89
201
/
/
vmulps
%
ymm1
%
ymm12
%
ymm1
.
byte
196
226
101
184
206
/
/
vfmadd231ps
%
ymm6
%
ymm3
%
ymm1
.
byte
98
49
12
40
89
208
/
/
vmulps
%
ymm16
%
ymm14
%
ymm10
.
byte
196
98
53
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm9
%
ymm10
.
byte
98
241
108
40
194
195
2
/
/
vcmpleps
%
ymm3
%
ymm2
%
k0
.
byte
98
242
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm2
.
byte
196
195
117
74
203
32
/
/
vblendvps
%
ymm2
%
ymm11
%
ymm1
%
ymm1
.
byte
197
172
88
209
/
/
vaddps
%
ymm1
%
ymm10
%
ymm2
.
byte
196
194
69
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
193
/
/
vmovaps
%
ymm8
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hue_skx
.
globl
_sk_hue_skx
FUNCTION
(
_sk_hue_skx
)
_sk_hue_skx
:
.
byte
98
225
124
40
40
210
/
/
vmovaps
%
ymm2
%
ymm18
.
byte
98
225
124
40
40
225
/
/
vmovaps
%
ymm1
%
ymm20
.
byte
197
124
89
203
/
/
vmulps
%
ymm3
%
ymm0
%
ymm9
.
byte
98
113
92
32
89
211
/
/
vmulps
%
ymm3
%
ymm20
%
ymm10
.
byte
98
113
108
32
89
219
/
/
vmulps
%
ymm3
%
ymm18
%
ymm11
.
byte
197
84
95
198
/
/
vmaxps
%
ymm6
%
ymm5
%
ymm8
.
byte
196
65
92
95
192
/
/
vmaxps
%
ymm8
%
ymm4
%
ymm8
.
byte
197
84
93
230
/
/
vminps
%
ymm6
%
ymm5
%
ymm12
.
byte
196
65
92
93
228
/
/
vminps
%
ymm12
%
ymm4
%
ymm12
.
byte
196
65
60
92
196
/
/
vsubps
%
ymm12
%
ymm8
%
ymm8
.
byte
197
60
89
227
/
/
vmulps
%
ymm3
%
ymm8
%
ymm12
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
232
/
/
vminps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
44
95
195
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
95
192
/
/
vmaxps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
65
60
92
245
/
/
vsubps
%
ymm13
%
ymm8
%
ymm14
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
98
241
12
40
194
193
0
/
/
vcmpeqps
%
ymm1
%
ymm14
%
k0
.
byte
98
114
126
40
56
248
/
/
vpmovm2d
%
k0
%
ymm15
.
byte
196
65
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm9
.
byte
196
65
28
89
201
/
/
vmulps
%
ymm9
%
ymm12
%
ymm9
.
byte
196
65
52
94
206
/
/
vdivps
%
ymm14
%
ymm9
%
ymm9
.
byte
196
99
53
74
201
240
/
/
vblendvps
%
ymm15
%
ymm1
%
ymm9
%
ymm9
.
byte
196
65
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
99
45
74
209
240
/
/
vblendvps
%
ymm15
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
99
37
74
225
240
/
/
vblendvps
%
ymm15
%
ymm1
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
48
184
3
0
/
/
vbroadcastss
0x3b830
(
%
rip
)
%
ymm13
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
53
43
184
3
0
/
/
vbroadcastss
0x3b82b
(
%
rip
)
%
ymm14
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
254
/
/
vmulps
%
ymm14
%
ymm5
%
ymm15
.
byte
196
66
93
184
253
/
/
vfmadd231ps
%
ymm13
%
ymm4
%
ymm15
.
byte
98
226
125
40
24
5
27
184
3
0
/
/
vbroadcastss
0x3b81b
(
%
rip
)
%
ymm16
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
98
50
77
40
184
248
/
/
vfmadd231ps
%
ymm16
%
ymm6
%
ymm15
.
byte
196
65
44
89
222
/
/
vmulps
%
ymm14
%
ymm10
%
ymm11
.
byte
196
66
53
184
221
/
/
vfmadd231ps
%
ymm13
%
ymm9
%
ymm11
.
byte
98
50
29
40
184
216
/
/
vfmadd231ps
%
ymm16
%
ymm12
%
ymm11
.
byte
196
66
101
170
251
/
/
vfmsub213ps
%
ymm11
%
ymm3
%
ymm15
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
44
88
223
/
/
vaddps
%
ymm15
%
ymm10
%
ymm11
.
byte
196
65
28
88
215
/
/
vaddps
%
ymm15
%
ymm12
%
ymm10
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
36
93
250
/
/
vminps
%
ymm10
%
ymm11
%
ymm15
.
byte
196
65
52
93
255
/
/
vminps
%
ymm15
%
ymm9
%
ymm15
.
byte
98
193
36
40
95
202
/
/
vmaxps
%
ymm10
%
ymm11
%
ymm17
.
byte
98
161
52
40
95
201
/
/
vmaxps
%
ymm17
%
ymm9
%
ymm17
.
byte
196
65
36
89
246
/
/
vmulps
%
ymm14
%
ymm11
%
ymm14
.
byte
196
66
53
184
245
/
/
vfmadd231ps
%
ymm13
%
ymm9
%
ymm14
.
byte
98
50
45
40
184
240
/
/
vfmadd231ps
%
ymm16
%
ymm10
%
ymm14
.
byte
98
209
116
40
194
199
2
/
/
vcmpleps
%
ymm15
%
ymm1
%
k0
.
byte
98
114
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm13
.
byte
98
193
52
40
92
198
/
/
vsubps
%
ymm14
%
ymm9
%
ymm16
.
byte
98
161
12
40
89
192
/
/
vmulps
%
ymm16
%
ymm14
%
ymm16
.
byte
196
65
12
92
255
/
/
vsubps
%
ymm15
%
ymm14
%
ymm15
.
byte
98
193
124
32
94
199
/
/
vdivps
%
ymm15
%
ymm16
%
ymm16
.
byte
98
49
12
40
88
192
/
/
vaddps
%
ymm16
%
ymm14
%
ymm8
.
byte
196
67
61
74
193
208
/
/
vblendvps
%
ymm13
%
ymm9
%
ymm8
%
ymm8
.
byte
98
177
28
40
194
193
1
/
/
vcmpltps
%
ymm17
%
ymm12
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
98
193
60
40
92
198
/
/
vsubps
%
ymm14
%
ymm8
%
ymm16
.
byte
98
193
28
40
92
222
/
/
vsubps
%
ymm14
%
ymm12
%
ymm19
.
byte
98
161
100
32
89
192
/
/
vmulps
%
ymm16
%
ymm19
%
ymm16
.
byte
98
193
116
32
92
206
/
/
vsubps
%
ymm14
%
ymm17
%
ymm17
.
byte
98
161
124
32
94
193
/
/
vdivps
%
ymm17
%
ymm16
%
ymm16
.
byte
98
177
12
40
88
208
/
/
vaddps
%
ymm16
%
ymm14
%
ymm2
.
byte
196
227
61
74
210
144
/
/
vblendvps
%
ymm9
%
ymm2
%
ymm8
%
ymm2
.
byte
197
236
95
209
/
/
vmaxps
%
ymm1
%
ymm2
%
ymm2
.
byte
196
65
36
92
198
/
/
vsubps
%
ymm14
%
ymm11
%
ymm8
.
byte
196
65
12
89
192
/
/
vmulps
%
ymm8
%
ymm14
%
ymm8
.
byte
196
65
60
94
199
/
/
vdivps
%
ymm15
%
ymm8
%
ymm8
.
byte
196
65
12
88
192
/
/
vaddps
%
ymm8
%
ymm14
%
ymm8
.
byte
196
67
61
74
195
208
/
/
vblendvps
%
ymm13
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
60
92
222
/
/
vsubps
%
ymm14
%
ymm8
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
217
/
/
vdivps
%
ymm17
%
ymm11
%
ymm11
.
byte
196
65
12
88
219
/
/
vaddps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
67
61
74
195
144
/
/
vblendvps
%
ymm9
%
ymm11
%
ymm8
%
ymm8
.
byte
197
60
95
193
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm8
.
byte
196
65
44
92
222
/
/
vsubps
%
ymm14
%
ymm10
%
ymm11
.
byte
196
65
12
89
219
/
/
vmulps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
36
94
223
/
/
vdivps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
65
12
88
219
/
/
vaddps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
67
37
74
210
208
/
/
vblendvps
%
ymm13
%
ymm10
%
ymm11
%
ymm10
.
byte
196
65
44
92
222
/
/
vsubps
%
ymm14
%
ymm10
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
217
/
/
vdivps
%
ymm17
%
ymm11
%
ymm11
.
byte
196
65
12
88
219
/
/
vaddps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
67
45
74
203
144
/
/
vblendvps
%
ymm9
%
ymm11
%
ymm10
%
ymm9
.
byte
197
52
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm9
.
byte
196
226
125
24
13
183
182
3
0
/
/
vbroadcastss
0x3b6b7
(
%
rip
)
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
116
92
215
/
/
vsubps
%
ymm7
%
ymm1
%
ymm10
.
byte
197
172
89
192
/
/
vmulps
%
ymm0
%
ymm10
%
ymm0
.
byte
197
116
92
219
/
/
vsubps
%
ymm3
%
ymm1
%
ymm11
.
byte
196
226
37
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm0
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
98
177
44
40
89
204
/
/
vmulps
%
ymm20
%
ymm10
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
98
177
44
40
89
210
/
/
vmulps
%
ymm18
%
ymm10
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
209
/
/
vaddps
%
ymm9
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_saturation_skx
.
globl
_sk_saturation_skx
FUNCTION
(
_sk_saturation_skx
)
_sk_saturation_skx
:
.
byte
98
225
124
40
40
210
/
/
vmovaps
%
ymm2
%
ymm18
.
byte
98
225
124
40
40
225
/
/
vmovaps
%
ymm1
%
ymm20
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
197
100
89
213
/
/
vmulps
%
ymm5
%
ymm3
%
ymm10
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
98
49
92
32
95
194
/
/
vmaxps
%
ymm18
%
ymm20
%
ymm8
.
byte
196
65
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm8
.
byte
98
49
92
32
93
226
/
/
vminps
%
ymm18
%
ymm20
%
ymm12
.
byte
196
65
124
93
228
/
/
vminps
%
ymm12
%
ymm0
%
ymm12
.
byte
196
65
60
92
196
/
/
vsubps
%
ymm12
%
ymm8
%
ymm8
.
byte
197
60
89
231
/
/
vmulps
%
ymm7
%
ymm8
%
ymm12
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
232
/
/
vminps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
44
95
195
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
95
192
/
/
vmaxps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
65
60
92
245
/
/
vsubps
%
ymm13
%
ymm8
%
ymm14
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
98
241
12
40
194
193
0
/
/
vcmpeqps
%
ymm1
%
ymm14
%
k0
.
byte
98
114
126
40
56
248
/
/
vpmovm2d
%
k0
%
ymm15
.
byte
196
65
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm9
.
byte
196
65
28
89
201
/
/
vmulps
%
ymm9
%
ymm12
%
ymm9
.
byte
196
65
52
94
206
/
/
vdivps
%
ymm14
%
ymm9
%
ymm9
.
byte
196
99
53
74
201
240
/
/
vblendvps
%
ymm15
%
ymm1
%
ymm9
%
ymm9
.
byte
196
65
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
99
45
74
209
240
/
/
vblendvps
%
ymm15
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
99
37
74
225
240
/
/
vblendvps
%
ymm15
%
ymm1
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
236
181
3
0
/
/
vbroadcastss
0x3b5ec
(
%
rip
)
%
ymm13
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
53
231
181
3
0
/
/
vbroadcastss
0x3b5e7
(
%
rip
)
%
ymm14
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
254
/
/
vmulps
%
ymm14
%
ymm5
%
ymm15
.
byte
196
66
93
184
253
/
/
vfmadd231ps
%
ymm13
%
ymm4
%
ymm15
.
byte
98
226
125
40
24
5
215
181
3
0
/
/
vbroadcastss
0x3b5d7
(
%
rip
)
%
ymm16
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
98
50
77
40
184
248
/
/
vfmadd231ps
%
ymm16
%
ymm6
%
ymm15
.
byte
196
65
44
89
222
/
/
vmulps
%
ymm14
%
ymm10
%
ymm11
.
byte
196
66
53
184
221
/
/
vfmadd231ps
%
ymm13
%
ymm9
%
ymm11
.
byte
98
50
29
40
184
216
/
/
vfmadd231ps
%
ymm16
%
ymm12
%
ymm11
.
byte
196
66
101
170
251
/
/
vfmsub213ps
%
ymm11
%
ymm3
%
ymm15
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
44
88
223
/
/
vaddps
%
ymm15
%
ymm10
%
ymm11
.
byte
196
65
28
88
215
/
/
vaddps
%
ymm15
%
ymm12
%
ymm10
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
36
93
250
/
/
vminps
%
ymm10
%
ymm11
%
ymm15
.
byte
196
65
52
93
255
/
/
vminps
%
ymm15
%
ymm9
%
ymm15
.
byte
98
193
36
40
95
202
/
/
vmaxps
%
ymm10
%
ymm11
%
ymm17
.
byte
98
161
52
40
95
201
/
/
vmaxps
%
ymm17
%
ymm9
%
ymm17
.
byte
196
65
36
89
246
/
/
vmulps
%
ymm14
%
ymm11
%
ymm14
.
byte
196
66
53
184
245
/
/
vfmadd231ps
%
ymm13
%
ymm9
%
ymm14
.
byte
98
50
45
40
184
240
/
/
vfmadd231ps
%
ymm16
%
ymm10
%
ymm14
.
byte
98
209
116
40
194
199
2
/
/
vcmpleps
%
ymm15
%
ymm1
%
k0
.
byte
98
114
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm13
.
byte
98
193
52
40
92
198
/
/
vsubps
%
ymm14
%
ymm9
%
ymm16
.
byte
98
161
12
40
89
192
/
/
vmulps
%
ymm16
%
ymm14
%
ymm16
.
byte
196
65
12
92
255
/
/
vsubps
%
ymm15
%
ymm14
%
ymm15
.
byte
98
193
124
32
94
199
/
/
vdivps
%
ymm15
%
ymm16
%
ymm16
.
byte
98
49
12
40
88
192
/
/
vaddps
%
ymm16
%
ymm14
%
ymm8
.
byte
196
67
61
74
193
208
/
/
vblendvps
%
ymm13
%
ymm9
%
ymm8
%
ymm8
.
byte
98
177
28
40
194
193
1
/
/
vcmpltps
%
ymm17
%
ymm12
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
98
193
60
40
92
198
/
/
vsubps
%
ymm14
%
ymm8
%
ymm16
.
byte
98
193
28
40
92
222
/
/
vsubps
%
ymm14
%
ymm12
%
ymm19
.
byte
98
161
100
32
89
192
/
/
vmulps
%
ymm16
%
ymm19
%
ymm16
.
byte
98
193
116
32
92
206
/
/
vsubps
%
ymm14
%
ymm17
%
ymm17
.
byte
98
161
124
32
94
193
/
/
vdivps
%
ymm17
%
ymm16
%
ymm16
.
byte
98
177
12
40
88
208
/
/
vaddps
%
ymm16
%
ymm14
%
ymm2
.
byte
196
227
61
74
210
144
/
/
vblendvps
%
ymm9
%
ymm2
%
ymm8
%
ymm2
.
byte
197
236
95
209
/
/
vmaxps
%
ymm1
%
ymm2
%
ymm2
.
byte
196
65
36
92
198
/
/
vsubps
%
ymm14
%
ymm11
%
ymm8
.
byte
196
65
12
89
192
/
/
vmulps
%
ymm8
%
ymm14
%
ymm8
.
byte
196
65
60
94
199
/
/
vdivps
%
ymm15
%
ymm8
%
ymm8
.
byte
196
65
12
88
192
/
/
vaddps
%
ymm8
%
ymm14
%
ymm8
.
byte
196
67
61
74
195
208
/
/
vblendvps
%
ymm13
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
60
92
222
/
/
vsubps
%
ymm14
%
ymm8
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
217
/
/
vdivps
%
ymm17
%
ymm11
%
ymm11
.
byte
196
65
12
88
219
/
/
vaddps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
67
61
74
195
144
/
/
vblendvps
%
ymm9
%
ymm11
%
ymm8
%
ymm8
.
byte
197
60
95
193
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm8
.
byte
196
65
44
92
222
/
/
vsubps
%
ymm14
%
ymm10
%
ymm11
.
byte
196
65
12
89
219
/
/
vmulps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
36
94
223
/
/
vdivps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
65
12
88
219
/
/
vaddps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
67
37
74
210
208
/
/
vblendvps
%
ymm13
%
ymm10
%
ymm11
%
ymm10
.
byte
196
65
44
92
222
/
/
vsubps
%
ymm14
%
ymm10
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
217
/
/
vdivps
%
ymm17
%
ymm11
%
ymm11
.
byte
196
65
12
88
219
/
/
vaddps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
67
45
74
203
144
/
/
vblendvps
%
ymm9
%
ymm11
%
ymm10
%
ymm9
.
byte
197
52
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm9
.
byte
196
226
125
24
13
115
180
3
0
/
/
vbroadcastss
0x3b473
(
%
rip
)
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
116
92
215
/
/
vsubps
%
ymm7
%
ymm1
%
ymm10
.
byte
197
172
89
192
/
/
vmulps
%
ymm0
%
ymm10
%
ymm0
.
byte
197
116
92
219
/
/
vsubps
%
ymm3
%
ymm1
%
ymm11
.
byte
196
226
37
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm0
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
98
177
44
40
89
204
/
/
vmulps
%
ymm20
%
ymm10
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
98
177
44
40
89
210
/
/
vmulps
%
ymm18
%
ymm10
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
209
/
/
vaddps
%
ymm9
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_color_skx
.
globl
_sk_color_skx
FUNCTION
(
_sk_color_skx
)
_sk_color_skx
:
.
byte
197
124
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm8
.
byte
197
116
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm9
.
byte
197
108
89
223
/
/
vmulps
%
ymm7
%
ymm2
%
ymm11
.
byte
196
98
125
24
37
60
180
3
0
/
/
vbroadcastss
0x3b43c
(
%
rip
)
%
ymm12
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
45
55
180
3
0
/
/
vbroadcastss
0x3b437
(
%
rip
)
%
ymm13
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
245
/
/
vmulps
%
ymm13
%
ymm5
%
ymm14
.
byte
196
66
93
184
244
/
/
vfmadd231ps
%
ymm12
%
ymm4
%
ymm14
.
byte
196
98
125
24
61
40
180
3
0
/
/
vbroadcastss
0x3b428
(
%
rip
)
%
ymm15
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
66
77
184
247
/
/
vfmadd231ps
%
ymm15
%
ymm6
%
ymm14
.
byte
196
65
52
89
213
/
/
vmulps
%
ymm13
%
ymm9
%
ymm10
.
byte
196
66
61
184
212
/
/
vfmadd231ps
%
ymm12
%
ymm8
%
ymm10
.
byte
196
66
37
184
215
/
/
vfmadd231ps
%
ymm15
%
ymm11
%
ymm10
.
byte
196
66
101
170
242
/
/
vfmsub213ps
%
ymm10
%
ymm3
%
ymm14
.
byte
196
65
60
88
198
/
/
vaddps
%
ymm14
%
ymm8
%
ymm8
.
byte
196
65
52
88
214
/
/
vaddps
%
ymm14
%
ymm9
%
ymm10
.
byte
196
65
36
88
206
/
/
vaddps
%
ymm14
%
ymm11
%
ymm9
.
byte
98
225
100
40
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm20
.
byte
196
65
44
93
241
/
/
vminps
%
ymm9
%
ymm10
%
ymm14
.
byte
196
65
60
93
246
/
/
vminps
%
ymm14
%
ymm8
%
ymm14
.
byte
98
193
44
40
95
193
/
/
vmaxps
%
ymm9
%
ymm10
%
ymm16
.
byte
98
161
60
40
95
192
/
/
vmaxps
%
ymm16
%
ymm8
%
ymm16
.
byte
196
65
44
89
237
/
/
vmulps
%
ymm13
%
ymm10
%
ymm13
.
byte
196
66
61
184
236
/
/
vfmadd231ps
%
ymm12
%
ymm8
%
ymm13
.
byte
196
66
53
184
239
/
/
vfmadd231ps
%
ymm15
%
ymm9
%
ymm13
.
byte
98
161
108
32
87
210
/
/
vxorps
%
ymm18
%
ymm18
%
ymm18
.
byte
98
209
108
32
194
198
2
/
/
vcmpleps
%
ymm14
%
ymm18
%
k0
.
byte
98
114
126
40
56
248
/
/
vpmovm2d
%
k0
%
ymm15
.
byte
98
193
60
40
92
205
/
/
vsubps
%
ymm13
%
ymm8
%
ymm17
.
byte
98
161
20
40
89
201
/
/
vmulps
%
ymm17
%
ymm13
%
ymm17
.
byte
196
65
20
92
246
/
/
vsubps
%
ymm14
%
ymm13
%
ymm14
.
byte
98
193
116
32
94
206
/
/
vdivps
%
ymm14
%
ymm17
%
ymm17
.
byte
98
49
20
40
88
225
/
/
vaddps
%
ymm17
%
ymm13
%
ymm12
.
byte
196
67
29
74
192
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm12
%
ymm8
.
byte
98
177
92
32
194
192
1
/
/
vcmpltps
%
ymm16
%
ymm20
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
98
193
60
40
92
205
/
/
vsubps
%
ymm13
%
ymm8
%
ymm17
.
byte
98
193
92
32
92
221
/
/
vsubps
%
ymm13
%
ymm20
%
ymm19
.
byte
98
161
100
32
89
201
/
/
vmulps
%
ymm17
%
ymm19
%
ymm17
.
byte
98
193
124
32
92
197
/
/
vsubps
%
ymm13
%
ymm16
%
ymm16
.
byte
98
161
116
32
94
200
/
/
vdivps
%
ymm16
%
ymm17
%
ymm17
.
byte
98
49
20
40
88
217
/
/
vaddps
%
ymm17
%
ymm13
%
ymm11
.
byte
196
67
61
74
195
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm8
%
ymm8
.
byte
98
49
60
40
95
194
/
/
vmaxps
%
ymm18
%
ymm8
%
ymm8
.
byte
196
65
44
92
221
/
/
vsubps
%
ymm13
%
ymm10
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
37
74
210
240
/
/
vblendvps
%
ymm15
%
ymm10
%
ymm11
%
ymm10
.
byte
196
65
44
92
221
/
/
vsubps
%
ymm13
%
ymm10
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
216
/
/
vdivps
%
ymm16
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
45
74
211
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm10
%
ymm10
.
byte
98
49
44
40
95
210
/
/
vmaxps
%
ymm18
%
ymm10
%
ymm10
.
byte
196
65
52
92
221
/
/
vsubps
%
ymm13
%
ymm9
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
37
74
201
240
/
/
vblendvps
%
ymm15
%
ymm9
%
ymm11
%
ymm9
.
byte
196
65
52
92
221
/
/
vsubps
%
ymm13
%
ymm9
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
216
/
/
vdivps
%
ymm16
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
53
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm9
%
ymm9
.
byte
98
49
52
40
95
202
/
/
vmaxps
%
ymm18
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
185
178
3
0
/
/
vbroadcastss
0x3b2b9
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
36
92
231
/
/
vsubps
%
ymm7
%
ymm11
%
ymm12
.
byte
197
156
89
192
/
/
vmulps
%
ymm0
%
ymm12
%
ymm0
.
byte
197
36
92
219
/
/
vsubps
%
ymm3
%
ymm11
%
ymm11
.
byte
196
226
37
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm0
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
156
89
201
/
/
vmulps
%
ymm1
%
ymm12
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
202
/
/
vaddps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
156
89
210
/
/
vmulps
%
ymm2
%
ymm12
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
209
/
/
vaddps
%
ymm9
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
98
177
100
40
92
220
/
/
vsubps
%
ymm20
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminosity_skx
.
globl
_sk_luminosity_skx
FUNCTION
(
_sk_luminosity_skx
)
_sk_luminosity_skx
:
.
byte
197
100
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm8
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
196
98
125
24
37
132
178
3
0
/
/
vbroadcastss
0x3b284
(
%
rip
)
%
ymm12
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
45
127
178
3
0
/
/
vbroadcastss
0x3b27f
(
%
rip
)
%
ymm13
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
116
89
245
/
/
vmulps
%
ymm13
%
ymm1
%
ymm14
.
byte
196
66
125
184
244
/
/
vfmadd231ps
%
ymm12
%
ymm0
%
ymm14
.
byte
196
98
125
24
61
112
178
3
0
/
/
vbroadcastss
0x3b270
(
%
rip
)
%
ymm15
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
66
109
184
247
/
/
vfmadd231ps
%
ymm15
%
ymm2
%
ymm14
.
byte
196
65
52
89
213
/
/
vmulps
%
ymm13
%
ymm9
%
ymm10
.
byte
196
66
61
184
212
/
/
vfmadd231ps
%
ymm12
%
ymm8
%
ymm10
.
byte
196
66
37
184
215
/
/
vfmadd231ps
%
ymm15
%
ymm11
%
ymm10
.
byte
196
66
69
170
242
/
/
vfmsub213ps
%
ymm10
%
ymm7
%
ymm14
.
byte
196
65
60
88
198
/
/
vaddps
%
ymm14
%
ymm8
%
ymm8
.
byte
196
65
52
88
214
/
/
vaddps
%
ymm14
%
ymm9
%
ymm10
.
byte
196
65
36
88
206
/
/
vaddps
%
ymm14
%
ymm11
%
ymm9
.
byte
98
225
100
40
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm20
.
byte
196
65
44
93
241
/
/
vminps
%
ymm9
%
ymm10
%
ymm14
.
byte
196
65
60
93
246
/
/
vminps
%
ymm14
%
ymm8
%
ymm14
.
byte
98
193
44
40
95
193
/
/
vmaxps
%
ymm9
%
ymm10
%
ymm16
.
byte
98
161
60
40
95
192
/
/
vmaxps
%
ymm16
%
ymm8
%
ymm16
.
byte
196
65
44
89
237
/
/
vmulps
%
ymm13
%
ymm10
%
ymm13
.
byte
196
66
61
184
236
/
/
vfmadd231ps
%
ymm12
%
ymm8
%
ymm13
.
byte
196
66
53
184
239
/
/
vfmadd231ps
%
ymm15
%
ymm9
%
ymm13
.
byte
98
161
108
32
87
210
/
/
vxorps
%
ymm18
%
ymm18
%
ymm18
.
byte
98
209
108
32
194
198
2
/
/
vcmpleps
%
ymm14
%
ymm18
%
k0
.
byte
98
114
126
40
56
248
/
/
vpmovm2d
%
k0
%
ymm15
.
byte
98
193
60
40
92
205
/
/
vsubps
%
ymm13
%
ymm8
%
ymm17
.
byte
98
161
20
40
89
201
/
/
vmulps
%
ymm17
%
ymm13
%
ymm17
.
byte
196
65
20
92
246
/
/
vsubps
%
ymm14
%
ymm13
%
ymm14
.
byte
98
193
116
32
94
206
/
/
vdivps
%
ymm14
%
ymm17
%
ymm17
.
byte
98
49
20
40
88
225
/
/
vaddps
%
ymm17
%
ymm13
%
ymm12
.
byte
196
67
29
74
192
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm12
%
ymm8
.
byte
98
177
92
32
194
192
1
/
/
vcmpltps
%
ymm16
%
ymm20
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
98
193
60
40
92
205
/
/
vsubps
%
ymm13
%
ymm8
%
ymm17
.
byte
98
193
92
32
92
221
/
/
vsubps
%
ymm13
%
ymm20
%
ymm19
.
byte
98
161
100
32
89
201
/
/
vmulps
%
ymm17
%
ymm19
%
ymm17
.
byte
98
193
124
32
92
197
/
/
vsubps
%
ymm13
%
ymm16
%
ymm16
.
byte
98
161
116
32
94
200
/
/
vdivps
%
ymm16
%
ymm17
%
ymm17
.
byte
98
49
20
40
88
217
/
/
vaddps
%
ymm17
%
ymm13
%
ymm11
.
byte
196
67
61
74
195
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm8
%
ymm8
.
byte
98
49
60
40
95
194
/
/
vmaxps
%
ymm18
%
ymm8
%
ymm8
.
byte
196
65
44
92
221
/
/
vsubps
%
ymm13
%
ymm10
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
37
74
210
240
/
/
vblendvps
%
ymm15
%
ymm10
%
ymm11
%
ymm10
.
byte
196
65
44
92
221
/
/
vsubps
%
ymm13
%
ymm10
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
216
/
/
vdivps
%
ymm16
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
45
74
211
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm10
%
ymm10
.
byte
98
49
44
40
95
210
/
/
vmaxps
%
ymm18
%
ymm10
%
ymm10
.
byte
196
65
52
92
221
/
/
vsubps
%
ymm13
%
ymm9
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
37
74
201
240
/
/
vblendvps
%
ymm15
%
ymm9
%
ymm11
%
ymm9
.
byte
196
65
52
92
221
/
/
vsubps
%
ymm13
%
ymm9
%
ymm11
.
byte
98
81
100
32
89
219
/
/
vmulps
%
ymm11
%
ymm19
%
ymm11
.
byte
98
49
36
40
94
216
/
/
vdivps
%
ymm16
%
ymm11
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
67
53
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm9
%
ymm9
.
byte
98
49
52
40
95
202
/
/
vmaxps
%
ymm18
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
1
177
3
0
/
/
vbroadcastss
0x3b101
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
36
92
231
/
/
vsubps
%
ymm7
%
ymm11
%
ymm12
.
byte
197
156
89
192
/
/
vmulps
%
ymm0
%
ymm12
%
ymm0
.
byte
197
36
92
219
/
/
vsubps
%
ymm3
%
ymm11
%
ymm11
.
byte
196
226
37
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm0
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
156
89
201
/
/
vmulps
%
ymm1
%
ymm12
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
202
/
/
vaddps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
156
89
210
/
/
vmulps
%
ymm2
%
ymm12
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
209
/
/
vaddps
%
ymm9
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
98
177
100
40
92
220
/
/
vsubps
%
ymm20
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_skx
.
globl
_sk_srcover_rgba_8888_skx
FUNCTION
(
_sk_srcover_rgba_8888_skx
)
_sk_srcover_rgba_8888_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
235
0
0
0
/
/
jne
1540
<
_sk_srcover_rgba_8888_skx
+
0x105
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
241
127
169
111
231
/
/
vmovdqu8
%
ymm7
%
ymm4
{
%
k1
}
{
z
}
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
197
213
114
215
8
/
/
vpsrld
0x8
%
ymm7
%
ymm5
.
byte
98
241
127
169
111
237
/
/
vmovdqu8
%
ymm5
%
ymm5
{
%
k1
}
{
z
}
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
197
205
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm6
.
byte
98
241
127
169
111
246
/
/
vmovdqu8
%
ymm6
%
ymm6
{
%
k1
}
{
z
}
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
13
89
176
3
0
/
/
vbroadcastss
0x3b059
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
203
/
/
vsubps
%
ymm3
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
120
176
3
0
/
/
vbroadcastss
0x3b078
(
%
rip
)
%
ymm10
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
194
93
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm4
%
ymm0
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
194
85
184
201
/
/
vfmadd231ps
%
ymm9
%
ymm5
%
ymm1
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
194
77
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm6
%
ymm2
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
194
69
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm7
%
ymm3
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
217
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
8
/
/
vpslld
0x8
%
ymm11
%
ymm11
.
byte
196
65
37
235
201
/
/
vpor
%
ymm9
%
ymm11
%
ymm9
.
byte
197
60
95
218
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
37
235
192
/
/
vpor
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
58
/
/
jne
1570
<
_sk_srcover_rgba_8888_skx
+
0x135
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
9
255
255
255
/
/
ja
145b
<
_sk_srcover_rgba_8888_skx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
252
0
0
0
/
/
lea
0xfc
(
%
rip
)
%
r9
#
1658
<
_sk_srcover_rgba_8888_skx
+
0x21d
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
235
254
255
255
/
/
jmpq
145b
<
_sk_srcover_rgba_8888_skx
+
0x20
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
194
/
/
ja
153c
<
_sk_srcover_rgba_8888_skx
+
0x101
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
240
0
0
0
/
/
lea
0xf0
(
%
rip
)
%
r9
#
1674
<
_sk_srcover_rgba_8888_skx
+
0x239
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
167
/
/
jmp
153c
<
_sk_srcover_rgba_8888_skx
+
0x101
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
194
121
53
36
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
232
/
/
vpshufd
0xe8
%
xmm4
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
157
254
255
255
/
/
jmpq
145b
<
_sk_srcover_rgba_8888_skx
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
81
254
255
255
/
/
jmpq
145b
<
_sk_srcover_rgba_8888_skx
+
0x20
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
66
121
53
192
/
/
vpmovzxdq
%
xmm8
%
xmm8
.
byte
98
82
126
8
53
4
144
/
/
vpmovqd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
25
255
255
255
/
/
jmpq
153c
<
_sk_srcover_rgba_8888_skx
+
0x101
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
229
254
255
255
/
/
jmpq
153c
<
_sk_srcover_rgba_8888_skx
+
0x101
>
.
byte
144
/
/
nop
.
byte
13
255
255
255
80
/
/
or
0x50ffffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
61
255
255
255
161
/
/
cmp
0xa1ffffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
141
255
255
255
121
/
/
decl
0x79ffffff
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
102
255
/
/
jmpq
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
25
/
/
lcall
*
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
158
255
255
255
150
/
/
lcall
*
-
0x69000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
203
/
/
dec
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
189
255
255
255
175
/
/
mov
0xafffffff
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_srcover_bgra_8888_skx
.
globl
_sk_srcover_bgra_8888_skx
FUNCTION
(
_sk_srcover_bgra_8888_skx
)
_sk_srcover_bgra_8888_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
235
0
0
0
/
/
jne
1795
<
_sk_srcover_bgra_8888_skx
+
0x105
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
241
127
169
111
231
/
/
vmovdqu8
%
ymm7
%
ymm4
{
%
k1
}
{
z
}
.
byte
197
252
91
244
/
/
vcvtdq2ps
%
ymm4
%
ymm6
.
byte
197
221
114
215
8
/
/
vpsrld
0x8
%
ymm7
%
ymm4
.
byte
98
241
127
169
111
228
/
/
vmovdqu8
%
ymm4
%
ymm4
{
%
k1
}
{
z
}
.
byte
197
252
91
236
/
/
vcvtdq2ps
%
ymm4
%
ymm5
.
byte
197
221
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm4
.
byte
98
241
127
169
111
228
/
/
vmovdqu8
%
ymm4
%
ymm4
{
%
k1
}
{
z
}
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
13
4
174
3
0
/
/
vbroadcastss
0x3ae04
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
203
/
/
vsubps
%
ymm3
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
35
174
3
0
/
/
vbroadcastss
0x3ae23
(
%
rip
)
%
ymm10
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
194
93
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm4
%
ymm0
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
194
85
184
201
/
/
vfmadd231ps
%
ymm9
%
ymm5
%
ymm1
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
194
77
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm6
%
ymm2
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
194
69
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm7
%
ymm3
.
byte
197
60
95
202
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm9
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
217
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
8
/
/
vpslld
0x8
%
ymm11
%
ymm11
.
byte
196
65
37
235
201
/
/
vpor
%
ymm9
%
ymm11
%
ymm9
.
byte
197
60
95
216
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
37
235
192
/
/
vpor
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
58
/
/
jne
17c5
<
_sk_srcover_bgra_8888_skx
+
0x135
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
9
255
255
255
/
/
ja
16b0
<
_sk_srcover_bgra_8888_skx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
251
0
0
0
/
/
lea
0xfb
(
%
rip
)
%
r9
#
18ac
<
_sk_srcover_bgra_8888_skx
+
0x21c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
235
254
255
255
/
/
jmpq
16b0
<
_sk_srcover_bgra_8888_skx
+
0x20
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
194
/
/
ja
1791
<
_sk_srcover_bgra_8888_skx
+
0x101
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
239
0
0
0
/
/
lea
0xef
(
%
rip
)
%
r9
#
18c8
<
_sk_srcover_bgra_8888_skx
+
0x238
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
167
/
/
jmp
1791
<
_sk_srcover_bgra_8888_skx
+
0x101
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
194
121
53
36
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
232
/
/
vpshufd
0xe8
%
xmm4
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
157
254
255
255
/
/
jmpq
16b0
<
_sk_srcover_bgra_8888_skx
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
81
254
255
255
/
/
jmpq
16b0
<
_sk_srcover_bgra_8888_skx
+
0x20
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
66
121
53
192
/
/
vpmovzxdq
%
xmm8
%
xmm8
.
byte
98
82
126
8
53
4
144
/
/
vpmovqd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
25
255
255
255
/
/
jmpq
1791
<
_sk_srcover_bgra_8888_skx
+
0x101
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
229
254
255
255
/
/
jmpq
1791
<
_sk_srcover_bgra_8888_skx
+
0x101
>
.
byte
14
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
81
255
/
/
callq
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
62
255
/
/
ds
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
162
255
255
255
142
/
/
jmpq
*
-
0x71000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
122
255
/
/
jp
18c1
<
_sk_srcover_bgra_8888_skx
+
0x231
>
.
byte
255
/
/
(
bad
)
.
byte
255
103
255
/
/
jmpq
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
159
255
255
255
151
/
/
lcall
*
-
0x68000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
204
/
/
dec
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
176
/
/
mov
0xb0ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_clamp_0_skx
.
globl
_sk_clamp_0_skx
FUNCTION
(
_sk_clamp_0_skx
)
_sk_clamp_0_skx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
95
200
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
108
95
208
/
/
vmaxps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
95
216
/
/
vmaxps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_1_skx
.
globl
_sk_clamp_1_skx
FUNCTION
(
_sk_clamp_1_skx
)
_sk_clamp_1_skx
:
.
byte
196
98
125
24
5
242
171
3
0
/
/
vbroadcastss
0x3abf2
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_skx
.
globl
_sk_clamp_a_skx
FUNCTION
(
_sk_clamp_a_skx
)
_sk_clamp_a_skx
:
.
byte
98
241
100
56
93
29
208
171
3
0
/
/
vminps
0x3abd0
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
93
195
/
/
vminps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
244
93
203
/
/
vminps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
236
93
211
/
/
vminps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_skx
.
globl
_sk_clamp_a_dst_skx
FUNCTION
(
_sk_clamp_a_dst_skx
)
_sk_clamp_a_dst_skx
:
.
byte
98
241
68
56
93
61
182
171
3
0
/
/
vminps
0x3abb6
(
%
rip
)
{
1to8
}
%
ymm7
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
220
93
231
/
/
vminps
%
ymm7
%
ymm4
%
ymm4
.
byte
197
212
93
239
/
/
vminps
%
ymm7
%
ymm5
%
ymm5
.
byte
197
204
93
247
/
/
vminps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_skx
.
globl
_sk_set_rgb_skx
FUNCTION
(
_sk_set_rgb_skx
)
_sk_set_rgb_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm0
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_skx
.
globl
_sk_swap_rb_skx
FUNCTION
(
_sk_swap_rb_skx
)
_sk_swap_rb_skx
:
.
byte
197
124
40
192
/
/
vmovaps
%
ymm0
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
194
/
/
vmovaps
%
ymm2
%
ymm0
.
byte
197
124
41
194
/
/
vmovaps
%
ymm8
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_skx
.
globl
_sk_invert_skx
FUNCTION
(
_sk_invert_skx
)
_sk_invert_skx
:
.
byte
196
98
125
24
5
118
171
3
0
/
/
vbroadcastss
0x3ab76
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
92
219
/
/
vsubps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_skx
.
globl
_sk_move_src_dst_skx
FUNCTION
(
_sk_move_src_dst_skx
)
_sk_move_src_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
224
/
/
vmovaps
%
ymm0
%
ymm4
.
byte
197
252
40
233
/
/
vmovaps
%
ymm1
%
ymm5
.
byte
197
252
40
242
/
/
vmovaps
%
ymm2
%
ymm6
.
byte
197
252
40
251
/
/
vmovaps
%
ymm3
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_skx
.
globl
_sk_move_dst_src_skx
FUNCTION
(
_sk_move_dst_src_skx
)
_sk_move_dst_src_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
196
/
/
vmovaps
%
ymm4
%
ymm0
.
byte
197
252
40
205
/
/
vmovaps
%
ymm5
%
ymm1
.
byte
197
252
40
214
/
/
vmovaps
%
ymm6
%
ymm2
.
byte
197
252
40
223
/
/
vmovaps
%
ymm7
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_skx
.
globl
_sk_premul_skx
FUNCTION
(
_sk_premul_skx
)
_sk_premul_skx
:
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_skx
.
globl
_sk_premul_dst_skx
FUNCTION
(
_sk_premul_dst_skx
)
_sk_premul_dst_skx
:
.
byte
197
220
89
231
/
/
vmulps
%
ymm7
%
ymm4
%
ymm4
.
byte
197
212
89
239
/
/
vmulps
%
ymm7
%
ymm5
%
ymm5
.
byte
197
204
89
247
/
/
vmulps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_unpremul_skx
.
globl
_sk_unpremul_skx
FUNCTION
(
_sk_unpremul_skx
)
_sk_unpremul_skx
:
.
byte
196
98
125
24
5
17
171
3
0
/
/
vbroadcastss
0x3ab11
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
94
195
/
/
vdivps
%
ymm3
%
ymm8
%
ymm8
.
byte
98
241
60
56
194
5
50
171
3
0
1
/
/
vcmpltps
0x3ab32
(
%
rip
)
{
1to8
}
%
ymm8
%
k0
#
3c52c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e0
>
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
45
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm10
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_skx
.
globl
_sk_force_opaque_skx
FUNCTION
(
_sk_force_opaque_skx
)
_sk_force_opaque_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
214
170
3
0
/
/
vbroadcastss
0x3aad6
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_skx
.
globl
_sk_force_opaque_dst_skx
FUNCTION
(
_sk_force_opaque_dst_skx
)
_sk_force_opaque_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
201
170
3
0
/
/
vbroadcastss
0x3aac9
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_skx
.
globl
_sk_from_srgb_skx
FUNCTION
(
_sk_from_srgb_skx
)
_sk_from_srgb_skx
:
.
byte
196
98
125
24
5
242
170
3
0
/
/
vbroadcastss
0x3aaf2
(
%
rip
)
%
ymm8
#
3c530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e4
>
.
byte
196
65
124
89
200
/
/
vmulps
%
ymm8
%
ymm0
%
ymm9
.
byte
197
124
89
208
/
/
vmulps
%
ymm0
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
228
170
3
0
/
/
vbroadcastss
0x3aae4
(
%
rip
)
%
ymm11
#
3c534
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e8
>
.
byte
196
98
125
24
37
195
170
3
0
/
/
vbroadcastss
0x3aac3
(
%
rip
)
%
ymm12
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
124
40
236
/
/
vmovaps
%
ymm12
%
ymm13
.
byte
196
66
125
168
235
/
/
vfmadd213ps
%
ymm11
%
ymm0
%
ymm13
.
byte
196
98
125
24
53
204
170
3
0
/
/
vbroadcastss
0x3aacc
(
%
rip
)
%
ymm14
#
3c538
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ec
>
.
byte
196
66
45
168
238
/
/
vfmadd213ps
%
ymm14
%
ymm10
%
ymm13
.
byte
196
98
125
24
21
194
170
3
0
/
/
vbroadcastss
0x3aac2
(
%
rip
)
%
ymm10
#
3c53c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f0
>
.
byte
98
209
124
40
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm0
%
k0
.
byte
98
242
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm0
.
byte
196
195
21
74
193
0
/
/
vblendvps
%
ymm0
%
ymm9
%
ymm13
%
ymm0
.
byte
196
65
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm9
.
byte
197
116
89
233
/
/
vmulps
%
ymm1
%
ymm1
%
ymm13
.
byte
196
65
124
40
252
/
/
vmovaps
%
ymm12
%
ymm15
.
byte
196
66
117
168
251
/
/
vfmadd213ps
%
ymm11
%
ymm1
%
ymm15
.
byte
196
66
21
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm13
%
ymm15
.
byte
98
209
116
40
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm1
%
k0
.
byte
98
242
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm1
.
byte
196
195
5
74
201
16
/
/
vblendvps
%
ymm1
%
ymm9
%
ymm15
%
ymm1
.
byte
196
65
108
89
192
/
/
vmulps
%
ymm8
%
ymm2
%
ymm8
.
byte
197
108
89
202
/
/
vmulps
%
ymm2
%
ymm2
%
ymm9
.
byte
196
66
109
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm2
%
ymm12
.
byte
196
66
53
168
230
/
/
vfmadd213ps
%
ymm14
%
ymm9
%
ymm12
.
byte
98
209
108
40
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm2
%
k0
.
byte
98
242
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm2
.
byte
196
195
29
74
208
32
/
/
vblendvps
%
ymm2
%
ymm8
%
ymm12
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_dst_skx
.
globl
_sk_from_srgb_dst_skx
FUNCTION
(
_sk_from_srgb_dst_skx
)
_sk_from_srgb_dst_skx
:
.
byte
196
98
125
24
5
69
170
3
0
/
/
vbroadcastss
0x3aa45
(
%
rip
)
%
ymm8
#
3c530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e4
>
.
byte
196
65
92
89
200
/
/
vmulps
%
ymm8
%
ymm4
%
ymm9
.
byte
197
92
89
212
/
/
vmulps
%
ymm4
%
ymm4
%
ymm10
.
byte
196
98
125
24
29
55
170
3
0
/
/
vbroadcastss
0x3aa37
(
%
rip
)
%
ymm11
#
3c534
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e8
>
.
byte
196
98
125
24
37
22
170
3
0
/
/
vbroadcastss
0x3aa16
(
%
rip
)
%
ymm12
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
124
40
236
/
/
vmovaps
%
ymm12
%
ymm13
.
byte
196
66
93
168
235
/
/
vfmadd213ps
%
ymm11
%
ymm4
%
ymm13
.
byte
196
98
125
24
53
31
170
3
0
/
/
vbroadcastss
0x3aa1f
(
%
rip
)
%
ymm14
#
3c538
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ec
>
.
byte
196
66
45
168
238
/
/
vfmadd213ps
%
ymm14
%
ymm10
%
ymm13
.
byte
196
98
125
24
21
21
170
3
0
/
/
vbroadcastss
0x3aa15
(
%
rip
)
%
ymm10
#
3c53c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f0
>
.
byte
98
209
92
40
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm4
%
k0
.
byte
98
242
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm4
.
byte
196
195
21
74
225
64
/
/
vblendvps
%
ymm4
%
ymm9
%
ymm13
%
ymm4
.
byte
196
65
84
89
200
/
/
vmulps
%
ymm8
%
ymm5
%
ymm9
.
byte
197
84
89
237
/
/
vmulps
%
ymm5
%
ymm5
%
ymm13
.
byte
196
65
124
40
252
/
/
vmovaps
%
ymm12
%
ymm15
.
byte
196
66
85
168
251
/
/
vfmadd213ps
%
ymm11
%
ymm5
%
ymm15
.
byte
196
66
21
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm13
%
ymm15
.
byte
98
209
84
40
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm5
%
k0
.
byte
98
242
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm5
.
byte
196
195
5
74
233
80
/
/
vblendvps
%
ymm5
%
ymm9
%
ymm15
%
ymm5
.
byte
196
65
76
89
192
/
/
vmulps
%
ymm8
%
ymm6
%
ymm8
.
byte
197
76
89
206
/
/
vmulps
%
ymm6
%
ymm6
%
ymm9
.
byte
196
66
77
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm6
%
ymm12
.
byte
196
66
53
168
230
/
/
vfmadd213ps
%
ymm14
%
ymm9
%
ymm12
.
byte
98
209
76
40
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm6
%
k0
.
byte
98
242
126
40
56
240
/
/
vpmovm2d
%
k0
%
ymm6
.
byte
196
195
29
74
240
96
/
/
vblendvps
%
ymm6
%
ymm8
%
ymm12
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_to_srgb_skx
.
globl
_sk_to_srgb_skx
FUNCTION
(
_sk_to_srgb_skx
)
_sk_to_srgb_skx
:
.
byte
98
114
125
40
78
192
/
/
vrsqrt14ps
%
ymm0
%
ymm8
.
byte
196
98
125
24
13
162
169
3
0
/
/
vbroadcastss
0x3a9a2
(
%
rip
)
%
ymm9
#
3c540
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f4
>
.
byte
196
65
124
89
209
/
/
vmulps
%
ymm9
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
152
169
3
0
/
/
vbroadcastss
0x3a998
(
%
rip
)
%
ymm11
#
3c544
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f8
>
.
byte
196
98
125
24
37
147
169
3
0
/
/
vbroadcastss
0x3a993
(
%
rip
)
%
ymm12
#
3c548
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2fc
>
.
byte
196
65
124
40
236
/
/
vmovaps
%
ymm12
%
ymm13
.
byte
196
66
61
168
235
/
/
vfmadd213ps
%
ymm11
%
ymm8
%
ymm13
.
byte
196
98
125
24
53
132
169
3
0
/
/
vbroadcastss
0x3a984
(
%
rip
)
%
ymm14
#
3c54c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x300
>
.
byte
196
66
61
168
238
/
/
vfmadd213ps
%
ymm14
%
ymm8
%
ymm13
.
byte
196
98
125
24
61
122
169
3
0
/
/
vbroadcastss
0x3a97a
(
%
rip
)
%
ymm15
#
3c550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x304
>
.
byte
196
65
60
88
199
/
/
vaddps
%
ymm15
%
ymm8
%
ymm8
.
byte
98
82
125
40
76
192
/
/
vrcp14ps
%
ymm8
%
ymm8
.
byte
196
65
20
89
192
/
/
vmulps
%
ymm8
%
ymm13
%
ymm8
.
byte
196
98
125
24
45
101
169
3
0
/
/
vbroadcastss
0x3a965
(
%
rip
)
%
ymm13
#
3c554
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x308
>
.
byte
98
209
124
40
194
197
1
/
/
vcmpltps
%
ymm13
%
ymm0
%
k0
.
byte
98
242
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm0
.
byte
196
195
61
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm8
%
ymm0
.
byte
98
114
125
40
78
193
/
/
vrsqrt14ps
%
ymm1
%
ymm8
.
byte
196
65
116
89
209
/
/
vmulps
%
ymm9
%
ymm1
%
ymm10
.
byte
98
193
124
40
40
196
/
/
vmovaps
%
ymm12
%
ymm16
.
byte
98
194
61
40
168
195
/
/
vfmadd213ps
%
ymm11
%
ymm8
%
ymm16
.
byte
98
194
61
40
168
198
/
/
vfmadd213ps
%
ymm14
%
ymm8
%
ymm16
.
byte
196
65
60
88
199
/
/
vaddps
%
ymm15
%
ymm8
%
ymm8
.
byte
98
82
125
40
76
192
/
/
vrcp14ps
%
ymm8
%
ymm8
.
byte
98
81
124
32
89
192
/
/
vmulps
%
ymm8
%
ymm16
%
ymm8
.
byte
98
209
116
40
194
197
1
/
/
vcmpltps
%
ymm13
%
ymm1
%
k0
.
byte
98
242
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm1
.
byte
196
195
61
74
202
16
/
/
vblendvps
%
ymm1
%
ymm10
%
ymm8
%
ymm1
.
byte
98
114
125
40
78
194
/
/
vrsqrt14ps
%
ymm2
%
ymm8
.
byte
196
65
108
89
201
/
/
vmulps
%
ymm9
%
ymm2
%
ymm9
.
byte
196
66
61
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm8
%
ymm12
.
byte
196
66
61
168
230
/
/
vfmadd213ps
%
ymm14
%
ymm8
%
ymm12
.
byte
196
65
60
88
199
/
/
vaddps
%
ymm15
%
ymm8
%
ymm8
.
byte
98
82
125
40
76
192
/
/
vrcp14ps
%
ymm8
%
ymm8
.
byte
196
65
28
89
192
/
/
vmulps
%
ymm8
%
ymm12
%
ymm8
.
byte
98
209
108
40
194
197
1
/
/
vcmpltps
%
ymm13
%
ymm2
%
k0
.
byte
98
242
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm2
.
byte
196
195
61
74
209
32
/
/
vblendvps
%
ymm2
%
ymm9
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_rgb_to_hsl_skx
.
globl
_sk_rgb_to_hsl_skx
FUNCTION
(
_sk_rgb_to_hsl_skx
)
_sk_rgb_to_hsl_skx
:
.
byte
98
225
124
40
40
207
/
/
vmovaps
%
ymm7
%
ymm17
.
byte
197
116
95
194
/
/
vmaxps
%
ymm2
%
ymm1
%
ymm8
.
byte
196
65
124
95
200
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm9
.
byte
197
116
93
194
/
/
vminps
%
ymm2
%
ymm1
%
ymm8
.
byte
196
65
124
93
208
/
/
vminps
%
ymm8
%
ymm0
%
ymm10
.
byte
98
193
52
40
92
194
/
/
vsubps
%
ymm10
%
ymm9
%
ymm16
.
byte
196
98
125
24
5
86
168
3
0
/
/
vbroadcastss
0x3a856
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
98
49
60
40
94
224
/
/
vdivps
%
ymm16
%
ymm8
%
ymm12
.
byte
98
209
52
40
194
194
0
/
/
vcmpeqps
%
ymm10
%
ymm9
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
98
241
52
40
194
192
0
/
/
vcmpeqps
%
ymm0
%
ymm9
%
k0
.
byte
98
114
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm13
.
byte
197
116
92
242
/
/
vsubps
%
ymm2
%
ymm1
%
ymm14
.
byte
98
241
116
40
194
194
1
/
/
vcmpltps
%
ymm2
%
ymm1
%
k0
.
byte
98
114
126
40
56
248
/
/
vpmovm2d
%
k0
%
ymm15
.
byte
196
98
125
24
29
120
168
3
0
/
/
vbroadcastss
0x3a878
(
%
rip
)
%
ymm11
#
3c558
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30c
>
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
196
67
69
74
219
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm7
%
ymm11
.
byte
196
66
29
168
243
/
/
vfmadd213ps
%
ymm11
%
ymm12
%
ymm14
.
byte
98
241
52
40
194
193
0
/
/
vcmpeqps
%
ymm1
%
ymm9
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
98
125
24
61
83
168
3
0
/
/
vbroadcastss
0x3a853
(
%
rip
)
%
ymm15
#
3c55c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x310
>
.
byte
196
194
29
168
215
/
/
vfmadd213ps
%
ymm15
%
ymm12
%
ymm2
.
byte
197
252
92
193
/
/
vsubps
%
ymm1
%
ymm0
%
ymm0
.
byte
98
242
29
56
168
5
68
168
3
0
/
/
vfmadd213ps
0x3a844
(
%
rip
)
{
1to8
}
%
ymm12
%
ymm0
#
3c560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x314
>
.
byte
196
227
125
74
194
176
/
/
vblendvps
%
ymm11
%
ymm2
%
ymm0
%
ymm0
.
byte
196
195
125
74
198
208
/
/
vblendvps
%
ymm13
%
ymm14
%
ymm0
%
ymm0
.
byte
196
227
125
74
199
128
/
/
vblendvps
%
ymm8
%
ymm7
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
44
168
3
0
/
/
vmulps
0x3a82c
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c564
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x318
>
.
byte
196
193
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm1
.
byte
196
98
125
24
29
178
167
3
0
/
/
vbroadcastss
0x3a7b2
(
%
rip
)
%
ymm11
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
116
89
211
/
/
vmulps
%
ymm11
%
ymm1
%
ymm2
.
byte
98
241
36
40
194
194
1
/
/
vcmpltps
%
ymm2
%
ymm11
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
196
65
4
92
201
/
/
vsubps
%
ymm9
%
ymm15
%
ymm9
.
byte
196
65
52
92
202
/
/
vsubps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
195
117
74
201
176
/
/
vblendvps
%
ymm11
%
ymm9
%
ymm1
%
ymm1
.
byte
98
241
124
32
94
201
/
/
vdivps
%
ymm1
%
ymm16
%
ymm1
.
byte
196
227
117
74
207
128
/
/
vblendvps
%
ymm8
%
ymm7
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
177
124
40
40
249
/
/
vmovaps
%
ymm17
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hsl_to_rgb_skx
.
globl
_sk_hsl_to_rgb_skx
FUNCTION
(
_sk_hsl_to_rgb_skx
)
_sk_hsl_to_rgb_skx
:
.
byte
98
225
124
40
40
215
/
/
vmovaps
%
ymm7
%
ymm18
.
byte
98
225
124
40
40
230
/
/
vmovaps
%
ymm6
%
ymm20
.
byte
98
226
125
40
24
5
100
167
3
0
/
/
vbroadcastss
0x3a764
(
%
rip
)
%
ymm16
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
98
241
124
32
194
194
2
/
/
vcmpleps
%
ymm2
%
ymm16
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
197
116
89
202
/
/
vmulps
%
ymm2
%
ymm1
%
ymm9
.
byte
196
65
116
92
209
/
/
vsubps
%
ymm9
%
ymm1
%
ymm10
.
byte
196
67
53
74
194
128
/
/
vblendvps
%
ymm8
%
ymm10
%
ymm9
%
ymm8
.
byte
197
60
88
218
/
/
vaddps
%
ymm2
%
ymm8
%
ymm11
.
byte
196
65
124
40
203
/
/
vmovaps
%
ymm11
%
ymm9
.
byte
98
114
109
56
186
13
153
167
3
0
/
/
vfmsub231ps
0x3a799
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm9
#
3c55c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x310
>
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
98
209
116
40
194
192
0
/
/
vcmpeqps
%
ymm8
%
ymm1
%
k0
.
byte
98
242
126
40
56
248
/
/
vpmovm2d
%
k0
%
ymm7
.
byte
98
241
124
56
88
13
137
167
3
0
/
/
vaddps
0x3a789
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm1
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
99
125
8
193
1
/
/
vroundps
0x1
%
ymm1
%
ymm8
.
byte
196
193
116
92
200
/
/
vsubps
%
ymm8
%
ymm1
%
ymm1
.
byte
98
226
125
40
24
29
120
167
3
0
/
/
vbroadcastss
0x3a778
(
%
rip
)
%
ymm19
#
3c56c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x320
>
.
byte
98
241
100
32
194
193
2
/
/
vcmpleps
%
ymm1
%
ymm19
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
65
36
92
241
/
/
vsubps
%
ymm9
%
ymm11
%
ymm14
.
byte
196
98
125
24
61
73
167
3
0
/
/
vbroadcastss
0x3a749
(
%
rip
)
%
ymm15
#
3c558
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30c
>
.
byte
196
65
116
89
231
/
/
vmulps
%
ymm15
%
ymm1
%
ymm12
.
byte
98
226
125
40
24
13
66
167
3
0
/
/
vbroadcastss
0x3a742
(
%
rip
)
%
ymm17
#
3c560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x314
>
.
byte
98
81
116
32
92
212
/
/
vsubps
%
ymm12
%
ymm17
%
ymm10
.
byte
196
66
13
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm14
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
98
241
124
32
194
193
2
/
/
vcmpleps
%
ymm1
%
ymm16
%
k0
.
byte
98
114
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm10
.
byte
196
67
37
74
192
160
/
/
vblendvps
%
ymm10
%
ymm8
%
ymm11
%
ymm8
.
byte
196
98
125
24
21
25
167
3
0
/
/
vbroadcastss
0x3a719
(
%
rip
)
%
ymm10
#
3c564
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x318
>
.
byte
98
241
44
40
194
193
2
/
/
vcmpleps
%
ymm1
%
ymm10
%
k0
.
byte
98
242
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm1
.
byte
196
66
13
168
225
/
/
vfmadd213ps
%
ymm9
%
ymm14
%
ymm12
.
byte
196
195
29
74
200
16
/
/
vblendvps
%
ymm1
%
ymm8
%
ymm12
%
ymm1
.
byte
196
227
117
74
242
112
/
/
vblendvps
%
ymm7
%
ymm2
%
ymm1
%
ymm6
.
byte
196
227
125
8
200
1
/
/
vroundps
0x1
%
ymm0
%
ymm1
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
98
241
100
32
194
193
2
/
/
vcmpleps
%
ymm1
%
ymm19
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
196
65
116
89
239
/
/
vmulps
%
ymm15
%
ymm1
%
ymm13
.
byte
98
81
116
32
92
197
/
/
vsubps
%
ymm13
%
ymm17
%
ymm8
.
byte
196
66
13
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm14
%
ymm8
.
byte
196
67
61
74
193
192
/
/
vblendvps
%
ymm12
%
ymm9
%
ymm8
%
ymm8
.
byte
98
241
124
32
194
193
2
/
/
vcmpleps
%
ymm1
%
ymm16
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
196
67
37
74
192
192
/
/
vblendvps
%
ymm12
%
ymm8
%
ymm11
%
ymm8
.
byte
98
241
44
40
194
193
2
/
/
vcmpleps
%
ymm1
%
ymm10
%
k0
.
byte
98
242
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm1
.
byte
196
66
13
168
233
/
/
vfmadd213ps
%
ymm9
%
ymm14
%
ymm13
.
byte
196
195
21
74
200
16
/
/
vblendvps
%
ymm1
%
ymm8
%
ymm13
%
ymm1
.
byte
196
227
117
74
202
112
/
/
vblendvps
%
ymm7
%
ymm2
%
ymm1
%
ymm1
.
byte
98
241
124
56
88
5
159
166
3
0
/
/
vaddps
0x3a69f
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
99
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
98
241
100
32
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm19
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
65
124
89
231
/
/
vmulps
%
ymm15
%
ymm0
%
ymm12
.
byte
98
81
116
32
92
236
/
/
vsubps
%
ymm12
%
ymm17
%
ymm13
.
byte
196
66
13
168
233
/
/
vfmadd213ps
%
ymm9
%
ymm14
%
ymm13
.
byte
196
67
21
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm13
%
ymm8
.
byte
98
241
124
32
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm16
%
k0
.
byte
98
114
126
40
56
232
/
/
vpmovm2d
%
k0
%
ymm13
.
byte
196
67
37
74
192
208
/
/
vblendvps
%
ymm13
%
ymm8
%
ymm11
%
ymm8
.
byte
98
241
44
40
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm10
%
k0
.
byte
98
242
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm0
.
byte
196
66
13
184
204
/
/
vfmadd231ps
%
ymm12
%
ymm14
%
ymm9
.
byte
196
195
53
74
192
0
/
/
vblendvps
%
ymm0
%
ymm8
%
ymm9
%
ymm0
.
byte
196
227
125
74
210
112
/
/
vblendvps
%
ymm7
%
ymm2
%
ymm0
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
198
/
/
vmovaps
%
ymm6
%
ymm0
.
byte
98
177
124
40
40
244
/
/
vmovaps
%
ymm20
%
ymm6
.
byte
98
177
124
40
40
250
/
/
vmovaps
%
ymm18
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_skx
.
globl
_sk_scale_1_float_skx
FUNCTION
(
_sk_scale_1_float_skx
)
_sk_scale_1_float_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_skx
.
globl
_sk_scale_u8_skx
FUNCTION
(
_sk_scale_u8_skx
)
_sk_scale_u8_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
54
/
/
jne
1fa7
<
_sk_scale_u8_skx
+
0x48
>
.
byte
196
66
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
197
57
219
5
49
175
3
0
/
/
vpand
0x3af31
(
%
rip
)
%
xmm8
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
98
113
60
56
89
5
225
165
3
0
/
/
vmulps
0x3a5e1
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
193
/
/
ja
1f77
<
_sk_scale_u8_skx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
140
0
0
0
/
/
lea
0x8c
(
%
rip
)
%
r9
#
204c
<
_sk_scale_u8_skx
+
0xed
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
235
163
/
/
jmp
1f77
<
_sk_scale_u8_skx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm8
%
xmm8
.
byte
196
66
121
50
12
16
/
/
vpmovzxbq
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
65
121
112
201
232
/
/
vpshufd
0xe8
%
xmm9
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
118
255
255
255
/
/
jmpq
1f77
<
_sk_scale_u8_skx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm8
%
xmm8
.
byte
196
66
121
49
12
16
/
/
vpmovzxbd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
122
112
201
232
/
/
vpshufhw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
121
112
201
232
/
/
vpshufd
0xe8
%
xmm9
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
233
45
255
255
255
/
/
jmpq
1f77
<
_sk_scale_u8_skx
+
0x18
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
125
255
/
/
jge
204d
<
_sk_scale_u8_skx
+
0xee
>
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
136
/
/
lcall
*
-
0x77000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
181
255
/
/
mov
0xff
%
ch
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_scale_565_skx
.
globl
_sk_scale_565_skx
FUNCTION
(
_sk_scale_565_skx
)
_sk_scale_565_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
145
0
0
0
/
/
jne
2112
<
_sk_scale_565_skx
+
0xaa
>
.
byte
196
65
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
98
113
61
56
219
13
226
164
3
0
/
/
vpandd
0x3a4e2
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
196
65
124
91
201
/
/
vcvtdq2ps
%
ymm9
%
ymm9
.
byte
98
113
52
56
89
13
215
164
3
0
/
/
vmulps
0x3a4d7
(
%
rip
)
{
1to8
}
%
ymm9
%
ymm9
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
98
113
61
56
219
21
209
164
3
0
/
/
vpandd
0x3a4d1
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm10
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
196
65
124
91
210
/
/
vcvtdq2ps
%
ymm10
%
ymm10
.
byte
98
113
44
56
89
21
198
164
3
0
/
/
vmulps
0x3a4c6
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
98
113
61
56
219
5
192
164
3
0
/
/
vpandd
0x3a4c0
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
98
113
60
56
89
5
181
164
3
0
/
/
vmulps
0x3a4b5
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
98
241
100
40
194
199
1
/
/
vcmpltps
%
ymm7
%
ymm3
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
196
65
44
93
224
/
/
vminps
%
ymm8
%
ymm10
%
ymm12
.
byte
196
65
52
93
228
/
/
vminps
%
ymm12
%
ymm9
%
ymm12
.
byte
196
65
44
95
232
/
/
vmaxps
%
ymm8
%
ymm10
%
ymm13
.
byte
196
65
52
95
237
/
/
vmaxps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
220
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
172
89
201
/
/
vmulps
%
ymm1
%
ymm10
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
164
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
98
255
255
255
/
/
ja
2087
<
_sk_scale_565_skx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
129
0
0
0
/
/
lea
0x81
(
%
rip
)
%
r9
#
21b0
<
_sk_scale_565_skx
+
0x148
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
65
255
255
255
/
/
jmpq
2087
<
_sk_scale_565_skx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
66
121
52
12
80
/
/
vpmovzxwq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
65
121
112
201
232
/
/
vpshufd
0xe8
%
xmm9
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
23
255
255
255
/
/
jmpq
2087
<
_sk_scale_565_skx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
66
121
51
12
80
/
/
vpmovzxwd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
122
112
201
232
/
/
vpshufhw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
121
112
201
132
/
/
vpshufd
0x84
%
xmm9
%
xmm9
.
byte
196
65
49
109
192
/
/
vpunpckhqdq
%
xmm8
%
xmm9
%
xmm8
.
byte
233
216
254
255
255
/
/
jmpq
2087
<
_sk_scale_565_skx
+
0x1f
>
.
byte
144
/
/
nop
.
byte
136
255
/
/
mov
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
163
255
255
255
150
/
/
jmpq
*
-
0x69000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
192
/
/
inc
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_1_float_skx
.
globl
_sk_lerp_1_float_skx
FUNCTION
(
_sk_lerp_1_float_skx
)
_sk_lerp_1_float_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
226
61
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
226
61
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
226
61
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_u8_skx
.
globl
_sk_lerp_u8_skx
FUNCTION
(
_sk_lerp_u8_skx
)
_sk_lerp_u8_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
74
/
/
jne
2257
<
_sk_lerp_u8_skx
+
0x5c
>
.
byte
196
66
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
197
57
219
5
149
172
3
0
/
/
vpand
0x3ac95
(
%
rip
)
%
xmm8
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
98
113
60
56
89
5
69
163
3
0
/
/
vmulps
0x3a345
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
226
61
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
226
61
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
226
61
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
173
/
/
ja
2213
<
_sk_lerp_u8_skx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
140
0
0
0
/
/
lea
0x8c
(
%
rip
)
%
r9
#
22fc
<
_sk_lerp_u8_skx
+
0x101
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
235
143
/
/
jmp
2213
<
_sk_lerp_u8_skx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm8
%
xmm8
.
byte
196
66
121
50
12
16
/
/
vpmovzxbq
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
65
121
112
201
232
/
/
vpshufd
0xe8
%
xmm9
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
98
255
255
255
/
/
jmpq
2213
<
_sk_lerp_u8_skx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm8
%
xmm8
.
byte
196
66
121
49
12
16
/
/
vpmovzxbd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
122
112
201
232
/
/
vpshufhw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
121
112
201
232
/
/
vpshufd
0xe8
%
xmm9
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
233
25
255
255
255
/
/
jmpq
2213
<
_sk_lerp_u8_skx
+
0x18
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
125
255
/
/
jge
22fd
<
_sk_lerp_u8_skx
+
0x102
>
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
136
/
/
lcall
*
-
0x77000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
181
255
/
/
mov
0xff
%
ch
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_565_skx
.
globl
_sk_lerp_565_skx
FUNCTION
(
_sk_lerp_565_skx
)
_sk_lerp_565_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
165
0
0
0
/
/
jne
23d6
<
_sk_lerp_565_skx
+
0xbe
>
.
byte
196
65
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
98
113
61
56
219
13
50
162
3
0
/
/
vpandd
0x3a232
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
196
65
124
91
201
/
/
vcvtdq2ps
%
ymm9
%
ymm9
.
byte
98
113
52
56
89
13
39
162
3
0
/
/
vmulps
0x3a227
(
%
rip
)
{
1to8
}
%
ymm9
%
ymm9
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
98
113
61
56
219
21
33
162
3
0
/
/
vpandd
0x3a221
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm10
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
196
65
124
91
210
/
/
vcvtdq2ps
%
ymm10
%
ymm10
.
byte
98
113
44
56
89
21
22
162
3
0
/
/
vmulps
0x3a216
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
98
113
61
56
219
5
16
162
3
0
/
/
vpandd
0x3a210
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
98
113
60
56
89
5
5
162
3
0
/
/
vmulps
0x3a205
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
98
241
100
40
194
199
1
/
/
vcmpltps
%
ymm7
%
ymm3
%
k0
.
byte
98
114
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm11
.
byte
196
65
44
93
224
/
/
vminps
%
ymm8
%
ymm10
%
ymm12
.
byte
196
65
52
93
228
/
/
vminps
%
ymm12
%
ymm9
%
ymm12
.
byte
196
65
44
95
232
/
/
vmaxps
%
ymm8
%
ymm10
%
ymm13
.
byte
196
65
52
95
237
/
/
vmaxps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
220
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
226
53
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm9
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
226
45
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm10
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
226
37
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm11
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
78
255
255
255
/
/
ja
2337
<
_sk_lerp_565_skx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
129
0
0
0
/
/
lea
0x81
(
%
rip
)
%
r9
#
2474
<
_sk_lerp_565_skx
+
0x15c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
45
255
255
255
/
/
jmpq
2337
<
_sk_lerp_565_skx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
66
121
52
12
80
/
/
vpmovzxwq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
65
121
112
201
232
/
/
vpshufd
0xe8
%
xmm9
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
3
255
255
255
/
/
jmpq
2337
<
_sk_lerp_565_skx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
66
121
51
12
80
/
/
vpmovzxwd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
65
123
112
201
232
/
/
vpshuflw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
122
112
201
232
/
/
vpshufhw
0xe8
%
xmm9
%
xmm9
.
byte
196
65
121
112
201
132
/
/
vpshufd
0x84
%
xmm9
%
xmm9
.
byte
196
65
49
109
192
/
/
vpunpckhqdq
%
xmm8
%
xmm9
%
xmm8
.
byte
233
196
254
255
255
/
/
jmpq
2337
<
_sk_lerp_565_skx
+
0x1f
>
.
byte
144
/
/
nop
.
byte
136
255
/
/
mov
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
163
255
255
255
150
/
/
jmpq
*
-
0x69000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
192
/
/
inc
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_tables_skx
.
globl
_sk_load_tables_skx
FUNCTION
(
_sk_load_tables_skx
)
_sk_load_tables_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
124
/
/
jne
2516
<
_sk_load_tables_skx
+
0x86
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
65
184
17
17
17
17
/
/
mov
0x11111111
%
r8d
.
byte
196
193
123
146
200
/
/
kmovd
%
r8d
%
k1
.
byte
98
241
127
169
111
203
/
/
vmovdqu8
%
ymm3
%
ymm1
{
%
k1
}
{
z
}
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
194
109
146
4
136
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm1
4
)
%
ymm0
.
byte
197
245
114
211
8
/
/
vpsrld
0x8
%
ymm3
%
ymm1
.
byte
98
241
127
169
111
209
/
/
vmovdqu8
%
ymm1
%
ymm2
{
%
k1
}
{
z
}
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
194
61
146
12
145
/
/
vgatherdps
%
ymm8
(
%
r9
%
ymm2
4
)
%
ymm1
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
197
181
114
211
16
/
/
vpsrld
0x10
%
ymm3
%
ymm9
.
byte
98
81
127
169
111
201
/
/
vmovdqu8
%
ymm9
%
ymm9
{
%
k1
}
{
z
}
.
byte
196
162
61
146
20
136
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm9
4
)
%
ymm2
.
byte
197
229
114
211
24
/
/
vpsrld
0x18
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
98
241
100
56
89
29
98
160
3
0
/
/
vmulps
0x3a062
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
137
249
/
/
mov
%
edi
%
r9d
.
byte
65
128
225
7
/
/
and
0x7
%
r9b
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
65
128
193
255
/
/
add
0xff
%
r9b
.
byte
65
128
249
6
/
/
cmp
0x6
%
r9b
.
byte
15
135
113
255
255
255
/
/
ja
24a0
<
_sk_load_tables_skx
+
0x10
>
.
byte
69
15
182
201
/
/
movzbl
%
r9b
%
r9d
.
byte
76
141
21
142
0
0
0
/
/
lea
0x8e
(
%
rip
)
%
r10
#
25c8
<
_sk_load_tables_skx
+
0x138
>
.
byte
79
99
12
138
/
/
movslq
(
%
r10
%
r9
4
)
%
r9
.
byte
77
1
209
/
/
add
%
r10
%
r9
.
byte
65
255
225
/
/
jmpq
*
%
r9
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
81
255
255
255
/
/
jmpq
24a0
<
_sk_load_tables_skx
+
0x10
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
65
177
4
/
/
mov
0x4
%
r9b
.
byte
196
193
123
146
201
/
/
kmovd
%
r9d
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
194
121
53
4
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
232
/
/
vpshufd
0xe8
%
xmm0
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
38
255
255
255
/
/
jmpq
24a0
<
_sk_load_tables_skx
+
0x10
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
65
177
64
/
/
mov
0x40
%
r9b
.
byte
196
193
123
146
201
/
/
kmovd
%
r9d
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
216
254
255
255
/
/
jmpq
24a0
<
_sk_load_tables_skx
+
0x10
>
.
byte
124
255
/
/
jl
25c9
<
_sk_load_tables_skx
+
0x139
>
.
byte
255
/
/
(
bad
)
.
byte
255
156
255
255
255
135
255
/
/
lcall
*
-
0x780001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
178
255
/
/
mov
0xff
%
dl
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_tables_u16_be_skx
.
globl
_sk_load_tables_u16_be_skx
FUNCTION
(
_sk_load_tables_u16_be_skx
)
_sk_load_tables_u16_be_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
210
0
0
0
/
/
jne
26cc
<
_sk_load_tables_u16_be_skx
+
0xe8
>
.
byte
196
1
121
16
4
72
/
/
vmovupd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
129
121
16
84
72
16
/
/
vmovupd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
196
129
121
16
92
72
32
/
/
vmovupd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
1
122
111
76
72
48
/
/
vmovdqu
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
121
105
202
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm9
.
byte
197
241
97
195
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm0
.
byte
197
113
105
219
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm11
.
byte
197
185
108
200
/
/
vpunpcklqdq
%
xmm0
%
xmm8
%
xmm1
.
byte
197
57
109
192
/
/
vpunpckhqdq
%
xmm0
%
xmm8
%
xmm8
.
byte
197
121
111
21
105
168
3
0
/
/
vmovdqa
0x3a869
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
193
113
219
194
/
/
vpand
%
xmm10
%
xmm1
%
xmm0
.
byte
196
226
125
51
200
/
/
vpmovzxwd
%
xmm0
%
ymm1
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
194
109
146
4
136
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm1
4
)
%
ymm0
.
byte
196
193
49
108
219
/
/
vpunpcklqdq
%
xmm11
%
xmm9
%
xmm3
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
193
57
219
202
/
/
vpand
%
xmm10
%
xmm8
%
xmm1
.
byte
196
98
125
51
193
/
/
vpmovzxwd
%
xmm1
%
ymm8
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
130
21
146
12
129
/
/
vgatherdps
%
ymm13
(
%
r9
%
ymm8
4
)
%
ymm1
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
196
193
97
219
218
/
/
vpand
%
xmm10
%
xmm3
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
196
226
29
146
20
152
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
196
193
49
109
219
/
/
vpunpckhqdq
%
xmm11
%
xmm9
%
xmm3
.
byte
197
185
113
243
8
/
/
vpsllw
0x8
%
xmm3
%
xmm8
.
byte
197
225
113
211
8
/
/
vpsrlw
0x8
%
xmm3
%
xmm3
.
byte
197
185
235
219
/
/
vpor
%
xmm3
%
xmm8
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
98
241
100
56
89
29
200
158
3
0
/
/
vmulps
0x39ec8
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
123
16
4
72
/
/
vmovsd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
2732
<
_sk_load_tables_u16_be_skx
+
0x14e
>
.
byte
196
1
57
22
68
72
8
/
/
vmovhpd
0x8
(
%
r8
%
r9
2
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
2732
<
_sk_load_tables_u16_be_skx
+
0x14e
>
.
byte
196
129
123
16
84
72
16
/
/
vmovsd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
273f
<
_sk_load_tables_u16_be_skx
+
0x15b
>
.
byte
196
129
105
22
84
72
24
/
/
vmovhpd
0x18
(
%
r8
%
r9
2
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
273f
<
_sk_load_tables_u16_be_skx
+
0x15b
>
.
byte
196
129
123
16
92
72
32
/
/
vmovsd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
0
255
255
255
/
/
je
2615
<
_sk_load_tables_u16_be_skx
+
0x31
>
.
byte
196
129
97
22
92
72
40
/
/
vmovhpd
0x28
(
%
r8
%
r9
2
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
239
254
255
255
/
/
jb
2615
<
_sk_load_tables_u16_be_skx
+
0x31
>
.
byte
196
1
122
126
76
72
48
/
/
vmovq
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
233
227
254
255
255
/
/
jmpq
2615
<
_sk_load_tables_u16_be_skx
+
0x31
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
214
254
255
255
/
/
jmpq
2615
<
_sk_load_tables_u16_be_skx
+
0x31
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
205
254
255
255
/
/
jmpq
2615
<
_sk_load_tables_u16_be_skx
+
0x31
>
HIDDEN
_sk_load_tables_rgb_u16_be_skx
.
globl
_sk_load_tables_rgb_u16_be_skx
FUNCTION
(
_sk_load_tables_rgb_u16_be_skx
)
_sk_load_tables_rgb_u16_be_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
203
0
0
0
/
/
jne
2825
<
_sk_load_tables_rgb_u16_be_skx
+
0xdd
>
.
byte
196
1
121
16
28
72
/
/
vmovupd
(
%
r8
%
r9
2
)
%
xmm11
.
byte
196
129
121
16
92
72
12
/
/
vmovupd
0xc
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
129
121
16
84
72
24
/
/
vmovupd
0x18
(
%
r8
%
r9
2
)
%
xmm2
.
byte
98
145
125
8
115
92
72
2
4
/
/
vpsrldq
0x4
0x20
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
193
57
115
219
6
/
/
vpsrldq
0x6
%
xmm11
%
xmm8
.
byte
197
169
115
219
6
/
/
vpsrldq
0x6
%
xmm3
%
xmm10
.
byte
197
241
115
218
6
/
/
vpsrldq
0x6
%
xmm2
%
xmm1
.
byte
197
177
115
216
6
/
/
vpsrldq
0x6
%
xmm0
%
xmm9
.
byte
196
193
113
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm1
%
xmm1
.
byte
197
233
97
192
/
/
vpunpcklwd
%
xmm0
%
xmm2
%
xmm0
.
byte
196
193
57
97
210
/
/
vpunpcklwd
%
xmm10
%
xmm8
%
xmm2
.
byte
197
161
97
219
/
/
vpunpcklwd
%
xmm3
%
xmm11
%
xmm3
.
byte
197
97
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm3
%
xmm8
.
byte
197
225
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm3
%
xmm2
.
byte
197
249
97
217
/
/
vpunpcklwd
%
xmm1
%
xmm0
%
xmm3
.
byte
197
249
105
193
/
/
vpunpckhwd
%
xmm1
%
xmm0
%
xmm0
.
byte
197
105
108
200
/
/
vpunpcklqdq
%
xmm0
%
xmm2
%
xmm9
.
byte
197
185
108
195
/
/
vpunpcklqdq
%
xmm3
%
xmm8
%
xmm0
.
byte
197
121
111
21
242
166
3
0
/
/
vmovdqa
0x3a6f2
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
193
121
219
194
/
/
vpand
%
xmm10
%
xmm0
%
xmm0
.
byte
196
226
125
51
200
/
/
vpmovzxwd
%
xmm0
%
ymm1
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
194
109
146
4
136
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm1
4
)
%
ymm0
.
byte
197
185
109
203
/
/
vpunpckhqdq
%
xmm3
%
xmm8
%
xmm1
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
193
113
219
202
/
/
vpand
%
xmm10
%
xmm1
%
xmm1
.
byte
196
98
125
51
193
/
/
vpmovzxwd
%
xmm1
%
ymm8
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
130
37
146
12
129
/
/
vgatherdps
%
ymm11
(
%
r9
%
ymm8
4
)
%
ymm1
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
196
193
49
219
218
/
/
vpand
%
xmm10
%
xmm9
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
196
226
29
146
20
152
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
217
156
3
0
/
/
vbroadcastss
0x39cd9
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
110
4
72
/
/
vmovd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
92
72
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
r9
2
)
%
xmm0
%
xmm11
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
31
/
/
jne
285d
<
_sk_load_tables_rgb_u16_be_skx
+
0x115
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
233
47
255
255
255
/
/
jmpq
278c
<
_sk_load_tables_rgb_u16_be_skx
+
0x44
>
.
byte
196
129
121
110
68
72
6
/
/
vmovd
0x6
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
68
72
10
2
/
/
vpinsrw
0x2
0xa
(
%
r8
%
r9
2
)
%
xmm0
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
48
/
/
jb
28a7
<
_sk_load_tables_rgb_u16_be_skx
+
0x15f
>
.
byte
196
129
121
110
68
72
12
/
/
vmovd
0xc
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
92
72
16
2
/
/
vpinsrw
0x2
0x10
(
%
r8
%
r9
2
)
%
xmm0
%
xmm3
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
117
48
/
/
jne
28c1
<
_sk_load_tables_rgb_u16_be_skx
+
0x179
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
233
229
254
255
255
/
/
jmpq
278c
<
_sk_load_tables_rgb_u16_be_skx
+
0x44
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
233
203
254
255
255
/
/
jmpq
278c
<
_sk_load_tables_rgb_u16_be_skx
+
0x44
>
.
byte
196
129
121
110
68
72
18
/
/
vmovd
0x12
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
84
72
22
2
/
/
vpinsrw
0x2
0x16
(
%
r8
%
r9
2
)
%
xmm0
%
xmm10
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
39
/
/
jb
2902
<
_sk_load_tables_rgb_u16_be_skx
+
0x1ba
>
.
byte
196
129
121
110
68
72
24
/
/
vmovd
0x18
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
84
72
28
2
/
/
vpinsrw
0x2
0x1c
(
%
r8
%
r9
2
)
%
xmm0
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
117
30
/
/
jne
2913
<
_sk_load_tables_rgb_u16_be_skx
+
0x1cb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
233
138
254
255
255
/
/
jmpq
278c
<
_sk_load_tables_rgb_u16_be_skx
+
0x44
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
233
121
254
255
255
/
/
jmpq
278c
<
_sk_load_tables_rgb_u16_be_skx
+
0x44
>
.
byte
196
129
121
110
68
72
30
/
/
vmovd
0x1e
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
76
72
34
2
/
/
vpinsrw
0x2
0x22
(
%
r8
%
r9
2
)
%
xmm0
%
xmm1
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
25
/
/
jb
2946
<
_sk_load_tables_rgb_u16_be_skx
+
0x1fe
>
.
byte
196
129
121
110
68
72
36
/
/
vmovd
0x24
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
68
72
40
2
/
/
vpinsrw
0x2
0x28
(
%
r8
%
r9
2
)
%
xmm0
%
xmm0
.
byte
233
70
254
255
255
/
/
jmpq
278c
<
_sk_load_tables_rgb_u16_be_skx
+
0x44
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
233
61
254
255
255
/
/
jmpq
278c
<
_sk_load_tables_rgb_u16_be_skx
+
0x44
>
HIDDEN
_sk_byte_tables_skx
.
globl
_sk_byte_tables_skx
FUNCTION
(
_sk_byte_tables_skx
)
_sk_byte_tables_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
29
153
155
3
0
/
/
vbroadcastss
0x39b99
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
211
/
/
vminps
%
ymm11
%
ymm9
%
ymm10
.
byte
196
98
125
24
13
183
155
3
0
/
/
vbroadcastss
0x39bb7
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
44
89
209
/
/
vmulps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
125
91
210
/
/
vcvtps2dq
%
ymm10
%
ymm10
.
byte
196
65
249
126
208
/
/
vmovq
%
xmm10
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
193
121
110
193
/
/
vmovd
%
r9d
%
xmm0
.
byte
196
67
249
22
209
1
/
/
vpextrq
0x1
%
xmm10
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
121
32
224
1
/
/
vpinsrb
0x1
%
r8d
%
xmm0
%
xmm12
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
99
125
57
208
1
/
/
vextracti128
0x1
%
ymm10
%
xmm0
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
25
32
208
2
/
/
vpinsrb
0x2
%
r8d
%
xmm12
%
xmm10
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
3
/
/
vpinsrb
0x3
%
r9d
%
xmm10
%
xmm10
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
4
/
/
vpinsrb
0x4
%
r9d
%
xmm10
%
xmm10
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
41
32
192
5
/
/
vpinsrb
0x5
%
r8d
%
xmm10
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
121
32
192
6
/
/
vpinsrb
0x6
%
r8d
%
xmm0
%
xmm0
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
121
32
209
7
/
/
vpinsrb
0x7
%
r9d
%
xmm0
%
xmm10
.
byte
197
188
95
193
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
200
/
/
vcvtps2dq
%
ymm0
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm0
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
4
/
/
vpinsrb
0x4
%
r10d
%
xmm0
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
5
/
/
vpinsrb
0x5
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
6
/
/
vpinsrb
0x6
%
r9d
%
xmm0
%
xmm0
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
195
121
32
200
7
/
/
vpinsrb
0x7
%
r8d
%
xmm0
%
xmm1
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
197
188
95
194
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
208
/
/
vcvtps2dq
%
ymm0
%
ymm2
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
210
1
/
/
vextracti128
0x1
%
ymm2
%
xmm2
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm0
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
4
/
/
vpinsrb
0x4
%
r10d
%
xmm0
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
5
/
/
vpinsrb
0x5
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
6
/
/
vpinsrb
0x6
%
r9d
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
195
121
32
208
7
/
/
vpinsrb
0x7
%
r8d
%
xmm0
%
xmm2
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
197
188
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
216
/
/
vmovd
%
eax
%
xmm3
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
97
32
216
1
/
/
vpinsrb
0x1
%
eax
%
xmm3
%
xmm3
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
97
32
216
2
/
/
vpinsrb
0x2
%
eax
%
xmm3
%
xmm3
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
227
97
32
216
3
/
/
vpinsrb
0x3
%
eax
%
xmm3
%
xmm3
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
97
32
216
4
/
/
vpinsrb
0x4
%
eax
%
xmm3
%
xmm3
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
97
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm3
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
121
32
216
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
194
125
49
194
/
/
vpmovzxbd
%
xmm10
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
109
153
3
0
/
/
vbroadcastss
0x3996d
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
226
125
49
201
/
/
vpmovzxbd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
226
125
49
210
/
/
vpmovzxbd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
227
97
32
216
7
/
/
vpinsrb
0x7
%
eax
%
xmm3
%
xmm3
.
byte
196
226
125
49
219
/
/
vpmovzxbd
%
xmm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_byte_tables_rgb_skx
.
globl
_sk_byte_tables_rgb_skx
FUNCTION
(
_sk_byte_tables_rgb_skx
)
_sk_byte_tables_rgb_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8d
.
byte
65
131
192
255
/
/
add
0xffffffff
%
r8d
.
byte
98
82
125
40
124
192
/
/
vpbroadcastd
%
r8d
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
52
95
208
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm10
.
byte
196
98
125
24
29
149
152
3
0
/
/
vbroadcastss
0x39895
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
44
93
211
/
/
vminps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
44
89
208
/
/
vmulps
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
125
91
210
/
/
vcvtps2dq
%
ymm10
%
ymm10
.
byte
196
65
249
126
208
/
/
vmovq
%
xmm10
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
193
121
110
193
/
/
vmovd
%
r9d
%
xmm0
.
byte
196
67
249
22
209
1
/
/
vpextrq
0x1
%
xmm10
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
121
32
224
1
/
/
vpinsrb
0x1
%
r8d
%
xmm0
%
xmm12
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
99
125
57
208
1
/
/
vextracti128
0x1
%
ymm10
%
xmm0
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
25
32
208
2
/
/
vpinsrb
0x2
%
r8d
%
xmm12
%
xmm10
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
3
/
/
vpinsrb
0x3
%
r9d
%
xmm10
%
xmm10
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
4
/
/
vpinsrb
0x4
%
r9d
%
xmm10
%
xmm10
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
41
32
192
5
/
/
vpinsrb
0x5
%
r8d
%
xmm10
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
121
32
192
6
/
/
vpinsrb
0x6
%
r8d
%
xmm0
%
xmm0
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
121
32
209
7
/
/
vpinsrb
0x7
%
r9d
%
xmm0
%
xmm10
.
byte
197
180
95
193
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
253
91
200
/
/
vcvtps2dq
%
ymm0
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm0
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
4
/
/
vpinsrb
0x4
%
r10d
%
xmm0
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
5
/
/
vpinsrb
0x5
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
6
/
/
vpinsrb
0x6
%
r9d
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
195
121
32
200
7
/
/
vpinsrb
0x7
%
r8d
%
xmm0
%
xmm1
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
180
95
194
/
/
vmaxps
%
ymm2
%
ymm9
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
105
32
208
1
/
/
vpinsrb
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
2
/
/
vpinsrb
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
227
105
32
208
3
/
/
vpinsrb
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
4
/
/
vpinsrb
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
105
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm2
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
121
32
208
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
194
125
49
194
/
/
vpmovzxbd
%
xmm10
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
25
151
3
0
/
/
vbroadcastss
0x39719
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
226
125
49
201
/
/
vpmovzxbd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
227
105
32
208
7
/
/
vpinsrb
0x7
%
eax
%
xmm2
%
xmm2
.
byte
196
226
125
49
210
/
/
vpmovzxbd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_r_skx
.
globl
_sk_table_r_skx
FUNCTION
(
_sk_table_r_skx
)
_sk_table_r_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
131
192
255
/
/
add
0xffffffff
%
eax
.
byte
98
114
125
40
124
192
/
/
vpbroadcastd
%
eax
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
98
241
124
56
93
5
77
150
3
0
/
/
vminps
0x3964d
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
128
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_g_skx
.
globl
_sk_table_g_skx
FUNCTION
(
_sk_table_g_skx
)
_sk_table_g_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
131
192
255
/
/
add
0xffffffff
%
eax
.
byte
98
114
125
40
124
192
/
/
vpbroadcastd
%
eax
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
98
241
116
56
93
13
8
150
3
0
/
/
vminps
0x39608
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
253
91
201
/
/
vcvtps2dq
%
ymm1
%
ymm1
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
136
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm1
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
193
/
/
vmovaps
%
ymm8
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_b_skx
.
globl
_sk_table_b_skx
FUNCTION
(
_sk_table_b_skx
)
_sk_table_b_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
131
192
255
/
/
add
0xffffffff
%
eax
.
byte
98
114
125
40
124
192
/
/
vpbroadcastd
%
eax
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm2
.
byte
98
241
108
56
93
21
195
149
3
0
/
/
vminps
0x395c3
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
253
91
210
/
/
vcvtps2dq
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
144
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm2
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
194
/
/
vmovaps
%
ymm8
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_a_skx
.
globl
_sk_table_a_skx
FUNCTION
(
_sk_table_a_skx
)
_sk_table_a_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
131
192
255
/
/
add
0xffffffff
%
eax
.
byte
98
114
125
40
124
192
/
/
vpbroadcastd
%
eax
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
219
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm3
.
byte
98
241
100
56
93
29
126
149
3
0
/
/
vminps
0x3957e
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
100
89
217
/
/
vmulps
%
ymm9
%
ymm3
%
ymm3
.
byte
197
253
91
219
/
/
vcvtps2dq
%
ymm3
%
ymm3
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
152
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm3
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
195
/
/
vmovaps
%
ymm8
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_r_skx
.
globl
_sk_parametric_r_skx
FUNCTION
(
_sk_parametric_r_skx
)
_sk_parametric_r_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
241
124
56
194
64
4
2
/
/
vcmpleps
0x10
(
%
rax
)
{
1to8
}
%
ymm0
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
98
114
125
56
168
72
6
/
/
vfmadd213ps
0x18
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
98
114
125
56
168
80
2
/
/
vfmadd213ps
0x8
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm10
.
byte
196
226
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm0
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
98
209
44
40
194
195
0
/
/
vcmpeqps
%
ymm11
%
ymm10
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
196
65
124
91
234
/
/
vcvtdq2ps
%
ymm10
%
ymm13
.
byte
196
98
125
24
53
171
149
3
0
/
/
vbroadcastss
0x395ab
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
98
114
21
56
168
53
169
149
3
0
/
/
vfmadd213ps
0x395a9
(
%
rip
)
{
1to8
}
%
ymm13
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
98
113
44
56
84
21
155
149
3
0
/
/
vandps
0x3959b
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
98
113
44
56
86
21
241
148
3
0
/
/
vorps
0x394f1
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
98
114
45
56
188
53
143
149
3
0
/
/
vfnmadd231ps
0x3958f
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
98
113
44
56
88
21
137
149
3
0
/
/
vaddps
0x39589
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
98
125
24
45
132
149
3
0
/
/
vbroadcastss
0x39584
(
%
rip
)
%
ymm13
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
12
92
210
/
/
vsubps
%
ymm10
%
ymm14
%
ymm10
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
99
125
8
208
1
/
/
vroundps
0x1
%
ymm0
%
ymm10
.
byte
196
65
124
92
210
/
/
vsubps
%
ymm10
%
ymm0
%
ymm10
.
byte
98
241
124
56
88
5
100
149
3
0
/
/
vaddps
0x39564
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
98
242
45
56
188
5
94
149
3
0
/
/
vfnmadd231ps
0x3955e
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm0
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
98
125
24
45
89
149
3
0
/
/
vbroadcastss
0x39559
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
210
/
/
vsubps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
98
125
24
45
79
149
3
0
/
/
vbroadcastss
0x3954f
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
124
88
194
/
/
vaddps
%
ymm10
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
63
149
3
0
/
/
vmulps
0x3953f
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
195
125
74
195
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm0
%
ymm0
.
byte
196
98
125
24
80
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm10
.
byte
196
193
124
88
194
/
/
vaddps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
195
125
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
124
95
195
/
/
vmaxps
%
ymm11
%
ymm0
%
ymm0
.
byte
98
241
124
56
93
5
85
148
3
0
/
/
vminps
0x39455
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_g_skx
.
globl
_sk_parametric_g_skx
FUNCTION
(
_sk_parametric_g_skx
)
_sk_parametric_g_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
241
116
56
194
64
4
2
/
/
vcmpleps
0x10
(
%
rax
)
{
1to8
}
%
ymm1
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
98
114
117
56
168
72
6
/
/
vfmadd213ps
0x18
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
98
114
117
56
168
80
2
/
/
vfmadd213ps
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm10
.
byte
196
226
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm1
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
98
209
44
40
194
195
0
/
/
vcmpeqps
%
ymm11
%
ymm10
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
196
65
124
91
234
/
/
vcvtdq2ps
%
ymm10
%
ymm13
.
byte
196
98
125
24
53
154
148
3
0
/
/
vbroadcastss
0x3949a
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
98
114
21
56
168
53
152
148
3
0
/
/
vfmadd213ps
0x39498
(
%
rip
)
{
1to8
}
%
ymm13
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
98
113
44
56
84
21
138
148
3
0
/
/
vandps
0x3948a
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
98
113
44
56
86
21
224
147
3
0
/
/
vorps
0x393e0
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
98
114
45
56
188
53
126
148
3
0
/
/
vfnmadd231ps
0x3947e
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
98
113
44
56
88
21
120
148
3
0
/
/
vaddps
0x39478
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
98
125
24
45
115
148
3
0
/
/
vbroadcastss
0x39473
(
%
rip
)
%
ymm13
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
12
92
210
/
/
vsubps
%
ymm10
%
ymm14
%
ymm10
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
99
125
8
209
1
/
/
vroundps
0x1
%
ymm1
%
ymm10
.
byte
196
65
116
92
210
/
/
vsubps
%
ymm10
%
ymm1
%
ymm10
.
byte
98
241
116
56
88
13
83
148
3
0
/
/
vaddps
0x39453
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
98
242
45
56
188
13
77
148
3
0
/
/
vfnmadd231ps
0x3944d
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm1
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
98
125
24
45
72
148
3
0
/
/
vbroadcastss
0x39448
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
210
/
/
vsubps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
98
125
24
45
62
148
3
0
/
/
vbroadcastss
0x3943e
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
116
88
202
/
/
vaddps
%
ymm10
%
ymm1
%
ymm1
.
byte
98
241
116
56
89
13
46
148
3
0
/
/
vmulps
0x3942e
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
253
91
201
/
/
vcvtps2dq
%
ymm1
%
ymm1
.
byte
196
195
117
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm1
%
ymm1
.
byte
196
98
125
24
80
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm10
.
byte
196
193
116
88
202
/
/
vaddps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
195
117
74
201
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
116
95
203
/
/
vmaxps
%
ymm11
%
ymm1
%
ymm1
.
byte
98
241
116
56
93
13
68
147
3
0
/
/
vminps
0x39344
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_b_skx
.
globl
_sk_parametric_b_skx
FUNCTION
(
_sk_parametric_b_skx
)
_sk_parametric_b_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
241
108
56
194
64
4
2
/
/
vcmpleps
0x10
(
%
rax
)
{
1to8
}
%
ymm2
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
98
114
109
56
168
72
6
/
/
vfmadd213ps
0x18
(
%
rax
)
{
1to8
}
%
ymm2
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
98
114
109
56
168
80
2
/
/
vfmadd213ps
0x8
(
%
rax
)
{
1to8
}
%
ymm2
%
ymm10
.
byte
196
226
125
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm2
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
98
209
44
40
194
195
0
/
/
vcmpeqps
%
ymm11
%
ymm10
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
196
65
124
91
234
/
/
vcvtdq2ps
%
ymm10
%
ymm13
.
byte
196
98
125
24
53
137
147
3
0
/
/
vbroadcastss
0x39389
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
98
114
21
56
168
53
135
147
3
0
/
/
vfmadd213ps
0x39387
(
%
rip
)
{
1to8
}
%
ymm13
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
98
113
44
56
84
21
121
147
3
0
/
/
vandps
0x39379
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
98
113
44
56
86
21
207
146
3
0
/
/
vorps
0x392cf
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
98
114
45
56
188
53
109
147
3
0
/
/
vfnmadd231ps
0x3936d
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
98
113
44
56
88
21
103
147
3
0
/
/
vaddps
0x39367
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
98
125
24
45
98
147
3
0
/
/
vbroadcastss
0x39362
(
%
rip
)
%
ymm13
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
12
92
210
/
/
vsubps
%
ymm10
%
ymm14
%
ymm10
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
99
125
8
210
1
/
/
vroundps
0x1
%
ymm2
%
ymm10
.
byte
196
65
108
92
210
/
/
vsubps
%
ymm10
%
ymm2
%
ymm10
.
byte
98
241
108
56
88
21
66
147
3
0
/
/
vaddps
0x39342
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
98
242
45
56
188
21
60
147
3
0
/
/
vfnmadd231ps
0x3933c
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm2
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
98
125
24
45
55
147
3
0
/
/
vbroadcastss
0x39337
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
210
/
/
vsubps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
98
125
24
45
45
147
3
0
/
/
vbroadcastss
0x3932d
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
108
88
210
/
/
vaddps
%
ymm10
%
ymm2
%
ymm2
.
byte
98
241
108
56
89
21
29
147
3
0
/
/
vmulps
0x3931d
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
253
91
210
/
/
vcvtps2dq
%
ymm2
%
ymm2
.
byte
196
195
109
74
211
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm2
%
ymm2
.
byte
196
98
125
24
80
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm10
.
byte
196
193
108
88
210
/
/
vaddps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
195
109
74
209
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
108
95
211
/
/
vmaxps
%
ymm11
%
ymm2
%
ymm2
.
byte
98
241
108
56
93
21
51
146
3
0
/
/
vminps
0x39233
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_a_skx
.
globl
_sk_parametric_a_skx
FUNCTION
(
_sk_parametric_a_skx
)
_sk_parametric_a_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
241
100
56
194
64
4
2
/
/
vcmpleps
0x10
(
%
rax
)
{
1to8
}
%
ymm3
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
98
114
101
56
168
72
6
/
/
vfmadd213ps
0x18
(
%
rax
)
{
1to8
}
%
ymm3
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
98
114
101
56
168
80
2
/
/
vfmadd213ps
0x8
(
%
rax
)
{
1to8
}
%
ymm3
%
ymm10
.
byte
196
226
125
24
24
/
/
vbroadcastss
(
%
rax
)
%
ymm3
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
98
209
44
40
194
195
0
/
/
vcmpeqps
%
ymm11
%
ymm10
%
k0
.
byte
98
114
126
40
56
224
/
/
vpmovm2d
%
k0
%
ymm12
.
byte
196
65
124
91
234
/
/
vcvtdq2ps
%
ymm10
%
ymm13
.
byte
196
98
125
24
53
120
146
3
0
/
/
vbroadcastss
0x39278
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
98
114
21
56
168
53
118
146
3
0
/
/
vfmadd213ps
0x39276
(
%
rip
)
{
1to8
}
%
ymm13
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
98
113
44
56
84
21
104
146
3
0
/
/
vandps
0x39268
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
98
113
44
56
86
21
190
145
3
0
/
/
vorps
0x391be
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
98
114
45
56
188
53
92
146
3
0
/
/
vfnmadd231ps
0x3925c
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
98
113
44
56
88
21
86
146
3
0
/
/
vaddps
0x39256
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm10
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
98
125
24
45
81
146
3
0
/
/
vbroadcastss
0x39251
(
%
rip
)
%
ymm13
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
12
92
210
/
/
vsubps
%
ymm10
%
ymm14
%
ymm10
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
99
125
8
211
1
/
/
vroundps
0x1
%
ymm3
%
ymm10
.
byte
196
65
100
92
210
/
/
vsubps
%
ymm10
%
ymm3
%
ymm10
.
byte
98
241
100
56
88
29
49
146
3
0
/
/
vaddps
0x39231
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
98
242
45
56
188
29
43
146
3
0
/
/
vfnmadd231ps
0x3922b
(
%
rip
)
{
1to8
}
%
ymm10
%
ymm3
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
98
125
24
45
38
146
3
0
/
/
vbroadcastss
0x39226
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
210
/
/
vsubps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
98
125
24
45
28
146
3
0
/
/
vbroadcastss
0x3921c
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
100
88
218
/
/
vaddps
%
ymm10
%
ymm3
%
ymm3
.
byte
98
241
100
56
89
29
12
146
3
0
/
/
vmulps
0x3920c
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
253
91
219
/
/
vcvtps2dq
%
ymm3
%
ymm3
.
byte
196
195
101
74
219
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm3
%
ymm3
.
byte
196
98
125
24
80
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm10
.
byte
196
193
100
88
218
/
/
vaddps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
195
101
74
217
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm3
%
ymm3
.
byte
196
193
100
95
219
/
/
vmaxps
%
ymm11
%
ymm3
%
ymm3
.
byte
98
241
100
56
93
29
34
145
3
0
/
/
vminps
0x39122
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_skx
.
globl
_sk_gamma_skx
FUNCTION
(
_sk_gamma_skx
)
_sk_gamma_skx
:
.
byte
98
97
124
40
40
199
/
/
vmovaps
%
ymm7
%
ymm24
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
226
125
40
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm18
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
98
241
124
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm0
%
k0
.
byte
197
124
91
208
/
/
vcvtdq2ps
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
144
145
3
0
/
/
vbroadcastss
0x39190
(
%
rip
)
%
ymm11
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
98
125
24
37
139
145
3
0
/
/
vbroadcastss
0x3918b
(
%
rip
)
%
ymm12
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
193
124
84
196
/
/
vandps
%
ymm12
%
ymm0
%
ymm0
.
byte
196
98
125
24
45
221
144
3
0
/
/
vbroadcastss
0x390dd
(
%
rip
)
%
ymm13
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
124
86
197
/
/
vorps
%
ymm13
%
ymm0
%
ymm0
.
byte
196
98
125
24
53
115
145
3
0
/
/
vbroadcastss
0x39173
(
%
rip
)
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
37
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm10
.
byte
196
98
125
24
61
105
145
3
0
/
/
vbroadcastss
0x39169
(
%
rip
)
%
ymm15
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
66
125
188
215
/
/
vfnmadd231ps
%
ymm15
%
ymm0
%
ymm10
.
byte
98
226
125
40
24
5
94
145
3
0
/
/
vbroadcastss
0x3915e
(
%
rip
)
%
ymm16
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
98
177
124
40
88
192
/
/
vaddps
%
ymm16
%
ymm0
%
ymm0
.
byte
98
226
125
40
24
13
82
145
3
0
/
/
vbroadcastss
0x39152
(
%
rip
)
%
ymm17
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
98
241
116
32
94
192
/
/
vdivps
%
ymm0
%
ymm17
%
ymm0
.
byte
197
172
92
192
/
/
vsubps
%
ymm0
%
ymm10
%
ymm0
.
byte
98
177
124
40
89
194
/
/
vmulps
%
ymm18
%
ymm0
%
ymm0
.
byte
196
99
125
8
208
1
/
/
vroundps
0x1
%
ymm0
%
ymm10
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
124
92
210
/
/
vsubps
%
ymm10
%
ymm0
%
ymm10
.
byte
98
226
125
40
24
29
43
145
3
0
/
/
vbroadcastss
0x3912b
(
%
rip
)
%
ymm19
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
98
177
124
40
88
195
/
/
vaddps
%
ymm19
%
ymm0
%
ymm0
.
byte
98
226
125
40
24
37
31
145
3
0
/
/
vbroadcastss
0x3911f
(
%
rip
)
%
ymm20
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
98
178
45
40
188
196
/
/
vfnmadd231ps
%
ymm20
%
ymm10
%
ymm0
.
byte
98
226
125
40
24
45
19
145
3
0
/
/
vbroadcastss
0x39113
(
%
rip
)
%
ymm21
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
98
81
84
32
92
210
/
/
vsubps
%
ymm10
%
ymm21
%
ymm10
.
byte
98
226
125
40
24
53
7
145
3
0
/
/
vbroadcastss
0x39107
(
%
rip
)
%
ymm22
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
98
81
76
32
94
210
/
/
vdivps
%
ymm10
%
ymm22
%
ymm10
.
byte
196
193
124
88
194
/
/
vaddps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
247
144
3
0
/
/
vbroadcastss
0x390f7
(
%
rip
)
%
ymm10
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
74
199
144
/
/
vblendvps
%
ymm9
%
ymm7
%
ymm0
%
ymm0
.
byte
98
241
116
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm1
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
98
225
124
40
91
249
/
/
vcvtdq2ps
%
ymm1
%
ymm23
.
byte
196
193
116
84
204
/
/
vandps
%
ymm12
%
ymm1
%
ymm1
.
byte
196
193
116
86
205
/
/
vorps
%
ymm13
%
ymm1
%
ymm1
.
byte
98
194
37
40
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm23
.
byte
98
194
117
40
188
255
/
/
vfnmadd231ps
%
ymm15
%
ymm1
%
ymm23
.
byte
98
177
116
40
88
200
/
/
vaddps
%
ymm16
%
ymm1
%
ymm1
.
byte
98
241
116
32
94
201
/
/
vdivps
%
ymm1
%
ymm17
%
ymm1
.
byte
98
241
68
32
92
201
/
/
vsubps
%
ymm1
%
ymm23
%
ymm1
.
byte
98
177
116
40
89
202
/
/
vmulps
%
ymm18
%
ymm1
%
ymm1
.
byte
196
99
125
8
193
1
/
/
vroundps
0x1
%
ymm1
%
ymm8
.
byte
196
65
116
92
192
/
/
vsubps
%
ymm8
%
ymm1
%
ymm8
.
byte
98
177
116
40
88
203
/
/
vaddps
%
ymm19
%
ymm1
%
ymm1
.
byte
98
178
61
40
188
204
/
/
vfnmadd231ps
%
ymm20
%
ymm8
%
ymm1
.
byte
98
81
84
32
92
192
/
/
vsubps
%
ymm8
%
ymm21
%
ymm8
.
byte
98
81
76
32
94
192
/
/
vdivps
%
ymm8
%
ymm22
%
ymm8
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
253
91
201
/
/
vcvtps2dq
%
ymm1
%
ymm1
.
byte
196
227
117
74
207
144
/
/
vblendvps
%
ymm9
%
ymm7
%
ymm1
%
ymm1
.
byte
98
241
108
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm2
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
197
124
91
202
/
/
vcvtdq2ps
%
ymm2
%
ymm9
.
byte
196
193
108
84
212
/
/
vandps
%
ymm12
%
ymm2
%
ymm2
.
byte
196
193
108
86
213
/
/
vorps
%
ymm13
%
ymm2
%
ymm2
.
byte
196
66
37
168
206
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm9
.
byte
196
66
109
188
207
/
/
vfnmadd231ps
%
ymm15
%
ymm2
%
ymm9
.
byte
98
177
108
40
88
208
/
/
vaddps
%
ymm16
%
ymm2
%
ymm2
.
byte
98
241
116
32
94
210
/
/
vdivps
%
ymm2
%
ymm17
%
ymm2
.
byte
197
180
92
210
/
/
vsubps
%
ymm2
%
ymm9
%
ymm2
.
byte
98
177
108
40
89
210
/
/
vmulps
%
ymm18
%
ymm2
%
ymm2
.
byte
196
99
125
8
202
1
/
/
vroundps
0x1
%
ymm2
%
ymm9
.
byte
196
65
108
92
201
/
/
vsubps
%
ymm9
%
ymm2
%
ymm9
.
byte
98
177
108
40
88
211
/
/
vaddps
%
ymm19
%
ymm2
%
ymm2
.
byte
98
178
53
40
188
212
/
/
vfnmadd231ps
%
ymm20
%
ymm9
%
ymm2
.
byte
98
81
84
32
92
201
/
/
vsubps
%
ymm9
%
ymm21
%
ymm9
.
byte
98
81
76
32
94
201
/
/
vdivps
%
ymm9
%
ymm22
%
ymm9
.
byte
196
193
108
88
209
/
/
vaddps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
197
253
91
210
/
/
vcvtps2dq
%
ymm2
%
ymm2
.
byte
196
227
109
74
215
128
/
/
vblendvps
%
ymm8
%
ymm7
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
145
124
40
40
248
/
/
vmovaps
%
ymm24
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_dst_skx
.
globl
_sk_gamma_dst_skx
FUNCTION
(
_sk_gamma_dst_skx
)
_sk_gamma_dst_skx
:
.
byte
98
97
124
40
40
199
/
/
vmovaps
%
ymm7
%
ymm24
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
226
125
40
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm18
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
98
241
92
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm4
%
k0
.
byte
197
124
91
212
/
/
vcvtdq2ps
%
ymm4
%
ymm10
.
byte
196
98
125
24
29
166
143
3
0
/
/
vbroadcastss
0x38fa6
(
%
rip
)
%
ymm11
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
98
125
24
37
161
143
3
0
/
/
vbroadcastss
0x38fa1
(
%
rip
)
%
ymm12
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
193
92
84
228
/
/
vandps
%
ymm12
%
ymm4
%
ymm4
.
byte
196
98
125
24
45
243
142
3
0
/
/
vbroadcastss
0x38ef3
(
%
rip
)
%
ymm13
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
92
86
229
/
/
vorps
%
ymm13
%
ymm4
%
ymm4
.
byte
196
98
125
24
53
137
143
3
0
/
/
vbroadcastss
0x38f89
(
%
rip
)
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
37
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm10
.
byte
196
98
125
24
61
127
143
3
0
/
/
vbroadcastss
0x38f7f
(
%
rip
)
%
ymm15
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
66
93
188
215
/
/
vfnmadd231ps
%
ymm15
%
ymm4
%
ymm10
.
byte
98
226
125
40
24
5
116
143
3
0
/
/
vbroadcastss
0x38f74
(
%
rip
)
%
ymm16
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
98
177
92
40
88
224
/
/
vaddps
%
ymm16
%
ymm4
%
ymm4
.
byte
98
226
125
40
24
13
104
143
3
0
/
/
vbroadcastss
0x38f68
(
%
rip
)
%
ymm17
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
98
241
116
32
94
228
/
/
vdivps
%
ymm4
%
ymm17
%
ymm4
.
byte
197
172
92
228
/
/
vsubps
%
ymm4
%
ymm10
%
ymm4
.
byte
98
177
92
40
89
226
/
/
vmulps
%
ymm18
%
ymm4
%
ymm4
.
byte
196
99
125
8
212
1
/
/
vroundps
0x1
%
ymm4
%
ymm10
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
92
92
210
/
/
vsubps
%
ymm10
%
ymm4
%
ymm10
.
byte
98
226
125
40
24
29
65
143
3
0
/
/
vbroadcastss
0x38f41
(
%
rip
)
%
ymm19
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
98
177
92
40
88
227
/
/
vaddps
%
ymm19
%
ymm4
%
ymm4
.
byte
98
226
125
40
24
37
53
143
3
0
/
/
vbroadcastss
0x38f35
(
%
rip
)
%
ymm20
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
98
178
45
40
188
228
/
/
vfnmadd231ps
%
ymm20
%
ymm10
%
ymm4
.
byte
98
226
125
40
24
45
41
143
3
0
/
/
vbroadcastss
0x38f29
(
%
rip
)
%
ymm21
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
98
81
84
32
92
210
/
/
vsubps
%
ymm10
%
ymm21
%
ymm10
.
byte
98
226
125
40
24
53
29
143
3
0
/
/
vbroadcastss
0x38f1d
(
%
rip
)
%
ymm22
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
98
81
76
32
94
210
/
/
vdivps
%
ymm10
%
ymm22
%
ymm10
.
byte
196
193
92
88
226
/
/
vaddps
%
ymm10
%
ymm4
%
ymm4
.
byte
196
98
125
24
21
13
143
3
0
/
/
vbroadcastss
0x38f0d
(
%
rip
)
%
ymm10
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
193
92
89
226
/
/
vmulps
%
ymm10
%
ymm4
%
ymm4
.
byte
197
253
91
228
/
/
vcvtps2dq
%
ymm4
%
ymm4
.
byte
196
227
93
74
231
144
/
/
vblendvps
%
ymm9
%
ymm7
%
ymm4
%
ymm4
.
byte
98
241
84
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm5
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
98
225
124
40
91
253
/
/
vcvtdq2ps
%
ymm5
%
ymm23
.
byte
196
193
84
84
236
/
/
vandps
%
ymm12
%
ymm5
%
ymm5
.
byte
196
193
84
86
237
/
/
vorps
%
ymm13
%
ymm5
%
ymm5
.
byte
98
194
37
40
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm23
.
byte
98
194
85
40
188
255
/
/
vfnmadd231ps
%
ymm15
%
ymm5
%
ymm23
.
byte
98
177
84
40
88
232
/
/
vaddps
%
ymm16
%
ymm5
%
ymm5
.
byte
98
241
116
32
94
237
/
/
vdivps
%
ymm5
%
ymm17
%
ymm5
.
byte
98
241
68
32
92
237
/
/
vsubps
%
ymm5
%
ymm23
%
ymm5
.
byte
98
177
84
40
89
234
/
/
vmulps
%
ymm18
%
ymm5
%
ymm5
.
byte
196
99
125
8
197
1
/
/
vroundps
0x1
%
ymm5
%
ymm8
.
byte
196
65
84
92
192
/
/
vsubps
%
ymm8
%
ymm5
%
ymm8
.
byte
98
177
84
40
88
235
/
/
vaddps
%
ymm19
%
ymm5
%
ymm5
.
byte
98
178
61
40
188
236
/
/
vfnmadd231ps
%
ymm20
%
ymm8
%
ymm5
.
byte
98
81
84
32
92
192
/
/
vsubps
%
ymm8
%
ymm21
%
ymm8
.
byte
98
81
76
32
94
192
/
/
vdivps
%
ymm8
%
ymm22
%
ymm8
.
byte
196
193
84
88
232
/
/
vaddps
%
ymm8
%
ymm5
%
ymm5
.
byte
196
193
84
89
234
/
/
vmulps
%
ymm10
%
ymm5
%
ymm5
.
byte
197
253
91
237
/
/
vcvtps2dq
%
ymm5
%
ymm5
.
byte
196
227
85
74
239
144
/
/
vblendvps
%
ymm9
%
ymm7
%
ymm5
%
ymm5
.
byte
98
241
76
40
194
199
0
/
/
vcmpeqps
%
ymm7
%
ymm6
%
k0
.
byte
98
114
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm8
.
byte
197
124
91
206
/
/
vcvtdq2ps
%
ymm6
%
ymm9
.
byte
196
193
76
84
244
/
/
vandps
%
ymm12
%
ymm6
%
ymm6
.
byte
196
193
76
86
245
/
/
vorps
%
ymm13
%
ymm6
%
ymm6
.
byte
196
66
37
168
206
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm9
.
byte
196
66
77
188
207
/
/
vfnmadd231ps
%
ymm15
%
ymm6
%
ymm9
.
byte
98
177
76
40
88
240
/
/
vaddps
%
ymm16
%
ymm6
%
ymm6
.
byte
98
241
116
32
94
246
/
/
vdivps
%
ymm6
%
ymm17
%
ymm6
.
byte
197
180
92
246
/
/
vsubps
%
ymm6
%
ymm9
%
ymm6
.
byte
98
177
76
40
89
242
/
/
vmulps
%
ymm18
%
ymm6
%
ymm6
.
byte
196
99
125
8
206
1
/
/
vroundps
0x1
%
ymm6
%
ymm9
.
byte
196
65
76
92
201
/
/
vsubps
%
ymm9
%
ymm6
%
ymm9
.
byte
98
177
76
40
88
243
/
/
vaddps
%
ymm19
%
ymm6
%
ymm6
.
byte
98
178
53
40
188
244
/
/
vfnmadd231ps
%
ymm20
%
ymm9
%
ymm6
.
byte
98
81
84
32
92
201
/
/
vsubps
%
ymm9
%
ymm21
%
ymm9
.
byte
98
81
76
32
94
201
/
/
vdivps
%
ymm9
%
ymm22
%
ymm9
.
byte
196
193
76
88
241
/
/
vaddps
%
ymm9
%
ymm6
%
ymm6
.
byte
196
193
76
89
242
/
/
vmulps
%
ymm10
%
ymm6
%
ymm6
.
byte
197
253
91
246
/
/
vcvtps2dq
%
ymm6
%
ymm6
.
byte
196
227
77
74
247
128
/
/
vblendvps
%
ymm8
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
145
124
40
40
248
/
/
vmovaps
%
ymm24
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lab_to_xyz_skx
.
globl
_sk_lab_to_xyz_skx
FUNCTION
(
_sk_lab_to_xyz_skx
)
_sk_lab_to_xyz_skx
:
.
byte
196
98
125
24
5
5
142
3
0
/
/
vbroadcastss
0x38e05
(
%
rip
)
%
ymm8
#
3c5c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x374
>
.
byte
196
98
125
24
13
100
141
3
0
/
/
vbroadcastss
0x38d64
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
98
125
24
21
247
141
3
0
/
/
vbroadcastss
0x38df7
(
%
rip
)
%
ymm10
#
3c5c4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x378
>
.
byte
196
194
53
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm9
%
ymm1
.
byte
196
194
53
168
210
/
/
vfmadd213ps
%
ymm10
%
ymm9
%
ymm2
.
byte
98
114
125
56
168
5
231
141
3
0
/
/
vfmadd213ps
0x38de7
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm8
#
3c5c8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x37c
>
.
byte
98
241
60
56
89
5
225
141
3
0
/
/
vmulps
0x38de1
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm0
#
3c5cc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x380
>
.
byte
98
242
125
56
152
13
219
141
3
0
/
/
vfmadd132ps
0x38ddb
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm1
#
3c5d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x384
>
.
byte
98
242
125
56
156
21
213
141
3
0
/
/
vfnmadd132ps
0x38dd5
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm2
#
3c5d4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x388
>
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
65
116
89
192
/
/
vmulps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
98
125
24
13
199
141
3
0
/
/
vbroadcastss
0x38dc7
(
%
rip
)
%
ymm9
#
3c5d8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x38c
>
.
byte
98
209
52
40
194
192
1
/
/
vcmpltps
%
ymm8
%
ymm9
%
k0
.
byte
98
114
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm10
.
byte
196
98
125
24
29
181
141
3
0
/
/
vbroadcastss
0x38db5
(
%
rip
)
%
ymm11
#
3c5dc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x390
>
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
196
98
125
24
37
171
141
3
0
/
/
vbroadcastss
0x38dab
(
%
rip
)
%
ymm12
#
3c5e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x394
>
.
byte
196
193
116
89
204
/
/
vmulps
%
ymm12
%
ymm1
%
ymm1
.
byte
196
67
117
74
192
160
/
/
vblendvps
%
ymm10
%
ymm8
%
ymm1
%
ymm8
.
byte
197
252
89
200
/
/
vmulps
%
ymm0
%
ymm0
%
ymm1
.
byte
197
252
89
201
/
/
vmulps
%
ymm1
%
ymm0
%
ymm1
.
byte
98
241
52
40
194
193
1
/
/
vcmpltps
%
ymm1
%
ymm9
%
k0
.
byte
98
114
126
40
56
208
/
/
vpmovm2d
%
k0
%
ymm10
.
byte
196
193
124
88
195
/
/
vaddps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
196
/
/
vmulps
%
ymm12
%
ymm0
%
ymm0
.
byte
196
227
125
74
201
160
/
/
vblendvps
%
ymm10
%
ymm1
%
ymm0
%
ymm1
.
byte
197
236
89
194
/
/
vmulps
%
ymm2
%
ymm2
%
ymm0
.
byte
197
236
89
192
/
/
vmulps
%
ymm0
%
ymm2
%
ymm0
.
byte
98
241
52
40
194
192
1
/
/
vcmpltps
%
ymm0
%
ymm9
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
193
108
88
211
/
/
vaddps
%
ymm11
%
ymm2
%
ymm2
.
byte
196
193
108
89
212
/
/
vmulps
%
ymm12
%
ymm2
%
ymm2
.
byte
196
227
109
74
208
144
/
/
vblendvps
%
ymm9
%
ymm0
%
ymm2
%
ymm2
.
byte
98
241
60
56
89
5
80
141
3
0
/
/
vmulps
0x38d50
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm0
#
3c5e4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x398
>
.
byte
98
241
108
56
89
21
74
141
3
0
/
/
vmulps
0x38d4a
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c5e8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x39c
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_skx
.
globl
_sk_load_a8_skx
FUNCTION
(
_sk_load_a8_skx
)
_sk_load_a8_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
49
/
/
jne
38e5
<
_sk_load_a8_skx
+
0x43
>
.
byte
196
194
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
197
249
219
5
238
149
3
0
/
/
vpand
0x395ee
(
%
rip
)
%
xmm0
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
29
159
140
3
0
/
/
vmulps
0x38c9f
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm3
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
199
/
/
ja
38ba
<
_sk_load_a8_skx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
131
0
0
0
/
/
lea
0x83
(
%
rip
)
%
r9
#
3980
<
_sk_load_a8_skx
+
0xde
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
235
169
/
/
jmp
38ba
<
_sk_load_a8_skx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm0
%
xmm0
.
byte
196
194
121
50
12
16
/
/
vpmovzxbq
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
235
130
/
/
jmp
38ba
<
_sk_load_a8_skx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
196
194
121
49
12
16
/
/
vpmovzxbd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
197
250
112
201
232
/
/
vpshufhw
0xe8
%
xmm1
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
233
61
255
255
255
/
/
jmpq
38ba
<
_sk_load_a8_skx
+
0x18
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
134
255
/
/
xchg
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
160
255
255
255
145
/
/
jmpq
*
-
0x6e000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
184
/
/
.
byte
0xb8
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_a8_dst_skx
.
globl
_sk_load_a8_dst_skx
FUNCTION
(
_sk_load_a8_dst_skx
)
_sk_load_a8_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
49
/
/
jne
39df
<
_sk_load_a8_dst_skx
+
0x43
>
.
byte
196
194
121
48
36
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
197
217
219
37
244
148
3
0
/
/
vpand
0x394f4
(
%
rip
)
%
xmm4
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
228
/
/
vpmovzxwd
%
xmm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
98
241
92
56
89
61
165
139
3
0
/
/
vmulps
0x38ba5
(
%
rip
)
{
1to8
}
%
ymm4
%
ymm7
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
199
/
/
ja
39b4
<
_sk_load_a8_dst_skx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
129
0
0
0
/
/
lea
0x81
(
%
rip
)
%
r9
#
3a78
<
_sk_load_a8_dst_skx
+
0xdc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
235
169
/
/
jmp
39b4
<
_sk_load_a8_dst_skx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
2
/
/
vpinsrw
0x2
%
eax
%
xmm4
%
xmm4
.
byte
196
194
121
50
44
16
/
/
vpmovzxbq
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
235
130
/
/
jmp
39b4
<
_sk_load_a8_dst_skx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
6
/
/
vpinsrw
0x6
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
5
/
/
vpinsrw
0x5
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
4
/
/
vpinsrw
0x4
%
eax
%
xmm4
%
xmm4
.
byte
196
194
121
49
44
16
/
/
vpmovzxbd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
197
250
112
237
232
/
/
vpshufhw
0xe8
%
xmm5
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
233
61
255
255
255
/
/
jmpq
39b4
<
_sk_load_a8_dst_skx
+
0x18
>
.
byte
144
/
/
nop
.
byte
136
255
/
/
mov
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
162
255
255
255
147
/
/
jmpq
*
-
0x6c000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
186
/
/
.
byte
0xba
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_a8_skx
.
globl
_sk_gather_a8_skx
FUNCTION
(
_sk_gather_a8_skx
)
_sk_gather_a8_skx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
195
121
32
194
1
/
/
vpinsrb
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
121
32
192
3
/
/
vpinsrb
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
eax
.
byte
196
227
121
32
192
4
/
/
vpinsrb
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
eax
.
byte
196
227
121
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
eax
.
byte
196
227
121
32
192
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
4
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
eax
.
byte
196
227
121
32
192
7
/
/
vpinsrb
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
49
192
/
/
vpmovzxbd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
29
242
137
3
0
/
/
vmulps
0x389f2
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm3
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_a8_skx
.
globl
_sk_store_a8_skx
FUNCTION
(
_sk_store_a8_skx
)
_sk_store_a8_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
98
113
60
56
93
5
67
137
3
0
/
/
vminps
0x38943
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
98
113
60
56
89
5
101
137
3
0
/
/
vmulps
0x38965
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
197
57
103
192
/
/
vpackuswb
%
xmm0
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
3be6
<
_sk_store_a8_skx
+
0x4d
>
.
byte
196
65
121
214
4
16
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
3be2
<
_sk_store_a8_skx
+
0x49
>
.
byte
196
66
121
48
192
/
/
vpmovzxbw
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
81
0
0
0
/
/
lea
0x51
(
%
rip
)
%
r9
#
3c50
<
_sk_store_a8_skx
+
0xb7
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
20
4
16
0
/
/
vpextrb
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
209
/
/
jmp
3be2
<
_sk_store_a8_skx
+
0x49
>
.
byte
196
67
121
20
68
16
2
4
/
/
vpextrb
0x4
%
xmm8
0x2
(
%
r8
%
rdx
1
)
.
byte
196
66
121
49
192
/
/
vpmovzxbd
%
xmm8
%
xmm8
.
byte
98
82
126
8
50
4
16
/
/
vpmovqb
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
187
/
/
jmp
3be2
<
_sk_store_a8_skx
+
0x49
>
.
byte
196
67
121
20
68
16
6
12
/
/
vpextrb
0xc
%
xmm8
0x6
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
5
10
/
/
vpextrb
0xa
%
xmm8
0x5
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
4
8
/
/
vpextrb
0x8
%
xmm8
0x4
(
%
r8
%
rdx
1
)
.
byte
196
66
121
48
192
/
/
vpmovzxbw
%
xmm8
%
xmm8
.
byte
98
82
126
8
49
4
16
/
/
vpmovdb
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
149
/
/
jmp
3be2
<
_sk_store_a8_skx
+
0x49
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
184
255
255
255
201
/
/
mov
0xc9ffffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
215
/
/
callq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_skx
.
globl
_sk_load_g8_skx
FUNCTION
(
_sk_load_g8_skx
)
_sk_load_g8_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
54
/
/
jne
3cb4
<
_sk_load_g8_skx
+
0x48
>
.
byte
196
194
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
197
249
219
5
36
146
3
0
/
/
vpand
0x39224
(
%
rip
)
%
xmm0
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
213
136
3
0
/
/
vmulps
0x388d5
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
82
136
3
0
/
/
vbroadcastss
0x38852
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
194
/
/
ja
3c84
<
_sk_load_g8_skx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
132
0
0
0
/
/
lea
0x84
(
%
rip
)
%
r9
#
3d50
<
_sk_load_g8_skx
+
0xe4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
235
164
/
/
jmp
3c84
<
_sk_load_g8_skx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm0
%
xmm0
.
byte
196
194
121
50
12
16
/
/
vpmovzxbq
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
122
255
255
255
/
/
jmpq
3c84
<
_sk_load_g8_skx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
196
194
121
49
12
16
/
/
vpmovzxbd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
197
250
112
201
232
/
/
vpshufhw
0xe8
%
xmm1
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
233
53
255
255
255
/
/
jmpq
3c84
<
_sk_load_g8_skx
+
0x18
>
.
byte
144
/
/
nop
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
159
255
255
255
144
/
/
lcall
*
-
0x6f000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
186
/
/
.
byte
0xba
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_dst_skx
.
globl
_sk_load_g8_dst_skx
FUNCTION
(
_sk_load_g8_dst_skx
)
_sk_load_g8_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
54
/
/
jne
3db4
<
_sk_load_g8_dst_skx
+
0x48
>
.
byte
196
194
121
48
36
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
197
217
219
37
36
145
3
0
/
/
vpand
0x39124
(
%
rip
)
%
xmm4
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
228
/
/
vpmovzxwd
%
xmm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
98
241
92
56
89
37
213
135
3
0
/
/
vmulps
0x387d5
(
%
rip
)
{
1to8
}
%
ymm4
%
ymm4
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
82
135
3
0
/
/
vbroadcastss
0x38752
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
236
/
/
vmovaps
%
ymm4
%
ymm5
.
byte
197
252
40
244
/
/
vmovaps
%
ymm4
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
194
/
/
ja
3d84
<
_sk_load_g8_dst_skx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
132
0
0
0
/
/
lea
0x84
(
%
rip
)
%
r9
#
3e50
<
_sk_load_g8_dst_skx
+
0xe4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
235
164
/
/
jmp
3d84
<
_sk_load_g8_dst_skx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
2
/
/
vpinsrw
0x2
%
eax
%
xmm4
%
xmm4
.
byte
196
194
121
50
44
16
/
/
vpmovzxbq
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
122
255
255
255
/
/
jmpq
3d84
<
_sk_load_g8_dst_skx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
6
/
/
vpinsrw
0x6
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
5
/
/
vpinsrw
0x5
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
4
/
/
vpinsrw
0x4
%
eax
%
xmm4
%
xmm4
.
byte
196
194
121
49
44
16
/
/
vpmovzxbd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
197
250
112
237
232
/
/
vpshufhw
0xe8
%
xmm5
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
233
53
255
255
255
/
/
jmpq
3d84
<
_sk_load_g8_dst_skx
+
0x18
>
.
byte
144
/
/
nop
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
159
255
255
255
144
/
/
lcall
*
-
0x6f000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
186
/
/
.
byte
0xba
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_g8_skx
.
globl
_sk_gather_g8_skx
FUNCTION
(
_sk_gather_g8_skx
)
_sk_gather_g8_skx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
195
121
32
194
1
/
/
vpinsrb
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
121
32
192
3
/
/
vpinsrb
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
eax
.
byte
196
227
121
32
192
4
/
/
vpinsrb
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
eax
.
byte
196
227
121
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
eax
.
byte
196
227
121
32
192
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
4
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
eax
.
byte
196
227
121
32
192
7
/
/
vpinsrb
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
49
192
/
/
vpmovzxbd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
26
134
3
0
/
/
vmulps
0x3861a
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
151
133
3
0
/
/
vbroadcastss
0x38597
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_skx
.
globl
_sk_load_565_skx
FUNCTION
(
_sk_load_565_skx
)
_sk_load_565_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
96
/
/
jne
3feb
<
_sk_load_565_skx
+
0x75
>
.
byte
196
193
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
196
226
125
51
208
/
/
vpmovzxwd
%
xmm0
%
ymm2
.
byte
98
241
109
56
219
5
216
133
3
0
/
/
vpandd
0x385d8
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm0
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
206
133
3
0
/
/
vmulps
0x385ce
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
98
241
109
56
219
13
200
133
3
0
/
/
vpandd
0x385c8
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm1
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
241
116
56
89
13
190
133
3
0
/
/
vmulps
0x385be
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
98
241
109
56
219
21
184
133
3
0
/
/
vpandd
0x385b8
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
98
241
108
56
89
21
174
133
3
0
/
/
vmulps
0x385ae
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
19
133
3
0
/
/
vbroadcastss
0x38513
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
152
/
/
ja
3f91
<
_sk_load_565_skx
+
0x1b
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
121
0
0
0
/
/
lea
0x79
(
%
rip
)
%
r9
#
407c
<
_sk_load_565_skx
+
0x106
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
119
255
255
255
/
/
jmpq
3f91
<
_sk_load_565_skx
+
0x1b
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
194
121
52
12
80
/
/
vpmovzxwq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
80
255
255
255
/
/
jmpq
3f91
<
_sk_load_565_skx
+
0x1b
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
194
121
51
12
80
/
/
vpmovzxwd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
197
250
112
201
232
/
/
vpshufhw
0xe8
%
xmm1
%
xmm1
.
byte
197
249
112
201
132
/
/
vpshufd
0x84
%
xmm1
%
xmm1
.
byte
197
241
109
192
/
/
vpunpckhqdq
%
xmm0
%
xmm1
%
xmm0
.
byte
233
22
255
255
255
/
/
jmpq
3f91
<
_sk_load_565_skx
+
0x1b
>
.
byte
144
/
/
nop
.
byte
144
/
/
nop
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
170
255
255
255
158
/
/
ljmp
*
-
0x61000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
225
/
/
jmpq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_565_dst_skx
.
globl
_sk_load_565_dst_skx
FUNCTION
(
_sk_load_565_dst_skx
)
_sk_load_565_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
96
/
/
jne
410d
<
_sk_load_565_dst_skx
+
0x75
>
.
byte
196
193
122
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
196
226
125
51
244
/
/
vpmovzxwd
%
xmm4
%
ymm6
.
byte
98
241
77
56
219
37
182
132
3
0
/
/
vpandd
0x384b6
(
%
rip
)
{
1to8
}
%
ymm6
%
ymm4
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
98
241
92
56
89
37
172
132
3
0
/
/
vmulps
0x384ac
(
%
rip
)
{
1to8
}
%
ymm4
%
ymm4
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
98
241
77
56
219
45
166
132
3
0
/
/
vpandd
0x384a6
(
%
rip
)
{
1to8
}
%
ymm6
%
ymm5
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
98
241
84
56
89
45
156
132
3
0
/
/
vmulps
0x3849c
(
%
rip
)
{
1to8
}
%
ymm5
%
ymm5
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
98
241
77
56
219
53
150
132
3
0
/
/
vpandd
0x38496
(
%
rip
)
{
1to8
}
%
ymm6
%
ymm6
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
98
241
76
56
89
53
140
132
3
0
/
/
vmulps
0x3848c
(
%
rip
)
{
1to8
}
%
ymm6
%
ymm6
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
241
131
3
0
/
/
vbroadcastss
0x383f1
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
152
/
/
ja
40b3
<
_sk_load_565_dst_skx
+
0x1b
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
123
0
0
0
/
/
lea
0x7b
(
%
rip
)
%
r9
#
41a0
<
_sk_load_565_dst_skx
+
0x108
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
119
255
255
255
/
/
jmpq
40b3
<
_sk_load_565_dst_skx
+
0x1b
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
194
121
52
44
80
/
/
vpmovzxwq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
80
255
255
255
/
/
jmpq
40b3
<
_sk_load_565_dst_skx
+
0x1b
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
194
121
51
44
80
/
/
vpmovzxwd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
197
250
112
237
232
/
/
vpshufhw
0xe8
%
xmm5
%
xmm5
.
byte
197
249
112
237
132
/
/
vpshufd
0x84
%
xmm5
%
xmm5
.
byte
197
209
109
228
/
/
vpunpckhqdq
%
xmm4
%
xmm5
%
xmm4
.
byte
233
22
255
255
255
/
/
jmpq
40b3
<
_sk_load_565_dst_skx
+
0x1b
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
142
255
/
/
mov
%
edi
%
?
.
byte
255
/
/
(
bad
)
.
byte
255
168
255
255
255
156
/
/
ljmp
*
-
0x63000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
215
/
/
callq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
207
/
/
dec
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_565_skx
.
globl
_sk_gather_565_skx
FUNCTION
(
_sk_gather_565_skx
)
_sk_gather_565_skx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
193
121
196
194
1
/
/
vpinsrw
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
196
193
121
196
193
2
/
/
vpinsrw
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
3
/
/
vpinsrw
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
eax
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
51
208
/
/
vpmovzxwd
%
xmm0
%
ymm2
.
byte
98
241
109
56
219
5
215
130
3
0
/
/
vpandd
0x382d7
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm0
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
205
130
3
0
/
/
vmulps
0x382cd
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
98
241
109
56
219
13
199
130
3
0
/
/
vpandd
0x382c7
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm1
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
241
116
56
89
13
189
130
3
0
/
/
vmulps
0x382bd
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
98
241
109
56
219
21
183
130
3
0
/
/
vpandd
0x382b7
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
98
241
108
56
89
21
173
130
3
0
/
/
vmulps
0x382ad
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
18
130
3
0
/
/
vbroadcastss
0x38212
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_565_skx
.
globl
_sk_store_565_skx
FUNCTION
(
_sk_store_565_skx
)
_sk_store_565_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
231
129
3
0
/
/
vbroadcastss
0x381e7
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
201
130
3
0
/
/
vbroadcastss
0x382c9
(
%
rip
)
%
ymm11
#
3c5ec
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a0
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
193
53
114
241
11
/
/
vpslld
0xb
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
98
113
28
56
89
37
170
130
3
0
/
/
vmulps
0x382aa
(
%
rip
)
{
1to8
}
%
ymm12
%
ymm12
#
3c5f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a4
>
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
5
/
/
vpslld
0x5
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
194
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
4388
<
_sk_store_565_skx
+
0x95
>
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
4384
<
_sk_store_565_skx
+
0x91
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
80
0
0
0
/
/
lea
0x50
(
%
rip
)
%
r9
#
43ec
<
_sk_store_565_skx
+
0xf9
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
214
/
/
jmp
4384
<
_sk_store_565_skx
+
0x91
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
66
121
52
192
/
/
vpmovzxwq
%
xmm8
%
xmm8
.
byte
98
82
126
8
52
4
80
/
/
vpmovqw
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
192
/
/
jmp
4384
<
_sk_store_565_skx
+
0x91
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
66
121
51
192
/
/
vpmovzxwd
%
xmm8
%
xmm8
.
byte
98
82
126
8
51
4
80
/
/
vpmovdw
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
154
/
/
jmp
4384
<
_sk_store_565_skx
+
0x91
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
185
255
255
255
202
/
/
mov
0xcaffffff
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
240
/
/
push
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
224
/
/
callq
ffffffffe1004400
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffe0fc81b4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_skx
.
globl
_sk_load_4444_skx
FUNCTION
(
_sk_load_4444_skx
)
_sk_load_4444_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
111
/
/
jne
448c
<
_sk_load_4444_skx
+
0x84
>
.
byte
196
193
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
196
226
125
51
216
/
/
vpmovzxwd
%
xmm0
%
ymm3
.
byte
98
241
101
56
219
5
194
129
3
0
/
/
vpandd
0x381c2
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm0
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
184
129
3
0
/
/
vmulps
0x381b8
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
98
241
101
56
219
13
178
129
3
0
/
/
vpandd
0x381b2
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm1
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
241
116
56
89
13
168
129
3
0
/
/
vmulps
0x381a8
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
98
241
101
56
219
21
162
129
3
0
/
/
vpandd
0x381a2
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm2
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
98
241
108
56
89
21
152
129
3
0
/
/
vmulps
0x38198
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
98
241
101
56
219
29
146
129
3
0
/
/
vpandd
0x38192
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
98
241
100
56
89
29
136
129
3
0
/
/
vmulps
0x38188
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
137
/
/
ja
4423
<
_sk_load_4444_skx
+
0x1b
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
120
0
0
0
/
/
lea
0x78
(
%
rip
)
%
r9
#
451c
<
_sk_load_4444_skx
+
0x114
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
104
255
255
255
/
/
jmpq
4423
<
_sk_load_4444_skx
+
0x1b
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
194
121
52
12
80
/
/
vpmovzxwq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
65
255
255
255
/
/
jmpq
4423
<
_sk_load_4444_skx
+
0x1b
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
194
121
51
12
80
/
/
vpmovzxwd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
197
251
112
201
232
/
/
vpshuflw
0xe8
%
xmm1
%
xmm1
.
byte
197
250
112
201
232
/
/
vpshufhw
0xe8
%
xmm1
%
xmm1
.
byte
197
249
112
201
132
/
/
vpshufd
0x84
%
xmm1
%
xmm1
.
byte
197
241
109
192
/
/
vpunpckhqdq
%
xmm0
%
xmm1
%
xmm0
.
byte
233
7
255
255
255
/
/
jmpq
4423
<
_sk_load_4444_skx
+
0x1b
>
.
byte
145
/
/
xchg
%
eax
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
171
255
255
255
159
/
/
ljmp
*
-
0x60000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
226
/
/
jmpq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_dst_skx
.
globl
_sk_load_4444_dst_skx
FUNCTION
(
_sk_load_4444_dst_skx
)
_sk_load_4444_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
111
/
/
jne
45bc
<
_sk_load_4444_dst_skx
+
0x84
>
.
byte
196
193
122
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
196
226
125
51
252
/
/
vpmovzxwd
%
xmm4
%
ymm7
.
byte
98
241
69
56
219
37
146
128
3
0
/
/
vpandd
0x38092
(
%
rip
)
{
1to8
}
%
ymm7
%
ymm4
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
98
241
92
56
89
37
136
128
3
0
/
/
vmulps
0x38088
(
%
rip
)
{
1to8
}
%
ymm4
%
ymm4
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
98
241
69
56
219
45
130
128
3
0
/
/
vpandd
0x38082
(
%
rip
)
{
1to8
}
%
ymm7
%
ymm5
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
98
241
84
56
89
45
120
128
3
0
/
/
vmulps
0x38078
(
%
rip
)
{
1to8
}
%
ymm5
%
ymm5
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
98
241
69
56
219
53
114
128
3
0
/
/
vpandd
0x38072
(
%
rip
)
{
1to8
}
%
ymm7
%
ymm6
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
98
241
76
56
89
53
104
128
3
0
/
/
vmulps
0x38068
(
%
rip
)
{
1to8
}
%
ymm6
%
ymm6
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
98
241
69
56
219
61
98
128
3
0
/
/
vpandd
0x38062
(
%
rip
)
{
1to8
}
%
ymm7
%
ymm7
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
98
241
68
56
89
61
88
128
3
0
/
/
vmulps
0x38058
(
%
rip
)
{
1to8
}
%
ymm7
%
ymm7
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
137
/
/
ja
4553
<
_sk_load_4444_dst_skx
+
0x1b
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
120
0
0
0
/
/
lea
0x78
(
%
rip
)
%
r9
#
464c
<
_sk_load_4444_dst_skx
+
0x114
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
104
255
255
255
/
/
jmpq
4553
<
_sk_load_4444_dst_skx
+
0x1b
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
194
121
52
44
80
/
/
vpmovzxwq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
65
255
255
255
/
/
jmpq
4553
<
_sk_load_4444_dst_skx
+
0x1b
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
194
121
51
44
80
/
/
vpmovzxwd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
197
251
112
237
232
/
/
vpshuflw
0xe8
%
xmm5
%
xmm5
.
byte
197
250
112
237
232
/
/
vpshufhw
0xe8
%
xmm5
%
xmm5
.
byte
197
249
112
237
132
/
/
vpshufd
0x84
%
xmm5
%
xmm5
.
byte
197
209
109
228
/
/
vpunpckhqdq
%
xmm4
%
xmm5
%
xmm4
.
byte
233
7
255
255
255
/
/
jmpq
4553
<
_sk_load_4444_dst_skx
+
0x1b
>
.
byte
145
/
/
xchg
%
eax
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
171
255
255
255
159
/
/
ljmp
*
-
0x60000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
226
/
/
jmpq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_4444_skx
.
globl
_sk_gather_4444_skx
FUNCTION
(
_sk_gather_4444_skx
)
_sk_gather_4444_skx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
193
121
196
194
1
/
/
vpinsrw
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
196
193
121
196
193
2
/
/
vpinsrw
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
3
/
/
vpinsrw
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
eax
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
51
216
/
/
vpmovzxwd
%
xmm0
%
ymm3
.
byte
98
241
101
56
219
5
167
126
3
0
/
/
vpandd
0x37ea7
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm0
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
98
241
124
56
89
5
157
126
3
0
/
/
vmulps
0x37e9d
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
98
241
101
56
219
13
151
126
3
0
/
/
vpandd
0x37e97
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm1
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
241
116
56
89
13
141
126
3
0
/
/
vmulps
0x37e8d
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
98
241
101
56
219
21
135
126
3
0
/
/
vpandd
0x37e87
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm2
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
98
241
108
56
89
21
125
126
3
0
/
/
vmulps
0x37e7d
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm2
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
98
241
101
56
219
29
119
126
3
0
/
/
vpandd
0x37e77
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
98
241
100
56
89
29
109
126
3
0
/
/
vmulps
0x37e6d
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_4444_skx
.
globl
_sk_store_4444_skx
FUNCTION
(
_sk_store_4444_skx
)
_sk_store_4444_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
44
125
3
0
/
/
vbroadcastss
0x37d2c
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
54
126
3
0
/
/
vbroadcastss
0x37e36
(
%
rip
)
%
ymm11
#
3c614
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c8
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
193
53
114
241
12
/
/
vpslld
0xc
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
8
/
/
vpslld
0x8
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
4
/
/
vpslld
0x4
%
ymm12
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
65
29
235
192
/
/
vpor
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
485c
<
_sk_store_4444_skx
+
0xae
>
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
4858
<
_sk_store_4444_skx
+
0xaa
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
80
0
0
0
/
/
lea
0x50
(
%
rip
)
%
r9
#
48c0
<
_sk_store_4444_skx
+
0x112
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
214
/
/
jmp
4858
<
_sk_store_4444_skx
+
0xaa
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
66
121
52
192
/
/
vpmovzxwq
%
xmm8
%
xmm8
.
byte
98
82
126
8
52
4
80
/
/
vpmovqw
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
192
/
/
jmp
4858
<
_sk_store_4444_skx
+
0xaa
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
66
121
51
192
/
/
vpmovzxwd
%
xmm8
%
xmm8
.
byte
98
82
126
8
51
4
80
/
/
vpmovdw
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
154
/
/
jmp
4858
<
_sk_store_4444_skx
+
0xaa
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
185
255
255
255
202
/
/
mov
0xcaffffff
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
240
/
/
push
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
224
/
/
callq
ffffffffe10048d4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffe0fc8688
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_skx
.
globl
_sk_load_8888_skx
FUNCTION
(
_sk_load_8888_skx
)
_sk_load_8888_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
97
/
/
jne
4953
<
_sk_load_8888_skx
+
0x77
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
241
127
169
111
195
/
/
vmovdqu8
%
ymm3
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
96
124
3
0
/
/
vbroadcastss
0x37c60
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
245
114
211
8
/
/
vpsrld
0x8
%
ymm3
%
ymm1
.
byte
98
241
127
169
111
201
/
/
vmovdqu8
%
ymm1
%
ymm1
{
%
k1
}
{
z
}
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
237
114
211
16
/
/
vpsrld
0x10
%
ymm3
%
ymm2
.
byte
98
241
127
169
111
210
/
/
vmovdqu8
%
ymm2
%
ymm2
{
%
k1
}
{
z
}
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
229
114
211
24
/
/
vpsrld
0x18
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
151
/
/
ja
48f8
<
_sk_load_8888_skx
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
137
0
0
0
/
/
lea
0x89
(
%
rip
)
%
r9
#
49f4
<
_sk_load_8888_skx
+
0x118
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
121
255
255
255
/
/
jmpq
48f8
<
_sk_load_8888_skx
+
0x1c
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
194
121
53
4
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
232
/
/
vpshufd
0xe8
%
xmm0
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
80
255
255
255
/
/
jmpq
48f8
<
_sk_load_8888_skx
+
0x1c
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
4
255
255
255
/
/
jmpq
48f8
<
_sk_load_8888_skx
+
0x1c
>
.
byte
128
255
255
/
/
cmp
0xff
%
bh
.
byte
255
158
255
255
255
139
/
/
lcall
*
-
0x74000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
180
255
/
/
mov
0xff
%
ah
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_dst_skx
.
globl
_sk_load_8888_dst_skx
FUNCTION
(
_sk_load_8888_dst_skx
)
_sk_load_8888_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
97
/
/
jne
4a87
<
_sk_load_8888_dst_skx
+
0x77
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
241
127
169
111
231
/
/
vmovdqu8
%
ymm7
%
ymm4
{
%
k1
}
{
z
}
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
44
123
3
0
/
/
vbroadcastss
0x37b2c
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
197
213
114
215
8
/
/
vpsrld
0x8
%
ymm7
%
ymm5
.
byte
98
241
127
169
111
237
/
/
vmovdqu8
%
ymm5
%
ymm5
{
%
k1
}
{
z
}
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
84
89
232
/
/
vmulps
%
ymm8
%
ymm5
%
ymm5
.
byte
197
205
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm6
.
byte
98
241
127
169
111
246
/
/
vmovdqu8
%
ymm6
%
ymm6
{
%
k1
}
{
z
}
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
151
/
/
ja
4a2c
<
_sk_load_8888_dst_skx
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
137
0
0
0
/
/
lea
0x89
(
%
rip
)
%
r9
#
4b28
<
_sk_load_8888_dst_skx
+
0x118
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
121
255
255
255
/
/
jmpq
4a2c
<
_sk_load_8888_dst_skx
+
0x1c
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
194
121
53
36
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
232
/
/
vpshufd
0xe8
%
xmm4
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
80
255
255
255
/
/
jmpq
4a2c
<
_sk_load_8888_dst_skx
+
0x1c
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
4
255
255
255
/
/
jmpq
4a2c
<
_sk_load_8888_dst_skx
+
0x1c
>
.
byte
128
255
255
/
/
cmp
0xff
%
bh
.
byte
255
158
255
255
255
139
/
/
lcall
*
-
0x74000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
180
255
/
/
mov
0xff
%
ah
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_8888_skx
.
globl
_sk_gather_8888_skx
FUNCTION
(
_sk_gather_8888_skx
)
_sk_gather_8888_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
66
101
144
4
128
/
/
vpgatherdd
%
ymm3
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
209
127
169
111
192
/
/
vmovdqu8
%
ymm8
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
29
201
121
3
0
/
/
vbroadcastss
0x379c9
(
%
rip
)
%
ymm3
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
193
117
114
208
8
/
/
vpsrld
0x8
%
ymm8
%
ymm1
.
byte
98
241
127
169
111
201
/
/
vmovdqu8
%
ymm1
%
ymm1
{
%
k1
}
{
z
}
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
193
109
114
208
16
/
/
vpsrld
0x10
%
ymm8
%
ymm2
.
byte
98
241
127
169
111
210
/
/
vmovdqu8
%
ymm2
%
ymm2
{
%
k1
}
{
z
}
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
193
61
114
208
24
/
/
vpsrld
0x18
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_8888_skx
.
globl
_sk_store_8888_skx
FUNCTION
(
_sk_store_8888_skx
)
_sk_store_8888_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
239
120
3
0
/
/
vbroadcastss
0x378ef
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
13
121
3
0
/
/
vbroadcastss
0x3790d
(
%
rip
)
%
ymm11
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
8
/
/
vpslld
0x8
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
16
/
/
vpslld
0x10
%
ymm12
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
29
235
192
/
/
vpor
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
4c8e
<
_sk_store_8888_skx
+
0xa4
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
4c8a
<
_sk_store_8888_skx
+
0xa0
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
90
0
0
0
/
/
lea
0x5a
(
%
rip
)
%
r9
#
4cfc
<
_sk_store_8888_skx
+
0x112
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
4c8a
<
_sk_store_8888_skx
+
0xa0
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
66
121
53
192
/
/
vpmovzxdq
%
xmm8
%
xmm8
.
byte
98
82
126
8
53
4
144
/
/
vpmovqd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
193
/
/
jmp
4c8a
<
_sk_store_8888_skx
+
0xa0
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
144
/
/
jmp
4c8a
<
_sk_store_8888_skx
+
0xa0
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
175
/
/
scas
%
es
:
(
%
rdi
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
183
/
/
mov
0xb7ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
246
/
/
push
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
233
255
255
255
219
/
/
jmpq
ffffffffdc004d10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffdbfc8ac4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_skx
.
globl
_sk_load_bgra_skx
FUNCTION
(
_sk_load_bgra_skx
)
_sk_load_bgra_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
97
/
/
jne
4d8f
<
_sk_load_bgra_skx
+
0x77
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
241
127
169
111
195
/
/
vmovdqu8
%
ymm3
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
36
120
3
0
/
/
vbroadcastss
0x37824
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
208
/
/
vmulps
%
ymm8
%
ymm0
%
ymm2
.
byte
197
253
114
211
8
/
/
vpsrld
0x8
%
ymm3
%
ymm0
.
byte
98
241
127
169
111
192
/
/
vmovdqu8
%
ymm0
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
200
/
/
vmulps
%
ymm8
%
ymm0
%
ymm1
.
byte
197
253
114
211
16
/
/
vpsrld
0x10
%
ymm3
%
ymm0
.
byte
98
241
127
169
111
192
/
/
vmovdqu8
%
ymm0
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
229
114
211
24
/
/
vpsrld
0x18
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
151
/
/
ja
4d34
<
_sk_load_bgra_skx
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
137
0
0
0
/
/
lea
0x89
(
%
rip
)
%
r9
#
4e30
<
_sk_load_bgra_skx
+
0x118
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
121
255
255
255
/
/
jmpq
4d34
<
_sk_load_bgra_skx
+
0x1c
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
194
121
53
4
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
232
/
/
vpshufd
0xe8
%
xmm0
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
80
255
255
255
/
/
jmpq
4d34
<
_sk_load_bgra_skx
+
0x1c
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
4
255
255
255
/
/
jmpq
4d34
<
_sk_load_bgra_skx
+
0x1c
>
.
byte
128
255
255
/
/
cmp
0xff
%
bh
.
byte
255
158
255
255
255
139
/
/
lcall
*
-
0x74000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
180
255
/
/
mov
0xff
%
ah
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_dst_skx
.
globl
_sk_load_bgra_dst_skx
FUNCTION
(
_sk_load_bgra_dst_skx
)
_sk_load_bgra_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
97
/
/
jne
4ec3
<
_sk_load_bgra_dst_skx
+
0x77
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
241
127
169
111
231
/
/
vmovdqu8
%
ymm7
%
ymm4
{
%
k1
}
{
z
}
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
240
118
3
0
/
/
vbroadcastss
0x376f0
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
92
89
240
/
/
vmulps
%
ymm8
%
ymm4
%
ymm6
.
byte
197
221
114
215
8
/
/
vpsrld
0x8
%
ymm7
%
ymm4
.
byte
98
241
127
169
111
228
/
/
vmovdqu8
%
ymm4
%
ymm4
{
%
k1
}
{
z
}
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
92
89
232
/
/
vmulps
%
ymm8
%
ymm4
%
ymm5
.
byte
197
221
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm4
.
byte
98
241
127
169
111
228
/
/
vmovdqu8
%
ymm4
%
ymm4
{
%
k1
}
{
z
}
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
151
/
/
ja
4e68
<
_sk_load_bgra_dst_skx
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
137
0
0
0
/
/
lea
0x89
(
%
rip
)
%
r9
#
4f64
<
_sk_load_bgra_dst_skx
+
0x118
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
121
255
255
255
/
/
jmpq
4e68
<
_sk_load_bgra_dst_skx
+
0x1c
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
194
121
53
36
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
232
/
/
vpshufd
0xe8
%
xmm4
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
80
255
255
255
/
/
jmpq
4e68
<
_sk_load_bgra_dst_skx
+
0x1c
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
4
255
255
255
/
/
jmpq
4e68
<
_sk_load_bgra_dst_skx
+
0x1c
>
.
byte
128
255
255
/
/
cmp
0xff
%
bh
.
byte
255
158
255
255
255
139
/
/
lcall
*
-
0x74000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
180
255
/
/
mov
0xff
%
ah
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_bgra_skx
.
globl
_sk_gather_bgra_skx
FUNCTION
(
_sk_gather_bgra_skx
)
_sk_gather_bgra_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
66
101
144
4
128
/
/
vpgatherdd
%
ymm3
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
209
127
169
111
192
/
/
vmovdqu8
%
ymm8
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
29
141
117
3
0
/
/
vbroadcastss
0x3758d
(
%
rip
)
%
ymm3
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
211
/
/
vmulps
%
ymm3
%
ymm0
%
ymm2
.
byte
196
193
125
114
208
8
/
/
vpsrld
0x8
%
ymm8
%
ymm0
.
byte
98
241
127
169
111
192
/
/
vmovdqu8
%
ymm0
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
89
203
/
/
vmulps
%
ymm3
%
ymm0
%
ymm1
.
byte
196
193
125
114
208
16
/
/
vpsrld
0x10
%
ymm8
%
ymm0
.
byte
98
241
127
169
111
192
/
/
vmovdqu8
%
ymm0
%
ymm0
{
%
k1
}
{
z
}
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
193
61
114
208
24
/
/
vpsrld
0x18
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_bgra_skx
.
globl
_sk_store_bgra_skx
FUNCTION
(
_sk_store_bgra_skx
)
_sk_store_bgra_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
202
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
179
116
3
0
/
/
vbroadcastss
0x374b3
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
209
116
3
0
/
/
vbroadcastss
0x374d1
(
%
rip
)
%
ymm11
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
8
/
/
vpslld
0x8
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
224
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
16
/
/
vpslld
0x10
%
ymm12
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
29
235
192
/
/
vpor
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
50ca
<
_sk_store_bgra_skx
+
0xa4
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
50c6
<
_sk_store_bgra_skx
+
0xa0
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
90
0
0
0
/
/
lea
0x5a
(
%
rip
)
%
r9
#
5138
<
_sk_store_bgra_skx
+
0x112
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
50c6
<
_sk_store_bgra_skx
+
0xa0
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
66
121
53
192
/
/
vpmovzxdq
%
xmm8
%
xmm8
.
byte
98
82
126
8
53
4
144
/
/
vpmovqd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
193
/
/
jmp
50c6
<
_sk_store_bgra_skx
+
0xa0
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
144
/
/
jmp
50c6
<
_sk_store_bgra_skx
+
0xa0
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
175
/
/
scas
%
es
:
(
%
rdi
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
183
/
/
mov
0xb7ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
246
/
/
push
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
233
255
255
255
219
/
/
jmpq
ffffffffdc00514c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffdbfc8f00
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_1010102_skx
.
globl
_sk_load_1010102_skx
FUNCTION
(
_sk_load_1010102_skx
)
_sk_load_1010102_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
96
/
/
jne
51ca
<
_sk_load_1010102_skx
+
0x76
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
196
226
125
88
21
159
116
3
0
/
/
vpbroadcastd
0x3749f
(
%
rip
)
%
ymm2
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
197
229
219
194
/
/
vpand
%
ymm2
%
ymm3
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
146
116
3
0
/
/
vbroadcastss
0x37492
(
%
rip
)
%
ymm8
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
245
114
211
10
/
/
vpsrld
0xa
%
ymm3
%
ymm1
.
byte
197
245
219
202
/
/
vpand
%
ymm2
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
181
114
211
20
/
/
vpsrld
0x14
%
ymm3
%
ymm9
.
byte
197
181
219
210
/
/
vpand
%
ymm2
%
ymm9
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
229
114
211
30
/
/
vpsrld
0x1e
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
98
241
100
56
89
29
162
115
3
0
/
/
vmulps
0x373a2
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
152
/
/
ja
5170
<
_sk_load_1010102_skx
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
138
0
0
0
/
/
lea
0x8a
(
%
rip
)
%
r9
#
526c
<
_sk_load_1010102_skx
+
0x118
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
122
255
255
255
/
/
jmpq
5170
<
_sk_load_1010102_skx
+
0x1c
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
194
121
53
4
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
232
/
/
vpshufd
0xe8
%
xmm0
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
81
255
255
255
/
/
jmpq
5170
<
_sk_load_1010102_skx
+
0x1c
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
216
/
/
vpexpandd
%
ymm0
%
ymm3
{
%
k1
}
{
z
}
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
5
255
255
255
/
/
jmpq
5170
<
_sk_load_1010102_skx
+
0x1c
>
.
byte
144
/
/
nop
.
byte
127
255
/
/
jg
526d
<
_sk_load_1010102_skx
+
0x119
>
.
byte
255
/
/
(
bad
)
.
byte
255
157
255
255
255
138
/
/
lcall
*
-
0x75000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
238
/
/
out
%
al
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
179
255
/
/
mov
0xff
%
bl
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_1010102_dst_skx
.
globl
_sk_load_1010102_dst_skx
FUNCTION
(
_sk_load_1010102_dst_skx
)
_sk_load_1010102_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
96
/
/
jne
52fe
<
_sk_load_1010102_dst_skx
+
0x76
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
196
226
125
88
53
107
115
3
0
/
/
vpbroadcastd
0x3736b
(
%
rip
)
%
ymm6
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
197
197
219
230
/
/
vpand
%
ymm6
%
ymm7
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
94
115
3
0
/
/
vbroadcastss
0x3735e
(
%
rip
)
%
ymm8
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
197
213
114
215
10
/
/
vpsrld
0xa
%
ymm7
%
ymm5
.
byte
197
213
219
238
/
/
vpand
%
ymm6
%
ymm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
84
89
232
/
/
vmulps
%
ymm8
%
ymm5
%
ymm5
.
byte
197
181
114
215
20
/
/
vpsrld
0x14
%
ymm7
%
ymm9
.
byte
197
181
219
246
/
/
vpand
%
ymm6
%
ymm9
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
197
197
114
215
30
/
/
vpsrld
0x1e
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
98
241
68
56
89
61
110
114
3
0
/
/
vmulps
0x3726e
(
%
rip
)
{
1to8
}
%
ymm7
%
ymm7
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
152
/
/
ja
52a4
<
_sk_load_1010102_dst_skx
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
138
0
0
0
/
/
lea
0x8a
(
%
rip
)
%
r9
#
53a0
<
_sk_load_1010102_dst_skx
+
0x118
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
122
255
255
255
/
/
jmpq
52a4
<
_sk_load_1010102_dst_skx
+
0x1c
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
4
/
/
mov
0x4
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
194
121
53
36
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
232
/
/
vpshufd
0xe8
%
xmm4
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
81
255
255
255
/
/
jmpq
52a4
<
_sk_load_1010102_dst_skx
+
0x1c
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
176
64
/
/
mov
0x40
%
al
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
242
125
169
137
252
/
/
vpexpandd
%
ymm4
%
ymm7
{
%
k1
}
{
z
}
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
5
255
255
255
/
/
jmpq
52a4
<
_sk_load_1010102_dst_skx
+
0x1c
>
.
byte
144
/
/
nop
.
byte
127
255
/
/
jg
53a1
<
_sk_load_1010102_dst_skx
+
0x119
>
.
byte
255
/
/
(
bad
)
.
byte
255
157
255
255
255
138
/
/
lcall
*
-
0x75000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
238
/
/
out
%
al
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
179
255
/
/
mov
0xff
%
bl
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_1010102_skx
.
globl
_sk_gather_1010102_skx
FUNCTION
(
_sk_gather_1010102_skx
)
_sk_gather_1010102_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
66
101
144
4
128
/
/
vpgatherdd
%
ymm3
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
196
226
125
88
21
8
114
3
0
/
/
vpbroadcastd
0x37208
(
%
rip
)
%
ymm2
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
197
189
219
194
/
/
vpand
%
ymm2
%
ymm8
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
29
251
113
3
0
/
/
vbroadcastss
0x371fb
(
%
rip
)
%
ymm3
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
193
117
114
208
10
/
/
vpsrld
0xa
%
ymm8
%
ymm1
.
byte
197
245
219
202
/
/
vpand
%
ymm2
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
193
53
114
208
20
/
/
vpsrld
0x14
%
ymm8
%
ymm9
.
byte
197
181
219
210
/
/
vpand
%
ymm2
%
ymm9
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
193
101
114
208
30
/
/
vpsrld
0x1e
%
ymm8
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
98
241
100
56
89
29
11
113
3
0
/
/
vmulps
0x3710b
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm3
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_1010102_skx
.
globl
_sk_store_1010102_skx
FUNCTION
(
_sk_store_1010102_skx
)
_sk_store_1010102_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
120
112
3
0
/
/
vbroadcastss
0x37078
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
142
113
3
0
/
/
vbroadcastss
0x3718e
(
%
rip
)
%
ymm11
#
3c620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d4
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
10
/
/
vpslld
0xa
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
20
/
/
vpslld
0x14
%
ymm11
%
ymm11
.
byte
196
65
53
235
203
/
/
vpor
%
ymm11
%
ymm9
%
ymm9
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
98
113
60
56
89
5
57
113
3
0
/
/
vmulps
0x37139
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm8
#
3c624
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d8
>
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
30
/
/
vpslld
0x1e
%
ymm8
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
550a
<
_sk_store_1010102_skx
+
0xa9
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
4
255
/
/
add
0xff
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
5506
<
_sk_store_1010102_skx
+
0xa5
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
90
0
0
0
/
/
lea
0x5a
(
%
rip
)
%
r9
#
5578
<
_sk_store_1010102_skx
+
0x117
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
5506
<
_sk_store_1010102_skx
+
0xa5
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
66
121
53
192
/
/
vpmovzxdq
%
xmm8
%
xmm8
.
byte
98
82
126
8
53
4
144
/
/
vpmovqd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
193
/
/
jmp
5506
<
_sk_store_1010102_skx
+
0xa5
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
144
/
/
jmp
5506
<
_sk_store_1010102_skx
+
0xa5
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
175
/
/
scas
%
es
:
(
%
rdi
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
183
/
/
mov
0xb7ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
246
/
/
push
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
233
255
255
255
219
/
/
jmpq
ffffffffdc00558c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffdbfc9340
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_f16_skx
.
globl
_sk_load_f16_skx
FUNCTION
(
_sk_load_f16_skx
)
_sk_load_f16_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
560f
<
_sk_load_f16_skx
+
0x7b
>
.
byte
196
65
121
16
4
208
/
/
vmovupd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
193
121
16
84
208
16
/
/
vmovupd
0x10
(
%
r8
%
rdx
8
)
%
xmm2
.
byte
196
193
121
16
76
208
32
/
/
vmovupd
0x20
(
%
r8
%
rdx
8
)
%
xmm1
.
byte
196
65
122
111
76
208
48
/
/
vmovdqu
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
65
113
97
193
/
/
vpunpcklwd
%
xmm9
%
xmm1
%
xmm8
.
byte
196
193
113
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm1
%
xmm3
.
byte
197
249
97
202
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm1
.
byte
197
121
105
202
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm9
.
byte
197
185
97
195
/
/
vpunpcklwd
%
xmm3
%
xmm8
%
xmm0
.
byte
197
241
108
208
/
/
vpunpcklqdq
%
xmm0
%
xmm1
%
xmm2
.
byte
197
241
109
200
/
/
vpunpckhqdq
%
xmm0
%
xmm1
%
xmm1
.
byte
196
226
125
19
194
/
/
vcvtph2ps
%
xmm2
%
ymm0
.
byte
196
226
125
19
201
/
/
vcvtph2ps
%
xmm1
%
ymm1
.
byte
197
185
105
219
/
/
vpunpckhwd
%
xmm3
%
xmm8
%
xmm3
.
byte
197
177
108
211
/
/
vpunpcklqdq
%
xmm3
%
xmm9
%
xmm2
.
byte
196
226
125
19
210
/
/
vcvtph2ps
%
xmm2
%
ymm2
.
byte
197
177
109
219
/
/
vpunpckhqdq
%
xmm3
%
xmm9
%
xmm3
.
byte
196
226
125
19
219
/
/
vcvtph2ps
%
xmm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
123
16
4
208
/
/
vmovsd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
5675
<
_sk_load_f16_skx
+
0xe1
>
.
byte
196
65
57
22
68
208
8
/
/
vmovhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
5675
<
_sk_load_f16_skx
+
0xe1
>
.
byte
196
193
123
16
84
208
16
/
/
vmovsd
0x10
(
%
r8
%
rdx
8
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
5682
<
_sk_load_f16_skx
+
0xee
>
.
byte
196
193
105
22
84
208
24
/
/
vmovhpd
0x18
(
%
r8
%
rdx
8
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
5682
<
_sk_load_f16_skx
+
0xee
>
.
byte
196
193
123
16
76
208
32
/
/
vmovsd
0x20
(
%
r8
%
rdx
8
)
%
xmm1
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
109
255
255
255
/
/
je
55c5
<
_sk_load_f16_skx
+
0x31
>
.
byte
196
193
113
22
76
208
40
/
/
vmovhpd
0x28
(
%
r8
%
rdx
8
)
%
xmm1
%
xmm1
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
92
255
255
255
/
/
jb
55c5
<
_sk_load_f16_skx
+
0x31
>
.
byte
196
65
122
126
76
208
48
/
/
vmovq
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
233
80
255
255
255
/
/
jmpq
55c5
<
_sk_load_f16_skx
+
0x31
>
.
byte
197
241
87
201
/
/
vxorpd
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
67
255
255
255
/
/
jmpq
55c5
<
_sk_load_f16_skx
+
0x31
>
.
byte
197
241
87
201
/
/
vxorpd
%
xmm1
%
xmm1
%
xmm1
.
byte
233
58
255
255
255
/
/
jmpq
55c5
<
_sk_load_f16_skx
+
0x31
>
HIDDEN
_sk_load_f16_dst_skx
.
globl
_sk_load_f16_dst_skx
FUNCTION
(
_sk_load_f16_dst_skx
)
_sk_load_f16_dst_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
5706
<
_sk_load_f16_dst_skx
+
0x7b
>
.
byte
196
65
121
16
4
208
/
/
vmovupd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
193
121
16
116
208
16
/
/
vmovupd
0x10
(
%
r8
%
rdx
8
)
%
xmm6
.
byte
196
193
121
16
108
208
32
/
/
vmovupd
0x20
(
%
r8
%
rdx
8
)
%
xmm5
.
byte
196
65
122
111
76
208
48
/
/
vmovdqu
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
197
185
97
230
/
/
vpunpcklwd
%
xmm6
%
xmm8
%
xmm4
.
byte
197
185
105
246
/
/
vpunpckhwd
%
xmm6
%
xmm8
%
xmm6
.
byte
196
65
81
97
193
/
/
vpunpcklwd
%
xmm9
%
xmm5
%
xmm8
.
byte
196
193
81
105
249
/
/
vpunpckhwd
%
xmm9
%
xmm5
%
xmm7
.
byte
197
217
97
238
/
/
vpunpcklwd
%
xmm6
%
xmm4
%
xmm5
.
byte
197
89
105
206
/
/
vpunpckhwd
%
xmm6
%
xmm4
%
xmm9
.
byte
197
185
97
231
/
/
vpunpcklwd
%
xmm7
%
xmm8
%
xmm4
.
byte
197
209
108
244
/
/
vpunpcklqdq
%
xmm4
%
xmm5
%
xmm6
.
byte
197
209
109
236
/
/
vpunpckhqdq
%
xmm4
%
xmm5
%
xmm5
.
byte
196
226
125
19
230
/
/
vcvtph2ps
%
xmm6
%
ymm4
.
byte
196
226
125
19
237
/
/
vcvtph2ps
%
xmm5
%
ymm5
.
byte
197
185
105
255
/
/
vpunpckhwd
%
xmm7
%
xmm8
%
xmm7
.
byte
197
177
108
247
/
/
vpunpcklqdq
%
xmm7
%
xmm9
%
xmm6
.
byte
196
226
125
19
246
/
/
vcvtph2ps
%
xmm6
%
ymm6
.
byte
197
177
109
255
/
/
vpunpckhqdq
%
xmm7
%
xmm9
%
xmm7
.
byte
196
226
125
19
255
/
/
vcvtph2ps
%
xmm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
123
16
4
208
/
/
vmovsd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
576c
<
_sk_load_f16_dst_skx
+
0xe1
>
.
byte
196
65
57
22
68
208
8
/
/
vmovhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
576c
<
_sk_load_f16_dst_skx
+
0xe1
>
.
byte
196
193
123
16
116
208
16
/
/
vmovsd
0x10
(
%
r8
%
rdx
8
)
%
xmm6
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
5779
<
_sk_load_f16_dst_skx
+
0xee
>
.
byte
196
193
73
22
116
208
24
/
/
vmovhpd
0x18
(
%
r8
%
rdx
8
)
%
xmm6
%
xmm6
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
5779
<
_sk_load_f16_dst_skx
+
0xee
>
.
byte
196
193
123
16
108
208
32
/
/
vmovsd
0x20
(
%
r8
%
rdx
8
)
%
xmm5
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
109
255
255
255
/
/
je
56bc
<
_sk_load_f16_dst_skx
+
0x31
>
.
byte
196
193
81
22
108
208
40
/
/
vmovhpd
0x28
(
%
r8
%
rdx
8
)
%
xmm5
%
xmm5
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
92
255
255
255
/
/
jb
56bc
<
_sk_load_f16_dst_skx
+
0x31
>
.
byte
196
65
122
126
76
208
48
/
/
vmovq
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
233
80
255
255
255
/
/
jmpq
56bc
<
_sk_load_f16_dst_skx
+
0x31
>
.
byte
197
209
87
237
/
/
vxorpd
%
xmm5
%
xmm5
%
xmm5
.
byte
197
201
87
246
/
/
vxorpd
%
xmm6
%
xmm6
%
xmm6
.
byte
233
67
255
255
255
/
/
jmpq
56bc
<
_sk_load_f16_dst_skx
+
0x31
>
.
byte
197
209
87
237
/
/
vxorpd
%
xmm5
%
xmm5
%
xmm5
.
byte
233
58
255
255
255
/
/
jmpq
56bc
<
_sk_load_f16_dst_skx
+
0x31
>
HIDDEN
_sk_gather_f16_skx
.
globl
_sk_gather_f16_skx
FUNCTION
(
_sk_gather_f16_skx
)
_sk_gather_f16_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
242
117
56
64
72
2
/
/
vpmulld
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
196
194
245
144
20
192
/
/
vpgatherdq
%
ymm1
(
%
r8
%
xmm0
8
)
%
ymm2
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
66
229
144
4
192
/
/
vpgatherdq
%
ymm3
(
%
r8
%
xmm0
8
)
%
ymm8
.
byte
98
211
253
72
58
192
1
/
/
vinserti64x4
0x1
%
ymm8
%
zmm0
%
zmm0
.
byte
98
211
237
72
58
200
1
/
/
vinserti64x4
0x1
%
ymm8
%
zmm2
%
zmm1
.
byte
98
243
253
72
57
202
1
/
/
vextracti64x2
0x1
%
zmm1
%
xmm2
.
byte
98
243
253
72
57
195
2
/
/
vextracti64x2
0x2
%
zmm0
%
xmm3
.
byte
98
243
253
72
57
192
3
/
/
vextracti64x2
0x3
%
zmm0
%
xmm0
.
byte
197
113
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm1
%
xmm8
.
byte
197
241
105
202
/
/
vpunpckhwd
%
xmm2
%
xmm1
%
xmm1
.
byte
197
225
97
208
/
/
vpunpcklwd
%
xmm0
%
xmm3
%
xmm2
.
byte
197
225
105
216
/
/
vpunpckhwd
%
xmm0
%
xmm3
%
xmm3
.
byte
197
185
97
193
/
/
vpunpcklwd
%
xmm1
%
xmm8
%
xmm0
.
byte
197
57
105
193
/
/
vpunpckhwd
%
xmm1
%
xmm8
%
xmm8
.
byte
197
233
97
203
/
/
vpunpcklwd
%
xmm3
%
xmm2
%
xmm1
.
byte
197
121
108
201
/
/
vpunpcklqdq
%
xmm1
%
xmm0
%
xmm9
.
byte
197
249
109
201
/
/
vpunpckhqdq
%
xmm1
%
xmm0
%
xmm1
.
byte
196
194
125
19
193
/
/
vcvtph2ps
%
xmm9
%
ymm0
.
byte
196
226
125
19
201
/
/
vcvtph2ps
%
xmm1
%
ymm1
.
byte
197
233
105
219
/
/
vpunpckhwd
%
xmm3
%
xmm2
%
xmm3
.
byte
197
185
108
211
/
/
vpunpcklqdq
%
xmm3
%
xmm8
%
xmm2
.
byte
196
226
125
19
210
/
/
vcvtph2ps
%
xmm2
%
ymm2
.
byte
197
185
109
219
/
/
vpunpckhqdq
%
xmm3
%
xmm8
%
xmm3
.
byte
196
226
125
19
219
/
/
vcvtph2ps
%
xmm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_f16_skx
.
globl
_sk_store_f16_skx
FUNCTION
(
_sk_store_f16_skx
)
_sk_store_f16_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
196
195
125
29
192
4
/
/
vcvtps2ph
0x4
%
ymm0
%
xmm8
.
byte
196
195
125
29
201
4
/
/
vcvtps2ph
0x4
%
ymm1
%
xmm9
.
byte
196
195
125
29
210
4
/
/
vcvtps2ph
0x4
%
ymm2
%
xmm10
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
195
125
29
219
4
/
/
vcvtps2ph
0x4
%
ymm3
%
xmm11
.
byte
196
65
57
97
225
/
/
vpunpcklwd
%
xmm9
%
xmm8
%
xmm12
.
byte
196
65
57
105
193
/
/
vpunpckhwd
%
xmm9
%
xmm8
%
xmm8
.
byte
196
65
41
97
203
/
/
vpunpcklwd
%
xmm11
%
xmm10
%
xmm9
.
byte
196
65
41
105
235
/
/
vpunpckhwd
%
xmm11
%
xmm10
%
xmm13
.
byte
196
65
25
98
217
/
/
vpunpckldq
%
xmm9
%
xmm12
%
xmm11
.
byte
196
65
25
106
209
/
/
vpunpckhdq
%
xmm9
%
xmm12
%
xmm10
.
byte
196
65
57
98
205
/
/
vpunpckldq
%
xmm13
%
xmm8
%
xmm9
.
byte
196
65
57
106
197
/
/
vpunpckhdq
%
xmm13
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
31
/
/
jne
58c1
<
_sk_store_f16_skx
+
0x75
>
.
byte
196
65
122
127
28
208
/
/
vmovdqu
%
xmm11
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
84
208
16
/
/
vmovdqu
%
xmm10
0x10
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
76
208
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
68
208
48
/
/
vmovdqu
%
xmm8
0x30
(
%
r8
%
rdx
8
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
214
28
208
/
/
vmovq
%
xmm11
(
%
r8
%
rdx
8
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
58bd
<
_sk_store_f16_skx
+
0x71
>
.
byte
196
65
121
23
92
208
8
/
/
vmovhpd
%
xmm11
0x8
(
%
r8
%
rdx
8
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
58bd
<
_sk_store_f16_skx
+
0x71
>
.
byte
196
65
121
214
84
208
16
/
/
vmovq
%
xmm10
0x10
(
%
r8
%
rdx
8
)
.
byte
116
218
/
/
je
58bd
<
_sk_store_f16_skx
+
0x71
>
.
byte
196
65
121
23
84
208
24
/
/
vmovhpd
%
xmm10
0x18
(
%
r8
%
rdx
8
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
58bd
<
_sk_store_f16_skx
+
0x71
>
.
byte
196
65
121
214
76
208
32
/
/
vmovq
%
xmm9
0x20
(
%
r8
%
rdx
8
)
.
byte
116
196
/
/
je
58bd
<
_sk_store_f16_skx
+
0x71
>
.
byte
196
65
121
23
76
208
40
/
/
vmovhpd
%
xmm9
0x28
(
%
r8
%
rdx
8
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
183
/
/
jb
58bd
<
_sk_store_f16_skx
+
0x71
>
.
byte
196
65
121
214
68
208
48
/
/
vmovq
%
xmm8
0x30
(
%
r8
%
rdx
8
)
.
byte
235
174
/
/
jmp
58bd
<
_sk_store_f16_skx
+
0x71
>
HIDDEN
_sk_load_u16_be_skx
.
globl
_sk_load_u16_be_skx
FUNCTION
(
_sk_load_u16_be_skx
)
_sk_load_u16_be_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
141
12
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r9
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
204
0
0
0
/
/
jne
59fc
<
_sk_load_u16_be_skx
+
0xed
>
.
byte
196
1
121
16
4
72
/
/
vmovupd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
129
121
16
84
72
16
/
/
vmovupd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
196
129
121
16
92
72
32
/
/
vmovupd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
1
122
111
76
72
48
/
/
vmovdqu
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
121
105
202
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm9
.
byte
197
241
97
211
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm2
.
byte
197
113
105
219
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm11
.
byte
197
185
108
194
/
/
vpunpcklqdq
%
xmm2
%
xmm8
%
xmm0
.
byte
197
241
113
240
8
/
/
vpsllw
0x8
%
xmm0
%
xmm1
.
byte
197
249
113
208
8
/
/
vpsrlw
0x8
%
xmm0
%
xmm0
.
byte
197
241
235
192
/
/
vpor
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
255
107
3
0
/
/
vbroadcastss
0x36bff
(
%
rip
)
%
ymm10
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
185
109
202
/
/
vpunpckhqdq
%
xmm2
%
xmm8
%
xmm1
.
byte
197
233
113
241
8
/
/
vpsllw
0x8
%
xmm1
%
xmm2
.
byte
197
241
113
209
8
/
/
vpsrlw
0x8
%
xmm1
%
xmm1
.
byte
197
233
235
201
/
/
vpor
%
xmm1
%
xmm2
%
xmm1
.
byte
196
226
125
51
201
/
/
vpmovzxwd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
49
108
211
/
/
vpunpcklqdq
%
xmm11
%
xmm9
%
xmm2
.
byte
197
225
113
242
8
/
/
vpsllw
0x8
%
xmm2
%
xmm3
.
byte
197
233
113
210
8
/
/
vpsrlw
0x8
%
xmm2
%
xmm2
.
byte
197
225
235
210
/
/
vpor
%
xmm2
%
xmm3
%
xmm2
.
byte
196
226
125
51
210
/
/
vpmovzxwd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
49
109
219
/
/
vpunpckhqdq
%
xmm11
%
xmm9
%
xmm3
.
byte
197
185
113
243
8
/
/
vpsllw
0x8
%
xmm3
%
xmm8
.
byte
197
225
113
211
8
/
/
vpsrlw
0x8
%
xmm3
%
xmm3
.
byte
197
185
235
219
/
/
vpor
%
xmm3
%
xmm8
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
123
16
4
72
/
/
vmovsd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
5a62
<
_sk_load_u16_be_skx
+
0x153
>
.
byte
196
1
57
22
68
72
8
/
/
vmovhpd
0x8
(
%
r8
%
r9
2
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
5a62
<
_sk_load_u16_be_skx
+
0x153
>
.
byte
196
129
123
16
84
72
16
/
/
vmovsd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
5a6f
<
_sk_load_u16_be_skx
+
0x160
>
.
byte
196
129
105
22
84
72
24
/
/
vmovhpd
0x18
(
%
r8
%
r9
2
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
5a6f
<
_sk_load_u16_be_skx
+
0x160
>
.
byte
196
129
123
16
92
72
32
/
/
vmovsd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
6
255
255
255
/
/
je
594b
<
_sk_load_u16_be_skx
+
0x3c
>
.
byte
196
129
97
22
92
72
40
/
/
vmovhpd
0x28
(
%
r8
%
r9
2
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
245
254
255
255
/
/
jb
594b
<
_sk_load_u16_be_skx
+
0x3c
>
.
byte
196
1
122
126
76
72
48
/
/
vmovq
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
233
233
254
255
255
/
/
jmpq
594b
<
_sk_load_u16_be_skx
+
0x3c
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
220
254
255
255
/
/
jmpq
594b
<
_sk_load_u16_be_skx
+
0x3c
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
211
254
255
255
/
/
jmpq
594b
<
_sk_load_u16_be_skx
+
0x3c
>
HIDDEN
_sk_load_rgb_u16_be_skx
.
globl
_sk_load_rgb_u16_be_skx
FUNCTION
(
_sk_load_rgb_u16_be_skx
)
_sk_load_rgb_u16_be_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
141
12
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r9
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
202
0
0
0
/
/
jne
5b5f
<
_sk_load_rgb_u16_be_skx
+
0xe7
>
.
byte
196
1
121
16
28
72
/
/
vmovupd
(
%
r8
%
r9
2
)
%
xmm11
.
byte
196
129
121
16
92
72
12
/
/
vmovupd
0xc
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
129
121
16
84
72
24
/
/
vmovupd
0x18
(
%
r8
%
r9
2
)
%
xmm2
.
byte
98
145
125
8
115
92
72
2
4
/
/
vpsrldq
0x4
0x20
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
193
57
115
219
6
/
/
vpsrldq
0x6
%
xmm11
%
xmm8
.
byte
197
169
115
219
6
/
/
vpsrldq
0x6
%
xmm3
%
xmm10
.
byte
197
241
115
218
6
/
/
vpsrldq
0x6
%
xmm2
%
xmm1
.
byte
197
177
115
216
6
/
/
vpsrldq
0x6
%
xmm0
%
xmm9
.
byte
196
193
113
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm1
%
xmm1
.
byte
197
233
97
192
/
/
vpunpcklwd
%
xmm0
%
xmm2
%
xmm0
.
byte
196
193
57
97
210
/
/
vpunpcklwd
%
xmm10
%
xmm8
%
xmm2
.
byte
197
161
97
219
/
/
vpunpcklwd
%
xmm3
%
xmm11
%
xmm3
.
byte
197
97
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm3
%
xmm8
.
byte
197
225
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm3
%
xmm2
.
byte
197
249
97
217
/
/
vpunpcklwd
%
xmm1
%
xmm0
%
xmm3
.
byte
197
249
105
193
/
/
vpunpckhwd
%
xmm1
%
xmm0
%
xmm0
.
byte
197
233
108
208
/
/
vpunpcklqdq
%
xmm0
%
xmm2
%
xmm2
.
byte
197
185
108
195
/
/
vpunpcklqdq
%
xmm3
%
xmm8
%
xmm0
.
byte
197
241
113
240
8
/
/
vpsllw
0x8
%
xmm0
%
xmm1
.
byte
197
249
113
208
8
/
/
vpsrlw
0x8
%
xmm0
%
xmm0
.
byte
197
241
235
192
/
/
vpor
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
127
106
3
0
/
/
vbroadcastss
0x36a7f
(
%
rip
)
%
ymm9
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
185
109
203
/
/
vpunpckhqdq
%
xmm3
%
xmm8
%
xmm1
.
byte
197
225
113
241
8
/
/
vpsllw
0x8
%
xmm1
%
xmm3
.
byte
197
241
113
209
8
/
/
vpsrlw
0x8
%
xmm1
%
xmm1
.
byte
197
225
235
201
/
/
vpor
%
xmm1
%
xmm3
%
xmm1
.
byte
196
226
125
51
201
/
/
vpmovzxwd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
225
113
242
8
/
/
vpsllw
0x8
%
xmm2
%
xmm3
.
byte
197
233
113
210
8
/
/
vpsrlw
0x8
%
xmm2
%
xmm2
.
byte
197
225
235
210
/
/
vpor
%
xmm2
%
xmm3
%
xmm2
.
byte
196
226
125
51
210
/
/
vpmovzxwd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
159
105
3
0
/
/
vbroadcastss
0x3699f
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
110
4
72
/
/
vmovd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
92
72
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
r9
2
)
%
xmm0
%
xmm11
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
31
/
/
jne
5b97
<
_sk_load_rgb_u16_be_skx
+
0x11f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
233
48
255
255
255
/
/
jmpq
5ac7
<
_sk_load_rgb_u16_be_skx
+
0x4f
>
.
byte
196
129
121
110
68
72
6
/
/
vmovd
0x6
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
68
72
10
2
/
/
vpinsrw
0x2
0xa
(
%
r8
%
r9
2
)
%
xmm0
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
48
/
/
jb
5be1
<
_sk_load_rgb_u16_be_skx
+
0x169
>
.
byte
196
129
121
110
68
72
12
/
/
vmovd
0xc
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
92
72
16
2
/
/
vpinsrw
0x2
0x10
(
%
r8
%
r9
2
)
%
xmm0
%
xmm3
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
117
48
/
/
jne
5bfb
<
_sk_load_rgb_u16_be_skx
+
0x183
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
233
230
254
255
255
/
/
jmpq
5ac7
<
_sk_load_rgb_u16_be_skx
+
0x4f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
233
204
254
255
255
/
/
jmpq
5ac7
<
_sk_load_rgb_u16_be_skx
+
0x4f
>
.
byte
196
129
121
110
68
72
18
/
/
vmovd
0x12
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
84
72
22
2
/
/
vpinsrw
0x2
0x16
(
%
r8
%
r9
2
)
%
xmm0
%
xmm10
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
39
/
/
jb
5c3c
<
_sk_load_rgb_u16_be_skx
+
0x1c4
>
.
byte
196
129
121
110
68
72
24
/
/
vmovd
0x18
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
84
72
28
2
/
/
vpinsrw
0x2
0x1c
(
%
r8
%
r9
2
)
%
xmm0
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
117
30
/
/
jne
5c4d
<
_sk_load_rgb_u16_be_skx
+
0x1d5
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
233
139
254
255
255
/
/
jmpq
5ac7
<
_sk_load_rgb_u16_be_skx
+
0x4f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
233
122
254
255
255
/
/
jmpq
5ac7
<
_sk_load_rgb_u16_be_skx
+
0x4f
>
.
byte
196
129
121
110
68
72
30
/
/
vmovd
0x1e
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
76
72
34
2
/
/
vpinsrw
0x2
0x22
(
%
r8
%
r9
2
)
%
xmm0
%
xmm1
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
25
/
/
jb
5c80
<
_sk_load_rgb_u16_be_skx
+
0x208
>
.
byte
196
129
121
110
68
72
36
/
/
vmovd
0x24
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
87
201
/
/
vxorpd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
68
72
40
2
/
/
vpinsrw
0x2
0x28
(
%
r8
%
r9
2
)
%
xmm0
%
xmm0
.
byte
233
71
254
255
255
/
/
jmpq
5ac7
<
_sk_load_rgb_u16_be_skx
+
0x4f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
233
62
254
255
255
/
/
jmpq
5ac7
<
_sk_load_rgb_u16_be_skx
+
0x4f
>
HIDDEN
_sk_store_u16_be_skx
.
globl
_sk_store_u16_be_skx
FUNCTION
(
_sk_store_u16_be_skx
)
_sk_store_u16_be_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
73
104
3
0
/
/
vbroadcastss
0x36849
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
103
105
3
0
/
/
vbroadcastss
0x36967
(
%
rip
)
%
ymm11
#
3c628
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
67
125
57
204
1
/
/
vextracti128
0x1
%
ymm9
%
xmm12
.
byte
196
66
49
43
204
/
/
vpackusdw
%
xmm12
%
xmm9
%
xmm9
.
byte
196
193
25
113
241
8
/
/
vpsllw
0x8
%
xmm9
%
xmm12
.
byte
196
193
49
113
209
8
/
/
vpsrlw
0x8
%
xmm9
%
xmm9
.
byte
196
65
25
235
201
/
/
vpor
%
xmm9
%
xmm12
%
xmm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
67
125
57
229
1
/
/
vextracti128
0x1
%
ymm12
%
xmm13
.
byte
196
66
25
43
229
/
/
vpackusdw
%
xmm13
%
xmm12
%
xmm12
.
byte
196
193
17
113
244
8
/
/
vpsllw
0x8
%
xmm12
%
xmm13
.
byte
196
193
25
113
212
8
/
/
vpsrlw
0x8
%
xmm12
%
xmm12
.
byte
196
65
17
235
228
/
/
vpor
%
xmm12
%
xmm13
%
xmm12
.
byte
197
60
95
234
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm13
.
byte
196
65
20
93
234
/
/
vminps
%
ymm10
%
ymm13
%
ymm13
.
byte
196
65
20
89
235
/
/
vmulps
%
ymm11
%
ymm13
%
ymm13
.
byte
196
65
125
91
237
/
/
vcvtps2dq
%
ymm13
%
ymm13
.
byte
196
67
125
57
238
1
/
/
vextracti128
0x1
%
ymm13
%
xmm14
.
byte
196
66
17
43
238
/
/
vpackusdw
%
xmm14
%
xmm13
%
xmm13
.
byte
196
193
9
113
245
8
/
/
vpsllw
0x8
%
xmm13
%
xmm14
.
byte
196
193
17
113
213
8
/
/
vpsrlw
0x8
%
xmm13
%
xmm13
.
byte
196
65
9
235
237
/
/
vpor
%
xmm13
%
xmm14
%
xmm13
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
67
125
57
194
1
/
/
vextracti128
0x1
%
ymm8
%
xmm10
.
byte
196
66
57
43
194
/
/
vpackusdw
%
xmm10
%
xmm8
%
xmm8
.
byte
196
193
41
113
240
8
/
/
vpsllw
0x8
%
xmm8
%
xmm10
.
byte
196
193
57
113
208
8
/
/
vpsrlw
0x8
%
xmm8
%
xmm8
.
byte
196
65
41
235
192
/
/
vpor
%
xmm8
%
xmm10
%
xmm8
.
byte
196
65
49
97
212
/
/
vpunpcklwd
%
xmm12
%
xmm9
%
xmm10
.
byte
196
65
49
105
228
/
/
vpunpckhwd
%
xmm12
%
xmm9
%
xmm12
.
byte
196
65
17
97
200
/
/
vpunpcklwd
%
xmm8
%
xmm13
%
xmm9
.
byte
196
65
17
105
192
/
/
vpunpckhwd
%
xmm8
%
xmm13
%
xmm8
.
byte
196
65
41
98
217
/
/
vpunpckldq
%
xmm9
%
xmm10
%
xmm11
.
byte
196
65
41
106
209
/
/
vpunpckhdq
%
xmm9
%
xmm10
%
xmm10
.
byte
196
65
25
98
200
/
/
vpunpckldq
%
xmm8
%
xmm12
%
xmm9
.
byte
196
65
25
106
192
/
/
vpunpckhdq
%
xmm8
%
xmm12
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
31
/
/
jne
5dc0
<
_sk_store_u16_be_skx
+
0x137
>
.
byte
196
1
122
127
28
65
/
/
vmovdqu
%
xmm11
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
84
65
16
/
/
vmovdqu
%
xmm10
0x10
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
76
65
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
68
65
48
/
/
vmovdqu
%
xmm8
0x30
(
%
r9
%
r8
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
121
214
28
65
/
/
vmovq
%
xmm11
(
%
r9
%
r8
2
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
5dbc
<
_sk_store_u16_be_skx
+
0x133
>
.
byte
196
1
121
23
92
65
8
/
/
vmovhpd
%
xmm11
0x8
(
%
r9
%
r8
2
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
5dbc
<
_sk_store_u16_be_skx
+
0x133
>
.
byte
196
1
121
214
84
65
16
/
/
vmovq
%
xmm10
0x10
(
%
r9
%
r8
2
)
.
byte
116
218
/
/
je
5dbc
<
_sk_store_u16_be_skx
+
0x133
>
.
byte
196
1
121
23
84
65
24
/
/
vmovhpd
%
xmm10
0x18
(
%
r9
%
r8
2
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
5dbc
<
_sk_store_u16_be_skx
+
0x133
>
.
byte
196
1
121
214
76
65
32
/
/
vmovq
%
xmm9
0x20
(
%
r9
%
r8
2
)
.
byte
116
196
/
/
je
5dbc
<
_sk_store_u16_be_skx
+
0x133
>
.
byte
196
1
121
23
76
65
40
/
/
vmovhpd
%
xmm9
0x28
(
%
r9
%
r8
2
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
183
/
/
jb
5dbc
<
_sk_store_u16_be_skx
+
0x133
>
.
byte
196
1
121
214
68
65
48
/
/
vmovq
%
xmm8
0x30
(
%
r9
%
r8
2
)
.
byte
235
174
/
/
jmp
5dbc
<
_sk_store_u16_be_skx
+
0x133
>
HIDDEN
_sk_load_f32_skx
.
globl
_sk_load_f32_skx
FUNCTION
(
_sk_load_f32_skx
)
_sk_load_f32_skx
:
.
byte
98
225
253
40
40
223
/
/
vmovapd
%
ymm7
%
ymm19
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
253
87
192
/
/
vxorpd
%
ymm0
%
ymm0
%
ymm0
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
135
57
1
0
0
/
/
ja
5f5d
<
_sk_load_f32_skx
+
0x14f
>
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
76
141
21
46
1
0
0
/
/
lea
0x12e
(
%
rip
)
%
r10
#
5f70
<
_sk_load_f32_skx
+
0x162
>
.
byte
73
99
4
186
/
/
movslq
(
%
r10
%
rdi
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
237
87
210
/
/
vxorpd
%
ymm2
%
ymm2
%
ymm2
.
byte
98
161
253
32
87
192
/
/
vxorpd
%
ymm16
%
ymm16
%
ymm16
.
byte
196
65
29
87
228
/
/
vxorpd
%
ymm12
%
ymm12
%
ymm12
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
37
87
219
/
/
vxorpd
%
ymm11
%
ymm11
%
ymm11
.
byte
98
161
237
32
87
210
/
/
vxorpd
%
ymm18
%
ymm18
%
ymm18
.
byte
196
65
5
87
255
/
/
vxorpd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
13
87
246
/
/
vxorpd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
21
87
237
/
/
vxorpd
%
ymm13
%
ymm13
%
ymm13
.
byte
98
161
245
32
87
201
/
/
vxorpd
%
ymm17
%
ymm17
%
ymm17
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
16
68
129
112
/
/
vmovupd
0x70
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
6
192
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm0
.
byte
196
129
121
16
76
129
96
/
/
vmovupd
0x60
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
249
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm7
.
byte
196
129
121
16
76
129
80
/
/
vmovupd
0x50
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
209
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm2
.
byte
197
125
40
215
/
/
vmovapd
%
ymm7
%
ymm10
.
byte
196
129
121
16
76
129
64
/
/
vmovupd
0x40
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
217
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm3
.
byte
98
193
253
40
40
194
/
/
vmovapd
%
ymm10
%
ymm16
.
byte
197
125
40
226
/
/
vmovapd
%
ymm2
%
ymm12
.
byte
196
129
121
16
76
129
48
/
/
vmovupd
0x30
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
117
13
192
12
/
/
vblendpd
0xc
%
ymm0
%
ymm1
%
ymm0
.
byte
98
49
253
40
40
216
/
/
vmovapd
%
ymm16
%
ymm11
.
byte
98
193
253
40
40
212
/
/
vmovapd
%
ymm12
%
ymm18
.
byte
197
125
40
251
/
/
vmovapd
%
ymm3
%
ymm15
.
byte
196
129
121
16
76
129
32
/
/
vmovupd
0x20
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
67
117
13
243
12
/
/
vblendpd
0xc
%
ymm11
%
ymm1
%
ymm14
.
byte
98
49
253
40
40
234
/
/
vmovapd
%
ymm18
%
ymm13
.
byte
98
193
253
40
40
207
/
/
vmovapd
%
ymm15
%
ymm17
.
byte
196
129
121
16
76
129
16
/
/
vmovupd
0x10
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
67
117
13
205
12
/
/
vblendpd
0xc
%
ymm13
%
ymm1
%
ymm9
.
byte
196
65
125
40
198
/
/
vmovapd
%
ymm14
%
ymm8
.
byte
98
177
253
40
40
201
/
/
vmovapd
%
ymm17
%
ymm1
.
byte
196
129
121
16
20
129
/
/
vmovupd
(
%
r9
%
r8
4
)
%
xmm2
.
byte
196
227
109
13
201
12
/
/
vblendpd
0xc
%
ymm1
%
ymm2
%
ymm1
.
byte
196
193
116
20
209
/
/
vunpcklps
%
ymm9
%
ymm1
%
ymm2
.
byte
196
193
116
21
217
/
/
vunpckhps
%
ymm9
%
ymm1
%
ymm3
.
byte
197
188
20
200
/
/
vunpcklps
%
ymm0
%
ymm8
%
ymm1
.
byte
197
188
21
248
/
/
vunpckhps
%
ymm0
%
ymm8
%
ymm7
.
byte
197
237
20
193
/
/
vunpcklpd
%
ymm1
%
ymm2
%
ymm0
.
byte
197
237
21
201
/
/
vunpckhpd
%
ymm1
%
ymm2
%
ymm1
.
byte
197
229
20
215
/
/
vunpcklpd
%
ymm7
%
ymm3
%
ymm2
.
byte
197
229
21
223
/
/
vunpckhpd
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
177
124
40
40
251
/
/
vmovaps
%
ymm19
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
235
196
/
/
jmp
5f31
<
_sk_load_f32_skx
+
0x123
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
37
255
255
255
181
/
/
and
0xb5ffffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
157
255
255
255
132
/
/
lcall
*
-
0x7b000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
103
255
/
/
jmpq
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
80
255
/
/
callq
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
63
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
50
/
/
pushq
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_f32_dst_skx
.
globl
_sk_load_f32_dst_skx
FUNCTION
(
_sk_load_f32_dst_skx
)
_sk_load_f32_dst_skx
:
.
byte
98
225
253
40
40
219
/
/
vmovapd
%
ymm3
%
ymm19
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
221
87
228
/
/
vxorpd
%
ymm4
%
ymm4
%
ymm4
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
135
59
1
0
0
/
/
ja
60e1
<
_sk_load_f32_dst_skx
+
0x151
>
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
76
141
21
48
1
0
0
/
/
lea
0x130
(
%
rip
)
%
r10
#
60f4
<
_sk_load_f32_dst_skx
+
0x164
>
.
byte
73
99
4
186
/
/
movslq
(
%
r10
%
rdi
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
205
87
246
/
/
vxorpd
%
ymm6
%
ymm6
%
ymm6
.
byte
98
161
253
32
87
192
/
/
vxorpd
%
ymm16
%
ymm16
%
ymm16
.
byte
196
65
29
87
228
/
/
vxorpd
%
ymm12
%
ymm12
%
ymm12
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
196
65
37
87
219
/
/
vxorpd
%
ymm11
%
ymm11
%
ymm11
.
byte
98
161
237
32
87
210
/
/
vxorpd
%
ymm18
%
ymm18
%
ymm18
.
byte
196
65
5
87
255
/
/
vxorpd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
13
87
246
/
/
vxorpd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
21
87
237
/
/
vxorpd
%
ymm13
%
ymm13
%
ymm13
.
byte
98
161
245
32
87
201
/
/
vxorpd
%
ymm17
%
ymm17
%
ymm17
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
16
92
129
112
/
/
vmovupd
0x70
(
%
r9
%
r8
4
)
%
xmm3
.
byte
196
227
125
6
227
40
/
/
vperm2f128
0x28
%
ymm3
%
ymm0
%
ymm4
.
byte
196
129
121
16
92
129
96
/
/
vmovupd
0x60
(
%
r9
%
r8
4
)
%
xmm3
.
byte
196
227
125
6
219
40
/
/
vperm2f128
0x28
%
ymm3
%
ymm0
%
ymm3
.
byte
196
129
121
16
108
129
80
/
/
vmovupd
0x50
(
%
r9
%
r8
4
)
%
xmm5
.
byte
196
227
125
6
245
40
/
/
vperm2f128
0x28
%
ymm5
%
ymm0
%
ymm6
.
byte
197
125
40
211
/
/
vmovapd
%
ymm3
%
ymm10
.
byte
196
129
121
16
92
129
64
/
/
vmovupd
0x40
(
%
r9
%
r8
4
)
%
xmm3
.
byte
196
227
125
6
251
40
/
/
vperm2f128
0x28
%
ymm3
%
ymm0
%
ymm7
.
byte
98
193
253
40
40
194
/
/
vmovapd
%
ymm10
%
ymm16
.
byte
197
125
40
230
/
/
vmovapd
%
ymm6
%
ymm12
.
byte
196
129
121
16
92
129
48
/
/
vmovupd
0x30
(
%
r9
%
r8
4
)
%
xmm3
.
byte
196
227
101
13
228
12
/
/
vblendpd
0xc
%
ymm4
%
ymm3
%
ymm4
.
byte
98
49
253
40
40
216
/
/
vmovapd
%
ymm16
%
ymm11
.
byte
98
193
253
40
40
212
/
/
vmovapd
%
ymm12
%
ymm18
.
byte
197
125
40
255
/
/
vmovapd
%
ymm7
%
ymm15
.
byte
196
129
121
16
92
129
32
/
/
vmovupd
0x20
(
%
r9
%
r8
4
)
%
xmm3
.
byte
196
67
101
13
243
12
/
/
vblendpd
0xc
%
ymm11
%
ymm3
%
ymm14
.
byte
98
49
253
40
40
234
/
/
vmovapd
%
ymm18
%
ymm13
.
byte
98
193
253
40
40
207
/
/
vmovapd
%
ymm15
%
ymm17
.
byte
196
129
121
16
92
129
16
/
/
vmovupd
0x10
(
%
r9
%
r8
4
)
%
xmm3
.
byte
196
67
101
13
205
12
/
/
vblendpd
0xc
%
ymm13
%
ymm3
%
ymm9
.
byte
196
65
125
40
198
/
/
vmovapd
%
ymm14
%
ymm8
.
byte
98
177
253
40
40
233
/
/
vmovapd
%
ymm17
%
ymm5
.
byte
196
129
121
16
28
129
/
/
vmovupd
(
%
r9
%
r8
4
)
%
xmm3
.
byte
196
227
101
13
237
12
/
/
vblendpd
0xc
%
ymm5
%
ymm3
%
ymm5
.
byte
196
193
84
20
217
/
/
vunpcklps
%
ymm9
%
ymm5
%
ymm3
.
byte
196
193
84
21
249
/
/
vunpckhps
%
ymm9
%
ymm5
%
ymm7
.
byte
197
188
20
236
/
/
vunpcklps
%
ymm4
%
ymm8
%
ymm5
.
byte
197
60
21
196
/
/
vunpckhps
%
ymm4
%
ymm8
%
ymm8
.
byte
197
229
20
229
/
/
vunpcklpd
%
ymm5
%
ymm3
%
ymm4
.
byte
197
229
21
237
/
/
vunpckhpd
%
ymm5
%
ymm3
%
ymm5
.
byte
196
193
69
20
240
/
/
vunpcklpd
%
ymm8
%
ymm7
%
ymm6
.
byte
196
193
69
21
248
/
/
vunpckhpd
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
177
124
40
40
219
/
/
vmovaps
%
ymm19
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
235
194
/
/
jmp
60b3
<
_sk_load_f32_dst_skx
+
0x123
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
35
255
/
/
and
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
179
255
255
255
155
/
/
pushq
-
0x64000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
130
255
255
255
101
/
/
incl
0x65ffffff
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
78
255
/
/
decl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
61
255
255
255
48
/
/
cmp
0x30ffffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_f32_skx
.
globl
_sk_store_f32_skx
FUNCTION
(
_sk_store_f32_skx
)
_sk_store_f32_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
141
12
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r9
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
197
124
20
193
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm8
.
byte
197
124
21
217
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm11
.
byte
197
108
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm9
.
byte
197
108
21
227
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm12
.
byte
196
65
61
20
209
/
/
vunpcklpd
%
ymm9
%
ymm8
%
ymm10
.
byte
196
65
61
21
201
/
/
vunpckhpd
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
37
20
196
/
/
vunpcklpd
%
ymm12
%
ymm11
%
ymm8
.
byte
196
65
37
21
220
/
/
vunpckhpd
%
ymm12
%
ymm11
%
ymm11
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
55
/
/
jne
618d
<
_sk_store_f32_skx
+
0x79
>
.
byte
196
67
45
24
225
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm12
.
byte
196
67
61
24
235
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm8
%
ymm13
.
byte
196
67
45
6
201
49
/
/
vperm2f128
0x31
%
ymm9
%
ymm10
%
ymm9
.
byte
196
67
61
6
195
49
/
/
vperm2f128
0x31
%
ymm11
%
ymm8
%
ymm8
.
byte
196
1
125
17
36
136
/
/
vmovupd
%
ymm12
(
%
r8
%
r9
4
)
.
byte
196
1
125
17
108
136
32
/
/
vmovupd
%
ymm13
0x20
(
%
r8
%
r9
4
)
.
byte
196
1
124
17
76
136
64
/
/
vmovups
%
ymm9
0x40
(
%
r8
%
r9
4
)
.
byte
196
1
125
17
68
136
96
/
/
vmovupd
%
ymm8
0x60
(
%
r8
%
r9
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
121
17
20
136
/
/
vmovupd
%
xmm10
(
%
r8
%
r9
4
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
6189
<
_sk_store_f32_skx
+
0x75
>
.
byte
196
1
121
17
76
136
16
/
/
vmovupd
%
xmm9
0x10
(
%
r8
%
r9
4
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
6189
<
_sk_store_f32_skx
+
0x75
>
.
byte
196
1
121
17
68
136
32
/
/
vmovupd
%
xmm8
0x20
(
%
r8
%
r9
4
)
.
byte
116
218
/
/
je
6189
<
_sk_store_f32_skx
+
0x75
>
.
byte
196
1
121
17
92
136
48
/
/
vmovupd
%
xmm11
0x30
(
%
r8
%
r9
4
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
6189
<
_sk_store_f32_skx
+
0x75
>
.
byte
196
3
125
25
84
136
64
1
/
/
vextractf128
0x1
%
ymm10
0x40
(
%
r8
%
r9
4
)
.
byte
116
195
/
/
je
6189
<
_sk_store_f32_skx
+
0x75
>
.
byte
196
3
125
25
76
136
80
1
/
/
vextractf128
0x1
%
ymm9
0x50
(
%
r8
%
r9
4
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
181
/
/
jb
6189
<
_sk_store_f32_skx
+
0x75
>
.
byte
196
3
125
25
68
136
96
1
/
/
vextractf128
0x1
%
ymm8
0x60
(
%
r8
%
r9
4
)
.
byte
235
171
/
/
jmp
6189
<
_sk_store_f32_skx
+
0x75
>
HIDDEN
_sk_repeat_x_skx
.
globl
_sk_repeat_x_skx
FUNCTION
(
_sk_repeat_x_skx
)
_sk_repeat_x_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
113
124
56
89
64
1
/
/
vmulps
0x4
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm8
.
byte
196
67
125
8
192
1
/
/
vroundps
0x1
%
ymm8
%
ymm8
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
194
61
188
193
/
/
vfnmadd231ps
%
ymm9
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_y_skx
.
globl
_sk_repeat_y_skx
FUNCTION
(
_sk_repeat_y_skx
)
_sk_repeat_y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
113
116
56
89
64
1
/
/
vmulps
0x4
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm8
.
byte
196
67
125
8
192
1
/
/
vroundps
0x1
%
ymm8
%
ymm8
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
194
61
188
201
/
/
vfnmadd231ps
%
ymm9
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_skx
.
globl
_sk_mirror_x_skx
FUNCTION
(
_sk_mirror_x_skx
)
_sk_mirror_x_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
72
4
/
/
vmovss
0x4
(
%
rax
)
%
xmm9
.
byte
196
66
125
24
208
/
/
vbroadcastss
%
xmm8
%
ymm10
.
byte
196
65
124
92
218
/
/
vsubps
%
ymm10
%
ymm0
%
ymm11
.
byte
196
193
58
88
192
/
/
vaddss
%
xmm8
%
xmm8
%
xmm0
.
byte
196
98
125
24
192
/
/
vbroadcastss
%
xmm0
%
ymm8
.
byte
197
178
89
5
185
98
3
0
/
/
vmulss
0x362b9
(
%
rip
)
%
xmm9
%
xmm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
226
125
24
192
/
/
vbroadcastss
%
xmm0
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
196
227
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm0
.
byte
196
194
61
172
195
/
/
vfnmadd213ps
%
ymm11
%
ymm8
%
ymm0
.
byte
196
193
124
92
194
/
/
vsubps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_y_skx
.
globl
_sk_mirror_y_skx
FUNCTION
(
_sk_mirror_y_skx
)
_sk_mirror_y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
72
4
/
/
vmovss
0x4
(
%
rax
)
%
xmm9
.
byte
196
66
125
24
208
/
/
vbroadcastss
%
xmm8
%
ymm10
.
byte
196
65
116
92
218
/
/
vsubps
%
ymm10
%
ymm1
%
ymm11
.
byte
196
193
58
88
200
/
/
vaddss
%
xmm8
%
xmm8
%
xmm1
.
byte
196
98
125
24
193
/
/
vbroadcastss
%
xmm1
%
ymm8
.
byte
197
178
89
13
104
98
3
0
/
/
vmulss
0x36268
(
%
rip
)
%
xmm9
%
xmm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
226
125
24
201
/
/
vbroadcastss
%
xmm1
%
ymm1
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
196
227
125
8
201
1
/
/
vroundps
0x1
%
ymm1
%
ymm1
.
byte
196
194
61
172
203
/
/
vfnmadd213ps
%
ymm11
%
ymm8
%
ymm1
.
byte
196
193
116
92
202
/
/
vsubps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
193
/
/
vsubps
%
ymm1
%
ymm8
%
ymm8
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_x_1_skx
.
globl
_sk_clamp_x_1_skx
FUNCTION
(
_sk_clamp_x_1_skx
)
_sk_clamp_x_1_skx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
98
241
124
56
93
5
47
98
3
0
/
/
vminps
0x3622f
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_skx
.
globl
_sk_repeat_x_1_skx
FUNCTION
(
_sk_repeat_x_1_skx
)
_sk_repeat_x_1_skx
:
.
byte
196
99
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
98
241
124
56
93
5
13
98
3
0
/
/
vminps
0x3620d
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_skx
.
globl
_sk_mirror_x_1_skx
FUNCTION
(
_sk_mirror_x_1_skx
)
_sk_mirror_x_1_skx
:
.
byte
196
98
125
24
5
24
98
3
0
/
/
vbroadcastss
0x36218
(
%
rip
)
%
ymm8
#
3c514
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c8
>
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
98
113
124
56
89
13
237
97
3
0
/
/
vmulps
0x361ed
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
67
125
8
201
1
/
/
vroundps
0x1
%
ymm9
%
ymm9
.
byte
196
65
52
88
201
/
/
vaddps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
193
124
92
193
/
/
vsubps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
200
/
/
vsubps
%
ymm0
%
ymm8
%
ymm9
.
byte
197
180
84
192
/
/
vandps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
98
241
124
56
93
5
193
97
3
0
/
/
vminps
0x361c1
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_skx
.
globl
_sk_decal_x_skx
FUNCTION
(
_sk_decal_x_skx
)
_sk_decal_x_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
98
241
60
40
194
200
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
k1
.
byte
98
241
124
57
194
64
16
1
/
/
vcmpltps
0x40
(
%
rax
)
{
1to8
}
%
ymm0
%
k0
{
%
k1
}
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_skx
.
globl
_sk_decal_y_skx
FUNCTION
(
_sk_decal_y_skx
)
_sk_decal_y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
98
241
60
40
194
201
2
/
/
vcmpleps
%
ymm1
%
ymm8
%
k1
.
byte
98
241
116
57
194
64
17
1
/
/
vcmpltps
0x44
(
%
rax
)
{
1to8
}
%
ymm1
%
k0
{
%
k1
}
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_skx
.
globl
_sk_decal_x_and_y_skx
FUNCTION
(
_sk_decal_x_and_y_skx
)
_sk_decal_x_and_y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
98
241
60
40
194
201
2
/
/
vcmpleps
%
ymm1
%
ymm8
%
k1
.
byte
98
241
60
41
194
200
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
k1
{
%
k1
}
.
byte
98
241
124
57
194
72
16
1
/
/
vcmpltps
0x40
(
%
rax
)
{
1to8
}
%
ymm0
%
k1
{
%
k1
}
.
byte
98
241
116
57
194
64
17
1
/
/
vcmpltps
0x44
(
%
rax
)
{
1to8
}
%
ymm1
%
k0
{
%
k1
}
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_skx
.
globl
_sk_check_decal_mask_skx
FUNCTION
(
_sk_check_decal_mask_skx
)
_sk_check_decal_mask_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
0
/
/
vmovups
(
%
rax
)
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
84
210
/
/
vandps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
84
219
/
/
vandps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminance_to_alpha_skx
.
globl
_sk_luminance_to_alpha_skx
FUNCTION
(
_sk_luminance_to_alpha_skx
)
_sk_luminance_to_alpha_skx
:
.
byte
98
241
116
56
89
29
45
98
3
0
/
/
vmulps
0x3622d
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm3
#
3c62c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e0
>
.
byte
98
242
125
56
184
29
39
98
3
0
/
/
vfmadd231ps
0x36227
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm3
#
3c630
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e4
>
.
byte
98
242
109
56
184
29
33
98
3
0
/
/
vfmadd231ps
0x36221
(
%
rip
)
{
1to8
}
%
ymm2
%
ymm3
#
3c634
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e8
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_skx
.
globl
_sk_matrix_translate_skx
FUNCTION
(
_sk_matrix_translate_skx
)
_sk_matrix_translate_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
241
124
56
88
0
/
/
vaddps
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm0
.
byte
98
241
116
56
88
72
1
/
/
vaddps
0x4
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_skx
.
globl
_sk_matrix_scale_translate_skx
FUNCTION
(
_sk_matrix_scale_translate_skx
)
_sk_matrix_scale_translate_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
98
242
61
56
168
64
2
/
/
vfmadd213ps
0x8
(
%
rax
)
{
1to8
}
%
ymm8
%
ymm0
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
98
242
61
56
168
72
3
/
/
vfmadd213ps
0xc
(
%
rax
)
{
1to8
}
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_skx
.
globl
_sk_matrix_2x3_skx
FUNCTION
(
_sk_matrix_2x3_skx
)
_sk_matrix_2x3_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
64
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm8
.
byte
98
114
117
56
168
64
4
/
/
vfmadd213ps
0x10
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm8
.
byte
196
66
125
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
98
114
117
56
168
72
5
/
/
vfmadd213ps
0x14
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm9
.
byte
196
66
125
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_3x4_skx
.
globl
_sk_matrix_3x4_skx
FUNCTION
(
_sk_matrix_3x4_skx
)
_sk_matrix_3x4_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
64
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm8
.
byte
98
114
109
56
168
64
9
/
/
vfmadd213ps
0x24
(
%
rax
)
{
1to8
}
%
ymm2
%
ymm8
.
byte
196
66
117
184
194
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm8
.
byte
196
66
125
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
72
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm9
.
byte
98
114
109
56
168
72
10
/
/
vfmadd213ps
0x28
(
%
rax
)
{
1to8
}
%
ymm2
%
ymm9
.
byte
196
66
117
184
203
/
/
vfmadd231ps
%
ymm11
%
ymm1
%
ymm9
.
byte
196
66
125
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm9
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
80
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm10
.
byte
98
114
109
56
168
80
11
/
/
vfmadd213ps
0x2c
(
%
rax
)
{
1to8
}
%
ymm2
%
ymm10
.
byte
196
66
117
184
212
/
/
vfmadd231ps
%
ymm12
%
ymm1
%
ymm10
.
byte
196
66
125
184
211
/
/
vfmadd231ps
%
ymm11
%
ymm0
%
ymm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
197
124
41
210
/
/
vmovaps
%
ymm10
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x5_skx
.
globl
_sk_matrix_4x5_skx
FUNCTION
(
_sk_matrix_4x5_skx
)
_sk_matrix_4x5_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
64
48
/
/
vbroadcastss
0x30
(
%
rax
)
%
ymm8
.
byte
98
114
101
56
168
64
16
/
/
vfmadd213ps
0x40
(
%
rax
)
{
1to8
}
%
ymm3
%
ymm8
.
byte
196
66
109
184
195
/
/
vfmadd231ps
%
ymm11
%
ymm2
%
ymm8
.
byte
196
66
117
184
194
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm8
.
byte
196
66
125
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
36
/
/
vbroadcastss
0x24
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
72
52
/
/
vbroadcastss
0x34
(
%
rax
)
%
ymm9
.
byte
98
114
101
56
168
72
17
/
/
vfmadd213ps
0x44
(
%
rax
)
{
1to8
}
%
ymm3
%
ymm9
.
byte
196
66
109
184
204
/
/
vfmadd231ps
%
ymm12
%
ymm2
%
ymm9
.
byte
196
66
117
184
203
/
/
vfmadd231ps
%
ymm11
%
ymm1
%
ymm9
.
byte
196
66
125
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm9
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
40
/
/
vbroadcastss
0x28
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
80
56
/
/
vbroadcastss
0x38
(
%
rax
)
%
ymm10
.
byte
98
114
101
56
168
80
18
/
/
vfmadd213ps
0x48
(
%
rax
)
{
1to8
}
%
ymm3
%
ymm10
.
byte
196
66
109
184
213
/
/
vfmadd231ps
%
ymm13
%
ymm2
%
ymm10
.
byte
196
66
117
184
212
/
/
vfmadd231ps
%
ymm12
%
ymm1
%
ymm10
.
byte
196
66
125
184
211
/
/
vfmadd231ps
%
ymm11
%
ymm0
%
ymm10
.
byte
196
98
125
24
96
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
112
44
/
/
vbroadcastss
0x2c
(
%
rax
)
%
ymm14
.
byte
196
98
125
24
88
60
/
/
vbroadcastss
0x3c
(
%
rax
)
%
ymm11
.
byte
98
114
101
56
168
88
19
/
/
vfmadd213ps
0x4c
(
%
rax
)
{
1to8
}
%
ymm3
%
ymm11
.
byte
196
66
109
184
222
/
/
vfmadd231ps
%
ymm14
%
ymm2
%
ymm11
.
byte
196
66
117
184
221
/
/
vfmadd231ps
%
ymm13
%
ymm1
%
ymm11
.
byte
196
66
125
184
220
/
/
vfmadd231ps
%
ymm12
%
ymm0
%
ymm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
197
124
41
210
/
/
vmovaps
%
ymm10
%
ymm2
.
byte
197
124
41
219
/
/
vmovaps
%
ymm11
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x3_skx
.
globl
_sk_matrix_4x3_skx
FUNCTION
(
_sk_matrix_4x3_skx
)
_sk_matrix_4x3_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm2
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
98
114
117
56
168
64
8
/
/
vfmadd213ps
0x20
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm8
.
byte
196
98
125
184
194
/
/
vfmadd231ps
%
ymm2
%
ymm0
%
ymm8
.
byte
196
226
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm2
.
byte
196
98
125
24
72
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm9
.
byte
98
114
117
56
168
72
9
/
/
vfmadd213ps
0x24
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm9
.
byte
196
98
125
184
202
/
/
vfmadd231ps
%
ymm2
%
ymm0
%
ymm9
.
byte
196
226
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm3
.
byte
196
226
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm2
.
byte
98
242
117
56
168
80
10
/
/
vfmadd213ps
0x28
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm2
.
byte
196
226
125
184
211
/
/
vfmadd231ps
%
ymm3
%
ymm0
%
ymm2
.
byte
196
98
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm10
.
byte
196
226
125
24
88
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm3
.
byte
98
242
117
56
168
88
11
/
/
vfmadd213ps
0x2c
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm3
.
byte
196
194
125
184
218
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_skx
.
globl
_sk_matrix_perspective_skx
FUNCTION
(
_sk_matrix_perspective_skx
)
_sk_matrix_perspective_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm9
.
byte
98
114
117
56
168
72
2
/
/
vfmadd213ps
0x8
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm9
.
byte
196
66
125
184
200
/
/
vfmadd231ps
%
ymm8
%
ymm0
%
ymm9
.
byte
196
98
125
24
64
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm10
.
byte
98
114
117
56
168
80
5
/
/
vfmadd213ps
0x14
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm10
.
byte
196
66
125
184
208
/
/
vfmadd231ps
%
ymm8
%
ymm0
%
ymm10
.
byte
196
98
125
24
64
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
88
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm11
.
byte
98
114
117
56
168
88
8
/
/
vfmadd213ps
0x20
(
%
rax
)
{
1to8
}
%
ymm1
%
ymm11
.
byte
196
66
125
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm0
%
ymm11
.
byte
98
210
125
40
76
203
/
/
vrcp14ps
%
ymm11
%
ymm1
.
byte
197
180
89
193
/
/
vmulps
%
ymm1
%
ymm9
%
ymm0
.
byte
197
172
89
201
/
/
vmulps
%
ymm1
%
ymm10
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_skx
.
globl
_sk_evenly_spaced_gradient_skx
FUNCTION
(
_sk_evenly_spaced_gradient_skx
)
_sk_evenly_spaced_gradient_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
8
/
/
mov
(
%
rax
)
%
r9
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
77
141
81
255
/
/
lea
-
0x1
(
%
r9
)
%
r10
.
byte
98
210
253
72
124
202
/
/
vpbroadcastq
%
r10
%
zmm1
.
byte
98
241
255
72
122
201
/
/
vcvtuqq2ps
%
zmm1
%
ymm1
.
byte
197
244
89
200
/
/
vmulps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
126
91
209
/
/
vcvttps2dq
%
ymm1
%
ymm10
.
byte
73
131
249
8
/
/
cmp
0x8
%
r9
.
byte
119
73
/
/
ja
6711
<
_sk_evenly_spaced_gradient_skx
+
0x70
>
.
byte
196
66
45
22
0
/
/
vpermps
(
%
r8
)
%
ymm10
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
66
45
22
8
/
/
vpermps
(
%
r8
)
%
ymm10
%
ymm9
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
196
194
45
22
8
/
/
vpermps
(
%
r8
)
%
ymm10
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
196
66
45
22
24
/
/
vpermps
(
%
r8
)
%
ymm10
%
ymm11
.
byte
196
194
45
22
17
/
/
vpermps
(
%
r9
)
%
ymm10
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
196
66
45
22
32
/
/
vpermps
(
%
r8
)
%
ymm10
%
ymm12
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
194
45
22
24
/
/
vpermps
(
%
r8
)
%
ymm10
%
ymm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
98
45
22
40
/
/
vpermps
(
%
rax
)
%
ymm10
%
ymm13
.
byte
233
147
0
0
0
/
/
jmpq
67a4
<
_sk_evenly_spaced_gradient_skx
+
0x103
>
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
2
117
146
4
144
/
/
vgatherdps
%
ymm1
(
%
r8
%
ymm10
4
)
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
2
117
146
12
144
/
/
vgatherdps
%
ymm1
(
%
r8
%
ymm10
4
)
%
ymm9
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
130
109
146
12
144
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm10
4
)
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
2
109
146
28
144
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm10
4
)
%
ymm11
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
130
101
146
20
145
/
/
vgatherdps
%
ymm3
(
%
r9
%
ymm10
4
)
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
196
2
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
r8
%
ymm10
4
)
%
ymm12
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
130
21
146
28
144
/
/
vgatherdps
%
ymm13
(
%
r8
%
ymm10
4
)
%
ymm3
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
34
13
146
44
144
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm10
4
)
%
ymm13
.
byte
196
66
125
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
194
125
168
203
/
/
vfmadd213ps
%
ymm11
%
ymm0
%
ymm1
.
byte
196
194
125
168
212
/
/
vfmadd213ps
%
ymm12
%
ymm0
%
ymm2
.
byte
196
194
125
168
221
/
/
vfmadd213ps
%
ymm13
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_skx
.
globl
_sk_gradient_skx
FUNCTION
(
_sk_gradient_skx
)
_sk_gradient_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
73
131
248
1
/
/
cmp
0x1
%
r8
.
byte
15
134
229
0
0
0
/
/
jbe
68b4
<
_sk_gradient_skx
+
0xf4
>
.
byte
76
139
72
72
/
/
mov
0x48
(
%
rax
)
%
r9
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
65
186
1
0
0
0
/
/
mov
0x1
%
r10d
.
byte
196
226
125
24
21
26
93
3
0
/
/
vbroadcastss
0x35d1a
(
%
rip
)
%
ymm2
#
3c500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b4
>
.
byte
196
65
53
239
201
/
/
vpxor
%
ymm9
%
ymm9
%
ymm9
.
byte
196
130
125
24
28
145
/
/
vbroadcastss
(
%
r9
%
r10
4
)
%
ymm3
.
byte
98
241
100
40
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm3
%
k0
.
byte
98
242
126
40
56
216
/
/
vpmovm2d
%
k0
%
ymm3
.
byte
196
227
117
74
218
48
/
/
vblendvps
%
ymm3
%
ymm2
%
ymm1
%
ymm3
.
byte
197
53
254
203
/
/
vpaddd
%
ymm3
%
ymm9
%
ymm9
.
byte
73
131
194
1
/
/
add
0x1
%
r10
.
byte
77
57
208
/
/
cmp
%
r10
%
r8
.
byte
117
218
/
/
jne
67eb
<
_sk_gradient_skx
+
0x2b
>
.
byte
76
139
72
8
/
/
mov
0x8
(
%
rax
)
%
r9
.
byte
73
131
248
8
/
/
cmp
0x8
%
r8
.
byte
15
134
158
0
0
0
/
/
jbe
68bd
<
_sk_gradient_skx
+
0xfd
>
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
2
117
146
4
137
/
/
vgatherdps
%
ymm1
(
%
r9
%
ymm9
4
)
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
2
117
146
20
136
/
/
vgatherdps
%
ymm1
(
%
r8
%
ymm9
4
)
%
ymm10
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
130
109
146
12
136
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm9
4
)
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
2
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm9
4
)
%
ymm11
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
130
101
146
20
137
/
/
vgatherdps
%
ymm3
(
%
r9
%
ymm9
4
)
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
196
2
101
146
36
136
/
/
vgatherdps
%
ymm3
(
%
r8
%
ymm9
4
)
%
ymm12
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
130
21
146
28
136
/
/
vgatherdps
%
ymm13
(
%
r8
%
ymm9
4
)
%
ymm3
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
34
13
146
44
136
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm9
4
)
%
ymm13
.
byte
235
77
/
/
jmp
6901
<
_sk_gradient_skx
+
0x141
>
.
byte
76
139
72
8
/
/
mov
0x8
(
%
rax
)
%
r9
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
22
1
/
/
vpermps
(
%
r9
)
%
ymm9
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
66
53
22
16
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm10
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
196
194
53
22
8
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
196
66
53
22
24
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm11
.
byte
196
194
53
22
17
/
/
vpermps
(
%
r9
)
%
ymm9
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
196
66
53
22
32
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm12
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
194
53
22
24
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
98
53
22
40
/
/
vpermps
(
%
rax
)
%
ymm9
%
ymm13
.
byte
196
66
125
168
194
/
/
vfmadd213ps
%
ymm10
%
ymm0
%
ymm8
.
byte
196
194
125
168
203
/
/
vfmadd213ps
%
ymm11
%
ymm0
%
ymm1
.
byte
196
194
125
168
212
/
/
vfmadd213ps
%
ymm12
%
ymm0
%
ymm2
.
byte
196
194
125
168
221
/
/
vfmadd213ps
%
ymm13
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_skx
.
globl
_sk_evenly_spaced_2_stop_gradient_skx
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_skx
)
_sk_evenly_spaced_2_stop_gradient_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
98
114
125
56
168
64
4
/
/
vfmadd213ps
0x10
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm8
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
98
242
125
56
168
72
5
/
/
vfmadd213ps
0x14
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
98
242
125
56
168
80
6
/
/
vfmadd213ps
0x18
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm2
.
byte
196
226
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm3
.
byte
98
242
125
56
168
88
7
/
/
vfmadd213ps
0x1c
(
%
rax
)
{
1to8
}
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_skx
.
globl
_sk_xy_to_unit_angle_skx
FUNCTION
(
_sk_xy_to_unit_angle_skx
)
_sk_xy_to_unit_angle_skx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
200
/
/
vsubps
%
ymm0
%
ymm8
%
ymm9
.
byte
197
52
84
200
/
/
vandps
%
ymm0
%
ymm9
%
ymm9
.
byte
197
60
92
209
/
/
vsubps
%
ymm1
%
ymm8
%
ymm10
.
byte
197
44
84
209
/
/
vandps
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
52
93
218
/
/
vminps
%
ymm10
%
ymm9
%
ymm11
.
byte
196
65
52
95
226
/
/
vmaxps
%
ymm10
%
ymm9
%
ymm12
.
byte
196
65
36
94
220
/
/
vdivps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
36
89
227
/
/
vmulps
%
ymm11
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
172
92
3
0
/
/
vbroadcastss
0x35cac
(
%
rip
)
%
ymm13
#
3c638
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ec
>
.
byte
98
114
29
56
168
45
166
92
3
0
/
/
vfmadd213ps
0x35ca6
(
%
rip
)
{
1to8
}
%
ymm12
%
ymm13
#
3c63c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f0
>
.
byte
98
114
29
56
168
45
160
92
3
0
/
/
vfmadd213ps
0x35ca0
(
%
rip
)
{
1to8
}
%
ymm12
%
ymm13
#
3c640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f4
>
.
byte
98
114
29
56
168
45
154
92
3
0
/
/
vfmadd213ps
0x35c9a
(
%
rip
)
{
1to8
}
%
ymm12
%
ymm13
#
3c644
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f8
>
.
byte
196
65
36
89
221
/
/
vmulps
%
ymm13
%
ymm11
%
ymm11
.
byte
98
209
52
40
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm9
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
98
125
24
21
131
92
3
0
/
/
vbroadcastss
0x35c83
(
%
rip
)
%
ymm10
#
3c648
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3fc
>
.
byte
196
65
44
92
211
/
/
vsubps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
37
74
202
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm11
%
ymm9
.
byte
98
209
124
40
194
192
1
/
/
vcmpltps
%
ymm8
%
ymm0
%
k0
.
byte
98
242
126
40
56
192
/
/
vpmovm2d
%
k0
%
ymm0
.
byte
196
98
125
24
21
18
91
3
0
/
/
vbroadcastss
0x35b12
(
%
rip
)
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
44
92
209
/
/
vsubps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
98
209
116
40
194
192
1
/
/
vcmpltps
%
ymm8
%
ymm1
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
98
125
24
21
245
90
3
0
/
/
vbroadcastss
0x35af5
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
44
92
208
/
/
vsubps
%
ymm0
%
ymm10
%
ymm10
.
byte
196
195
125
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm0
%
ymm0
.
byte
98
209
124
40
194
192
3
/
/
vcmpunordps
%
ymm8
%
ymm0
%
k0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
195
125
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_skx
.
globl
_sk_xy_to_radius_skx
FUNCTION
(
_sk_xy_to_radius_skx
)
_sk_xy_to_radius_skx
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
184
192
/
/
vfmadd231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
193
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_negate_x_skx
.
globl
_sk_negate_x_skx
FUNCTION
(
_sk_negate_x_skx
)
_sk_negate_x_skx
:
.
byte
98
241
124
56
87
5
8
92
3
0
/
/
vxorps
0x35c08
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c64c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x400
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_strip_skx
.
globl
_sk_xy_to_2pt_conical_strip_skx
FUNCTION
(
_sk_xy_to_2pt_conical_strip_skx
)
_sk_xy_to_2pt_conical_strip_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm8
.
byte
196
98
117
188
193
/
/
vfnmadd231ps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_focal_on_circle_skx
.
globl
_sk_xy_to_2pt_conical_focal_on_circle_skx
FUNCTION
(
_sk_xy_to_2pt_conical_focal_on_circle_skx
)
_sk_xy_to_2pt_conical_focal_on_circle_skx
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
197
60
94
192
/
/
vdivps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_well_behaved_skx
.
globl
_sk_xy_to_2pt_conical_well_behaved_skx
FUNCTION
(
_sk_xy_to_2pt_conical_well_behaved_skx
)
_sk_xy_to_2pt_conical_well_behaved_skx
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
184
192
/
/
vfmadd231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
196
194
53
172
192
/
/
vfnmadd213ps
%
ymm8
%
ymm9
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_greater_skx
.
globl
_sk_xy_to_2pt_conical_greater_skx
FUNCTION
(
_sk_xy_to_2pt_conical_greater_skx
)
_sk_xy_to_2pt_conical_greater_skx
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
186
192
/
/
vfmsub231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
196
194
53
172
192
/
/
vfnmadd213ps
%
ymm8
%
ymm9
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_smaller_skx
.
globl
_sk_xy_to_2pt_conical_smaller_skx
FUNCTION
(
_sk_xy_to_2pt_conical_smaller_skx
)
_sk_xy_to_2pt_conical_smaller_skx
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
186
192
/
/
vfmsub231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
196
194
53
174
192
/
/
vfnmsub213ps
%
ymm8
%
ymm9
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_compensate_focal_skx
.
globl
_sk_alter_2pt_conical_compensate_focal_skx
FUNCTION
(
_sk_alter_2pt_conical_compensate_focal_skx
)
_sk_alter_2pt_conical_compensate_focal_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_unswap_skx
.
globl
_sk_alter_2pt_conical_unswap_skx
FUNCTION
(
_sk_alter_2pt_conical_unswap_skx
)
_sk_alter_2pt_conical_unswap_skx
:
.
byte
196
98
125
24
5
20
90
3
0
/
/
vbroadcastss
0x35a14
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_nan_skx
.
globl
_sk_mask_2pt_conical_nan_skx
FUNCTION
(
_sk_mask_2pt_conical_nan_skx
)
_sk_mask_2pt_conical_nan_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
98
209
124
40
194
192
7
/
/
vcmpordps
%
ymm8
%
ymm0
%
k0
.
byte
98
209
124
40
194
200
3
/
/
vcmpunordps
%
ymm8
%
ymm0
%
k1
.
byte
98
114
126
40
56
201
/
/
vpmovm2d
%
k1
%
ymm9
.
byte
196
195
125
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm0
%
ymm0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_degenerates_skx
.
globl
_sk_mask_2pt_conical_degenerates_skx
FUNCTION
(
_sk_mask_2pt_conical_degenerates_skx
)
_sk_mask_2pt_conical_degenerates_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
98
241
60
40
194
192
1
/
/
vcmpltps
%
ymm0
%
ymm8
%
k0
.
byte
98
241
60
40
194
200
5
/
/
vcmpnltps
%
ymm0
%
ymm8
%
k1
.
byte
98
114
126
40
56
201
/
/
vpmovm2d
%
k1
%
ymm9
.
byte
196
195
125
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm0
%
ymm0
.
byte
98
114
126
40
56
200
/
/
vpmovm2d
%
k0
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_apply_vector_mask_skx
.
globl
_sk_apply_vector_mask_skx
FUNCTION
(
_sk_apply_vector_mask_skx
)
_sk_apply_vector_mask_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
0
/
/
vmovups
(
%
rax
)
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
84
210
/
/
vandps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
84
219
/
/
vandps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_save_xy_skx
.
globl
_sk_save_xy_skx
FUNCTION
(
_sk_save_xy_skx
)
_sk_save_xy_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
111
89
3
0
/
/
vbroadcastss
0x3596f
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
124
88
200
/
/
vaddps
%
ymm8
%
ymm0
%
ymm9
.
byte
196
67
125
8
209
1
/
/
vroundps
0x1
%
ymm9
%
ymm10
.
byte
196
65
52
92
202
/
/
vsubps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
116
88
192
/
/
vaddps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
67
125
8
208
1
/
/
vroundps
0x1
%
ymm8
%
ymm10
.
byte
196
65
60
92
194
/
/
vsubps
%
ymm10
%
ymm8
%
ymm8
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
197
252
17
72
64
/
/
vmovups
%
ymm1
0x40
(
%
rax
)
.
byte
197
124
17
136
128
0
0
0
/
/
vmovups
%
ymm9
0x80
(
%
rax
)
.
byte
197
124
17
128
192
0
0
0
/
/
vmovups
%
ymm8
0xc0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_accumulate_skx
.
globl
_sk_accumulate_skx
FUNCTION
(
_sk_accumulate_skx
)
_sk_accumulate_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
128
0
1
0
0
/
/
vmovups
0x100
(
%
rax
)
%
ymm8
.
byte
197
60
89
128
64
1
0
0
/
/
vmulps
0x140
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
226
61
184
224
/
/
vfmadd231ps
%
ymm0
%
ymm8
%
ymm4
.
byte
196
226
61
184
233
/
/
vfmadd231ps
%
ymm1
%
ymm8
%
ymm5
.
byte
196
226
61
184
242
/
/
vfmadd231ps
%
ymm2
%
ymm8
%
ymm6
.
byte
196
98
101
168
199
/
/
vfmadd213ps
%
ymm7
%
ymm3
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
199
/
/
vmovaps
%
ymm8
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_nx_skx
.
globl
_sk_bilinear_nx_skx
FUNCTION
(
_sk_bilinear_nx_skx
)
_sk_bilinear_nx_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
98
241
124
56
88
5
76
90
3
0
/
/
vaddps
0x35a4c
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
98
125
24
5
239
88
3
0
/
/
vbroadcastss
0x358ef
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_px_skx
.
globl
_sk_bilinear_px_skx
FUNCTION
(
_sk_bilinear_px_skx
)
_sk_bilinear_px_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
197
124
16
128
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm8
.
byte
98
241
124
56
88
5
191
88
3
0
/
/
vaddps
0x358bf
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_ny_skx
.
globl
_sk_bilinear_ny_skx
FUNCTION
(
_sk_bilinear_ny_skx
)
_sk_bilinear_ny_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
72
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm1
.
byte
98
241
116
56
88
13
250
89
3
0
/
/
vaddps
0x359fa
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
98
125
24
5
157
88
3
0
/
/
vbroadcastss
0x3589d
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_py_skx
.
globl
_sk_bilinear_py_skx
FUNCTION
(
_sk_bilinear_py_skx
)
_sk_bilinear_py_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
72
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm1
.
byte
197
124
16
128
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm8
.
byte
98
241
116
56
88
13
108
88
3
0
/
/
vaddps
0x3586c
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3x_skx
.
globl
_sk_bicubic_n3x_skx
FUNCTION
(
_sk_bicubic_n3x_skx
)
_sk_bicubic_n3x_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
98
241
124
56
88
5
172
89
3
0
/
/
vaddps
0x359ac
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c654
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x408
>
.
byte
196
98
125
24
5
75
88
3
0
/
/
vbroadcastss
0x3584b
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
145
89
3
0
/
/
vbroadcastss
0x35991
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
98
114
61
56
168
21
159
88
3
0
/
/
vfmadd213ps
0x3589f
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
44
89
193
/
/
vmulps
%
ymm9
%
ymm10
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1x_skx
.
globl
_sk_bicubic_n1x_skx
FUNCTION
(
_sk_bicubic_n1x_skx
)
_sk_bicubic_n1x_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
98
241
124
56
88
5
94
89
3
0
/
/
vaddps
0x3595e
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
98
125
24
5
1
88
3
0
/
/
vbroadcastss
0x35801
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
80
89
3
0
/
/
vbroadcastss
0x35950
(
%
rip
)
%
ymm9
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
98
114
61
56
168
13
74
89
3
0
/
/
vfmadd213ps
0x3594a
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
98
114
61
56
168
13
216
87
3
0
/
/
vfmadd213ps
0x357d8
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
98
114
61
56
168
13
58
89
3
0
/
/
vfmadd213ps
0x3593a
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
197
124
17
136
0
1
0
0
/
/
vmovups
%
ymm9
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1x_skx
.
globl
_sk_bicubic_p1x_skx
FUNCTION
(
_sk_bicubic_p1x_skx
)
_sk_bicubic_p1x_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
183
87
3
0
/
/
vbroadcastss
0x357b7
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
188
88
0
/
/
vaddps
(
%
rax
)
%
ymm8
%
ymm0
.
byte
197
124
16
136
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
21
6
89
3
0
/
/
vbroadcastss
0x35906
(
%
rip
)
%
ymm10
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
98
114
53
56
168
21
0
89
3
0
/
/
vfmadd213ps
0x35900
(
%
rip
)
{
1to8
}
%
ymm9
%
ymm10
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
66
53
168
208
/
/
vfmadd213ps
%
ymm8
%
ymm9
%
ymm10
.
byte
98
114
53
56
168
21
245
88
3
0
/
/
vfmadd213ps
0x358f5
(
%
rip
)
{
1to8
}
%
ymm9
%
ymm10
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
197
124
17
144
0
1
0
0
/
/
vmovups
%
ymm10
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3x_skx
.
globl
_sk_bicubic_p3x_skx
FUNCTION
(
_sk_bicubic_p3x_skx
)
_sk_bicubic_p3x_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
197
124
16
128
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm8
.
byte
98
241
124
56
88
5
205
88
3
0
/
/
vaddps
0x358cd
(
%
rip
)
{
1to8
}
%
ymm0
%
ymm0
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
183
88
3
0
/
/
vbroadcastss
0x358b7
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
98
114
61
56
168
21
197
87
3
0
/
/
vfmadd213ps
0x357c5
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
52
89
194
/
/
vmulps
%
ymm10
%
ymm9
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3y_skx
.
globl
_sk_bicubic_n3y_skx
FUNCTION
(
_sk_bicubic_n3y_skx
)
_sk_bicubic_n3y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
72
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm1
.
byte
98
241
116
56
88
13
135
88
3
0
/
/
vaddps
0x35887
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c654
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x408
>
.
byte
196
98
125
24
5
38
87
3
0
/
/
vbroadcastss
0x35726
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
108
88
3
0
/
/
vbroadcastss
0x3586c
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
98
114
61
56
168
21
122
87
3
0
/
/
vfmadd213ps
0x3577a
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
44
89
193
/
/
vmulps
%
ymm9
%
ymm10
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1y_skx
.
globl
_sk_bicubic_n1y_skx
FUNCTION
(
_sk_bicubic_n1y_skx
)
_sk_bicubic_n1y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
72
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm1
.
byte
98
241
116
56
88
13
56
88
3
0
/
/
vaddps
0x35838
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
98
125
24
5
219
86
3
0
/
/
vbroadcastss
0x356db
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
42
88
3
0
/
/
vbroadcastss
0x3582a
(
%
rip
)
%
ymm9
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
98
114
61
56
168
13
36
88
3
0
/
/
vfmadd213ps
0x35824
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
98
114
61
56
168
13
178
86
3
0
/
/
vfmadd213ps
0x356b2
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
98
114
61
56
168
13
20
88
3
0
/
/
vfmadd213ps
0x35814
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm9
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
197
124
17
136
64
1
0
0
/
/
vmovups
%
ymm9
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1y_skx
.
globl
_sk_bicubic_p1y_skx
FUNCTION
(
_sk_bicubic_p1y_skx
)
_sk_bicubic_p1y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
145
86
3
0
/
/
vbroadcastss
0x35691
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
188
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm8
%
ymm1
.
byte
197
124
16
136
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
21
223
87
3
0
/
/
vbroadcastss
0x357df
(
%
rip
)
%
ymm10
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
98
114
53
56
168
21
217
87
3
0
/
/
vfmadd213ps
0x357d9
(
%
rip
)
{
1to8
}
%
ymm9
%
ymm10
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
66
53
168
208
/
/
vfmadd213ps
%
ymm8
%
ymm9
%
ymm10
.
byte
98
114
53
56
168
21
206
87
3
0
/
/
vfmadd213ps
0x357ce
(
%
rip
)
{
1to8
}
%
ymm9
%
ymm10
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
197
124
17
144
64
1
0
0
/
/
vmovups
%
ymm10
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3y_skx
.
globl
_sk_bicubic_p3y_skx
FUNCTION
(
_sk_bicubic_p3y_skx
)
_sk_bicubic_p3y_skx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
72
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm1
.
byte
197
124
16
128
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm8
.
byte
98
241
116
56
88
13
165
87
3
0
/
/
vaddps
0x357a5
(
%
rip
)
{
1to8
}
%
ymm1
%
ymm1
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
143
87
3
0
/
/
vbroadcastss
0x3578f
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
98
114
61
56
168
21
157
86
3
0
/
/
vfmadd213ps
0x3569d
(
%
rip
)
{
1to8
}
%
ymm8
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
52
89
194
/
/
vmulps
%
ymm10
%
ymm9
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_callback_skx
.
globl
_sk_callback_skx
FUNCTION
(
_sk_callback_skx
)
_sk_callback_skx
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
228
224
/
/
and
0xffffffffffffffe0
%
rsp
.
byte
72
129
236
160
0
0
0
/
/
sub
0xa0
%
rsp
.
byte
197
252
41
124
36
96
/
/
vmovaps
%
ymm7
0x60
(
%
rsp
)
.
byte
197
252
41
116
36
64
/
/
vmovaps
%
ymm6
0x40
(
%
rsp
)
.
byte
197
252
41
108
36
32
/
/
vmovaps
%
ymm5
0x20
(
%
rsp
)
.
byte
197
252
41
36
36
/
/
vmovaps
%
ymm4
(
%
rsp
)
.
byte
73
137
206
/
/
mov
%
rcx
%
r14
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
73
137
253
/
/
mov
%
rdi
%
r13
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
137
195
/
/
mov
%
rax
%
rbx
.
byte
73
137
244
/
/
mov
%
rsi
%
r12
.
byte
197
252
20
225
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm4
.
byte
197
252
21
193
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
236
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm1
.
byte
197
236
21
211
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
221
20
217
/
/
vunpcklpd
%
ymm1
%
ymm4
%
ymm3
.
byte
197
221
21
201
/
/
vunpckhpd
%
ymm1
%
ymm4
%
ymm1
.
byte
197
253
20
226
/
/
vunpcklpd
%
ymm2
%
ymm0
%
ymm4
.
byte
197
253
21
194
/
/
vunpckhpd
%
ymm2
%
ymm0
%
ymm0
.
byte
196
227
101
24
209
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm2
.
byte
196
227
93
24
232
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm4
%
ymm5
.
byte
196
227
101
6
201
49
/
/
vperm2f128
0x31
%
ymm1
%
ymm3
%
ymm1
.
byte
196
227
93
6
192
49
/
/
vperm2f128
0x31
%
ymm0
%
ymm4
%
ymm0
.
byte
197
253
17
83
8
/
/
vmovupd
%
ymm2
0x8
(
%
rbx
)
.
byte
197
253
17
107
40
/
/
vmovupd
%
ymm5
0x28
(
%
rbx
)
.
byte
197
253
17
75
72
/
/
vmovupd
%
ymm1
0x48
(
%
rbx
)
.
byte
197
253
17
67
104
/
/
vmovupd
%
ymm0
0x68
(
%
rbx
)
.
byte
77
133
237
/
/
test
%
r13
%
r13
.
byte
190
8
0
0
0
/
/
mov
0x8
%
esi
.
byte
65
15
69
245
/
/
cmovne
%
r13d
%
esi
.
byte
72
137
223
/
/
mov
%
rbx
%
rdi
.
byte
197
248
119
/
/
vzeroupper
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
72
139
131
8
1
0
0
/
/
mov
0x108
(
%
rbx
)
%
rax
.
byte
197
248
16
0
/
/
vmovups
(
%
rax
)
%
xmm0
.
byte
197
248
16
72
16
/
/
vmovups
0x10
(
%
rax
)
%
xmm1
.
byte
197
248
16
80
32
/
/
vmovups
0x20
(
%
rax
)
%
xmm2
.
byte
197
248
16
88
48
/
/
vmovups
0x30
(
%
rax
)
%
xmm3
.
byte
196
227
101
24
88
112
1
/
/
vinsertf128
0x1
0x70
(
%
rax
)
%
ymm3
%
ymm3
.
byte
196
227
109
24
80
96
1
/
/
vinsertf128
0x1
0x60
(
%
rax
)
%
ymm2
%
ymm2
.
byte
196
227
117
24
72
80
1
/
/
vinsertf128
0x1
0x50
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
227
125
24
64
64
1
/
/
vinsertf128
0x1
0x40
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
252
20
225
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm4
.
byte
197
252
21
233
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm5
.
byte
197
236
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm1
.
byte
197
236
21
219
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm3
.
byte
197
221
20
193
/
/
vunpcklpd
%
ymm1
%
ymm4
%
ymm0
.
byte
197
221
21
201
/
/
vunpckhpd
%
ymm1
%
ymm4
%
ymm1
.
byte
197
213
20
211
/
/
vunpcklpd
%
ymm3
%
ymm5
%
ymm2
.
byte
197
213
21
219
/
/
vunpckhpd
%
ymm3
%
ymm5
%
ymm3
.
byte
76
137
230
/
/
mov
%
r12
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
137
239
/
/
mov
%
r13
%
rdi
.
byte
76
137
250
/
/
mov
%
r15
%
rdx
.
byte
76
137
241
/
/
mov
%
r14
%
rcx
.
byte
197
252
40
36
36
/
/
vmovaps
(
%
rsp
)
%
ymm4
.
byte
197
252
40
108
36
32
/
/
vmovaps
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
40
116
36
64
/
/
vmovaps
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
40
124
36
96
/
/
vmovaps
0x60
(
%
rsp
)
%
ymm7
.
byte
72
141
101
216
/
/
lea
-
0x28
(
%
rbp
)
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_3D_skx
.
globl
_sk_clut_3D_skx
FUNCTION
(
_sk_clut_3D_skx
)
_sk_clut_3D_skx
:
.
byte
72
129
236
24
1
0
0
/
/
sub
0x118
%
rsp
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
98
82
125
40
124
193
/
/
vpbroadcastd
%
r9d
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
126
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm10
.
byte
197
124
40
242
/
/
vmovaps
%
ymm2
%
ymm14
.
byte
197
124
17
116
36
224
/
/
vmovups
%
ymm14
-
0x20
(
%
rsp
)
.
byte
196
65
121
110
192
/
/
vmovd
%
r8d
%
xmm8
.
byte
98
210
125
40
124
208
/
/
vpbroadcastd
%
r8d
%
ymm2
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
98
82
125
40
124
201
/
/
vpbroadcastd
%
r9d
%
ymm9
.
byte
196
65
124
91
201
/
/
vcvtdq2ps
%
ymm9
%
ymm9
.
byte
98
97
52
40
89
225
/
/
vmulps
%
ymm1
%
ymm9
%
ymm28
.
byte
98
145
126
40
91
204
/
/
vcvttps2dq
%
ymm28
%
ymm1
.
byte
197
254
127
140
36
128
0
0
0
/
/
vmovdqu
%
ymm1
0x80
(
%
rsp
)
.
byte
98
98
109
40
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm25
.
byte
98
81
53
32
254
202
/
/
vpaddd
%
ymm10
%
ymm25
%
ymm9
.
byte
196
65
125
111
250
/
/
vmovdqa
%
ymm10
%
ymm15
.
byte
197
126
127
124
36
160
/
/
vmovdqu
%
ymm15
-
0x60
(
%
rsp
)
.
byte
196
193
121
110
200
/
/
vmovd
%
r8d
%
xmm1
.
byte
196
194
117
64
200
/
/
vpmulld
%
ymm8
%
ymm1
%
ymm1
.
byte
196
98
125
88
193
/
/
vpbroadcastd
%
xmm1
%
ymm8
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
131
192
255
/
/
add
0xffffffff
%
r8d
.
byte
98
210
125
40
124
200
/
/
vpbroadcastd
%
r8d
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
97
116
40
89
192
/
/
vmulps
%
ymm0
%
ymm1
%
ymm24
.
byte
98
145
126
40
91
192
/
/
vcvttps2dq
%
ymm24
%
ymm0
.
byte
197
254
127
68
36
128
/
/
vmovdqu
%
ymm0
-
0x80
(
%
rsp
)
.
byte
98
226
61
40
64
224
/
/
vpmulld
%
ymm0
%
ymm8
%
ymm20
.
byte
98
209
93
32
254
193
/
/
vpaddd
%
ymm9
%
ymm20
%
ymm0
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
98
98
125
40
88
29
153
85
3
0
/
/
vpbroadcastd
0x35599
(
%
rip
)
%
ymm27
#
3c66c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x420
>
.
byte
98
146
125
40
64
195
/
/
vpmulld
%
ymm27
%
ymm0
%
ymm0
.
byte
196
65
45
239
210
/
/
vpxor
%
ymm10
%
ymm10
%
ymm10
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
98
117
146
20
128
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm0
4
)
%
ymm10
.
byte
98
193
124
40
40
210
/
/
vmovaps
%
ymm10
%
ymm18
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
197
125
250
209
/
/
vpsubd
%
ymm1
%
ymm0
%
ymm10
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
162
37
146
12
144
/
/
vgatherdps
%
ymm11
(
%
rax
%
ymm10
4
)
%
ymm1
.
byte
98
97
124
40
40
233
/
/
vmovaps
%
ymm1
%
ymm29
.
byte
196
98
125
24
21
79
85
3
0
/
/
vbroadcastss
0x3554f
(
%
rip
)
%
ymm10
#
3c668
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x41c
>
.
byte
98
81
60
32
88
218
/
/
vaddps
%
ymm10
%
ymm24
%
ymm11
.
byte
98
226
125
40
88
5
223
83
3
0
/
/
vpbroadcastd
0x353df
(
%
rip
)
%
ymm16
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
98
177
125
40
254
192
/
/
vpaddd
%
ymm16
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
226
29
146
12
128
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm0
4
)
%
ymm1
.
byte
98
97
124
40
40
241
/
/
vmovaps
%
ymm1
%
ymm30
.
byte
196
193
126
91
195
/
/
vcvttps2dq
%
ymm11
%
ymm0
.
byte
98
226
61
40
64
232
/
/
vpmulld
%
ymm0
%
ymm8
%
ymm21
.
byte
98
209
85
32
254
193
/
/
vpaddd
%
ymm9
%
ymm21
%
ymm0
.
byte
98
146
125
40
64
195
/
/
vpmulld
%
ymm27
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
12
128
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm0
4
)
%
ymm1
.
byte
197
252
17
140
36
224
0
0
0
/
/
vmovups
%
ymm1
0xe0
(
%
rsp
)
.
byte
196
65
125
250
197
/
/
vpsubd
%
ymm13
%
ymm0
%
ymm8
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
12
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm1
.
byte
197
252
17
140
36
192
0
0
0
/
/
vmovups
%
ymm1
0xc0
(
%
rsp
)
.
byte
98
81
28
32
88
194
/
/
vaddps
%
ymm10
%
ymm28
%
ymm8
.
byte
196
65
126
91
192
/
/
vcvttps2dq
%
ymm8
%
ymm8
.
byte
98
177
125
40
254
192
/
/
vpaddd
%
ymm16
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
226
53
146
12
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm0
4
)
%
ymm1
.
byte
197
252
17
140
36
160
0
0
0
/
/
vmovups
%
ymm1
0xa0
(
%
rsp
)
.
byte
98
66
109
40
64
208
/
/
vpmulld
%
ymm8
%
ymm2
%
ymm26
.
byte
98
209
45
32
254
199
/
/
vpaddd
%
ymm15
%
ymm26
%
ymm0
.
byte
98
177
125
40
254
212
/
/
vpaddd
%
ymm20
%
ymm0
%
ymm2
.
byte
98
146
109
40
64
211
/
/
vpmulld
%
ymm27
%
ymm2
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
12
144
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm2
4
)
%
ymm1
.
byte
197
252
17
76
36
96
/
/
vmovups
%
ymm1
0x60
(
%
rsp
)
.
byte
196
65
109
250
197
/
/
vpsubd
%
ymm13
%
ymm2
%
ymm8
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
12
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm1
.
byte
197
252
17
76
36
64
/
/
vmovups
%
ymm1
0x40
(
%
rsp
)
.
byte
98
177
109
40
254
208
/
/
vpaddd
%
ymm16
%
ymm2
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
12
144
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm2
4
)
%
ymm1
.
byte
197
252
17
76
36
32
/
/
vmovups
%
ymm1
0x20
(
%
rsp
)
.
byte
98
241
85
32
254
192
/
/
vpaddd
%
ymm0
%
ymm21
%
ymm0
.
byte
98
146
125
40
64
195
/
/
vpmulld
%
ymm27
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
12
128
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm0
4
)
%
ymm1
.
byte
197
252
17
12
36
/
/
vmovups
%
ymm1
(
%
rsp
)
.
byte
196
193
125
250
213
/
/
vpsubd
%
ymm13
%
ymm0
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
12
144
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm2
4
)
%
ymm1
.
byte
197
252
17
76
36
192
/
/
vmovups
%
ymm1
-
0x40
(
%
rsp
)
.
byte
98
177
125
40
254
192
/
/
vpaddd
%
ymm16
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
12
128
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm0
4
)
%
ymm1
.
byte
98
97
124
40
40
249
/
/
vmovaps
%
ymm1
%
ymm31
.
byte
196
193
12
88
194
/
/
vaddps
%
ymm10
%
ymm14
%
ymm0
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
98
113
53
32
254
208
/
/
vpaddd
%
ymm0
%
ymm25
%
ymm10
.
byte
98
209
93
32
254
210
/
/
vpaddd
%
ymm10
%
ymm20
%
ymm2
.
byte
98
146
109
40
64
211
/
/
vpmulld
%
ymm27
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
98
53
146
4
144
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm2
4
)
%
ymm8
.
byte
196
65
109
250
221
/
/
vpsubd
%
ymm13
%
ymm2
%
ymm11
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
34
29
146
12
152
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm11
4
)
%
ymm9
.
byte
98
49
109
40
254
216
/
/
vpaddd
%
ymm16
%
ymm2
%
ymm11
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
162
29
146
12
152
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm11
4
)
%
ymm1
.
byte
98
225
124
40
40
249
/
/
vmovaps
%
ymm1
%
ymm23
.
byte
98
81
85
32
254
210
/
/
vpaddd
%
ymm10
%
ymm21
%
ymm10
.
byte
98
18
45
40
64
211
/
/
vpmulld
%
ymm27
%
ymm10
%
ymm10
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
162
29
146
12
144
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm10
4
)
%
ymm1
.
byte
98
225
124
40
40
201
/
/
vmovaps
%
ymm1
%
ymm17
.
byte
196
65
45
250
245
/
/
vpsubd
%
ymm13
%
ymm10
%
ymm14
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
162
5
146
12
176
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm14
4
)
%
ymm1
.
byte
98
225
124
40
40
241
/
/
vmovaps
%
ymm1
%
ymm22
.
byte
98
49
45
40
254
240
/
/
vpaddd
%
ymm16
%
ymm10
%
ymm14
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
162
5
146
12
176
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm14
4
)
%
ymm1
.
byte
98
225
124
40
40
217
/
/
vmovaps
%
ymm1
%
ymm19
.
byte
98
97
45
32
254
200
/
/
vpaddd
%
ymm0
%
ymm26
%
ymm25
.
byte
98
177
53
32
254
196
/
/
vpaddd
%
ymm20
%
ymm25
%
ymm0
.
byte
98
18
125
40
64
243
/
/
vpmulld
%
ymm27
%
ymm0
%
ymm14
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
162
5
146
4
176
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm14
4
)
%
ymm0
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
13
250
202
/
/
vpsubd
%
ymm10
%
ymm14
%
ymm1
.
byte
196
65
4
87
255
/
/
vxorps
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
98
29
146
60
136
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm1
4
)
%
ymm15
.
byte
98
177
13
40
254
200
/
/
vpaddd
%
ymm16
%
ymm14
%
ymm1
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
98
29
146
52
136
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm1
4
)
%
ymm14
.
byte
98
145
85
32
254
201
/
/
vpaddd
%
ymm25
%
ymm21
%
ymm1
.
byte
98
146
117
40
64
203
/
/
vpmulld
%
ymm27
%
ymm1
%
ymm1
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
98
37
146
36
136
/
/
vgatherdps
%
ymm11
(
%
rax
%
ymm1
4
)
%
ymm12
.
byte
196
193
117
250
210
/
/
vpsubd
%
ymm10
%
ymm1
%
ymm2
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
21
239
237
/
/
vpxor
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
98
45
146
44
144
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm2
4
)
%
ymm13
.
byte
98
177
117
40
254
200
/
/
vpaddd
%
ymm16
%
ymm1
%
ymm1
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
98
37
146
20
136
/
/
vgatherdps
%
ymm11
(
%
rax
%
ymm1
4
)
%
ymm10
.
byte
197
252
16
140
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm1
.
byte
98
177
116
40
92
202
/
/
vsubps
%
ymm18
%
ymm1
%
ymm1
.
byte
197
252
16
84
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm2
.
byte
197
124
91
218
/
/
vcvtdq2ps
%
ymm2
%
ymm11
.
byte
98
81
60
32
92
219
/
/
vsubps
%
ymm11
%
ymm24
%
ymm11
.
byte
98
178
37
40
168
202
/
/
vfmadd213ps
%
ymm18
%
ymm11
%
ymm1
.
byte
197
252
16
148
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm2
.
byte
98
129
108
40
92
197
/
/
vsubps
%
ymm29
%
ymm2
%
ymm16
.
byte
98
130
37
40
168
197
/
/
vfmadd213ps
%
ymm29
%
ymm11
%
ymm16
.
byte
197
252
16
148
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm2
.
byte
98
129
108
40
92
214
/
/
vsubps
%
ymm30
%
ymm2
%
ymm18
.
byte
98
130
37
40
168
214
/
/
vfmadd213ps
%
ymm30
%
ymm11
%
ymm18
.
byte
197
252
16
84
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm2
.
byte
98
225
124
40
16
36
36
/
/
vmovups
(
%
rsp
)
%
ymm20
.
byte
98
225
92
32
92
226
/
/
vsubps
%
ymm2
%
ymm20
%
ymm20
.
byte
98
226
37
40
168
226
/
/
vfmadd213ps
%
ymm2
%
ymm11
%
ymm20
.
byte
197
252
16
84
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm2
.
byte
98
225
124
40
16
108
36
254
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm21
.
byte
98
225
84
32
92
234
/
/
vsubps
%
ymm2
%
ymm21
%
ymm21
.
byte
98
226
37
40
168
234
/
/
vfmadd213ps
%
ymm2
%
ymm11
%
ymm21
.
byte
197
252
16
84
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm2
.
byte
98
97
4
32
92
194
/
/
vsubps
%
ymm2
%
ymm31
%
ymm24
.
byte
98
98
37
40
168
194
/
/
vfmadd213ps
%
ymm2
%
ymm11
%
ymm24
.
byte
98
65
116
32
92
200
/
/
vsubps
%
ymm8
%
ymm17
%
ymm25
.
byte
98
66
37
40
168
200
/
/
vfmadd213ps
%
ymm8
%
ymm11
%
ymm25
.
byte
197
252
16
148
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm2
.
byte
197
124
91
194
/
/
vcvtdq2ps
%
ymm2
%
ymm8
.
byte
98
81
28
32
92
192
/
/
vsubps
%
ymm8
%
ymm28
%
ymm8
.
byte
98
225
92
32
92
201
/
/
vsubps
%
ymm1
%
ymm20
%
ymm17
.
byte
98
226
61
40
168
201
/
/
vfmadd213ps
%
ymm1
%
ymm8
%
ymm17
.
byte
98
161
84
32
92
224
/
/
vsubps
%
ymm16
%
ymm21
%
ymm20
.
byte
98
162
61
40
168
224
/
/
vfmadd213ps
%
ymm16
%
ymm8
%
ymm20
.
byte
98
161
60
32
92
194
/
/
vsubps
%
ymm18
%
ymm24
%
ymm16
.
byte
98
162
61
40
168
194
/
/
vfmadd213ps
%
ymm18
%
ymm8
%
ymm16
.
byte
98
209
76
32
92
201
/
/
vsubps
%
ymm9
%
ymm22
%
ymm1
.
byte
196
194
37
168
201
/
/
vfmadd213ps
%
ymm9
%
ymm11
%
ymm1
.
byte
98
49
100
32
92
207
/
/
vsubps
%
ymm23
%
ymm19
%
ymm9
.
byte
98
50
37
40
168
207
/
/
vfmadd213ps
%
ymm23
%
ymm11
%
ymm9
.
byte
197
156
92
208
/
/
vsubps
%
ymm0
%
ymm12
%
ymm2
.
byte
196
226
37
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm11
%
ymm2
.
byte
196
193
20
92
199
/
/
vsubps
%
ymm15
%
ymm13
%
ymm0
.
byte
196
194
37
168
199
/
/
vfmadd213ps
%
ymm15
%
ymm11
%
ymm0
.
byte
196
65
44
92
214
/
/
vsubps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
66
37
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm10
.
byte
98
145
108
40
92
209
/
/
vsubps
%
ymm25
%
ymm2
%
ymm2
.
byte
98
146
61
40
168
209
/
/
vfmadd213ps
%
ymm25
%
ymm8
%
ymm2
.
byte
197
124
92
217
/
/
vsubps
%
ymm1
%
ymm0
%
ymm11
.
byte
196
98
61
168
217
/
/
vfmadd213ps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
65
44
92
209
/
/
vsubps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
66
61
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm10
.
byte
197
252
16
68
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
16
76
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm1
.
byte
197
116
92
192
/
/
vsubps
%
ymm0
%
ymm1
%
ymm8
.
byte
98
177
108
40
92
193
/
/
vsubps
%
ymm17
%
ymm2
%
ymm0
.
byte
98
178
61
40
168
193
/
/
vfmadd213ps
%
ymm17
%
ymm8
%
ymm0
.
byte
98
177
36
40
92
204
/
/
vsubps
%
ymm20
%
ymm11
%
ymm1
.
byte
98
178
61
40
168
204
/
/
vfmadd213ps
%
ymm20
%
ymm8
%
ymm1
.
byte
98
177
44
40
92
208
/
/
vsubps
%
ymm16
%
ymm10
%
ymm2
.
byte
98
178
61
40
168
208
/
/
vfmadd213ps
%
ymm16
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
129
196
24
1
0
0
/
/
add
0x118
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_4D_skx
.
globl
_sk_clut_4D_skx
FUNCTION
(
_sk_clut_4D_skx
)
_sk_clut_4D_skx
:
.
byte
72
129
236
88
5
0
0
/
/
sub
0x558
%
rsp
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
20
/
/
mov
0x14
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
98
82
125
40
124
193
/
/
vpbroadcastd
%
r9d
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
197
126
91
211
/
/
vcvttps2dq
%
ymm3
%
ymm10
.
byte
197
124
40
251
/
/
vmovaps
%
ymm3
%
ymm15
.
byte
197
124
17
188
36
160
2
0
0
/
/
vmovups
%
ymm15
0x2a0
(
%
rsp
)
.
byte
196
65
121
110
200
/
/
vmovd
%
r8d
%
xmm9
.
byte
98
210
125
40
124
216
/
/
vpbroadcastd
%
r8d
%
ymm3
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
98
82
125
40
124
193
/
/
vpbroadcastd
%
r9d
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
197
60
89
194
/
/
vmulps
%
ymm2
%
ymm8
%
ymm8
.
byte
196
193
126
91
208
/
/
vcvttps2dq
%
ymm8
%
ymm2
.
byte
98
193
124
40
40
248
/
/
vmovaps
%
ymm8
%
ymm23
.
byte
98
225
124
40
17
124
36
14
/
/
vmovups
%
ymm23
0x1c0
(
%
rsp
)
.
byte
197
254
127
148
36
32
5
0
0
/
/
vmovdqu
%
ymm2
0x520
(
%
rsp
)
.
byte
98
226
101
40
64
218
/
/
vpmulld
%
ymm2
%
ymm3
%
ymm19
.
byte
98
81
101
32
254
194
/
/
vpaddd
%
ymm10
%
ymm19
%
ymm8
.
byte
98
193
253
40
111
210
/
/
vmovdqa64
%
ymm10
%
ymm18
.
byte
98
225
254
40
127
84
36
33
/
/
vmovdqu64
%
ymm18
0x420
(
%
rsp
)
.
byte
196
65
121
110
208
/
/
vmovd
%
r8d
%
xmm10
.
byte
196
66
45
64
209
/
/
vpmulld
%
ymm9
%
ymm10
%
ymm10
.
byte
196
66
125
88
202
/
/
vpbroadcastd
%
xmm10
%
ymm9
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
98
82
125
40
124
217
/
/
vpbroadcastd
%
r9d
%
ymm11
.
byte
196
65
124
91
219
/
/
vcvtdq2ps
%
ymm11
%
ymm11
.
byte
197
164
89
209
/
/
vmulps
%
ymm1
%
ymm11
%
ymm2
.
byte
197
254
91
202
/
/
vcvttps2dq
%
ymm2
%
ymm1
.
byte
98
97
124
40
40
202
/
/
vmovaps
%
ymm2
%
ymm25
.
byte
98
97
124
40
17
76
36
13
/
/
vmovups
%
ymm25
0x1a0
(
%
rsp
)
.
byte
197
254
127
140
36
0
5
0
0
/
/
vmovdqu
%
ymm1
0x500
(
%
rsp
)
.
byte
98
98
53
40
64
193
/
/
vpmulld
%
ymm1
%
ymm9
%
ymm24
.
byte
98
209
61
32
254
200
/
/
vpaddd
%
ymm8
%
ymm24
%
ymm1
.
byte
196
65
121
110
216
/
/
vmovd
%
r8d
%
xmm11
.
byte
196
66
37
64
210
/
/
vpmulld
%
ymm10
%
ymm11
%
ymm10
.
byte
196
66
125
88
210
/
/
vpbroadcastd
%
xmm10
%
ymm10
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
131
192
255
/
/
add
0xffffffff
%
r8d
.
byte
98
82
125
40
124
216
/
/
vpbroadcastd
%
r8d
%
ymm11
.
byte
196
65
124
91
219
/
/
vcvtdq2ps
%
ymm11
%
ymm11
.
byte
98
97
36
40
89
224
/
/
vmulps
%
ymm0
%
ymm11
%
ymm28
.
byte
98
129
126
40
91
236
/
/
vcvttps2dq
%
ymm28
%
ymm21
.
byte
98
162
45
40
64
229
/
/
vpmulld
%
ymm21
%
ymm10
%
ymm20
.
byte
98
241
93
32
254
193
/
/
vpaddd
%
ymm1
%
ymm20
%
ymm0
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
98
226
125
40
88
5
43
80
3
0
/
/
vpbroadcastd
0x3502b
(
%
rip
)
%
ymm16
#
3c66c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x420
>
.
byte
98
50
125
40
64
216
/
/
vpmulld
%
ymm16
%
ymm0
%
ymm11
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
253
118
192
/
/
vpcmpeqd
%
ymm0
%
ymm0
%
ymm0
.
byte
196
34
125
146
36
152
/
/
vgatherdps
%
ymm0
(
%
rax
%
ymm11
4
)
%
ymm12
.
byte
197
124
17
164
36
224
4
0
0
/
/
vmovups
%
ymm12
0x4e0
(
%
rsp
)
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
193
37
250
198
/
/
vpsubd
%
ymm14
%
ymm11
%
ymm0
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
98
29
146
44
128
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm0
4
)
%
ymm13
.
byte
197
124
17
172
36
192
4
0
0
/
/
vmovups
%
ymm13
0x4c0
(
%
rsp
)
.
byte
196
226
125
24
5
221
79
3
0
/
/
vbroadcastss
0x34fdd
(
%
rip
)
%
ymm0
#
3c668
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x41c
>
.
byte
98
113
28
32
88
224
/
/
vaddps
%
ymm0
%
ymm28
%
ymm12
.
byte
98
226
125
40
88
13
109
78
3
0
/
/
vpbroadcastd
0x34e6d
(
%
rip
)
%
ymm17
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
98
49
37
40
254
217
/
/
vpaddd
%
ymm17
%
ymm11
%
ymm11
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
196
162
21
146
20
152
/
/
vgatherdps
%
ymm13
(
%
rax
%
ymm11
4
)
%
ymm2
.
byte
197
252
17
148
36
160
4
0
0
/
/
vmovups
%
ymm2
0x4a0
(
%
rsp
)
.
byte
196
65
126
91
220
/
/
vcvttps2dq
%
ymm12
%
ymm11
.
byte
98
194
45
40
64
243
/
/
vpmulld
%
ymm11
%
ymm10
%
ymm22
.
byte
98
241
77
32
254
201
/
/
vpaddd
%
ymm1
%
ymm22
%
ymm1
.
byte
98
178
117
40
64
200
/
/
vpmulld
%
ymm16
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
226
45
146
20
136
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
148
36
128
4
0
0
/
/
vmovups
%
ymm2
0x480
(
%
rsp
)
.
byte
196
65
117
250
214
/
/
vpsubd
%
ymm14
%
ymm1
%
ymm10
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
162
37
146
20
144
/
/
vgatherdps
%
ymm11
(
%
rax
%
ymm10
4
)
%
ymm2
.
byte
197
252
17
148
36
96
4
0
0
/
/
vmovups
%
ymm2
0x460
(
%
rsp
)
.
byte
98
113
52
32
88
208
/
/
vaddps
%
ymm0
%
ymm25
%
ymm10
.
byte
196
65
126
91
210
/
/
vcvttps2dq
%
ymm10
%
ymm10
.
byte
98
177
117
40
254
201
/
/
vpaddd
%
ymm17
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
226
37
146
20
136
/
/
vgatherdps
%
ymm11
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
148
36
64
4
0
0
/
/
vmovups
%
ymm2
0x440
(
%
rsp
)
.
byte
98
66
53
40
64
234
/
/
vpmulld
%
ymm10
%
ymm9
%
ymm29
.
byte
98
209
21
32
254
200
/
/
vpaddd
%
ymm8
%
ymm29
%
ymm1
.
byte
98
49
117
40
254
196
/
/
vpaddd
%
ymm20
%
ymm1
%
ymm8
.
byte
98
50
61
40
64
192
/
/
vpmulld
%
ymm16
%
ymm8
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
20
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm2
.
byte
197
252
17
148
36
0
4
0
0
/
/
vmovups
%
ymm2
0x400
(
%
rsp
)
.
byte
196
65
61
250
206
/
/
vpsubd
%
ymm14
%
ymm8
%
ymm9
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
162
45
146
20
136
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm9
4
)
%
ymm2
.
byte
197
252
17
148
36
224
3
0
0
/
/
vmovups
%
ymm2
0x3e0
(
%
rsp
)
.
byte
98
49
61
40
254
193
/
/
vpaddd
%
ymm17
%
ymm8
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
20
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm2
.
byte
197
252
17
148
36
192
3
0
0
/
/
vmovups
%
ymm2
0x3c0
(
%
rsp
)
.
byte
98
241
77
32
254
201
/
/
vpaddd
%
ymm1
%
ymm22
%
ymm1
.
byte
98
178
117
40
64
200
/
/
vpmulld
%
ymm16
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
20
136
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
148
36
160
3
0
0
/
/
vmovups
%
ymm2
0x3a0
(
%
rsp
)
.
byte
196
65
117
250
198
/
/
vpsubd
%
ymm14
%
ymm1
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
20
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm2
.
byte
197
252
17
148
36
128
3
0
0
/
/
vmovups
%
ymm2
0x380
(
%
rsp
)
.
byte
98
177
117
40
254
201
/
/
vpaddd
%
ymm17
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
20
136
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
148
36
96
3
0
0
/
/
vmovups
%
ymm2
0x360
(
%
rsp
)
.
byte
98
241
68
32
88
200
/
/
vaddps
%
ymm0
%
ymm23
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
98
98
101
40
64
249
/
/
vpmulld
%
ymm1
%
ymm3
%
ymm31
.
byte
98
177
5
32
254
202
/
/
vpaddd
%
ymm18
%
ymm31
%
ymm1
.
byte
98
145
117
40
254
216
/
/
vpaddd
%
ymm24
%
ymm1
%
ymm3
.
byte
98
49
101
40
254
196
/
/
vpaddd
%
ymm20
%
ymm3
%
ymm8
.
byte
98
50
61
40
64
192
/
/
vpmulld
%
ymm16
%
ymm8
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
20
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm2
.
byte
197
252
17
148
36
64
3
0
0
/
/
vmovups
%
ymm2
0x340
(
%
rsp
)
.
byte
196
65
61
250
206
/
/
vpsubd
%
ymm14
%
ymm8
%
ymm9
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
162
45
146
20
136
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm9
4
)
%
ymm2
.
byte
197
252
17
148
36
32
3
0
0
/
/
vmovups
%
ymm2
0x320
(
%
rsp
)
.
byte
98
49
61
40
254
193
/
/
vpaddd
%
ymm17
%
ymm8
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
20
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm2
.
byte
197
252
17
148
36
0
3
0
0
/
/
vmovups
%
ymm2
0x300
(
%
rsp
)
.
byte
98
241
77
32
254
219
/
/
vpaddd
%
ymm3
%
ymm22
%
ymm3
.
byte
98
178
101
40
64
216
/
/
vpmulld
%
ymm16
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
20
152
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
197
252
17
148
36
224
2
0
0
/
/
vmovups
%
ymm2
0x2e0
(
%
rsp
)
.
byte
196
65
101
250
198
/
/
vpsubd
%
ymm14
%
ymm3
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
20
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm2
.
byte
197
252
17
148
36
192
2
0
0
/
/
vmovups
%
ymm2
0x2c0
(
%
rsp
)
.
byte
98
177
101
40
254
217
/
/
vpaddd
%
ymm17
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
20
152
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
197
252
17
148
36
128
2
0
0
/
/
vmovups
%
ymm2
0x280
(
%
rsp
)
.
byte
98
241
21
32
254
201
/
/
vpaddd
%
ymm1
%
ymm29
%
ymm1
.
byte
98
177
117
40
254
220
/
/
vpaddd
%
ymm20
%
ymm1
%
ymm3
.
byte
98
178
101
40
64
216
/
/
vpmulld
%
ymm16
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
20
152
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
197
252
17
148
36
96
2
0
0
/
/
vmovups
%
ymm2
0x260
(
%
rsp
)
.
byte
196
65
101
250
198
/
/
vpsubd
%
ymm14
%
ymm3
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
162
53
146
20
128
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm8
4
)
%
ymm2
.
byte
197
252
17
148
36
64
2
0
0
/
/
vmovups
%
ymm2
0x240
(
%
rsp
)
.
byte
98
177
101
40
254
217
/
/
vpaddd
%
ymm17
%
ymm3
%
ymm3
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
98
61
146
12
152
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm3
4
)
%
ymm9
.
byte
98
241
77
32
254
201
/
/
vpaddd
%
ymm1
%
ymm22
%
ymm1
.
byte
98
178
117
40
64
200
/
/
vpmulld
%
ymm16
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
101
146
20
136
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
148
36
32
2
0
0
/
/
vmovups
%
ymm2
0x220
(
%
rsp
)
.
byte
196
193
117
250
222
/
/
vpsubd
%
ymm14
%
ymm1
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
226
61
146
20
152
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
197
252
17
148
36
0
2
0
0
/
/
vmovups
%
ymm2
0x200
(
%
rsp
)
.
byte
98
177
117
40
254
201
/
/
vpaddd
%
ymm17
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
101
146
20
136
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
148
36
224
1
0
0
/
/
vmovups
%
ymm2
0x1e0
(
%
rsp
)
.
byte
197
132
88
192
/
/
vaddps
%
ymm0
%
ymm15
%
ymm0
.
byte
98
97
126
40
91
240
/
/
vcvttps2dq
%
ymm0
%
ymm30
.
byte
98
145
101
32
254
198
/
/
vpaddd
%
ymm30
%
ymm19
%
ymm0
.
byte
98
241
61
32
254
200
/
/
vpaddd
%
ymm0
%
ymm24
%
ymm1
.
byte
98
241
93
32
254
209
/
/
vpaddd
%
ymm1
%
ymm20
%
ymm2
.
byte
98
178
109
40
64
208
/
/
vpmulld
%
ymm16
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
98
101
146
4
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm8
.
byte
197
124
17
132
36
128
1
0
0
/
/
vmovups
%
ymm8
0x180
(
%
rsp
)
.
byte
196
193
109
250
222
/
/
vpsubd
%
ymm14
%
ymm2
%
ymm3
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
98
61
146
20
152
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm3
4
)
%
ymm10
.
byte
197
124
17
148
36
96
1
0
0
/
/
vmovups
%
ymm10
0x160
(
%
rsp
)
.
byte
98
177
109
40
254
209
/
/
vpaddd
%
ymm17
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
98
101
146
4
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm8
.
byte
98
241
77
32
254
201
/
/
vpaddd
%
ymm1
%
ymm22
%
ymm1
.
byte
98
178
117
40
64
200
/
/
vpmulld
%
ymm16
%
ymm1
%
ymm1
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
64
1
0
0
/
/
vmovups
%
ymm3
0x140
(
%
rsp
)
.
byte
196
193
117
250
214
/
/
vpsubd
%
ymm14
%
ymm1
%
ymm2
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
98
101
146
20
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm10
.
byte
197
124
17
148
36
32
1
0
0
/
/
vmovups
%
ymm10
0x120
(
%
rsp
)
.
byte
98
177
117
40
254
201
/
/
vpaddd
%
ymm17
%
ymm1
%
ymm1
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
0
1
0
0
/
/
vmovups
%
ymm3
0x100
(
%
rsp
)
.
byte
98
241
21
32
254
192
/
/
vpaddd
%
ymm0
%
ymm29
%
ymm0
.
byte
98
177
125
40
254
204
/
/
vpaddd
%
ymm20
%
ymm0
%
ymm1
.
byte
98
178
117
40
64
200
/
/
vpmulld
%
ymm16
%
ymm1
%
ymm1
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
224
0
0
0
/
/
vmovups
%
ymm3
0xe0
(
%
rsp
)
.
byte
196
193
117
250
214
/
/
vpsubd
%
ymm14
%
ymm1
%
ymm2
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
98
101
146
20
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm10
.
byte
197
124
17
148
36
192
0
0
0
/
/
vmovups
%
ymm10
0xc0
(
%
rsp
)
.
byte
98
177
117
40
254
201
/
/
vpaddd
%
ymm17
%
ymm1
%
ymm1
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
160
0
0
0
/
/
vmovups
%
ymm3
0xa0
(
%
rsp
)
.
byte
98
241
77
32
254
192
/
/
vpaddd
%
ymm0
%
ymm22
%
ymm0
.
byte
98
178
125
40
64
192
/
/
vpmulld
%
ymm16
%
ymm0
%
ymm0
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
226
117
146
20
128
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm0
4
)
%
ymm2
.
byte
197
252
17
148
36
128
0
0
0
/
/
vmovups
%
ymm2
0x80
(
%
rsp
)
.
byte
196
193
125
250
206
/
/
vpsubd
%
ymm14
%
ymm0
%
ymm1
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
92
36
96
/
/
vmovups
%
ymm3
0x60
(
%
rsp
)
.
byte
98
177
125
40
254
193
/
/
vpaddd
%
ymm17
%
ymm0
%
ymm0
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
226
117
146
20
128
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm0
4
)
%
ymm2
.
byte
197
252
17
84
36
64
/
/
vmovups
%
ymm2
0x40
(
%
rsp
)
.
byte
98
145
5
32
254
198
/
/
vpaddd
%
ymm30
%
ymm31
%
ymm0
.
byte
98
145
125
40
254
200
/
/
vpaddd
%
ymm24
%
ymm0
%
ymm1
.
byte
98
177
117
40
254
212
/
/
vpaddd
%
ymm20
%
ymm1
%
ymm2
.
byte
98
178
109
40
64
208
/
/
vpmulld
%
ymm16
%
ymm2
%
ymm2
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
98
101
146
20
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm10
.
byte
197
124
17
84
36
32
/
/
vmovups
%
ymm10
0x20
(
%
rsp
)
.
byte
196
193
109
250
222
/
/
vpsubd
%
ymm14
%
ymm2
%
ymm3
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
98
45
146
28
152
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm3
4
)
%
ymm11
.
byte
197
124
17
28
36
/
/
vmovups
%
ymm11
(
%
rsp
)
.
byte
98
177
109
40
254
209
/
/
vpaddd
%
ymm17
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
226
45
146
28
144
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm2
4
)
%
ymm3
.
byte
98
225
124
40
40
251
/
/
vmovaps
%
ymm3
%
ymm23
.
byte
98
241
77
32
254
201
/
/
vpaddd
%
ymm1
%
ymm22
%
ymm1
.
byte
98
178
117
40
64
200
/
/
vpmulld
%
ymm16
%
ymm1
%
ymm1
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
92
36
192
/
/
vmovups
%
ymm3
-
0x40
(
%
rsp
)
.
byte
196
193
117
250
214
/
/
vpsubd
%
ymm14
%
ymm1
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
226
45
146
28
144
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm2
4
)
%
ymm3
.
byte
98
97
124
40
40
211
/
/
vmovaps
%
ymm3
%
ymm26
.
byte
98
177
117
40
254
201
/
/
vpaddd
%
ymm17
%
ymm1
%
ymm1
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
98
97
124
40
40
203
/
/
vmovaps
%
ymm3
%
ymm25
.
byte
98
97
21
32
254
192
/
/
vpaddd
%
ymm0
%
ymm29
%
ymm24
.
byte
98
177
61
32
254
196
/
/
vpaddd
%
ymm20
%
ymm24
%
ymm0
.
byte
98
178
125
40
64
192
/
/
vpmulld
%
ymm16
%
ymm0
%
ymm0
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
226
117
146
20
128
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm0
4
)
%
ymm2
.
byte
98
225
124
40
40
218
/
/
vmovaps
%
ymm2
%
ymm19
.
byte
196
193
125
250
214
/
/
vpsubd
%
ymm14
%
ymm0
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
226
5
146
12
144
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm2
4
)
%
ymm1
.
byte
98
225
124
40
40
209
/
/
vmovaps
%
ymm1
%
ymm18
.
byte
98
177
125
40
254
209
/
/
vpaddd
%
ymm17
%
ymm0
%
ymm2
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
98
5
146
36
144
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm2
4
)
%
ymm12
.
byte
98
145
77
32
254
208
/
/
vpaddd
%
ymm24
%
ymm22
%
ymm2
.
byte
98
178
109
40
64
208
/
/
vpmulld
%
ymm16
%
ymm2
%
ymm2
.
byte
196
65
4
87
255
/
/
vxorps
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
196
98
21
146
60
144
/
/
vgatherdps
%
ymm13
(
%
rax
%
ymm2
4
)
%
ymm15
.
byte
197
252
17
124
36
224
/
/
vmovups
%
ymm7
-
0x20
(
%
rsp
)
.
byte
197
252
17
116
36
160
/
/
vmovups
%
ymm6
-
0x60
(
%
rsp
)
.
byte
197
252
17
108
36
128
/
/
vmovups
%
ymm5
-
0x80
(
%
rsp
)
.
byte
98
97
124
40
40
220
/
/
vmovaps
%
ymm4
%
ymm27
.
byte
196
65
109
250
238
/
/
vpsubd
%
ymm14
%
ymm2
%
ymm13
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
34
45
146
28
168
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm13
4
)
%
ymm11
.
byte
98
177
109
40
254
209
/
/
vpaddd
%
ymm17
%
ymm2
%
ymm2
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
98
13
146
20
144
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm2
4
)
%
ymm10
.
byte
197
252
16
132
36
224
4
0
0
/
/
vmovups
0x4e0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
128
4
0
0
/
/
vmovups
0x480
(
%
rsp
)
%
ymm1
.
byte
197
244
92
240
/
/
vsubps
%
ymm0
%
ymm1
%
ymm6
.
byte
98
49
124
40
91
237
/
/
vcvtdq2ps
%
ymm21
%
ymm13
.
byte
98
81
28
32
92
237
/
/
vsubps
%
ymm13
%
ymm28
%
ymm13
.
byte
196
226
21
168
240
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm6
.
byte
197
252
16
132
36
192
4
0
0
/
/
vmovups
0x4c0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
96
4
0
0
/
/
vmovups
0x460
(
%
rsp
)
%
ymm1
.
byte
197
116
92
240
/
/
vsubps
%
ymm0
%
ymm1
%
ymm14
.
byte
196
98
21
168
240
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm14
.
byte
197
252
16
132
36
160
4
0
0
/
/
vmovups
0x4a0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
64
4
0
0
/
/
vmovups
0x440
(
%
rsp
)
%
ymm1
.
byte
98
225
116
40
92
192
/
/
vsubps
%
ymm0
%
ymm1
%
ymm16
.
byte
98
226
21
40
168
192
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm16
.
byte
197
252
16
132
36
0
4
0
0
/
/
vmovups
0x400
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
160
3
0
0
/
/
vmovups
0x3a0
(
%
rsp
)
%
ymm1
.
byte
98
225
116
40
92
200
/
/
vsubps
%
ymm0
%
ymm1
%
ymm17
.
byte
98
226
21
40
168
200
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm17
.
byte
197
252
16
132
36
224
3
0
0
/
/
vmovups
0x3e0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
128
3
0
0
/
/
vmovups
0x380
(
%
rsp
)
%
ymm1
.
byte
98
225
116
40
92
224
/
/
vsubps
%
ymm0
%
ymm1
%
ymm20
.
byte
98
226
21
40
168
224
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm20
.
byte
197
252
16
132
36
192
3
0
0
/
/
vmovups
0x3c0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
96
3
0
0
/
/
vmovups
0x360
(
%
rsp
)
%
ymm1
.
byte
98
225
116
40
92
232
/
/
vsubps
%
ymm0
%
ymm1
%
ymm21
.
byte
98
226
21
40
168
232
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm21
.
byte
197
252
16
132
36
64
3
0
0
/
/
vmovups
0x340
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
224
2
0
0
/
/
vmovups
0x2e0
(
%
rsp
)
%
ymm1
.
byte
98
225
116
40
92
240
/
/
vsubps
%
ymm0
%
ymm1
%
ymm22
.
byte
98
226
21
40
168
240
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm22
.
byte
197
252
16
132
36
32
3
0
0
/
/
vmovups
0x320
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
192
2
0
0
/
/
vmovups
0x2c0
(
%
rsp
)
%
ymm1
.
byte
98
97
116
40
92
192
/
/
vsubps
%
ymm0
%
ymm1
%
ymm24
.
byte
98
98
21
40
168
192
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm24
.
byte
197
252
16
132
36
0
3
0
0
/
/
vmovups
0x300
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
128
2
0
0
/
/
vmovups
0x280
(
%
rsp
)
%
ymm1
.
byte
98
97
116
40
92
224
/
/
vsubps
%
ymm0
%
ymm1
%
ymm28
.
byte
98
98
21
40
168
224
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm28
.
byte
197
252
16
132
36
96
2
0
0
/
/
vmovups
0x260
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
32
2
0
0
/
/
vmovups
0x220
(
%
rsp
)
%
ymm1
.
byte
98
97
116
40
92
232
/
/
vsubps
%
ymm0
%
ymm1
%
ymm29
.
byte
98
98
21
40
168
232
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm29
.
byte
197
252
16
132
36
64
2
0
0
/
/
vmovups
0x240
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
0
2
0
0
/
/
vmovups
0x200
(
%
rsp
)
%
ymm1
.
byte
98
97
116
40
92
240
/
/
vsubps
%
ymm0
%
ymm1
%
ymm30
.
byte
98
98
21
40
168
240
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm30
.
byte
197
252
16
132
36
224
1
0
0
/
/
vmovups
0x1e0
(
%
rsp
)
%
ymm0
.
byte
98
65
124
40
92
249
/
/
vsubps
%
ymm9
%
ymm0
%
ymm31
.
byte
98
66
21
40
168
249
/
/
vfmadd213ps
%
ymm9
%
ymm13
%
ymm31
.
byte
197
252
16
132
36
128
1
0
0
/
/
vmovups
0x180
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
64
1
0
0
/
/
vmovups
0x140
(
%
rsp
)
%
ymm1
.
byte
197
244
92
216
/
/
vsubps
%
ymm0
%
ymm1
%
ymm3
.
byte
196
226
21
168
216
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm3
.
byte
197
252
16
132
36
96
1
0
0
/
/
vmovups
0x160
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
32
1
0
0
/
/
vmovups
0x120
(
%
rsp
)
%
ymm1
.
byte
197
244
92
224
/
/
vsubps
%
ymm0
%
ymm1
%
ymm4
.
byte
196
226
21
168
224
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm4
.
byte
197
252
16
132
36
0
1
0
0
/
/
vmovups
0x100
(
%
rsp
)
%
ymm0
.
byte
196
193
124
92
232
/
/
vsubps
%
ymm8
%
ymm0
%
ymm5
.
byte
196
194
21
168
232
/
/
vfmadd213ps
%
ymm8
%
ymm13
%
ymm5
.
byte
197
252
16
132
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm1
.
byte
197
244
92
248
/
/
vsubps
%
ymm0
%
ymm1
%
ymm7
.
byte
196
226
21
168
248
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm7
.
byte
197
252
16
132
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
76
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm1
.
byte
197
116
92
192
/
/
vsubps
%
ymm0
%
ymm1
%
ymm8
.
byte
196
98
21
168
192
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm8
.
byte
197
252
16
132
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
76
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm1
.
byte
197
116
92
200
/
/
vsubps
%
ymm0
%
ymm1
%
ymm9
.
byte
196
98
21
168
200
/
/
vfmadd213ps
%
ymm0
%
ymm13
%
ymm9
.
byte
197
252
16
76
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm1
.
byte
197
252
16
68
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
92
193
/
/
vsubps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
21
168
193
/
/
vfmadd213ps
%
ymm1
%
ymm13
%
ymm0
.
byte
197
252
16
20
36
/
/
vmovups
(
%
rsp
)
%
ymm2
.
byte
98
241
44
32
92
202
/
/
vsubps
%
ymm2
%
ymm26
%
ymm1
.
byte
196
226
21
168
202
/
/
vfmadd213ps
%
ymm2
%
ymm13
%
ymm1
.
byte
98
177
52
32
92
215
/
/
vsubps
%
ymm23
%
ymm25
%
ymm2
.
byte
98
178
21
40
168
215
/
/
vfmadd213ps
%
ymm23
%
ymm13
%
ymm2
.
byte
98
49
4
40
92
251
/
/
vsubps
%
ymm19
%
ymm15
%
ymm15
.
byte
98
50
21
40
168
251
/
/
vfmadd213ps
%
ymm19
%
ymm13
%
ymm15
.
byte
98
49
36
40
92
218
/
/
vsubps
%
ymm18
%
ymm11
%
ymm11
.
byte
98
50
21
40
168
218
/
/
vfmadd213ps
%
ymm18
%
ymm13
%
ymm11
.
byte
196
65
44
92
212
/
/
vsubps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
66
21
168
212
/
/
vfmadd213ps
%
ymm12
%
ymm13
%
ymm10
.
byte
197
124
16
164
36
0
5
0
0
/
/
vmovups
0x500
(
%
rsp
)
%
ymm12
.
byte
196
65
124
91
228
/
/
vcvtdq2ps
%
ymm12
%
ymm12
.
byte
197
124
16
172
36
160
1
0
0
/
/
vmovups
0x1a0
(
%
rsp
)
%
ymm13
.
byte
196
65
20
92
228
/
/
vsubps
%
ymm12
%
ymm13
%
ymm12
.
byte
98
113
116
32
92
238
/
/
vsubps
%
ymm6
%
ymm17
%
ymm13
.
byte
196
98
29
168
238
/
/
vfmadd213ps
%
ymm6
%
ymm12
%
ymm13
.
byte
98
209
92
32
92
246
/
/
vsubps
%
ymm14
%
ymm20
%
ymm6
.
byte
196
194
29
168
246
/
/
vfmadd213ps
%
ymm14
%
ymm12
%
ymm6
.
byte
98
49
84
32
92
240
/
/
vsubps
%
ymm16
%
ymm21
%
ymm14
.
byte
98
50
29
40
168
240
/
/
vfmadd213ps
%
ymm16
%
ymm12
%
ymm14
.
byte
98
161
20
32
92
198
/
/
vsubps
%
ymm22
%
ymm29
%
ymm16
.
byte
98
162
29
40
168
198
/
/
vfmadd213ps
%
ymm22
%
ymm12
%
ymm16
.
byte
98
129
12
32
92
200
/
/
vsubps
%
ymm24
%
ymm30
%
ymm17
.
byte
98
130
29
40
168
200
/
/
vfmadd213ps
%
ymm24
%
ymm12
%
ymm17
.
byte
98
129
4
32
92
228
/
/
vsubps
%
ymm28
%
ymm31
%
ymm20
.
byte
98
130
29
40
168
228
/
/
vfmadd213ps
%
ymm28
%
ymm12
%
ymm20
.
byte
197
196
92
251
/
/
vsubps
%
ymm3
%
ymm7
%
ymm7
.
byte
196
226
29
168
251
/
/
vfmadd213ps
%
ymm3
%
ymm12
%
ymm7
.
byte
197
188
92
220
/
/
vsubps
%
ymm4
%
ymm8
%
ymm3
.
byte
196
226
29
168
220
/
/
vfmadd213ps
%
ymm4
%
ymm12
%
ymm3
.
byte
197
180
92
229
/
/
vsubps
%
ymm5
%
ymm9
%
ymm4
.
byte
196
226
29
168
229
/
/
vfmadd213ps
%
ymm5
%
ymm12
%
ymm4
.
byte
197
132
92
232
/
/
vsubps
%
ymm0
%
ymm15
%
ymm5
.
byte
196
226
29
168
232
/
/
vfmadd213ps
%
ymm0
%
ymm12
%
ymm5
.
byte
197
164
92
193
/
/
vsubps
%
ymm1
%
ymm11
%
ymm0
.
byte
196
226
29
168
193
/
/
vfmadd213ps
%
ymm1
%
ymm12
%
ymm0
.
byte
197
172
92
202
/
/
vsubps
%
ymm2
%
ymm10
%
ymm1
.
byte
196
226
29
168
202
/
/
vfmadd213ps
%
ymm2
%
ymm12
%
ymm1
.
byte
197
252
16
148
36
32
5
0
0
/
/
vmovups
0x520
(
%
rsp
)
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
197
124
16
132
36
192
1
0
0
/
/
vmovups
0x1c0
(
%
rsp
)
%
ymm8
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
98
81
124
32
92
197
/
/
vsubps
%
ymm13
%
ymm16
%
ymm8
.
byte
196
66
109
168
197
/
/
vfmadd213ps
%
ymm13
%
ymm2
%
ymm8
.
byte
98
113
116
32
92
206
/
/
vsubps
%
ymm6
%
ymm17
%
ymm9
.
byte
196
98
109
168
206
/
/
vfmadd213ps
%
ymm6
%
ymm2
%
ymm9
.
byte
98
209
92
32
92
246
/
/
vsubps
%
ymm14
%
ymm20
%
ymm6
.
byte
196
194
109
168
246
/
/
vfmadd213ps
%
ymm14
%
ymm2
%
ymm6
.
byte
197
212
92
239
/
/
vsubps
%
ymm7
%
ymm5
%
ymm5
.
byte
196
226
109
168
239
/
/
vfmadd213ps
%
ymm7
%
ymm2
%
ymm5
.
byte
197
252
92
251
/
/
vsubps
%
ymm3
%
ymm0
%
ymm7
.
byte
196
226
109
168
251
/
/
vfmadd213ps
%
ymm3
%
ymm2
%
ymm7
.
byte
197
244
92
220
/
/
vsubps
%
ymm4
%
ymm1
%
ymm3
.
byte
196
226
109
168
220
/
/
vfmadd213ps
%
ymm4
%
ymm2
%
ymm3
.
byte
197
252
16
132
36
32
4
0
0
/
/
vmovups
0x420
(
%
rsp
)
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
16
140
36
160
2
0
0
/
/
vmovups
0x2a0
(
%
rsp
)
%
ymm1
.
byte
197
244
92
224
/
/
vsubps
%
ymm0
%
ymm1
%
ymm4
.
byte
196
193
84
92
192
/
/
vsubps
%
ymm8
%
ymm5
%
ymm0
.
byte
196
194
93
168
192
/
/
vfmadd213ps
%
ymm8
%
ymm4
%
ymm0
.
byte
196
193
68
92
201
/
/
vsubps
%
ymm9
%
ymm7
%
ymm1
.
byte
196
194
93
168
201
/
/
vfmadd213ps
%
ymm9
%
ymm4
%
ymm1
.
byte
197
228
92
214
/
/
vsubps
%
ymm6
%
ymm3
%
ymm2
.
byte
196
226
93
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm4
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
11
69
3
0
/
/
vbroadcastss
0x3450b
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
98
145
124
40
40
227
/
/
vmovaps
%
ymm27
%
ymm4
.
byte
197
252
16
108
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm7
.
byte
72
129
196
88
5
0
0
/
/
add
0x558
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gauss_a_to_rgba_skx
.
globl
_sk_gauss_a_to_rgba_skx
FUNCTION
(
_sk_gauss_a_to_rgba_skx
)
_sk_gauss_a_to_rgba_skx
:
.
byte
196
226
125
24
5
85
70
3
0
/
/
vbroadcastss
0x34655
(
%
rip
)
%
ymm0
#
3c670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x424
>
.
byte
98
242
101
56
168
5
79
70
3
0
/
/
vfmadd213ps
0x3464f
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm0
#
3c674
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x428
>
.
byte
98
242
101
56
168
5
73
70
3
0
/
/
vfmadd213ps
0x34649
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm0
#
3c678
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x42c
>
.
byte
98
242
101
56
168
5
67
70
3
0
/
/
vfmadd213ps
0x34643
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm0
#
3c67c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x430
>
.
byte
98
242
101
56
168
5
61
70
3
0
/
/
vfmadd213ps
0x3463d
(
%
rip
)
{
1to8
}
%
ymm3
%
ymm0
#
3c680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x434
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
252
40
216
/
/
vmovaps
%
ymm0
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilerp_clamp_8888_skx
.
globl
_sk_bilerp_clamp_8888_skx
FUNCTION
(
_sk_bilerp_clamp_8888_skx
)
_sk_bilerp_clamp_8888_skx
:
.
byte
197
252
17
124
36
200
/
/
vmovups
%
ymm7
-
0x38
(
%
rsp
)
.
byte
197
252
17
116
36
168
/
/
vmovups
%
ymm6
-
0x58
(
%
rsp
)
.
byte
197
252
17
108
36
136
/
/
vmovups
%
ymm5
-
0x78
(
%
rsp
)
.
byte
98
225
124
40
40
244
/
/
vmovaps
%
ymm4
%
ymm22
.
byte
98
97
124
40
40
249
/
/
vmovaps
%
ymm1
%
ymm31
.
byte
196
226
125
24
21
126
68
3
0
/
/
vbroadcastss
0x3447e
(
%
rip
)
%
ymm2
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
88
218
/
/
vaddps
%
ymm2
%
ymm0
%
ymm3
.
byte
196
227
125
8
227
1
/
/
vroundps
0x1
%
ymm3
%
ymm4
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
98
241
4
32
88
210
/
/
vaddps
%
ymm2
%
ymm31
%
ymm2
.
byte
196
227
125
8
234
1
/
/
vroundps
0x1
%
ymm2
%
ymm5
.
byte
197
100
92
212
/
/
vsubps
%
ymm4
%
ymm3
%
ymm10
.
byte
197
236
92
205
/
/
vsubps
%
ymm5
%
ymm2
%
ymm1
.
byte
196
226
125
24
21
89
68
3
0
/
/
vbroadcastss
0x34459
(
%
rip
)
%
ymm2
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
108
92
226
/
/
vsubps
%
ymm10
%
ymm2
%
ymm12
.
byte
98
225
124
40
40
217
/
/
vmovaps
%
ymm1
%
ymm19
.
byte
98
225
108
40
92
233
/
/
vsubps
%
ymm1
%
ymm2
%
ymm21
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
109
254
243
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm14
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
98
225
109
40
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm18
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
98
226
125
40
88
64
2
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm16
.
byte
98
225
126
8
16
13
110
69
3
0
/
/
vmovss
0x3456e
(
%
rip
)
%
xmm17
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
192
87
255
/
/
vxorps
%
xmm7
%
xmm7
%
xmm7
.
byte
184
17
17
17
17
/
/
mov
0x11111111
%
eax
.
byte
98
226
125
40
24
37
122
68
3
0
/
/
vbroadcastss
0x3447a
(
%
rip
)
%
ymm20
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
250
16
53
250
67
3
0
/
/
vmovss
0x343fa
(
%
rip
)
%
xmm6
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
250
16
45
238
67
3
0
/
/
vmovss
0x343ee
(
%
rip
)
%
xmm5
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
98
33
124
8
40
201
/
/
vmovaps
%
xmm17
%
xmm25
.
byte
98
146
125
40
24
225
/
/
vbroadcastss
%
xmm25
%
ymm4
.
byte
98
145
92
40
88
231
/
/
vaddps
%
ymm31
%
ymm4
%
ymm4
.
byte
197
148
95
228
/
/
vmaxps
%
ymm4
%
ymm13
%
ymm4
.
byte
98
97
124
8
46
207
/
/
vucomiss
%
xmm7
%
xmm25
.
byte
98
177
124
40
40
203
/
/
vmovaps
%
ymm19
%
ymm1
.
byte
98
225
124
40
40
249
/
/
vmovaps
%
ymm1
%
ymm23
.
byte
119
12
/
/
ja
8152
<
_sk_bilerp_clamp_8888_skx
+
0xff
>
.
byte
98
177
124
40
40
205
/
/
vmovaps
%
ymm21
%
ymm1
.
byte
98
225
124
40
40
249
/
/
vmovaps
%
ymm1
%
ymm23
.
byte
98
177
92
40
93
226
/
/
vminps
%
ymm18
%
ymm4
%
ymm4
.
byte
197
254
91
228
/
/
vcvttps2dq
%
ymm4
%
ymm4
.
byte
98
98
125
32
64
196
/
/
vpmulld
%
ymm4
%
ymm16
%
ymm24
.
byte
98
177
124
8
40
225
/
/
vmovaps
%
xmm17
%
xmm4
.
byte
98
98
125
40
24
212
/
/
vbroadcastss
%
xmm4
%
ymm26
.
byte
98
97
44
32
88
208
/
/
vaddps
%
ymm0
%
ymm26
%
ymm26
.
byte
98
1
20
40
95
210
/
/
vmaxps
%
ymm26
%
ymm13
%
ymm26
.
byte
98
65
44
32
93
214
/
/
vminps
%
ymm14
%
ymm26
%
ymm26
.
byte
98
1
126
40
91
210
/
/
vcvttps2dq
%
ymm26
%
ymm26
.
byte
98
17
61
32
254
218
/
/
vpaddd
%
ymm26
%
ymm24
%
ymm11
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
5
239
255
/
/
vpxor
%
ymm15
%
ymm15
%
ymm15
.
byte
196
2
117
144
60
152
/
/
vpgatherdd
%
ymm1
(
%
r8
%
ymm11
4
)
%
ymm15
.
byte
197
251
146
200
/
/
kmovd
%
eax
%
k1
.
byte
98
209
127
169
111
207
/
/
vmovdqu8
%
ymm15
%
ymm1
{
%
k1
}
{
z
}
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
33
116
40
89
212
/
/
vmulps
%
ymm20
%
ymm1
%
ymm26
.
byte
196
193
117
114
215
8
/
/
vpsrld
0x8
%
ymm15
%
ymm1
.
byte
98
241
127
169
111
201
/
/
vmovdqu8
%
ymm1
%
ymm1
{
%
k1
}
{
z
}
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
33
116
40
89
220
/
/
vmulps
%
ymm20
%
ymm1
%
ymm27
.
byte
196
193
117
114
215
16
/
/
vpsrld
0x10
%
ymm15
%
ymm1
.
byte
98
241
127
169
111
201
/
/
vmovdqu8
%
ymm1
%
ymm1
{
%
k1
}
{
z
}
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
33
116
40
89
228
/
/
vmulps
%
ymm20
%
ymm1
%
ymm28
.
byte
196
193
117
114
215
24
/
/
vpsrld
0x18
%
ymm15
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
98
33
116
40
89
236
/
/
vmulps
%
ymm20
%
ymm1
%
ymm29
.
byte
197
248
46
231
/
/
vucomiss
%
xmm7
%
xmm4
.
byte
98
65
124
40
40
242
/
/
vmovaps
%
ymm10
%
ymm30
.
byte
119
6
/
/
ja
81fd
<
_sk_bilerp_clamp_8888_skx
+
0x1aa
>
.
byte
98
65
124
40
40
244
/
/
vmovaps
%
ymm12
%
ymm30
.
byte
98
145
68
32
89
206
/
/
vmulps
%
ymm30
%
ymm23
%
ymm1
.
byte
98
18
117
40
184
202
/
/
vfmadd231ps
%
ymm26
%
ymm1
%
ymm9
.
byte
98
18
117
40
184
195
/
/
vfmadd231ps
%
ymm27
%
ymm1
%
ymm8
.
byte
98
146
117
40
184
212
/
/
vfmadd231ps
%
ymm28
%
ymm1
%
ymm2
.
byte
98
146
117
40
184
221
/
/
vfmadd231ps
%
ymm29
%
ymm1
%
ymm3
.
byte
197
218
88
230
/
/
vaddss
%
xmm6
%
xmm4
%
xmm4
.
byte
197
248
46
236
/
/
vucomiss
%
xmm4
%
xmm5
.
byte
15
131
63
255
255
255
/
/
jae
8168
<
_sk_bilerp_clamp_8888_skx
+
0x115
>
.
byte
98
97
54
0
88
206
/
/
vaddss
%
xmm6
%
xmm25
%
xmm25
.
byte
98
145
124
8
46
233
/
/
vucomiss
%
xmm25
%
xmm5
.
byte
15
131
231
254
255
255
/
/
jae
8122
<
_sk_bilerp_clamp_8888_skx
+
0xcf
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
200
/
/
vmovaps
%
ymm9
%
ymm0
.
byte
197
124
41
193
/
/
vmovaps
%
ymm8
%
ymm1
.
byte
98
177
124
40
40
230
/
/
vmovaps
%
ymm22
%
ymm4
.
byte
197
252
16
108
36
136
/
/
vmovups
-
0x78
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
168
/
/
vmovups
-
0x58
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
HIDDEN
_sk_start_pipeline_hsw
.
globl
_sk_start_pipeline_hsw
FUNCTION
(
_sk_start_pipeline_hsw
)
_sk_start_pipeline_hsw
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
151
0
0
0
/
/
jae
832a
<
_sk_start_pipeline_hsw
+
0xca
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
8
/
/
lea
0x8
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
119
67
/
/
ja
82ec
<
_sk_start_pipeline_hsw
+
0x8c
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
8
/
/
lea
0x8
(
%
r12
)
%
rdx
.
byte
73
131
196
16
/
/
add
0x10
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
193
/
/
jbe
82ad
<
_sk_start_pipeline_hsw
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
41
/
/
je
831d
<
_sk_start_pipeline_hsw
+
0xbd
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
255
195
/
/
inc
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
15
133
117
255
255
255
/
/
jne
829f
<
_sk_start_pipeline_hsw
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
197
248
119
/
/
vzeroupper
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_hsw
.
globl
_sk_just_return_hsw
FUNCTION
(
_sk_just_return_hsw
)
_sk_just_return_hsw
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_hsw
.
globl
_sk_seed_shader_hsw
FUNCTION
(
_sk_seed_shader_hsw
)
_sk_seed_shader_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
249
110
194
/
/
vmovd
%
edx
%
xmm0
.
byte
196
226
125
88
192
/
/
vpbroadcastd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
249
110
201
/
/
vmovd
%
ecx
%
xmm1
.
byte
196
226
125
88
201
/
/
vpbroadcastd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
146
65
3
0
/
/
vbroadcastss
0x34192
(
%
rip
)
%
ymm2
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
244
88
202
/
/
vaddps
%
ymm2
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
21
135
65
3
0
/
/
vbroadcastss
0x34187
(
%
rip
)
%
ymm2
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dither_hsw
.
globl
_sk_dither_hsw
FUNCTION
(
_sk_dither_hsw
)
_sk_dither_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
121
110
194
/
/
vmovd
%
edx
%
xmm8
.
byte
196
66
125
88
192
/
/
vpbroadcastd
%
xmm8
%
ymm8
.
byte
197
61
254
5
66
67
3
0
/
/
vpaddd
0x34342
(
%
rip
)
%
ymm8
%
ymm8
#
3c6e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x494
>
.
byte
197
121
110
201
/
/
vmovd
%
ecx
%
xmm9
.
byte
196
66
125
88
201
/
/
vpbroadcastd
%
xmm9
%
ymm9
.
byte
196
65
53
239
200
/
/
vpxor
%
ymm8
%
ymm9
%
ymm9
.
byte
196
98
125
88
21
75
65
3
0
/
/
vpbroadcastd
0x3414b
(
%
rip
)
%
ymm10
#
3c500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b4
>
.
byte
196
65
53
219
218
/
/
vpand
%
ymm10
%
ymm9
%
ymm11
.
byte
196
193
37
114
243
5
/
/
vpslld
0x5
%
ymm11
%
ymm11
.
byte
196
65
61
219
210
/
/
vpand
%
ymm10
%
ymm8
%
ymm10
.
byte
196
193
45
114
242
4
/
/
vpslld
0x4
%
ymm10
%
ymm10
.
byte
196
98
125
88
37
48
65
3
0
/
/
vpbroadcastd
0x34130
(
%
rip
)
%
ymm12
#
3c504
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b8
>
.
byte
196
98
125
88
45
43
65
3
0
/
/
vpbroadcastd
0x3412b
(
%
rip
)
%
ymm13
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
196
65
53
219
245
/
/
vpand
%
ymm13
%
ymm9
%
ymm14
.
byte
196
193
13
114
246
2
/
/
vpslld
0x2
%
ymm14
%
ymm14
.
byte
196
65
37
235
222
/
/
vpor
%
ymm14
%
ymm11
%
ymm11
.
byte
196
65
61
219
237
/
/
vpand
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
21
254
237
/
/
vpaddd
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
21
235
210
/
/
vpor
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
53
219
204
/
/
vpand
%
ymm12
%
ymm9
%
ymm9
.
byte
196
193
53
114
209
1
/
/
vpsrld
0x1
%
ymm9
%
ymm9
.
byte
196
65
61
219
196
/
/
vpand
%
ymm12
%
ymm8
%
ymm8
.
byte
196
193
61
114
208
2
/
/
vpsrld
0x2
%
ymm8
%
ymm8
.
byte
196
65
45
235
192
/
/
vpor
%
ymm8
%
ymm10
%
ymm8
.
byte
196
65
61
235
195
/
/
vpor
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
221
64
3
0
/
/
vbroadcastss
0x340dd
(
%
rip
)
%
ymm9
#
3c50c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c0
>
.
byte
196
98
125
24
21
216
64
3
0
/
/
vbroadcastss
0x340d8
(
%
rip
)
%
ymm10
#
3c510
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c4
>
.
byte
196
66
61
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm8
%
ymm10
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
65
44
89
192
/
/
vmulps
%
ymm8
%
ymm10
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
88
201
/
/
vaddps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
88
210
/
/
vaddps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
252
93
195
/
/
vminps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
244
93
203
/
/
vminps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
236
93
211
/
/
vminps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_hsw
.
globl
_sk_uniform_color_hsw
FUNCTION
(
_sk_uniform_color_hsw
)
_sk_uniform_color_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm0
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_hsw
.
globl
_sk_black_color_hsw
FUNCTION
(
_sk_black_color_hsw
)
_sk_black_color_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
96
64
3
0
/
/
vbroadcastss
0x34060
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_hsw
.
globl
_sk_white_color_hsw
FUNCTION
(
_sk_white_color_hsw
)
_sk_white_color_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
71
64
3
0
/
/
vbroadcastss
0x34047
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
252
40
216
/
/
vmovaps
%
ymm0
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_rgba_hsw
.
globl
_sk_load_rgba_hsw
FUNCTION
(
_sk_load_rgba_hsw
)
_sk_load_rgba_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
197
252
16
72
32
/
/
vmovups
0x20
(
%
rax
)
%
ymm1
.
byte
197
252
16
80
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm2
.
byte
197
252
16
88
96
/
/
vmovups
0x60
(
%
rax
)
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_rgba_hsw
.
globl
_sk_store_rgba_hsw
FUNCTION
(
_sk_store_rgba_hsw
)
_sk_store_rgba_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
197
252
17
72
32
/
/
vmovups
%
ymm1
0x20
(
%
rax
)
.
byte
197
252
17
80
64
/
/
vmovups
%
ymm2
0x40
(
%
rax
)
.
byte
197
252
17
88
96
/
/
vmovups
%
ymm3
0x60
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_hsw
.
globl
_sk_clear_hsw
FUNCTION
(
_sk_clear_hsw
)
_sk_clear_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_hsw
.
globl
_sk_srcatop_hsw
FUNCTION
(
_sk_srcatop_hsw
)
_sk_srcatop_hsw
:
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
230
63
3
0
/
/
vbroadcastss
0x33fe6
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
226
61
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
196
226
61
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
196
226
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
196
194
69
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_hsw
.
globl
_sk_dstatop_hsw
FUNCTION
(
_sk_dstatop_hsw
)
_sk_dstatop_hsw
:
.
byte
196
98
125
24
5
181
63
3
0
/
/
vbroadcastss
0x33fb5
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
226
101
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
226
101
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
226
101
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
60
89
195
/
/
vmulps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_hsw
.
globl
_sk_srcin_hsw
FUNCTION
(
_sk_srcin_hsw
)
_sk_srcin_hsw
:
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_hsw
.
globl
_sk_dstin_hsw
FUNCTION
(
_sk_dstin_hsw
)
_sk_dstin_hsw
:
.
byte
197
228
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
228
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
228
89
214
/
/
vmulps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_hsw
.
globl
_sk_srcout_hsw
FUNCTION
(
_sk_srcout_hsw
)
_sk_srcout_hsw
:
.
byte
196
98
125
24
5
88
63
3
0
/
/
vbroadcastss
0x33f58
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_hsw
.
globl
_sk_dstout_hsw
FUNCTION
(
_sk_dstout_hsw
)
_sk_dstout_hsw
:
.
byte
196
226
125
24
5
55
63
3
0
/
/
vbroadcastss
0x33f37
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
92
219
/
/
vsubps
%
ymm3
%
ymm0
%
ymm3
.
byte
197
228
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
228
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
228
89
214
/
/
vmulps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_hsw
.
globl
_sk_srcover_hsw
FUNCTION
(
_sk_srcover_hsw
)
_sk_srcover_hsw
:
.
byte
196
98
125
24
5
22
63
3
0
/
/
vbroadcastss
0x33f16
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
93
184
192
/
/
vfmadd231ps
%
ymm8
%
ymm4
%
ymm0
.
byte
196
194
85
184
200
/
/
vfmadd231ps
%
ymm8
%
ymm5
%
ymm1
.
byte
196
194
77
184
208
/
/
vfmadd231ps
%
ymm8
%
ymm6
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_hsw
.
globl
_sk_dstover_hsw
FUNCTION
(
_sk_dstover_hsw
)
_sk_dstover_hsw
:
.
byte
196
98
125
24
5
241
62
3
0
/
/
vbroadcastss
0x33ef1
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
196
226
61
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm8
%
ymm0
.
byte
196
226
61
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm8
%
ymm1
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
196
226
61
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_hsw
.
globl
_sk_modulate_hsw
FUNCTION
(
_sk_modulate_hsw
)
_sk_modulate_hsw
:
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_hsw
.
globl
_sk_multiply_hsw
FUNCTION
(
_sk_multiply_hsw
)
_sk_multiply_hsw
:
.
byte
196
98
125
24
5
184
62
3
0
/
/
vbroadcastss
0x33eb8
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
52
89
208
/
/
vmulps
%
ymm0
%
ymm9
%
ymm10
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
61
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm10
.
byte
196
194
93
168
194
/
/
vfmadd213ps
%
ymm10
%
ymm4
%
ymm0
.
byte
197
52
89
209
/
/
vmulps
%
ymm1
%
ymm9
%
ymm10
.
byte
196
98
61
184
213
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm10
.
byte
196
194
85
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm5
%
ymm1
.
byte
197
52
89
210
/
/
vmulps
%
ymm2
%
ymm9
%
ymm10
.
byte
196
98
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm10
.
byte
196
194
77
168
210
/
/
vfmadd213ps
%
ymm10
%
ymm6
%
ymm2
.
byte
197
52
89
203
/
/
vmulps
%
ymm3
%
ymm9
%
ymm9
.
byte
196
66
69
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm7
%
ymm8
.
byte
196
194
69
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__hsw
.
globl
_sk_plus__hsw
FUNCTION
(
_sk_plus__hsw
)
_sk_plus__hsw
:
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
103
62
3
0
/
/
vbroadcastss
0x33e67
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
244
88
205
/
/
vaddps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
236
88
214
/
/
vaddps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_hsw
.
globl
_sk_screen_hsw
FUNCTION
(
_sk_screen_hsw
)
_sk_screen_hsw
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
196
194
93
172
192
/
/
vfnmadd213ps
%
ymm8
%
ymm4
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
196
194
85
172
200
/
/
vfnmadd213ps
%
ymm8
%
ymm5
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
196
194
77
172
208
/
/
vfnmadd213ps
%
ymm8
%
ymm6
%
ymm2
.
byte
197
100
88
199
/
/
vaddps
%
ymm7
%
ymm3
%
ymm8
.
byte
196
194
69
172
216
/
/
vfnmadd213ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__hsw
.
globl
_sk_xor__hsw
FUNCTION
(
_sk_xor__hsw
)
_sk_xor__hsw
:
.
byte
196
98
125
24
5
18
62
3
0
/
/
vbroadcastss
0x33e12
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
226
61
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
180
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
226
61
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
180
89
210
/
/
vmulps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
226
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
180
89
219
/
/
vmulps
%
ymm3
%
ymm9
%
ymm3
.
byte
196
98
69
168
195
/
/
vfmadd213ps
%
ymm3
%
ymm7
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
195
/
/
vmovaps
%
ymm8
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_hsw
.
globl
_sk_darken_hsw
FUNCTION
(
_sk_darken_hsw
)
_sk_darken_hsw
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
95
193
/
/
vmaxps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
95
201
/
/
vmaxps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
95
209
/
/
vmaxps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
150
61
3
0
/
/
vbroadcastss
0x33d96
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_hsw
.
globl
_sk_lighten_hsw
FUNCTION
(
_sk_lighten_hsw
)
_sk_lighten_hsw
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
93
193
/
/
vminps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
93
201
/
/
vminps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
93
209
/
/
vminps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
65
61
3
0
/
/
vbroadcastss
0x33d41
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_hsw
.
globl
_sk_difference_hsw
FUNCTION
(
_sk_difference_hsw
)
_sk_difference_hsw
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
93
193
/
/
vminps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
93
201
/
/
vminps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
93
209
/
/
vminps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
224
60
3
0
/
/
vbroadcastss
0x33ce0
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_hsw
.
globl
_sk_exclusion_hsw
FUNCTION
(
_sk_exclusion_hsw
)
_sk_exclusion_hsw
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
154
60
3
0
/
/
vbroadcastss
0x33c9a
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colorburn_hsw
.
globl
_sk_colorburn_hsw
FUNCTION
(
_sk_colorburn_hsw
)
_sk_colorburn_hsw
:
.
byte
196
98
125
24
5
132
60
3
0
/
/
vbroadcastss
0x33c84
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
52
89
216
/
/
vmulps
%
ymm0
%
ymm9
%
ymm11
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
65
124
194
226
0
/
/
vcmpeqps
%
ymm10
%
ymm0
%
ymm12
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
68
92
236
/
/
vsubps
%
ymm4
%
ymm7
%
ymm13
.
byte
197
20
89
235
/
/
vmulps
%
ymm3
%
ymm13
%
ymm13
.
byte
197
252
83
192
/
/
vrcpps
%
ymm0
%
ymm0
.
byte
197
148
89
192
/
/
vmulps
%
ymm0
%
ymm13
%
ymm0
.
byte
197
60
89
236
/
/
vmulps
%
ymm4
%
ymm8
%
ymm13
.
byte
197
196
93
192
/
/
vminps
%
ymm0
%
ymm7
%
ymm0
.
byte
197
196
92
192
/
/
vsubps
%
ymm0
%
ymm7
%
ymm0
.
byte
196
194
101
168
195
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm0
.
byte
197
148
88
192
/
/
vaddps
%
ymm0
%
ymm13
%
ymm0
.
byte
196
195
125
74
197
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm0
%
ymm0
.
byte
197
92
194
231
0
/
/
vcmpeqps
%
ymm7
%
ymm4
%
ymm12
.
byte
197
36
88
220
/
/
vaddps
%
ymm4
%
ymm11
%
ymm11
.
byte
196
195
125
74
195
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm0
%
ymm0
.
byte
197
52
89
217
/
/
vmulps
%
ymm1
%
ymm9
%
ymm11
.
byte
196
65
116
194
226
0
/
/
vcmpeqps
%
ymm10
%
ymm1
%
ymm12
.
byte
197
68
92
237
/
/
vsubps
%
ymm5
%
ymm7
%
ymm13
.
byte
197
20
89
235
/
/
vmulps
%
ymm3
%
ymm13
%
ymm13
.
byte
197
252
83
201
/
/
vrcpps
%
ymm1
%
ymm1
.
byte
197
148
89
201
/
/
vmulps
%
ymm1
%
ymm13
%
ymm1
.
byte
197
60
89
237
/
/
vmulps
%
ymm5
%
ymm8
%
ymm13
.
byte
197
196
93
201
/
/
vminps
%
ymm1
%
ymm7
%
ymm1
.
byte
197
196
92
201
/
/
vsubps
%
ymm1
%
ymm7
%
ymm1
.
byte
196
194
101
168
203
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm1
.
byte
197
148
88
201
/
/
vaddps
%
ymm1
%
ymm13
%
ymm1
.
byte
196
195
117
74
205
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm1
%
ymm1
.
byte
197
84
194
231
0
/
/
vcmpeqps
%
ymm7
%
ymm5
%
ymm12
.
byte
197
36
88
221
/
/
vaddps
%
ymm5
%
ymm11
%
ymm11
.
byte
196
195
117
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm1
%
ymm1
.
byte
197
52
89
202
/
/
vmulps
%
ymm2
%
ymm9
%
ymm9
.
byte
196
65
108
194
210
0
/
/
vcmpeqps
%
ymm10
%
ymm2
%
ymm10
.
byte
197
68
92
222
/
/
vsubps
%
ymm6
%
ymm7
%
ymm11
.
byte
197
36
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm11
.
byte
197
252
83
210
/
/
vrcpps
%
ymm2
%
ymm2
.
byte
197
164
89
210
/
/
vmulps
%
ymm2
%
ymm11
%
ymm2
.
byte
197
60
89
222
/
/
vmulps
%
ymm6
%
ymm8
%
ymm11
.
byte
197
196
93
210
/
/
vminps
%
ymm2
%
ymm7
%
ymm2
.
byte
197
196
92
210
/
/
vsubps
%
ymm2
%
ymm7
%
ymm2
.
byte
196
194
101
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm3
%
ymm2
.
byte
197
164
88
210
/
/
vaddps
%
ymm2
%
ymm11
%
ymm2
.
byte
196
195
109
74
211
160
/
/
vblendvps
%
ymm10
%
ymm11
%
ymm2
%
ymm2
.
byte
197
76
194
215
0
/
/
vcmpeqps
%
ymm7
%
ymm6
%
ymm10
.
byte
197
52
88
206
/
/
vaddps
%
ymm6
%
ymm9
%
ymm9
.
byte
196
195
109
74
209
160
/
/
vblendvps
%
ymm10
%
ymm9
%
ymm2
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colordodge_hsw
.
globl
_sk_colordodge_hsw
FUNCTION
(
_sk_colordodge_hsw
)
_sk_colordodge_hsw
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
148
59
3
0
/
/
vbroadcastss
0x33b94
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
215
/
/
vsubps
%
ymm7
%
ymm9
%
ymm10
.
byte
197
44
89
216
/
/
vmulps
%
ymm0
%
ymm10
%
ymm11
.
byte
197
52
92
203
/
/
vsubps
%
ymm3
%
ymm9
%
ymm9
.
byte
197
100
89
228
/
/
vmulps
%
ymm4
%
ymm3
%
ymm12
.
byte
197
100
92
232
/
/
vsubps
%
ymm0
%
ymm3
%
ymm13
.
byte
196
65
124
83
237
/
/
vrcpps
%
ymm13
%
ymm13
.
byte
196
65
28
89
229
/
/
vmulps
%
ymm13
%
ymm12
%
ymm12
.
byte
197
52
89
236
/
/
vmulps
%
ymm4
%
ymm9
%
ymm13
.
byte
196
65
68
93
228
/
/
vminps
%
ymm12
%
ymm7
%
ymm12
.
byte
196
66
101
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm12
.
byte
196
65
20
88
228
/
/
vaddps
%
ymm12
%
ymm13
%
ymm12
.
byte
197
20
88
232
/
/
vaddps
%
ymm0
%
ymm13
%
ymm13
.
byte
197
252
194
195
0
/
/
vcmpeqps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
195
29
74
197
0
/
/
vblendvps
%
ymm0
%
ymm13
%
ymm12
%
ymm0
.
byte
196
65
92
194
224
0
/
/
vcmpeqps
%
ymm8
%
ymm4
%
ymm12
.
byte
196
195
125
74
195
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm0
%
ymm0
.
byte
197
44
89
217
/
/
vmulps
%
ymm1
%
ymm10
%
ymm11
.
byte
197
100
89
229
/
/
vmulps
%
ymm5
%
ymm3
%
ymm12
.
byte
197
100
92
233
/
/
vsubps
%
ymm1
%
ymm3
%
ymm13
.
byte
196
65
124
83
237
/
/
vrcpps
%
ymm13
%
ymm13
.
byte
196
65
28
89
229
/
/
vmulps
%
ymm13
%
ymm12
%
ymm12
.
byte
197
52
89
237
/
/
vmulps
%
ymm5
%
ymm9
%
ymm13
.
byte
196
65
68
93
228
/
/
vminps
%
ymm12
%
ymm7
%
ymm12
.
byte
196
66
101
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm12
.
byte
196
65
20
88
228
/
/
vaddps
%
ymm12
%
ymm13
%
ymm12
.
byte
197
20
88
233
/
/
vaddps
%
ymm1
%
ymm13
%
ymm13
.
byte
197
244
194
203
0
/
/
vcmpeqps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
195
29
74
205
16
/
/
vblendvps
%
ymm1
%
ymm13
%
ymm12
%
ymm1
.
byte
196
65
84
194
224
0
/
/
vcmpeqps
%
ymm8
%
ymm5
%
ymm12
.
byte
196
195
117
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm1
%
ymm1
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
197
100
92
226
/
/
vsubps
%
ymm2
%
ymm3
%
ymm12
.
byte
196
65
124
83
228
/
/
vrcpps
%
ymm12
%
ymm12
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
52
89
230
/
/
vmulps
%
ymm6
%
ymm9
%
ymm12
.
byte
196
65
68
93
219
/
/
vminps
%
ymm11
%
ymm7
%
ymm11
.
byte
196
66
101
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm3
%
ymm11
.
byte
196
65
28
88
219
/
/
vaddps
%
ymm11
%
ymm12
%
ymm11
.
byte
197
28
88
226
/
/
vaddps
%
ymm2
%
ymm12
%
ymm12
.
byte
197
236
194
211
0
/
/
vcmpeqps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
195
37
74
212
32
/
/
vblendvps
%
ymm2
%
ymm12
%
ymm11
%
ymm2
.
byte
196
65
76
194
192
0
/
/
vcmpeqps
%
ymm8
%
ymm6
%
ymm8
.
byte
196
195
109
74
210
128
/
/
vblendvps
%
ymm8
%
ymm10
%
ymm2
%
ymm2
.
byte
196
194
69
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_hsw
.
globl
_sk_hardlight_hsw
FUNCTION
(
_sk_hardlight_hsw
)
_sk_hardlight_hsw
:
.
byte
196
98
125
24
5
174
58
3
0
/
/
vbroadcastss
0x33aae
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
215
/
/
vsubps
%
ymm7
%
ymm8
%
ymm10
.
byte
197
44
89
216
/
/
vmulps
%
ymm0
%
ymm10
%
ymm11
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
61
184
220
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm11
.
byte
197
124
88
200
/
/
vaddps
%
ymm0
%
ymm0
%
ymm9
.
byte
197
52
194
227
2
/
/
vcmpleps
%
ymm3
%
ymm9
%
ymm12
.
byte
197
124
89
204
/
/
vmulps
%
ymm4
%
ymm0
%
ymm9
.
byte
196
65
52
88
233
/
/
vaddps
%
ymm9
%
ymm9
%
ymm13
.
byte
197
100
89
207
/
/
vmulps
%
ymm7
%
ymm3
%
ymm9
.
byte
197
68
92
244
/
/
vsubps
%
ymm4
%
ymm7
%
ymm14
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
180
92
192
/
/
vsubps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
197
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm0
%
ymm0
.
byte
197
164
88
192
/
/
vaddps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
44
89
217
/
/
vmulps
%
ymm1
%
ymm10
%
ymm11
.
byte
196
98
61
184
221
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm11
.
byte
197
116
88
225
/
/
vaddps
%
ymm1
%
ymm1
%
ymm12
.
byte
197
28
194
227
2
/
/
vcmpleps
%
ymm3
%
ymm12
%
ymm12
.
byte
197
116
89
237
/
/
vmulps
%
ymm5
%
ymm1
%
ymm13
.
byte
196
65
20
88
237
/
/
vaddps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
68
92
245
/
/
vsubps
%
ymm5
%
ymm7
%
ymm14
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
116
89
206
/
/
vmulps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
180
92
201
/
/
vsubps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
195
117
74
205
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm1
%
ymm1
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
196
98
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm10
.
byte
197
108
88
218
/
/
vaddps
%
ymm2
%
ymm2
%
ymm11
.
byte
197
36
194
219
2
/
/
vcmpleps
%
ymm3
%
ymm11
%
ymm11
.
byte
197
108
89
230
/
/
vmulps
%
ymm6
%
ymm2
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
68
92
238
/
/
vsubps
%
ymm6
%
ymm7
%
ymm13
.
byte
197
228
92
210
/
/
vsubps
%
ymm2
%
ymm3
%
ymm2
.
byte
196
193
108
89
213
/
/
vmulps
%
ymm13
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
180
92
210
/
/
vsubps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
195
109
74
212
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm2
%
ymm2
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_hsw
.
globl
_sk_overlay_hsw
FUNCTION
(
_sk_overlay_hsw
)
_sk_overlay_hsw
:
.
byte
196
98
125
24
5
226
57
3
0
/
/
vbroadcastss
0x339e2
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
215
/
/
vsubps
%
ymm7
%
ymm8
%
ymm10
.
byte
197
44
89
216
/
/
vmulps
%
ymm0
%
ymm10
%
ymm11
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
61
184
220
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm11
.
byte
197
92
88
204
/
/
vaddps
%
ymm4
%
ymm4
%
ymm9
.
byte
197
52
194
231
2
/
/
vcmpleps
%
ymm7
%
ymm9
%
ymm12
.
byte
197
124
89
204
/
/
vmulps
%
ymm4
%
ymm0
%
ymm9
.
byte
196
65
52
88
233
/
/
vaddps
%
ymm9
%
ymm9
%
ymm13
.
byte
197
100
89
207
/
/
vmulps
%
ymm7
%
ymm3
%
ymm9
.
byte
197
68
92
244
/
/
vsubps
%
ymm4
%
ymm7
%
ymm14
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
180
92
192
/
/
vsubps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
197
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm0
%
ymm0
.
byte
197
164
88
192
/
/
vaddps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
44
89
217
/
/
vmulps
%
ymm1
%
ymm10
%
ymm11
.
byte
196
98
61
184
221
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm11
.
byte
197
84
88
229
/
/
vaddps
%
ymm5
%
ymm5
%
ymm12
.
byte
197
28
194
231
2
/
/
vcmpleps
%
ymm7
%
ymm12
%
ymm12
.
byte
197
116
89
237
/
/
vmulps
%
ymm5
%
ymm1
%
ymm13
.
byte
196
65
20
88
237
/
/
vaddps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
68
92
245
/
/
vsubps
%
ymm5
%
ymm7
%
ymm14
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
116
89
206
/
/
vmulps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
180
92
201
/
/
vsubps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
195
117
74
205
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm1
%
ymm1
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
196
98
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm10
.
byte
197
76
88
222
/
/
vaddps
%
ymm6
%
ymm6
%
ymm11
.
byte
197
36
194
223
2
/
/
vcmpleps
%
ymm7
%
ymm11
%
ymm11
.
byte
197
108
89
230
/
/
vmulps
%
ymm6
%
ymm2
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
68
92
238
/
/
vsubps
%
ymm6
%
ymm7
%
ymm13
.
byte
197
228
92
210
/
/
vsubps
%
ymm2
%
ymm3
%
ymm2
.
byte
196
193
108
89
213
/
/
vmulps
%
ymm13
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
180
92
210
/
/
vsubps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
195
109
74
212
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm2
%
ymm2
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_softlight_hsw
.
globl
_sk_softlight_hsw
FUNCTION
(
_sk_softlight_hsw
)
_sk_softlight_hsw
:
.
byte
197
252
17
84
36
200
/
/
vmovups
%
ymm2
-
0x38
(
%
rsp
)
.
byte
197
252
40
209
/
/
vmovaps
%
ymm1
%
ymm2
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
52
194
215
1
/
/
vcmpltps
%
ymm7
%
ymm9
%
ymm10
.
byte
197
92
94
199
/
/
vdivps
%
ymm7
%
ymm4
%
ymm8
.
byte
196
67
53
74
232
160
/
/
vblendvps
%
ymm10
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
20
88
197
/
/
vaddps
%
ymm13
%
ymm13
%
ymm8
.
byte
196
65
60
88
192
/
/
vaddps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
66
61
168
192
/
/
vfmadd213ps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
98
125
24
29
253
56
3
0
/
/
vbroadcastss
0x338fd
(
%
rip
)
%
ymm11
#
3c514
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c8
>
.
byte
196
65
20
88
227
/
/
vaddps
%
ymm11
%
ymm13
%
ymm12
.
byte
196
65
28
89
192
/
/
vmulps
%
ymm8
%
ymm12
%
ymm8
.
byte
196
98
125
24
37
238
56
3
0
/
/
vbroadcastss
0x338ee
(
%
rip
)
%
ymm12
#
3c518
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2cc
>
.
byte
196
66
21
184
196
/
/
vfmadd231ps
%
ymm12
%
ymm13
%
ymm8
.
byte
196
65
124
82
245
/
/
vrsqrtps
%
ymm13
%
ymm14
.
byte
196
65
124
83
246
/
/
vrcpps
%
ymm14
%
ymm14
.
byte
196
65
12
92
245
/
/
vsubps
%
ymm13
%
ymm14
%
ymm14
.
byte
197
92
88
252
/
/
vaddps
%
ymm4
%
ymm4
%
ymm15
.
byte
196
65
4
88
255
/
/
vaddps
%
ymm15
%
ymm15
%
ymm15
.
byte
197
4
194
255
2
/
/
vcmpleps
%
ymm7
%
ymm15
%
ymm15
.
byte
196
67
13
74
240
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm14
%
ymm14
.
byte
197
116
88
249
/
/
vaddps
%
ymm1
%
ymm1
%
ymm15
.
byte
196
98
125
24
5
157
56
3
0
/
/
vbroadcastss
0x3389d
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
60
92
237
/
/
vsubps
%
ymm13
%
ymm8
%
ymm13
.
byte
197
132
92
195
/
/
vsubps
%
ymm3
%
ymm15
%
ymm0
.
byte
196
98
125
168
235
/
/
vfmadd213ps
%
ymm3
%
ymm0
%
ymm13
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
20
89
236
/
/
vmulps
%
ymm4
%
ymm13
%
ymm13
.
byte
196
226
101
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
4
194
243
2
/
/
vcmpleps
%
ymm3
%
ymm15
%
ymm14
.
byte
196
195
125
74
197
224
/
/
vblendvps
%
ymm14
%
ymm13
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
168
/
/
vmovups
%
ymm0
-
0x58
(
%
rsp
)
.
byte
197
212
94
199
/
/
vdivps
%
ymm7
%
ymm5
%
ymm0
.
byte
196
227
53
74
192
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm9
%
ymm0
.
byte
197
124
88
240
/
/
vaddps
%
ymm0
%
ymm0
%
ymm14
.
byte
196
65
12
88
246
/
/
vaddps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
66
13
168
246
/
/
vfmadd213ps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
124
88
251
/
/
vaddps
%
ymm11
%
ymm0
%
ymm15
.
byte
196
65
4
89
246
/
/
vmulps
%
ymm14
%
ymm15
%
ymm14
.
byte
196
66
125
184
244
/
/
vfmadd231ps
%
ymm12
%
ymm0
%
ymm14
.
byte
197
124
82
248
/
/
vrsqrtps
%
ymm0
%
ymm15
.
byte
196
65
124
83
255
/
/
vrcpps
%
ymm15
%
ymm15
.
byte
197
4
92
248
/
/
vsubps
%
ymm0
%
ymm15
%
ymm15
.
byte
197
84
88
237
/
/
vaddps
%
ymm5
%
ymm5
%
ymm13
.
byte
196
65
20
88
237
/
/
vaddps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
20
194
239
2
/
/
vcmpleps
%
ymm7
%
ymm13
%
ymm13
.
byte
196
67
5
74
238
208
/
/
vblendvps
%
ymm13
%
ymm14
%
ymm15
%
ymm13
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
108
88
242
/
/
vaddps
%
ymm2
%
ymm2
%
ymm14
.
byte
197
12
92
251
/
/
vsubps
%
ymm3
%
ymm14
%
ymm15
.
byte
196
226
5
168
195
/
/
vfmadd213ps
%
ymm3
%
ymm15
%
ymm0
.
byte
197
4
89
255
/
/
vmulps
%
ymm7
%
ymm15
%
ymm15
.
byte
196
65
4
89
237
/
/
vmulps
%
ymm13
%
ymm15
%
ymm13
.
byte
197
252
89
197
/
/
vmulps
%
ymm5
%
ymm0
%
ymm0
.
byte
196
98
101
184
237
/
/
vfmadd231ps
%
ymm5
%
ymm3
%
ymm13
.
byte
197
12
194
243
2
/
/
vcmpleps
%
ymm3
%
ymm14
%
ymm14
.
byte
196
99
21
74
240
224
/
/
vblendvps
%
ymm14
%
ymm0
%
ymm13
%
ymm14
.
byte
197
204
94
199
/
/
vdivps
%
ymm7
%
ymm6
%
ymm0
.
byte
196
227
53
74
192
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm9
%
ymm0
.
byte
197
124
88
200
/
/
vaddps
%
ymm0
%
ymm0
%
ymm9
.
byte
196
65
52
88
201
/
/
vaddps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
168
201
/
/
vfmadd213ps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
124
88
211
/
/
vaddps
%
ymm11
%
ymm0
%
ymm10
.
byte
196
65
44
89
201
/
/
vmulps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
66
125
184
204
/
/
vfmadd231ps
%
ymm12
%
ymm0
%
ymm9
.
byte
197
124
82
208
/
/
vrsqrtps
%
ymm0
%
ymm10
.
byte
196
65
124
83
210
/
/
vrcpps
%
ymm10
%
ymm10
.
byte
197
44
92
208
/
/
vsubps
%
ymm0
%
ymm10
%
ymm10
.
byte
197
76
88
222
/
/
vaddps
%
ymm6
%
ymm6
%
ymm11
.
byte
196
65
36
88
219
/
/
vaddps
%
ymm11
%
ymm11
%
ymm11
.
byte
197
36
194
223
2
/
/
vcmpleps
%
ymm7
%
ymm11
%
ymm11
.
byte
196
67
45
74
201
176
/
/
vblendvps
%
ymm11
%
ymm9
%
ymm10
%
ymm9
.
byte
197
124
16
100
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm12
.
byte
196
65
28
88
212
/
/
vaddps
%
ymm12
%
ymm12
%
ymm10
.
byte
197
44
92
219
/
/
vsubps
%
ymm3
%
ymm10
%
ymm11
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
226
37
168
195
/
/
vfmadd213ps
%
ymm3
%
ymm11
%
ymm0
.
byte
197
36
89
223
/
/
vmulps
%
ymm7
%
ymm11
%
ymm11
.
byte
196
65
36
89
201
/
/
vmulps
%
ymm9
%
ymm11
%
ymm9
.
byte
197
252
89
198
/
/
vmulps
%
ymm6
%
ymm0
%
ymm0
.
byte
196
98
101
184
206
/
/
vfmadd231ps
%
ymm6
%
ymm3
%
ymm9
.
byte
197
44
194
211
2
/
/
vcmpleps
%
ymm3
%
ymm10
%
ymm10
.
byte
196
99
53
74
200
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm9
%
ymm9
.
byte
197
60
92
215
/
/
vsubps
%
ymm7
%
ymm8
%
ymm10
.
byte
197
172
89
193
/
/
vmulps
%
ymm1
%
ymm10
%
ymm0
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
226
61
184
196
/
/
vfmadd231ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
252
88
68
36
168
/
/
vaddps
-
0x58
(
%
rsp
)
%
ymm0
%
ymm0
.
byte
197
172
89
202
/
/
vmulps
%
ymm2
%
ymm10
%
ymm1
.
byte
196
226
61
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm8
%
ymm1
.
byte
196
193
116
88
206
/
/
vaddps
%
ymm14
%
ymm1
%
ymm1
.
byte
196
193
44
89
212
/
/
vmulps
%
ymm12
%
ymm10
%
ymm2
.
byte
196
226
61
184
214
/
/
vfmadd231ps
%
ymm6
%
ymm8
%
ymm2
.
byte
196
193
108
88
209
/
/
vaddps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
194
69
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm7
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hue_hsw
.
globl
_sk_hue_hsw
FUNCTION
(
_sk_hue_hsw
)
_sk_hue_hsw
:
.
byte
197
124
40
194
/
/
vmovaps
%
ymm2
%
ymm8
.
byte
197
124
17
68
36
200
/
/
vmovups
%
ymm8
-
0x38
(
%
rsp
)
.
byte
197
252
17
76
36
168
/
/
vmovups
%
ymm1
-
0x58
(
%
rsp
)
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
108
89
203
/
/
vmulps
%
ymm3
%
ymm2
%
ymm9
.
byte
197
116
89
211
/
/
vmulps
%
ymm3
%
ymm1
%
ymm10
.
byte
197
60
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm11
.
byte
197
84
95
198
/
/
vmaxps
%
ymm6
%
ymm5
%
ymm8
.
byte
196
65
92
95
192
/
/
vmaxps
%
ymm8
%
ymm4
%
ymm8
.
byte
197
84
93
230
/
/
vminps
%
ymm6
%
ymm5
%
ymm12
.
byte
196
65
92
93
228
/
/
vminps
%
ymm12
%
ymm4
%
ymm12
.
byte
196
65
60
92
196
/
/
vsubps
%
ymm12
%
ymm8
%
ymm8
.
byte
197
60
89
227
/
/
vmulps
%
ymm3
%
ymm8
%
ymm12
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
232
/
/
vminps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
44
95
195
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
95
192
/
/
vmaxps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
65
60
92
245
/
/
vsubps
%
ymm13
%
ymm8
%
ymm14
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
12
194
248
0
/
/
vcmpeqps
%
ymm8
%
ymm14
%
ymm15
.
byte
196
65
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm9
.
byte
196
65
28
89
201
/
/
vmulps
%
ymm9
%
ymm12
%
ymm9
.
byte
196
65
52
94
206
/
/
vdivps
%
ymm14
%
ymm9
%
ymm9
.
byte
196
67
53
74
200
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm9
%
ymm9
.
byte
196
65
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
195
45
74
200
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm10
%
ymm1
.
byte
196
65
36
92
213
/
/
vsubps
%
ymm13
%
ymm11
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
67
45
74
224
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm10
%
ymm12
.
byte
196
98
125
24
53
181
54
3
0
/
/
vbroadcastss
0x336b5
(
%
rip
)
%
ymm14
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
61
176
54
3
0
/
/
vbroadcastss
0x336b0
(
%
rip
)
%
ymm15
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
239
/
/
vmulps
%
ymm15
%
ymm5
%
ymm13
.
byte
196
66
93
184
238
/
/
vfmadd231ps
%
ymm14
%
ymm4
%
ymm13
.
byte
196
226
125
24
5
161
54
3
0
/
/
vbroadcastss
0x336a1
(
%
rip
)
%
ymm0
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
98
77
184
232
/
/
vfmadd231ps
%
ymm0
%
ymm6
%
ymm13
.
byte
196
65
116
89
215
/
/
vmulps
%
ymm15
%
ymm1
%
ymm10
.
byte
196
66
53
184
214
/
/
vfmadd231ps
%
ymm14
%
ymm9
%
ymm10
.
byte
196
98
29
184
208
/
/
vfmadd231ps
%
ymm0
%
ymm12
%
ymm10
.
byte
196
66
101
170
234
/
/
vfmsub213ps
%
ymm10
%
ymm3
%
ymm13
.
byte
196
65
52
88
213
/
/
vaddps
%
ymm13
%
ymm9
%
ymm10
.
byte
196
65
116
88
221
/
/
vaddps
%
ymm13
%
ymm1
%
ymm11
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
193
36
93
204
/
/
vminps
%
ymm12
%
ymm11
%
ymm1
.
byte
197
44
93
233
/
/
vminps
%
ymm1
%
ymm10
%
ymm13
.
byte
196
65
36
89
207
/
/
vmulps
%
ymm15
%
ymm11
%
ymm9
.
byte
196
66
45
184
206
/
/
vfmadd231ps
%
ymm14
%
ymm10
%
ymm9
.
byte
196
98
29
184
200
/
/
vfmadd231ps
%
ymm0
%
ymm12
%
ymm9
.
byte
196
193
44
92
193
/
/
vsubps
%
ymm9
%
ymm10
%
ymm0
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
193
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm1
.
byte
197
252
94
193
/
/
vdivps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
65
36
92
241
/
/
vsubps
%
ymm9
%
ymm11
%
ymm14
.
byte
196
65
52
89
246
/
/
vmulps
%
ymm14
%
ymm9
%
ymm14
.
byte
197
12
94
241
/
/
vdivps
%
ymm1
%
ymm14
%
ymm14
.
byte
196
65
28
92
249
/
/
vsubps
%
ymm9
%
ymm12
%
ymm15
.
byte
196
65
52
89
255
/
/
vmulps
%
ymm15
%
ymm9
%
ymm15
.
byte
197
132
94
201
/
/
vdivps
%
ymm1
%
ymm15
%
ymm1
.
byte
196
65
60
194
237
2
/
/
vcmpleps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
52
88
246
/
/
vaddps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
67
13
74
243
208
/
/
vblendvps
%
ymm13
%
ymm11
%
ymm14
%
ymm14
.
byte
196
65
36
95
220
/
/
vmaxps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
180
88
201
/
/
vaddps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
195
117
74
204
208
/
/
vblendvps
%
ymm13
%
ymm12
%
ymm1
%
ymm1
.
byte
197
180
88
192
/
/
vaddps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
194
208
/
/
vblendvps
%
ymm13
%
ymm10
%
ymm0
%
ymm0
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
44
95
211
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
124
92
217
/
/
vsubps
%
ymm9
%
ymm0
%
ymm11
.
byte
196
65
28
92
233
/
/
vsubps
%
ymm9
%
ymm12
%
ymm13
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
194
250
1
/
/
vcmpltps
%
ymm10
%
ymm12
%
ymm15
.
byte
196
65
44
92
209
/
/
vsubps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
36
94
218
/
/
vdivps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
52
88
219
/
/
vaddps
%
ymm11
%
ymm9
%
ymm11
.
byte
196
195
125
74
195
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm0
%
ymm0
.
byte
196
65
12
92
217
/
/
vsubps
%
ymm9
%
ymm14
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
218
/
/
vdivps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
52
88
219
/
/
vaddps
%
ymm11
%
ymm9
%
ymm11
.
byte
196
67
13
74
219
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
116
92
241
/
/
vsubps
%
ymm9
%
ymm1
%
ymm14
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
195
117
74
201
240
/
/
vblendvps
%
ymm15
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
36
95
200
/
/
vmaxps
%
ymm8
%
ymm11
%
ymm9
.
byte
196
65
116
95
192
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
226
125
24
13
98
53
3
0
/
/
vbroadcastss
0x33562
(
%
rip
)
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
116
92
215
/
/
vsubps
%
ymm7
%
ymm1
%
ymm10
.
byte
197
172
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
116
92
219
/
/
vsubps
%
ymm3
%
ymm1
%
ymm11
.
byte
196
226
37
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm2
.
byte
197
236
88
192
/
/
vaddps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
172
89
76
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm10
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
172
89
84
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm10
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
208
/
/
vaddps
%
ymm8
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_saturation_hsw
.
globl
_sk_saturation_hsw
FUNCTION
(
_sk_saturation_hsw
)
_sk_saturation_hsw
:
.
byte
197
124
40
194
/
/
vmovaps
%
ymm2
%
ymm8
.
byte
197
252
17
76
36
168
/
/
vmovups
%
ymm1
-
0x58
(
%
rsp
)
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
197
100
89
213
/
/
vmulps
%
ymm5
%
ymm3
%
ymm10
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
252
17
68
36
200
/
/
vmovups
%
ymm0
-
0x38
(
%
rsp
)
.
byte
197
116
95
192
/
/
vmaxps
%
ymm0
%
ymm1
%
ymm8
.
byte
196
65
108
95
192
/
/
vmaxps
%
ymm8
%
ymm2
%
ymm8
.
byte
197
116
93
224
/
/
vminps
%
ymm0
%
ymm1
%
ymm12
.
byte
196
65
108
93
228
/
/
vminps
%
ymm12
%
ymm2
%
ymm12
.
byte
196
65
60
92
196
/
/
vsubps
%
ymm12
%
ymm8
%
ymm8
.
byte
197
60
89
231
/
/
vmulps
%
ymm7
%
ymm8
%
ymm12
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
232
/
/
vminps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
44
95
195
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
95
192
/
/
vmaxps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
65
60
92
245
/
/
vsubps
%
ymm13
%
ymm8
%
ymm14
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
12
194
248
0
/
/
vcmpeqps
%
ymm8
%
ymm14
%
ymm15
.
byte
196
65
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm9
.
byte
196
65
28
89
201
/
/
vmulps
%
ymm9
%
ymm12
%
ymm9
.
byte
196
65
52
94
206
/
/
vdivps
%
ymm14
%
ymm9
%
ymm9
.
byte
196
67
53
74
200
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm9
%
ymm9
.
byte
196
65
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
195
45
74
200
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm10
%
ymm1
.
byte
196
65
36
92
213
/
/
vsubps
%
ymm13
%
ymm11
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
67
45
74
224
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm10
%
ymm12
.
byte
196
98
125
24
53
149
52
3
0
/
/
vbroadcastss
0x33495
(
%
rip
)
%
ymm14
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
61
144
52
3
0
/
/
vbroadcastss
0x33490
(
%
rip
)
%
ymm15
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
239
/
/
vmulps
%
ymm15
%
ymm5
%
ymm13
.
byte
196
66
93
184
238
/
/
vfmadd231ps
%
ymm14
%
ymm4
%
ymm13
.
byte
196
226
125
24
5
129
52
3
0
/
/
vbroadcastss
0x33481
(
%
rip
)
%
ymm0
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
98
77
184
232
/
/
vfmadd231ps
%
ymm0
%
ymm6
%
ymm13
.
byte
196
65
116
89
215
/
/
vmulps
%
ymm15
%
ymm1
%
ymm10
.
byte
196
66
53
184
214
/
/
vfmadd231ps
%
ymm14
%
ymm9
%
ymm10
.
byte
196
98
29
184
208
/
/
vfmadd231ps
%
ymm0
%
ymm12
%
ymm10
.
byte
196
66
101
170
234
/
/
vfmsub213ps
%
ymm10
%
ymm3
%
ymm13
.
byte
196
65
52
88
213
/
/
vaddps
%
ymm13
%
ymm9
%
ymm10
.
byte
196
65
116
88
221
/
/
vaddps
%
ymm13
%
ymm1
%
ymm11
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
193
36
93
204
/
/
vminps
%
ymm12
%
ymm11
%
ymm1
.
byte
197
44
93
233
/
/
vminps
%
ymm1
%
ymm10
%
ymm13
.
byte
196
65
36
89
207
/
/
vmulps
%
ymm15
%
ymm11
%
ymm9
.
byte
196
66
45
184
206
/
/
vfmadd231ps
%
ymm14
%
ymm10
%
ymm9
.
byte
196
98
29
184
200
/
/
vfmadd231ps
%
ymm0
%
ymm12
%
ymm9
.
byte
196
193
44
92
193
/
/
vsubps
%
ymm9
%
ymm10
%
ymm0
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
193
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm1
.
byte
197
252
94
193
/
/
vdivps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
65
36
92
241
/
/
vsubps
%
ymm9
%
ymm11
%
ymm14
.
byte
196
65
52
89
246
/
/
vmulps
%
ymm14
%
ymm9
%
ymm14
.
byte
197
12
94
241
/
/
vdivps
%
ymm1
%
ymm14
%
ymm14
.
byte
196
65
28
92
249
/
/
vsubps
%
ymm9
%
ymm12
%
ymm15
.
byte
196
65
52
89
255
/
/
vmulps
%
ymm15
%
ymm9
%
ymm15
.
byte
197
132
94
201
/
/
vdivps
%
ymm1
%
ymm15
%
ymm1
.
byte
196
65
60
194
237
2
/
/
vcmpleps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
52
88
246
/
/
vaddps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
67
13
74
243
208
/
/
vblendvps
%
ymm13
%
ymm11
%
ymm14
%
ymm14
.
byte
196
65
36
95
220
/
/
vmaxps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
180
88
201
/
/
vaddps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
195
117
74
204
208
/
/
vblendvps
%
ymm13
%
ymm12
%
ymm1
%
ymm1
.
byte
197
180
88
192
/
/
vaddps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
194
208
/
/
vblendvps
%
ymm13
%
ymm10
%
ymm0
%
ymm0
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
44
95
211
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
124
92
217
/
/
vsubps
%
ymm9
%
ymm0
%
ymm11
.
byte
196
65
28
92
233
/
/
vsubps
%
ymm9
%
ymm12
%
ymm13
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
194
250
1
/
/
vcmpltps
%
ymm10
%
ymm12
%
ymm15
.
byte
196
65
44
92
209
/
/
vsubps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
36
94
218
/
/
vdivps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
52
88
219
/
/
vaddps
%
ymm11
%
ymm9
%
ymm11
.
byte
196
195
125
74
195
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm0
%
ymm0
.
byte
196
65
12
92
217
/
/
vsubps
%
ymm9
%
ymm14
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
218
/
/
vdivps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
52
88
219
/
/
vaddps
%
ymm11
%
ymm9
%
ymm11
.
byte
196
67
13
74
219
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
116
92
241
/
/
vsubps
%
ymm9
%
ymm1
%
ymm14
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
20
94
210
/
/
vdivps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
195
117
74
201
240
/
/
vblendvps
%
ymm15
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
36
95
200
/
/
vmaxps
%
ymm8
%
ymm11
%
ymm9
.
byte
196
65
116
95
192
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
226
125
24
13
66
51
3
0
/
/
vbroadcastss
0x33342
(
%
rip
)
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
116
92
215
/
/
vsubps
%
ymm7
%
ymm1
%
ymm10
.
byte
197
172
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
116
92
219
/
/
vsubps
%
ymm3
%
ymm1
%
ymm11
.
byte
196
226
37
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm2
.
byte
197
236
88
192
/
/
vaddps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
172
89
76
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm10
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
172
89
84
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm10
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
208
/
/
vaddps
%
ymm8
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_color_hsw
.
globl
_sk_color_hsw
FUNCTION
(
_sk_color_hsw
)
_sk_color_hsw
:
.
byte
197
124
40
202
/
/
vmovaps
%
ymm2
%
ymm9
.
byte
197
124
17
76
36
200
/
/
vmovups
%
ymm9
-
0x38
(
%
rsp
)
.
byte
197
252
17
76
36
168
/
/
vmovups
%
ymm1
-
0x58
(
%
rsp
)
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
108
89
199
/
/
vmulps
%
ymm7
%
ymm2
%
ymm8
.
byte
197
116
89
215
/
/
vmulps
%
ymm7
%
ymm1
%
ymm10
.
byte
197
52
89
223
/
/
vmulps
%
ymm7
%
ymm9
%
ymm11
.
byte
196
98
125
24
45
247
50
3
0
/
/
vbroadcastss
0x332f7
(
%
rip
)
%
ymm13
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
53
242
50
3
0
/
/
vbroadcastss
0x332f2
(
%
rip
)
%
ymm14
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
230
/
/
vmulps
%
ymm14
%
ymm5
%
ymm12
.
byte
196
66
93
184
229
/
/
vfmadd231ps
%
ymm13
%
ymm4
%
ymm12
.
byte
196
98
125
24
61
227
50
3
0
/
/
vbroadcastss
0x332e3
(
%
rip
)
%
ymm15
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
66
77
184
231
/
/
vfmadd231ps
%
ymm15
%
ymm6
%
ymm12
.
byte
196
65
44
89
206
/
/
vmulps
%
ymm14
%
ymm10
%
ymm9
.
byte
196
66
61
184
205
/
/
vfmadd231ps
%
ymm13
%
ymm8
%
ymm9
.
byte
196
66
37
184
207
/
/
vfmadd231ps
%
ymm15
%
ymm11
%
ymm9
.
byte
196
66
101
170
225
/
/
vfmsub213ps
%
ymm9
%
ymm3
%
ymm12
.
byte
196
65
60
88
204
/
/
vaddps
%
ymm12
%
ymm8
%
ymm9
.
byte
196
65
44
88
212
/
/
vaddps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
224
/
/
vminps
%
ymm8
%
ymm9
%
ymm12
.
byte
196
65
44
89
198
/
/
vmulps
%
ymm14
%
ymm10
%
ymm8
.
byte
196
66
53
184
197
/
/
vfmadd231ps
%
ymm13
%
ymm9
%
ymm8
.
byte
196
66
37
184
199
/
/
vfmadd231ps
%
ymm15
%
ymm11
%
ymm8
.
byte
196
65
52
92
232
/
/
vsubps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
60
89
237
/
/
vmulps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
60
92
244
/
/
vsubps
%
ymm12
%
ymm8
%
ymm14
.
byte
196
193
20
94
198
/
/
vdivps
%
ymm14
%
ymm13
%
ymm0
.
byte
196
65
44
92
248
/
/
vsubps
%
ymm8
%
ymm10
%
ymm15
.
byte
196
65
60
89
255
/
/
vmulps
%
ymm15
%
ymm8
%
ymm15
.
byte
196
65
4
94
254
/
/
vdivps
%
ymm14
%
ymm15
%
ymm15
.
byte
196
65
36
92
232
/
/
vsubps
%
ymm8
%
ymm11
%
ymm13
.
byte
196
65
60
89
237
/
/
vmulps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
20
94
238
/
/
vdivps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
12
194
228
2
/
/
vcmpleps
%
ymm12
%
ymm14
%
ymm12
.
byte
196
65
60
88
255
/
/
vaddps
%
ymm15
%
ymm8
%
ymm15
.
byte
196
67
5
74
250
192
/
/
vblendvps
%
ymm12
%
ymm10
%
ymm15
%
ymm15
.
byte
196
65
44
95
211
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
60
88
237
/
/
vaddps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
67
21
74
219
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm13
%
ymm11
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
195
125
74
201
192
/
/
vblendvps
%
ymm12
%
ymm9
%
ymm0
%
ymm1
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
52
95
202
/
/
vmaxps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
116
92
208
/
/
vsubps
%
ymm8
%
ymm1
%
ymm10
.
byte
196
65
28
92
232
/
/
vsubps
%
ymm8
%
ymm12
%
ymm13
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
28
194
193
1
/
/
vcmpltps
%
ymm9
%
ymm12
%
ymm0
.
byte
196
65
52
92
200
/
/
vsubps
%
ymm8
%
ymm9
%
ymm9
.
byte
196
65
44
94
209
/
/
vdivps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
60
88
210
/
/
vaddps
%
ymm10
%
ymm8
%
ymm10
.
byte
196
195
117
74
202
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
4
92
208
/
/
vsubps
%
ymm8
%
ymm15
%
ymm10
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
44
94
209
/
/
vdivps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
60
88
210
/
/
vaddps
%
ymm10
%
ymm8
%
ymm10
.
byte
196
67
5
74
210
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm15
%
ymm10
.
byte
196
65
36
92
248
/
/
vsubps
%
ymm8
%
ymm11
%
ymm15
.
byte
196
65
20
89
239
/
/
vmulps
%
ymm15
%
ymm13
%
ymm13
.
byte
196
65
20
94
201
/
/
vdivps
%
ymm9
%
ymm13
%
ymm9
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
195
37
74
192
0
/
/
vblendvps
%
ymm0
%
ymm8
%
ymm11
%
ymm0
.
byte
196
193
116
95
206
/
/
vmaxps
%
ymm14
%
ymm1
%
ymm1
.
byte
196
65
44
95
198
/
/
vmaxps
%
ymm14
%
ymm10
%
ymm8
.
byte
196
65
124
95
206
/
/
vmaxps
%
ymm14
%
ymm0
%
ymm9
.
byte
196
226
125
24
5
153
49
3
0
/
/
vbroadcastss
0x33199
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
124
92
215
/
/
vsubps
%
ymm7
%
ymm0
%
ymm10
.
byte
197
172
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
124
92
219
/
/
vsubps
%
ymm3
%
ymm0
%
ymm11
.
byte
196
226
37
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm2
.
byte
197
236
88
193
/
/
vaddps
%
ymm1
%
ymm2
%
ymm0
.
byte
197
172
89
76
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm10
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
172
89
84
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm10
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
209
/
/
vaddps
%
ymm9
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminosity_hsw
.
globl
_sk_luminosity_hsw
FUNCTION
(
_sk_luminosity_hsw
)
_sk_luminosity_hsw
:
.
byte
197
124
40
202
/
/
vmovaps
%
ymm2
%
ymm9
.
byte
197
124
17
76
36
168
/
/
vmovups
%
ymm9
-
0x58
(
%
rsp
)
.
byte
197
252
17
76
36
200
/
/
vmovups
%
ymm1
-
0x38
(
%
rsp
)
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
100
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm8
.
byte
197
100
89
213
/
/
vmulps
%
ymm5
%
ymm3
%
ymm10
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
196
98
125
24
45
78
49
3
0
/
/
vbroadcastss
0x3314e
(
%
rip
)
%
ymm13
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
98
125
24
53
73
49
3
0
/
/
vbroadcastss
0x33149
(
%
rip
)
%
ymm14
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
116
89
230
/
/
vmulps
%
ymm14
%
ymm1
%
ymm12
.
byte
196
66
109
184
229
/
/
vfmadd231ps
%
ymm13
%
ymm2
%
ymm12
.
byte
196
98
125
24
61
58
49
3
0
/
/
vbroadcastss
0x3313a
(
%
rip
)
%
ymm15
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
66
53
184
231
/
/
vfmadd231ps
%
ymm15
%
ymm9
%
ymm12
.
byte
196
65
44
89
206
/
/
vmulps
%
ymm14
%
ymm10
%
ymm9
.
byte
196
66
61
184
205
/
/
vfmadd231ps
%
ymm13
%
ymm8
%
ymm9
.
byte
196
66
37
184
207
/
/
vfmadd231ps
%
ymm15
%
ymm11
%
ymm9
.
byte
196
66
69
170
225
/
/
vfmsub213ps
%
ymm9
%
ymm7
%
ymm12
.
byte
196
65
60
88
204
/
/
vaddps
%
ymm12
%
ymm8
%
ymm9
.
byte
196
65
44
88
212
/
/
vaddps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
224
/
/
vminps
%
ymm8
%
ymm9
%
ymm12
.
byte
196
65
44
89
198
/
/
vmulps
%
ymm14
%
ymm10
%
ymm8
.
byte
196
66
53
184
197
/
/
vfmadd231ps
%
ymm13
%
ymm9
%
ymm8
.
byte
196
66
37
184
199
/
/
vfmadd231ps
%
ymm15
%
ymm11
%
ymm8
.
byte
196
65
52
92
232
/
/
vsubps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
60
89
237
/
/
vmulps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
60
92
244
/
/
vsubps
%
ymm12
%
ymm8
%
ymm14
.
byte
196
193
20
94
198
/
/
vdivps
%
ymm14
%
ymm13
%
ymm0
.
byte
196
65
44
92
248
/
/
vsubps
%
ymm8
%
ymm10
%
ymm15
.
byte
196
65
60
89
255
/
/
vmulps
%
ymm15
%
ymm8
%
ymm15
.
byte
196
65
4
94
254
/
/
vdivps
%
ymm14
%
ymm15
%
ymm15
.
byte
196
65
36
92
232
/
/
vsubps
%
ymm8
%
ymm11
%
ymm13
.
byte
196
65
60
89
237
/
/
vmulps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
20
94
238
/
/
vdivps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
12
194
228
2
/
/
vcmpleps
%
ymm12
%
ymm14
%
ymm12
.
byte
196
65
60
88
255
/
/
vaddps
%
ymm15
%
ymm8
%
ymm15
.
byte
196
67
5
74
250
192
/
/
vblendvps
%
ymm12
%
ymm10
%
ymm15
%
ymm15
.
byte
196
65
44
95
211
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
60
88
237
/
/
vaddps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
67
21
74
219
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm13
%
ymm11
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
195
125
74
201
192
/
/
vblendvps
%
ymm12
%
ymm9
%
ymm0
%
ymm1
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
52
95
202
/
/
vmaxps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
116
92
208
/
/
vsubps
%
ymm8
%
ymm1
%
ymm10
.
byte
196
65
28
92
232
/
/
vsubps
%
ymm8
%
ymm12
%
ymm13
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
28
194
193
1
/
/
vcmpltps
%
ymm9
%
ymm12
%
ymm0
.
byte
196
65
52
92
200
/
/
vsubps
%
ymm8
%
ymm9
%
ymm9
.
byte
196
65
44
94
209
/
/
vdivps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
60
88
210
/
/
vaddps
%
ymm10
%
ymm8
%
ymm10
.
byte
196
195
117
74
202
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
4
92
208
/
/
vsubps
%
ymm8
%
ymm15
%
ymm10
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
44
94
209
/
/
vdivps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
60
88
210
/
/
vaddps
%
ymm10
%
ymm8
%
ymm10
.
byte
196
67
5
74
210
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm15
%
ymm10
.
byte
196
65
36
92
248
/
/
vsubps
%
ymm8
%
ymm11
%
ymm15
.
byte
196
65
20
89
239
/
/
vmulps
%
ymm15
%
ymm13
%
ymm13
.
byte
196
65
20
94
201
/
/
vdivps
%
ymm9
%
ymm13
%
ymm9
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
195
37
74
192
0
/
/
vblendvps
%
ymm0
%
ymm8
%
ymm11
%
ymm0
.
byte
196
193
116
95
206
/
/
vmaxps
%
ymm14
%
ymm1
%
ymm1
.
byte
196
65
44
95
198
/
/
vmaxps
%
ymm14
%
ymm10
%
ymm8
.
byte
196
65
124
95
206
/
/
vmaxps
%
ymm14
%
ymm0
%
ymm9
.
byte
196
226
125
24
5
240
47
3
0
/
/
vbroadcastss
0x32ff0
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
124
92
215
/
/
vsubps
%
ymm7
%
ymm0
%
ymm10
.
byte
197
172
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
124
92
219
/
/
vsubps
%
ymm3
%
ymm0
%
ymm11
.
byte
196
226
37
184
212
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm2
.
byte
197
236
88
193
/
/
vaddps
%
ymm1
%
ymm2
%
ymm0
.
byte
197
172
89
76
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm10
%
ymm1
.
byte
196
226
37
184
205
/
/
vfmadd231ps
%
ymm5
%
ymm11
%
ymm1
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
172
89
84
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm10
%
ymm2
.
byte
196
98
77
168
218
/
/
vfmadd213ps
%
ymm2
%
ymm6
%
ymm11
.
byte
196
193
36
88
209
/
/
vaddps
%
ymm9
%
ymm11
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_hsw
.
globl
_sk_srcover_rgba_8888_hsw
FUNCTION
(
_sk_srcover_rgba_8888_hsw
)
_sk_srcover_rgba_8888_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
224
0
0
0
/
/
jne
9648
<
_sk_srcover_rgba_8888_hsw
+
0xfa
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
197
197
219
37
138
49
3
0
/
/
vpand
0x3318a
(
%
rip
)
%
ymm7
%
ymm4
#
3c700
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x4b4
>
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
69
0
45
157
49
3
0
/
/
vpshufb
0x3319d
(
%
rip
)
%
ymm7
%
ymm5
#
3c720
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x4d4
>
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
226
69
0
53
176
49
3
0
/
/
vpshufb
0x331b0
(
%
rip
)
%
ymm7
%
ymm6
#
3c740
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x4f4
>
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
13
81
47
3
0
/
/
vbroadcastss
0x32f51
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
203
/
/
vsubps
%
ymm3
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
112
47
3
0
/
/
vbroadcastss
0x32f70
(
%
rip
)
%
ymm10
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
194
93
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm4
%
ymm0
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
194
85
184
201
/
/
vfmadd231ps
%
ymm9
%
ymm5
%
ymm1
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
194
77
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm6
%
ymm2
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
194
69
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm7
%
ymm3
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
217
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
8
/
/
vpslld
0x8
%
ymm11
%
ymm11
.
byte
196
65
37
235
201
/
/
vpor
%
ymm9
%
ymm11
%
ymm9
.
byte
197
60
95
218
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
37
235
192
/
/
vpor
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
58
/
/
jne
9678
<
_sk_srcover_rgba_8888_hsw
+
0x12a
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
20
255
255
255
/
/
ja
956e
<
_sk_srcover_rgba_8888_hsw
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
248
0
0
0
/
/
lea
0xf8
(
%
rip
)
%
r9
#
975c
<
_sk_srcover_rgba_8888_hsw
+
0x20e
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
246
254
255
255
/
/
jmpq
956e
<
_sk_srcover_rgba_8888_hsw
+
0x20
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
194
/
/
ja
9644
<
_sk_srcover_rgba_8888_hsw
+
0xf6
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
236
0
0
0
/
/
lea
0xec
(
%
rip
)
%
r9
#
9778
<
_sk_srcover_rgba_8888_hsw
+
0x22a
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
167
/
/
jmp
9644
<
_sk_srcover_rgba_8888_hsw
+
0xf6
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm7
.
byte
196
193
122
126
36
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
170
254
255
255
/
/
jmpq
956e
<
_sk_srcover_rgba_8888_hsw
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
91
254
255
255
/
/
jmpq
956e
<
_sk_srcover_rgba_8888_hsw
+
0x20
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
30
255
255
255
/
/
jmpq
9644
<
_sk_srcover_rgba_8888_hsw
+
0xf6
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
234
254
255
255
/
/
jmpq
9644
<
_sk_srcover_rgba_8888_hsw
+
0xf6
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
17
255
/
/
adc
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
87
255
/
/
callq
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
65
255
/
/
incl
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
166
255
255
255
146
/
/
jmpq
*
-
0x6d000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
126
255
/
/
jle
9771
<
_sk_srcover_rgba_8888_hsw
+
0x223
>
.
byte
255
/
/
(
bad
)
.
byte
255
104
255
/
/
ljmp
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
163
/
/
lcall
*
-
0x5c000001
(
%
rip
)
#
ffffffffa400977c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffa3fcd530
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
155
255
255
255
215
/
/
lcall
*
-
0x28000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
202
/
/
dec
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
255
255
255
174
/
/
mov
0xaeffffff
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_srcover_bgra_8888_hsw
.
globl
_sk_srcover_bgra_8888_hsw
FUNCTION
(
_sk_srcover_bgra_8888_hsw
)
_sk_srcover_bgra_8888_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
224
0
0
0
/
/
jne
988e
<
_sk_srcover_bgra_8888_hsw
+
0xfa
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
197
197
219
37
164
47
3
0
/
/
vpand
0x32fa4
(
%
rip
)
%
ymm7
%
ymm4
#
3c760
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x514
>
.
byte
197
252
91
244
/
/
vcvtdq2ps
%
ymm4
%
ymm6
.
byte
196
226
69
0
37
183
47
3
0
/
/
vpshufb
0x32fb7
(
%
rip
)
%
ymm7
%
ymm4
#
3c780
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x534
>
.
byte
197
252
91
236
/
/
vcvtdq2ps
%
ymm4
%
ymm5
.
byte
196
226
69
0
37
202
47
3
0
/
/
vpshufb
0x32fca
(
%
rip
)
%
ymm7
%
ymm4
#
3c7a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x554
>
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
13
11
45
3
0
/
/
vbroadcastss
0x32d0b
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
203
/
/
vsubps
%
ymm3
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
42
45
3
0
/
/
vbroadcastss
0x32d2a
(
%
rip
)
%
ymm10
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
194
93
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm4
%
ymm0
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
194
85
184
201
/
/
vfmadd231ps
%
ymm9
%
ymm5
%
ymm1
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
194
77
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm6
%
ymm2
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
194
69
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm7
%
ymm3
.
byte
197
60
95
202
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm9
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
217
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
8
/
/
vpslld
0x8
%
ymm11
%
ymm11
.
byte
196
65
37
235
201
/
/
vpor
%
ymm9
%
ymm11
%
ymm9
.
byte
197
60
95
216
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm11
.
byte
196
65
36
93
218
/
/
vminps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
37
235
192
/
/
vpor
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
58
/
/
jne
98be
<
_sk_srcover_bgra_8888_hsw
+
0x12a
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
20
255
255
255
/
/
ja
97b4
<
_sk_srcover_bgra_8888_hsw
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
246
0
0
0
/
/
lea
0xf6
(
%
rip
)
%
r9
#
99a0
<
_sk_srcover_bgra_8888_hsw
+
0x20c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
246
254
255
255
/
/
jmpq
97b4
<
_sk_srcover_bgra_8888_hsw
+
0x20
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
194
/
/
ja
988a
<
_sk_srcover_bgra_8888_hsw
+
0xf6
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
234
0
0
0
/
/
lea
0xea
(
%
rip
)
%
r9
#
99bc
<
_sk_srcover_bgra_8888_hsw
+
0x228
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
167
/
/
jmp
988a
<
_sk_srcover_bgra_8888_hsw
+
0xf6
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm7
.
byte
196
193
122
126
36
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
170
254
255
255
/
/
jmpq
97b4
<
_sk_srcover_bgra_8888_hsw
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
91
254
255
255
/
/
jmpq
97b4
<
_sk_srcover_bgra_8888_hsw
+
0x20
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
30
255
255
255
/
/
jmpq
988a
<
_sk_srcover_bgra_8888_hsw
+
0xf6
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
234
254
255
255
/
/
jmpq
988a
<
_sk_srcover_bgra_8888_hsw
+
0xf6
>
.
byte
19
255
/
/
adc
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
89
255
/
/
lcall
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
67
255
/
/
incl
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
168
255
255
255
148
/
/
ljmp
*
-
0x6b000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
128
255
255
255
106
/
/
incl
0x6affffff
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
31
/
/
lcall
*
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
165
255
255
255
157
/
/
jmpq
*
-
0x62000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
204
/
/
dec
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
176
/
/
mov
0xb0ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_clamp_0_hsw
.
globl
_sk_clamp_0_hsw
FUNCTION
(
_sk_clamp_0_hsw
)
_sk_clamp_0_hsw
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
95
200
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
108
95
208
/
/
vmaxps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
95
216
/
/
vmaxps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_1_hsw
.
globl
_sk_clamp_1_hsw
FUNCTION
(
_sk_clamp_1_hsw
)
_sk_clamp_1_hsw
:
.
byte
196
98
125
24
5
254
42
3
0
/
/
vbroadcastss
0x32afe
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_hsw
.
globl
_sk_clamp_a_hsw
FUNCTION
(
_sk_clamp_a_hsw
)
_sk_clamp_a_hsw
:
.
byte
196
98
125
24
5
221
42
3
0
/
/
vbroadcastss
0x32add
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
252
93
195
/
/
vminps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
244
93
203
/
/
vminps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
236
93
211
/
/
vminps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_hsw
.
globl
_sk_clamp_a_dst_hsw
FUNCTION
(
_sk_clamp_a_dst_hsw
)
_sk_clamp_a_dst_hsw
:
.
byte
196
98
125
24
5
191
42
3
0
/
/
vbroadcastss
0x32abf
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
68
93
248
/
/
vminps
%
ymm8
%
ymm7
%
ymm7
.
byte
197
220
93
231
/
/
vminps
%
ymm7
%
ymm4
%
ymm4
.
byte
197
212
93
239
/
/
vminps
%
ymm7
%
ymm5
%
ymm5
.
byte
197
204
93
247
/
/
vminps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_hsw
.
globl
_sk_set_rgb_hsw
FUNCTION
(
_sk_set_rgb_hsw
)
_sk_set_rgb_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm0
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_hsw
.
globl
_sk_swap_rb_hsw
FUNCTION
(
_sk_swap_rb_hsw
)
_sk_swap_rb_hsw
:
.
byte
197
124
40
192
/
/
vmovaps
%
ymm0
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
194
/
/
vmovaps
%
ymm2
%
ymm0
.
byte
197
124
41
194
/
/
vmovaps
%
ymm8
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_hsw
.
globl
_sk_invert_hsw
FUNCTION
(
_sk_invert_hsw
)
_sk_invert_hsw
:
.
byte
196
98
125
24
5
122
42
3
0
/
/
vbroadcastss
0x32a7a
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
92
219
/
/
vsubps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_hsw
.
globl
_sk_move_src_dst_hsw
FUNCTION
(
_sk_move_src_dst_hsw
)
_sk_move_src_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
224
/
/
vmovaps
%
ymm0
%
ymm4
.
byte
197
252
40
233
/
/
vmovaps
%
ymm1
%
ymm5
.
byte
197
252
40
242
/
/
vmovaps
%
ymm2
%
ymm6
.
byte
197
252
40
251
/
/
vmovaps
%
ymm3
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_hsw
.
globl
_sk_move_dst_src_hsw
FUNCTION
(
_sk_move_dst_src_hsw
)
_sk_move_dst_src_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
196
/
/
vmovaps
%
ymm4
%
ymm0
.
byte
197
252
40
205
/
/
vmovaps
%
ymm5
%
ymm1
.
byte
197
252
40
214
/
/
vmovaps
%
ymm6
%
ymm2
.
byte
197
252
40
223
/
/
vmovaps
%
ymm7
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_hsw
.
globl
_sk_premul_hsw
FUNCTION
(
_sk_premul_hsw
)
_sk_premul_hsw
:
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_hsw
.
globl
_sk_premul_dst_hsw
FUNCTION
(
_sk_premul_dst_hsw
)
_sk_premul_dst_hsw
:
.
byte
197
220
89
231
/
/
vmulps
%
ymm7
%
ymm4
%
ymm4
.
byte
197
212
89
239
/
/
vmulps
%
ymm7
%
ymm5
%
ymm5
.
byte
197
204
89
247
/
/
vmulps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_unpremul_hsw
.
globl
_sk_unpremul_hsw
FUNCTION
(
_sk_unpremul_hsw
)
_sk_unpremul_hsw
:
.
byte
196
98
125
24
5
21
42
3
0
/
/
vbroadcastss
0x32a15
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
94
195
/
/
vdivps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
56
42
3
0
/
/
vbroadcastss
0x32a38
(
%
rip
)
%
ymm9
#
3c52c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e0
>
.
byte
196
65
60
194
201
1
/
/
vcmpltps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
45
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm10
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_hsw
.
globl
_sk_force_opaque_hsw
FUNCTION
(
_sk_force_opaque_hsw
)
_sk_force_opaque_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
220
41
3
0
/
/
vbroadcastss
0x329dc
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_hsw
.
globl
_sk_force_opaque_dst_hsw
FUNCTION
(
_sk_force_opaque_dst_hsw
)
_sk_force_opaque_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
207
41
3
0
/
/
vbroadcastss
0x329cf
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_hsw
.
globl
_sk_from_srgb_hsw
FUNCTION
(
_sk_from_srgb_hsw
)
_sk_from_srgb_hsw
:
.
byte
196
98
125
24
5
248
41
3
0
/
/
vbroadcastss
0x329f8
(
%
rip
)
%
ymm8
#
3c530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e4
>
.
byte
196
65
124
89
200
/
/
vmulps
%
ymm8
%
ymm0
%
ymm9
.
byte
197
124
89
208
/
/
vmulps
%
ymm0
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
234
41
3
0
/
/
vbroadcastss
0x329ea
(
%
rip
)
%
ymm11
#
3c534
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e8
>
.
byte
196
98
125
24
37
201
41
3
0
/
/
vbroadcastss
0x329c9
(
%
rip
)
%
ymm12
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
124
40
236
/
/
vmovaps
%
ymm12
%
ymm13
.
byte
196
66
125
168
235
/
/
vfmadd213ps
%
ymm11
%
ymm0
%
ymm13
.
byte
196
98
125
24
53
210
41
3
0
/
/
vbroadcastss
0x329d2
(
%
rip
)
%
ymm14
#
3c538
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ec
>
.
byte
196
66
45
168
238
/
/
vfmadd213ps
%
ymm14
%
ymm10
%
ymm13
.
byte
196
98
125
24
21
200
41
3
0
/
/
vbroadcastss
0x329c8
(
%
rip
)
%
ymm10
#
3c53c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f0
>
.
byte
196
193
124
194
194
1
/
/
vcmpltps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
195
21
74
193
0
/
/
vblendvps
%
ymm0
%
ymm9
%
ymm13
%
ymm0
.
byte
196
65
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm9
.
byte
197
116
89
233
/
/
vmulps
%
ymm1
%
ymm1
%
ymm13
.
byte
196
65
124
40
252
/
/
vmovaps
%
ymm12
%
ymm15
.
byte
196
66
117
168
251
/
/
vfmadd213ps
%
ymm11
%
ymm1
%
ymm15
.
byte
196
66
21
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm13
%
ymm15
.
byte
196
193
116
194
202
1
/
/
vcmpltps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
195
5
74
201
16
/
/
vblendvps
%
ymm1
%
ymm9
%
ymm15
%
ymm1
.
byte
196
65
108
89
192
/
/
vmulps
%
ymm8
%
ymm2
%
ymm8
.
byte
197
108
89
202
/
/
vmulps
%
ymm2
%
ymm2
%
ymm9
.
byte
196
66
109
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm2
%
ymm12
.
byte
196
66
53
168
230
/
/
vfmadd213ps
%
ymm14
%
ymm9
%
ymm12
.
byte
196
193
108
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
195
29
74
208
32
/
/
vblendvps
%
ymm2
%
ymm8
%
ymm12
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_dst_hsw
.
globl
_sk_from_srgb_dst_hsw
FUNCTION
(
_sk_from_srgb_dst_hsw
)
_sk_from_srgb_dst_hsw
:
.
byte
196
98
125
24
5
96
41
3
0
/
/
vbroadcastss
0x32960
(
%
rip
)
%
ymm8
#
3c530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e4
>
.
byte
196
65
92
89
200
/
/
vmulps
%
ymm8
%
ymm4
%
ymm9
.
byte
197
92
89
212
/
/
vmulps
%
ymm4
%
ymm4
%
ymm10
.
byte
196
98
125
24
29
82
41
3
0
/
/
vbroadcastss
0x32952
(
%
rip
)
%
ymm11
#
3c534
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e8
>
.
byte
196
98
125
24
37
49
41
3
0
/
/
vbroadcastss
0x32931
(
%
rip
)
%
ymm12
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
124
40
236
/
/
vmovaps
%
ymm12
%
ymm13
.
byte
196
66
93
168
235
/
/
vfmadd213ps
%
ymm11
%
ymm4
%
ymm13
.
byte
196
98
125
24
53
58
41
3
0
/
/
vbroadcastss
0x3293a
(
%
rip
)
%
ymm14
#
3c538
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ec
>
.
byte
196
66
45
168
238
/
/
vfmadd213ps
%
ymm14
%
ymm10
%
ymm13
.
byte
196
98
125
24
21
48
41
3
0
/
/
vbroadcastss
0x32930
(
%
rip
)
%
ymm10
#
3c53c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f0
>
.
byte
196
193
92
194
226
1
/
/
vcmpltps
%
ymm10
%
ymm4
%
ymm4
.
byte
196
195
21
74
225
64
/
/
vblendvps
%
ymm4
%
ymm9
%
ymm13
%
ymm4
.
byte
196
65
84
89
200
/
/
vmulps
%
ymm8
%
ymm5
%
ymm9
.
byte
197
84
89
237
/
/
vmulps
%
ymm5
%
ymm5
%
ymm13
.
byte
196
65
124
40
252
/
/
vmovaps
%
ymm12
%
ymm15
.
byte
196
66
85
168
251
/
/
vfmadd213ps
%
ymm11
%
ymm5
%
ymm15
.
byte
196
66
21
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm13
%
ymm15
.
byte
196
193
84
194
234
1
/
/
vcmpltps
%
ymm10
%
ymm5
%
ymm5
.
byte
196
195
5
74
233
80
/
/
vblendvps
%
ymm5
%
ymm9
%
ymm15
%
ymm5
.
byte
196
65
76
89
192
/
/
vmulps
%
ymm8
%
ymm6
%
ymm8
.
byte
197
76
89
206
/
/
vmulps
%
ymm6
%
ymm6
%
ymm9
.
byte
196
66
77
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm6
%
ymm12
.
byte
196
66
53
168
230
/
/
vfmadd213ps
%
ymm14
%
ymm9
%
ymm12
.
byte
196
193
76
194
242
1
/
/
vcmpltps
%
ymm10
%
ymm6
%
ymm6
.
byte
196
195
29
74
240
96
/
/
vblendvps
%
ymm6
%
ymm8
%
ymm12
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_to_srgb_hsw
.
globl
_sk_to_srgb_hsw
FUNCTION
(
_sk_to_srgb_hsw
)
_sk_to_srgb_hsw
:
.
byte
197
124
82
200
/
/
vrsqrtps
%
ymm0
%
ymm9
.
byte
196
98
125
24
5
212
40
3
0
/
/
vbroadcastss
0x328d4
(
%
rip
)
%
ymm8
#
3c540
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f4
>
.
byte
196
65
124
89
208
/
/
vmulps
%
ymm8
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
202
40
3
0
/
/
vbroadcastss
0x328ca
(
%
rip
)
%
ymm11
#
3c544
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f8
>
.
byte
196
98
125
24
37
197
40
3
0
/
/
vbroadcastss
0x328c5
(
%
rip
)
%
ymm12
#
3c548
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2fc
>
.
byte
196
65
124
40
236
/
/
vmovaps
%
ymm12
%
ymm13
.
byte
196
66
53
168
235
/
/
vfmadd213ps
%
ymm11
%
ymm9
%
ymm13
.
byte
196
98
125
24
53
238
41
3
0
/
/
vbroadcastss
0x329ee
(
%
rip
)
%
ymm14
#
3c684
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x438
>
.
byte
196
66
53
168
238
/
/
vfmadd213ps
%
ymm14
%
ymm9
%
ymm13
.
byte
196
98
125
24
61
228
41
3
0
/
/
vbroadcastss
0x329e4
(
%
rip
)
%
ymm15
#
3c688
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x43c
>
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
124
83
201
/
/
vrcpps
%
ymm9
%
ymm9
.
byte
196
65
20
89
201
/
/
vmulps
%
ymm9
%
ymm13
%
ymm9
.
byte
196
98
125
24
45
152
40
3
0
/
/
vbroadcastss
0x32898
(
%
rip
)
%
ymm13
#
3c554
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x308
>
.
byte
196
193
124
194
197
1
/
/
vcmpltps
%
ymm13
%
ymm0
%
ymm0
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
197
124
82
201
/
/
vrsqrtps
%
ymm1
%
ymm9
.
byte
196
65
124
40
212
/
/
vmovaps
%
ymm12
%
ymm10
.
byte
196
66
53
168
211
/
/
vfmadd213ps
%
ymm11
%
ymm9
%
ymm10
.
byte
196
66
53
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm9
%
ymm10
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
124
83
201
/
/
vrcpps
%
ymm9
%
ymm9
.
byte
196
65
44
89
201
/
/
vmulps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
65
116
89
208
/
/
vmulps
%
ymm8
%
ymm1
%
ymm10
.
byte
196
193
116
194
205
1
/
/
vcmpltps
%
ymm13
%
ymm1
%
ymm1
.
byte
196
195
53
74
202
16
/
/
vblendvps
%
ymm1
%
ymm10
%
ymm9
%
ymm1
.
byte
197
124
82
202
/
/
vrsqrtps
%
ymm2
%
ymm9
.
byte
196
66
53
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm9
%
ymm12
.
byte
196
66
53
168
230
/
/
vfmadd213ps
%
ymm14
%
ymm9
%
ymm12
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
124
83
201
/
/
vrcpps
%
ymm9
%
ymm9
.
byte
196
65
28
89
201
/
/
vmulps
%
ymm9
%
ymm12
%
ymm9
.
byte
196
65
108
89
192
/
/
vmulps
%
ymm8
%
ymm2
%
ymm8
.
byte
196
193
108
194
213
1
/
/
vcmpltps
%
ymm13
%
ymm2
%
ymm2
.
byte
196
195
53
74
208
32
/
/
vblendvps
%
ymm2
%
ymm8
%
ymm9
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_rgb_to_hsl_hsw
.
globl
_sk_rgb_to_hsl_hsw
FUNCTION
(
_sk_rgb_to_hsl_hsw
)
_sk_rgb_to_hsl_hsw
:
.
byte
197
116
95
194
/
/
vmaxps
%
ymm2
%
ymm1
%
ymm8
.
byte
196
65
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm8
.
byte
197
116
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm9
.
byte
196
65
124
93
201
/
/
vminps
%
ymm9
%
ymm0
%
ymm9
.
byte
196
65
60
92
209
/
/
vsubps
%
ymm9
%
ymm8
%
ymm10
.
byte
196
98
125
24
29
175
39
3
0
/
/
vbroadcastss
0x327af
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
36
94
218
/
/
vdivps
%
ymm10
%
ymm11
%
ymm11
.
byte
197
116
92
226
/
/
vsubps
%
ymm2
%
ymm1
%
ymm12
.
byte
197
116
194
234
1
/
/
vcmpltps
%
ymm2
%
ymm1
%
ymm13
.
byte
196
98
125
24
53
244
39
3
0
/
/
vbroadcastss
0x327f4
(
%
rip
)
%
ymm14
#
3c558
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30c
>
.
byte
196
65
4
87
255
/
/
vxorps
%
ymm15
%
ymm15
%
ymm15
.
byte
196
67
5
74
238
208
/
/
vblendvps
%
ymm13
%
ymm14
%
ymm15
%
ymm13
.
byte
196
66
37
168
229
/
/
vfmadd213ps
%
ymm13
%
ymm11
%
ymm12
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
197
124
92
233
/
/
vsubps
%
ymm1
%
ymm0
%
ymm13
.
byte
196
98
125
24
53
219
39
3
0
/
/
vbroadcastss
0x327db
(
%
rip
)
%
ymm14
#
3c560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x314
>
.
byte
196
66
37
168
238
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
201
39
3
0
/
/
vbroadcastss
0x327c9
(
%
rip
)
%
ymm14
#
3c55c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x310
>
.
byte
196
194
37
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm11
%
ymm2
.
byte
197
188
194
201
0
/
/
vcmpeqps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
227
21
74
202
16
/
/
vblendvps
%
ymm1
%
ymm2
%
ymm13
%
ymm1
.
byte
197
188
194
192
0
/
/
vcmpeqps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
195
117
74
196
0
/
/
vblendvps
%
ymm0
%
ymm12
%
ymm1
%
ymm0
.
byte
196
193
60
88
201
/
/
vaddps
%
ymm9
%
ymm8
%
ymm1
.
byte
196
98
125
24
29
60
39
3
0
/
/
vbroadcastss
0x3273c
(
%
rip
)
%
ymm11
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
116
89
211
/
/
vmulps
%
ymm11
%
ymm1
%
ymm2
.
byte
197
36
194
218
1
/
/
vcmpltps
%
ymm2
%
ymm11
%
ymm11
.
byte
196
65
12
92
224
/
/
vsubps
%
ymm8
%
ymm14
%
ymm12
.
byte
196
65
28
92
225
/
/
vsubps
%
ymm9
%
ymm12
%
ymm12
.
byte
196
195
117
74
204
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm1
%
ymm1
.
byte
196
65
60
194
193
0
/
/
vcmpeqps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
172
94
201
/
/
vdivps
%
ymm1
%
ymm10
%
ymm1
.
byte
196
195
125
74
199
128
/
/
vblendvps
%
ymm8
%
ymm15
%
ymm0
%
ymm0
.
byte
196
195
117
74
207
128
/
/
vblendvps
%
ymm8
%
ymm15
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
111
39
3
0
/
/
vbroadcastss
0x3276f
(
%
rip
)
%
ymm8
#
3c564
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x318
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hsl_to_rgb_hsw
.
globl
_sk_hsl_to_rgb_hsw
FUNCTION
(
_sk_hsl_to_rgb_hsw
)
_sk_hsl_to_rgb_hsw
:
.
byte
72
131
236
56
/
/
sub
0x38
%
rsp
.
byte
197
252
17
60
36
/
/
vmovups
%
ymm7
(
%
rsp
)
.
byte
197
252
17
116
36
224
/
/
vmovups
%
ymm6
-
0x20
(
%
rsp
)
.
byte
197
252
17
108
36
192
/
/
vmovups
%
ymm5
-
0x40
(
%
rsp
)
.
byte
197
252
17
100
36
160
/
/
vmovups
%
ymm4
-
0x60
(
%
rsp
)
.
byte
197
252
17
92
36
128
/
/
vmovups
%
ymm3
-
0x80
(
%
rsp
)
.
byte
197
252
40
233
/
/
vmovaps
%
ymm1
%
ymm5
.
byte
197
252
40
224
/
/
vmovaps
%
ymm0
%
ymm4
.
byte
196
98
125
24
5
200
38
3
0
/
/
vbroadcastss
0x326c8
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
60
194
202
2
/
/
vcmpleps
%
ymm2
%
ymm8
%
ymm9
.
byte
197
84
89
210
/
/
vmulps
%
ymm2
%
ymm5
%
ymm10
.
byte
196
65
84
92
218
/
/
vsubps
%
ymm10
%
ymm5
%
ymm11
.
byte
196
67
45
74
203
144
/
/
vblendvps
%
ymm9
%
ymm11
%
ymm10
%
ymm9
.
byte
197
52
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm10
.
byte
196
98
125
24
13
11
39
3
0
/
/
vbroadcastss
0x3270b
(
%
rip
)
%
ymm9
#
3c55c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x310
>
.
byte
196
66
109
170
202
/
/
vfmsub213ps
%
ymm10
%
ymm2
%
ymm9
.
byte
196
98
125
24
29
9
39
3
0
/
/
vbroadcastss
0x32709
(
%
rip
)
%
ymm11
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
65
92
88
219
/
/
vaddps
%
ymm11
%
ymm4
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
252
/
/
vsubps
%
ymm12
%
ymm11
%
ymm15
.
byte
196
65
44
92
217
/
/
vsubps
%
ymm9
%
ymm10
%
ymm11
.
byte
196
98
125
24
45
219
38
3
0
/
/
vbroadcastss
0x326db
(
%
rip
)
%
ymm13
#
3c558
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30c
>
.
byte
196
193
4
89
197
/
/
vmulps
%
ymm13
%
ymm15
%
ymm0
.
byte
196
98
125
24
53
213
38
3
0
/
/
vbroadcastss
0x326d5
(
%
rip
)
%
ymm14
#
3c560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x314
>
.
byte
197
12
92
224
/
/
vsubps
%
ymm0
%
ymm14
%
ymm12
.
byte
196
66
37
168
225
/
/
vfmadd213ps
%
ymm9
%
ymm11
%
ymm12
.
byte
196
226
125
24
29
207
38
3
0
/
/
vbroadcastss
0x326cf
(
%
rip
)
%
ymm3
#
3c56c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x320
>
.
byte
196
193
100
194
255
2
/
/
vcmpleps
%
ymm15
%
ymm3
%
ymm7
.
byte
196
195
29
74
249
112
/
/
vblendvps
%
ymm7
%
ymm9
%
ymm12
%
ymm7
.
byte
196
65
60
194
231
2
/
/
vcmpleps
%
ymm15
%
ymm8
%
ymm12
.
byte
196
227
45
74
255
192
/
/
vblendvps
%
ymm12
%
ymm7
%
ymm10
%
ymm7
.
byte
196
98
125
24
37
166
38
3
0
/
/
vbroadcastss
0x326a6
(
%
rip
)
%
ymm12
#
3c564
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x318
>
.
byte
196
65
28
194
255
2
/
/
vcmpleps
%
ymm15
%
ymm12
%
ymm15
.
byte
196
194
37
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm11
%
ymm0
.
byte
196
99
125
74
255
240
/
/
vblendvps
%
ymm15
%
ymm7
%
ymm0
%
ymm15
.
byte
196
227
125
8
196
1
/
/
vroundps
0x1
%
ymm4
%
ymm0
.
byte
197
220
92
192
/
/
vsubps
%
ymm0
%
ymm4
%
ymm0
.
byte
196
193
124
89
253
/
/
vmulps
%
ymm13
%
ymm0
%
ymm7
.
byte
197
140
92
207
/
/
vsubps
%
ymm7
%
ymm14
%
ymm1
.
byte
196
194
37
168
201
/
/
vfmadd213ps
%
ymm9
%
ymm11
%
ymm1
.
byte
197
228
194
240
2
/
/
vcmpleps
%
ymm0
%
ymm3
%
ymm6
.
byte
196
195
117
74
201
96
/
/
vblendvps
%
ymm6
%
ymm9
%
ymm1
%
ymm1
.
byte
197
188
194
240
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm6
.
byte
196
227
45
74
201
96
/
/
vblendvps
%
ymm6
%
ymm1
%
ymm10
%
ymm1
.
byte
197
156
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm12
%
ymm0
.
byte
196
194
37
168
249
/
/
vfmadd213ps
%
ymm9
%
ymm11
%
ymm7
.
byte
196
227
69
74
201
0
/
/
vblendvps
%
ymm0
%
ymm1
%
ymm7
%
ymm1
.
byte
196
226
125
24
5
90
38
3
0
/
/
vbroadcastss
0x3265a
(
%
rip
)
%
ymm0
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
196
227
125
8
224
1
/
/
vroundps
0x1
%
ymm0
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
228
194
216
2
/
/
vcmpleps
%
ymm0
%
ymm3
%
ymm3
.
byte
196
193
124
89
229
/
/
vmulps
%
ymm13
%
ymm0
%
ymm4
.
byte
197
140
92
244
/
/
vsubps
%
ymm4
%
ymm14
%
ymm6
.
byte
196
194
37
168
241
/
/
vfmadd213ps
%
ymm9
%
ymm11
%
ymm6
.
byte
196
195
77
74
217
48
/
/
vblendvps
%
ymm3
%
ymm9
%
ymm6
%
ymm3
.
byte
197
188
194
240
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm6
.
byte
196
227
45
74
219
96
/
/
vblendvps
%
ymm6
%
ymm3
%
ymm10
%
ymm3
.
byte
196
98
37
184
204
/
/
vfmadd231ps
%
ymm4
%
ymm11
%
ymm9
.
byte
197
156
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm12
%
ymm0
.
byte
196
227
53
74
219
0
/
/
vblendvps
%
ymm0
%
ymm3
%
ymm9
%
ymm3
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
212
194
224
0
/
/
vcmpeqps
%
ymm0
%
ymm5
%
ymm4
.
byte
196
227
5
74
194
64
/
/
vblendvps
%
ymm4
%
ymm2
%
ymm15
%
ymm0
.
byte
196
227
117
74
202
64
/
/
vblendvps
%
ymm4
%
ymm2
%
ymm1
%
ymm1
.
byte
196
227
101
74
210
64
/
/
vblendvps
%
ymm4
%
ymm2
%
ymm3
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
92
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm3
.
byte
197
252
16
100
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm6
.
byte
197
252
16
60
36
/
/
vmovups
(
%
rsp
)
%
ymm7
.
byte
72
131
196
56
/
/
add
0x38
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_hsw
.
globl
_sk_scale_1_float_hsw
FUNCTION
(
_sk_scale_1_float_hsw
)
_sk_scale_1_float_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_hsw
.
globl
_sk_scale_u8_hsw
FUNCTION
(
_sk_scale_u8_hsw
)
_sk_scale_u8_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
58
/
/
jne
9fff
<
_sk_scale_u8_hsw
+
0x4c
>
.
byte
196
66
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
197
57
219
5
221
46
3
0
/
/
vpand
0x32edd
(
%
rip
)
%
xmm8
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
142
37
3
0
/
/
vbroadcastss
0x3258e
(
%
rip
)
%
ymm9
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
189
/
/
ja
9fcb
<
_sk_scale_u8_hsw
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
124
0
0
0
/
/
lea
0x7c
(
%
rip
)
%
r9
#
a094
<
_sk_scale_u8_hsw
+
0xe1
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
235
159
/
/
jmp
9fcb
<
_sk_scale_u8_hsw
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
200
/
/
vmovd
%
eax
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
118
255
255
255
/
/
jmpq
9fcb
<
_sk_scale_u8_hsw
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
233
58
255
255
255
/
/
jmpq
9fcb
<
_sk_scale_u8_hsw
+
0x18
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
141
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
168
255
255
255
152
/
/
ljmp
*
-
0x67000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_scale_565_hsw
.
globl
_sk_scale_565_hsw
FUNCTION
(
_sk_scale_565_hsw
)
_sk_scale_565_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
161
0
0
0
/
/
jne
a16a
<
_sk_scale_565_hsw
+
0xba
>
.
byte
196
65
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
98
125
88
13
155
36
3
0
/
/
vpbroadcastd
0x3249b
(
%
rip
)
%
ymm9
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
196
65
61
219
201
/
/
vpand
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
124
91
201
/
/
vcvtdq2ps
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
140
36
3
0
/
/
vbroadcastss
0x3248c
(
%
rip
)
%
ymm10
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
88
21
130
36
3
0
/
/
vpbroadcastd
0x32482
(
%
rip
)
%
ymm10
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
196
65
61
219
210
/
/
vpand
%
ymm10
%
ymm8
%
ymm10
.
byte
196
65
124
91
210
/
/
vcvtdq2ps
%
ymm10
%
ymm10
.
byte
196
98
125
24
29
115
36
3
0
/
/
vbroadcastss
0x32473
(
%
rip
)
%
ymm11
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
196
65
44
89
211
/
/
vmulps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
88
29
105
36
3
0
/
/
vpbroadcastd
0x32469
(
%
rip
)
%
ymm11
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
196
65
61
219
195
/
/
vpand
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
29
90
36
3
0
/
/
vbroadcastss
0x3245a
(
%
rip
)
%
ymm11
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
197
100
194
223
1
/
/
vcmpltps
%
ymm7
%
ymm3
%
ymm11
.
byte
196
65
44
93
224
/
/
vminps
%
ymm8
%
ymm10
%
ymm12
.
byte
196
65
52
93
228
/
/
vminps
%
ymm12
%
ymm9
%
ymm12
.
byte
196
65
44
95
232
/
/
vmaxps
%
ymm8
%
ymm10
%
ymm13
.
byte
196
65
52
95
237
/
/
vmaxps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
220
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
172
89
201
/
/
vmulps
%
ymm1
%
ymm10
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
164
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
82
255
255
255
/
/
ja
a0cf
<
_sk_scale_565_hsw
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
101
0
0
0
/
/
lea
0x65
(
%
rip
)
%
r9
#
a1ec
<
_sk_scale_565_hsw
+
0x13c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
49
255
255
255
/
/
jmpq
a0cf
<
_sk_scale_565_hsw
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
19
255
255
255
/
/
jmpq
a0cf
<
_sk_scale_565_hsw
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
233
229
254
255
255
/
/
jmpq
a0cf
<
_sk_scale_565_hsw
+
0x1f
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
164
/
/
movsb
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
178
/
/
mov
0xb2ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
229
/
/
jmpq
*
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_1_float_hsw
.
globl
_sk_lerp_1_float_hsw
FUNCTION
(
_sk_lerp_1_float_hsw
)
_sk_lerp_1_float_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
226
61
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
226
61
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
226
61
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_u8_hsw
.
globl
_sk_lerp_u8_hsw
FUNCTION
(
_sk_lerp_u8_hsw
)
_sk_lerp_u8_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
78
/
/
jne
a297
<
_sk_lerp_u8_hsw
+
0x60
>
.
byte
196
66
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
197
57
219
5
89
44
3
0
/
/
vpand
0x32c59
(
%
rip
)
%
xmm8
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
10
35
3
0
/
/
vbroadcastss
0x3230a
(
%
rip
)
%
ymm9
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
226
61
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm8
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
226
61
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
226
61
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
169
/
/
ja
a24f
<
_sk_lerp_u8_hsw
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
124
0
0
0
/
/
lea
0x7c
(
%
rip
)
%
r9
#
a32c
<
_sk_lerp_u8_hsw
+
0xf5
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
235
139
/
/
jmp
a24f
<
_sk_lerp_u8_hsw
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
200
/
/
vmovd
%
eax
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
98
255
255
255
/
/
jmpq
a24f
<
_sk_lerp_u8_hsw
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
233
38
255
255
255
/
/
jmpq
a24f
<
_sk_lerp_u8_hsw
+
0x18
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
141
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
168
255
255
255
152
/
/
ljmp
*
-
0x67000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_565_hsw
.
globl
_sk_lerp_565_hsw
FUNCTION
(
_sk_lerp_565_hsw
)
_sk_lerp_565_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
181
0
0
0
/
/
jne
a416
<
_sk_lerp_565_hsw
+
0xce
>
.
byte
196
65
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
98
125
88
13
3
34
3
0
/
/
vpbroadcastd
0x32203
(
%
rip
)
%
ymm9
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
196
65
61
219
201
/
/
vpand
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
124
91
201
/
/
vcvtdq2ps
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
244
33
3
0
/
/
vbroadcastss
0x321f4
(
%
rip
)
%
ymm10
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
88
21
234
33
3
0
/
/
vpbroadcastd
0x321ea
(
%
rip
)
%
ymm10
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
196
65
61
219
210
/
/
vpand
%
ymm10
%
ymm8
%
ymm10
.
byte
196
65
124
91
210
/
/
vcvtdq2ps
%
ymm10
%
ymm10
.
byte
196
98
125
24
29
219
33
3
0
/
/
vbroadcastss
0x321db
(
%
rip
)
%
ymm11
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
196
65
44
89
211
/
/
vmulps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
88
29
209
33
3
0
/
/
vpbroadcastd
0x321d1
(
%
rip
)
%
ymm11
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
196
65
61
219
195
/
/
vpand
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
29
194
33
3
0
/
/
vbroadcastss
0x321c2
(
%
rip
)
%
ymm11
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
197
100
194
223
1
/
/
vcmpltps
%
ymm7
%
ymm3
%
ymm11
.
byte
196
65
44
93
224
/
/
vminps
%
ymm8
%
ymm10
%
ymm12
.
byte
196
65
52
93
228
/
/
vminps
%
ymm12
%
ymm9
%
ymm12
.
byte
196
65
44
95
232
/
/
vmaxps
%
ymm8
%
ymm10
%
ymm13
.
byte
196
65
52
95
237
/
/
vmaxps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
220
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
226
53
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm9
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
226
45
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm10
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
226
61
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
226
37
168
223
/
/
vfmadd213ps
%
ymm7
%
ymm11
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
62
255
255
255
/
/
ja
a367
<
_sk_lerp_565_hsw
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
101
0
0
0
/
/
lea
0x65
(
%
rip
)
%
r9
#
a498
<
_sk_lerp_565_hsw
+
0x150
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
29
255
255
255
/
/
jmpq
a367
<
_sk_lerp_565_hsw
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
255
254
255
255
/
/
jmpq
a367
<
_sk_lerp_565_hsw
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
233
209
254
255
255
/
/
jmpq
a367
<
_sk_lerp_565_hsw
+
0x1f
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
164
/
/
movsb
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
178
/
/
mov
0xb2ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
229
/
/
jmpq
*
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_tables_hsw
.
globl
_sk_load_tables_hsw
FUNCTION
(
_sk_load_tables_hsw
)
_sk_load_tables_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
116
/
/
jne
a532
<
_sk_load_tables_hsw
+
0x7e
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
197
229
219
13
244
34
3
0
/
/
vpand
0x322f4
(
%
rip
)
%
ymm3
%
ymm1
#
3c7c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x574
>
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
194
53
146
4
136
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm1
4
)
%
ymm0
.
byte
196
98
101
0
13
235
34
3
0
/
/
vpshufb
0x322eb
(
%
rip
)
%
ymm3
%
ymm9
#
3c7e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x594
>
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
130
45
146
12
137
/
/
vgatherdps
%
ymm10
(
%
r9
%
ymm9
4
)
%
ymm1
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
196
98
101
0
13
239
34
3
0
/
/
vpshufb
0x322ef
(
%
rip
)
%
ymm3
%
ymm9
#
3c800
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x5b4
>
.
byte
196
162
61
146
20
136
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm9
4
)
%
ymm2
.
byte
197
229
114
211
24
/
/
vpsrld
0x18
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
75
32
3
0
/
/
vbroadcastss
0x3204b
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
137
249
/
/
mov
%
edi
%
r9d
.
byte
65
128
225
7
/
/
and
0x7
%
r9b
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
65
254
201
/
/
dec
%
r9b
.
byte
65
128
249
6
/
/
cmp
0x6
%
r9b
.
byte
15
135
122
255
255
255
/
/
ja
a4c4
<
_sk_load_tables_hsw
+
0x10
>
.
byte
69
15
182
201
/
/
movzbl
%
r9b
%
r9d
.
byte
76
141
21
139
0
0
0
/
/
lea
0x8b
(
%
rip
)
%
r10
#
a5e0
<
_sk_load_tables_hsw
+
0x12c
>
.
byte
79
99
12
138
/
/
movslq
(
%
r10
%
r9
4
)
%
r9
.
byte
77
1
209
/
/
add
%
r10
%
r9
.
byte
65
255
225
/
/
jmpq
*
%
r9
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
90
255
255
255
/
/
jmpq
a4c4
<
_sk_load_tables_hsw
+
0x10
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
4
/
/
vpblendd
0x4
%
ymm0
%
ymm1
%
ymm3
.
byte
196
193
122
126
4
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
51
255
255
255
/
/
jmpq
a4c4
<
_sk_load_tables_hsw
+
0x10
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
228
254
255
255
/
/
jmpq
a4c4
<
_sk_load_tables_hsw
+
0x10
>
.
byte
127
255
/
/
jg
a5e1
<
_sk_load_tables_hsw
+
0x12d
>
.
byte
255
/
/
(
bad
)
.
byte
255
160
255
255
255
138
/
/
jmpq
*
-
0x75000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
177
255
/
/
mov
0xff
%
cl
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_tables_u16_be_hsw
.
globl
_sk_load_tables_u16_be_hsw
FUNCTION
(
_sk_load_tables_u16_be_hsw
)
_sk_load_tables_u16_be_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
215
0
0
0
/
/
jne
a6e9
<
_sk_load_tables_u16_be_hsw
+
0xed
>
.
byte
196
1
121
16
4
72
/
/
vmovupd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
129
121
16
84
72
16
/
/
vmovupd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
196
129
121
16
92
72
32
/
/
vmovupd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
1
122
111
76
72
48
/
/
vmovdqu
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
121
105
202
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm9
.
byte
197
241
97
195
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm0
.
byte
197
113
105
243
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm14
.
byte
197
185
108
200
/
/
vpunpcklqdq
%
xmm0
%
xmm8
%
xmm1
.
byte
197
57
109
192
/
/
vpunpckhqdq
%
xmm0
%
xmm8
%
xmm8
.
byte
196
65
49
108
214
/
/
vpunpcklqdq
%
xmm14
%
xmm9
%
xmm10
.
byte
197
121
111
29
76
40
3
0
/
/
vmovdqa
0x3284c
(
%
rip
)
%
xmm11
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
193
113
219
195
/
/
vpand
%
xmm11
%
xmm1
%
xmm0
.
byte
196
226
125
51
200
/
/
vpmovzxwd
%
xmm0
%
ymm1
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
194
21
146
4
136
/
/
vgatherdps
%
ymm13
(
%
r8
%
ymm1
4
)
%
ymm0
.
byte
196
193
57
219
203
/
/
vpand
%
xmm11
%
xmm8
%
xmm1
.
byte
196
98
125
51
193
/
/
vpmovzxwd
%
xmm1
%
ymm8
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
130
21
146
12
129
/
/
vgatherdps
%
ymm13
(
%
r9
%
ymm8
4
)
%
ymm1
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
196
193
41
219
219
/
/
vpand
%
xmm11
%
xmm10
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
196
226
29
146
20
152
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
196
193
49
109
222
/
/
vpunpckhqdq
%
xmm14
%
xmm9
%
xmm3
.
byte
197
185
113
243
8
/
/
vpsllw
0x8
%
xmm3
%
xmm8
.
byte
197
225
113
211
8
/
/
vpsrlw
0x8
%
xmm3
%
xmm3
.
byte
197
185
235
219
/
/
vpor
%
xmm3
%
xmm8
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
176
30
3
0
/
/
vbroadcastss
0x31eb0
(
%
rip
)
%
ymm8
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
123
16
4
72
/
/
vmovsd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
a74f
<
_sk_load_tables_u16_be_hsw
+
0x153
>
.
byte
196
1
57
22
68
72
8
/
/
vmovhpd
0x8
(
%
r8
%
r9
2
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
a74f
<
_sk_load_tables_u16_be_hsw
+
0x153
>
.
byte
196
129
123
16
84
72
16
/
/
vmovsd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
a75c
<
_sk_load_tables_u16_be_hsw
+
0x160
>
.
byte
196
129
105
22
84
72
24
/
/
vmovhpd
0x18
(
%
r8
%
r9
2
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
a75c
<
_sk_load_tables_u16_be_hsw
+
0x160
>
.
byte
196
129
123
16
92
72
32
/
/
vmovsd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
251
254
255
255
/
/
je
a62d
<
_sk_load_tables_u16_be_hsw
+
0x31
>
.
byte
196
129
97
22
92
72
40
/
/
vmovhpd
0x28
(
%
r8
%
r9
2
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
234
254
255
255
/
/
jb
a62d
<
_sk_load_tables_u16_be_hsw
+
0x31
>
.
byte
196
1
122
126
76
72
48
/
/
vmovq
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
233
222
254
255
255
/
/
jmpq
a62d
<
_sk_load_tables_u16_be_hsw
+
0x31
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
209
254
255
255
/
/
jmpq
a62d
<
_sk_load_tables_u16_be_hsw
+
0x31
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
200
254
255
255
/
/
jmpq
a62d
<
_sk_load_tables_u16_be_hsw
+
0x31
>
HIDDEN
_sk_load_tables_rgb_u16_be_hsw
.
globl
_sk_load_tables_rgb_u16_be_hsw
FUNCTION
(
_sk_load_tables_rgb_u16_be_hsw
)
_sk_load_tables_rgb_u16_be_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
206
0
0
0
/
/
jne
a845
<
_sk_load_tables_rgb_u16_be_hsw
+
0xe0
>
.
byte
196
1
122
111
28
72
/
/
vmovdqu
(
%
r8
%
r9
2
)
%
xmm11
.
byte
196
129
122
111
92
72
12
/
/
vmovdqu
0xc
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
129
122
111
84
72
24
/
/
vmovdqu
0x18
(
%
r8
%
r9
2
)
%
xmm2
.
byte
196
129
122
111
68
72
32
/
/
vmovdqu
0x20
(
%
r8
%
r9
2
)
%
xmm0
.
byte
197
249
115
216
4
/
/
vpsrldq
0x4
%
xmm0
%
xmm0
.
byte
196
193
57
115
219
6
/
/
vpsrldq
0x6
%
xmm11
%
xmm8
.
byte
197
169
115
219
6
/
/
vpsrldq
0x6
%
xmm3
%
xmm10
.
byte
197
241
115
218
6
/
/
vpsrldq
0x6
%
xmm2
%
xmm1
.
byte
197
177
115
216
6
/
/
vpsrldq
0x6
%
xmm0
%
xmm9
.
byte
196
65
113
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm1
%
xmm9
.
byte
197
233
97
192
/
/
vpunpcklwd
%
xmm0
%
xmm2
%
xmm0
.
byte
196
193
57
97
210
/
/
vpunpcklwd
%
xmm10
%
xmm8
%
xmm2
.
byte
197
161
97
219
/
/
vpunpcklwd
%
xmm3
%
xmm11
%
xmm3
.
byte
197
225
97
202
/
/
vpunpcklwd
%
xmm2
%
xmm3
%
xmm1
.
byte
197
225
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm3
%
xmm2
.
byte
196
193
121
97
217
/
/
vpunpcklwd
%
xmm9
%
xmm0
%
xmm3
.
byte
196
193
121
105
193
/
/
vpunpckhwd
%
xmm9
%
xmm0
%
xmm0
.
byte
197
105
108
192
/
/
vpunpcklqdq
%
xmm0
%
xmm2
%
xmm8
.
byte
197
241
108
195
/
/
vpunpcklqdq
%
xmm3
%
xmm1
%
xmm0
.
byte
197
241
109
203
/
/
vpunpckhqdq
%
xmm3
%
xmm1
%
xmm1
.
byte
197
249
111
29
204
38
3
0
/
/
vmovdqa
0x326cc
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
197
249
219
195
/
/
vpand
%
xmm3
%
xmm0
%
xmm0
.
byte
196
98
125
51
200
/
/
vpmovzxwd
%
xmm0
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
130
37
146
4
136
/
/
vgatherdps
%
ymm11
(
%
r8
%
ymm9
4
)
%
ymm0
.
byte
197
241
219
203
/
/
vpand
%
xmm3
%
xmm1
%
xmm1
.
byte
196
98
125
51
201
/
/
vpmovzxwd
%
xmm1
%
ymm9
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
130
37
146
12
137
/
/
vgatherdps
%
ymm11
(
%
r9
%
ymm9
4
)
%
ymm1
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
197
185
219
219
/
/
vpand
%
xmm3
%
xmm8
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
196
226
45
146
20
152
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm3
4
)
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
185
28
3
0
/
/
vbroadcastss
0x31cb9
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
110
4
72
/
/
vmovd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
92
72
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
r9
2
)
%
xmm0
%
xmm11
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
31
/
/
jne
a87d
<
_sk_load_tables_rgb_u16_be_hsw
+
0x118
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
233
47
255
255
255
/
/
jmpq
a7ac
<
_sk_load_tables_rgb_u16_be_hsw
+
0x47
>
.
byte
196
129
121
110
68
72
6
/
/
vmovd
0x6
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
68
72
10
2
/
/
vpinsrw
0x2
0xa
(
%
r8
%
r9
2
)
%
xmm0
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
48
/
/
jb
a8c7
<
_sk_load_tables_rgb_u16_be_hsw
+
0x162
>
.
byte
196
129
121
110
68
72
12
/
/
vmovd
0xc
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
92
72
16
2
/
/
vpinsrw
0x2
0x10
(
%
r8
%
r9
2
)
%
xmm0
%
xmm3
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
117
48
/
/
jne
a8e1
<
_sk_load_tables_rgb_u16_be_hsw
+
0x17c
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
233
229
254
255
255
/
/
jmpq
a7ac
<
_sk_load_tables_rgb_u16_be_hsw
+
0x47
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
233
203
254
255
255
/
/
jmpq
a7ac
<
_sk_load_tables_rgb_u16_be_hsw
+
0x47
>
.
byte
196
129
121
110
68
72
18
/
/
vmovd
0x12
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
84
72
22
2
/
/
vpinsrw
0x2
0x16
(
%
r8
%
r9
2
)
%
xmm0
%
xmm10
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
39
/
/
jb
a922
<
_sk_load_tables_rgb_u16_be_hsw
+
0x1bd
>
.
byte
196
129
121
110
68
72
24
/
/
vmovd
0x18
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
84
72
28
2
/
/
vpinsrw
0x2
0x1c
(
%
r8
%
r9
2
)
%
xmm0
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
117
30
/
/
jne
a933
<
_sk_load_tables_rgb_u16_be_hsw
+
0x1ce
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
233
138
254
255
255
/
/
jmpq
a7ac
<
_sk_load_tables_rgb_u16_be_hsw
+
0x47
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
233
121
254
255
255
/
/
jmpq
a7ac
<
_sk_load_tables_rgb_u16_be_hsw
+
0x47
>
.
byte
196
129
121
110
68
72
30
/
/
vmovd
0x1e
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
76
72
34
2
/
/
vpinsrw
0x2
0x22
(
%
r8
%
r9
2
)
%
xmm0
%
xmm1
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
25
/
/
jb
a966
<
_sk_load_tables_rgb_u16_be_hsw
+
0x201
>
.
byte
196
129
121
110
68
72
36
/
/
vmovd
0x24
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
68
72
40
2
/
/
vpinsrw
0x2
0x28
(
%
r8
%
r9
2
)
%
xmm0
%
xmm0
.
byte
233
70
254
255
255
/
/
jmpq
a7ac
<
_sk_load_tables_rgb_u16_be_hsw
+
0x47
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
233
61
254
255
255
/
/
jmpq
a7ac
<
_sk_load_tables_rgb_u16_be_hsw
+
0x47
>
HIDDEN
_sk_byte_tables_hsw
.
globl
_sk_byte_tables_hsw
FUNCTION
(
_sk_byte_tables_hsw
)
_sk_byte_tables_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
29
121
27
3
0
/
/
vbroadcastss
0x31b79
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
211
/
/
vminps
%
ymm11
%
ymm9
%
ymm10
.
byte
196
98
125
24
13
151
27
3
0
/
/
vbroadcastss
0x31b97
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
44
89
209
/
/
vmulps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
125
91
210
/
/
vcvtps2dq
%
ymm10
%
ymm10
.
byte
196
65
249
126
208
/
/
vmovq
%
xmm10
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
193
121
110
193
/
/
vmovd
%
r9d
%
xmm0
.
byte
196
67
249
22
209
1
/
/
vpextrq
0x1
%
xmm10
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
121
32
224
1
/
/
vpinsrb
0x1
%
r8d
%
xmm0
%
xmm12
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
99
125
57
208
1
/
/
vextracti128
0x1
%
ymm10
%
xmm0
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
25
32
208
2
/
/
vpinsrb
0x2
%
r8d
%
xmm12
%
xmm10
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
3
/
/
vpinsrb
0x3
%
r9d
%
xmm10
%
xmm10
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
4
/
/
vpinsrb
0x4
%
r9d
%
xmm10
%
xmm10
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
41
32
192
5
/
/
vpinsrb
0x5
%
r8d
%
xmm10
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
121
32
192
6
/
/
vpinsrb
0x6
%
r8d
%
xmm0
%
xmm0
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
121
32
209
7
/
/
vpinsrb
0x7
%
r9d
%
xmm0
%
xmm10
.
byte
197
188
95
193
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
200
/
/
vcvtps2dq
%
ymm0
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm0
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
4
/
/
vpinsrb
0x4
%
r10d
%
xmm0
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
5
/
/
vpinsrb
0x5
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
6
/
/
vpinsrb
0x6
%
r9d
%
xmm0
%
xmm0
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
195
121
32
200
7
/
/
vpinsrb
0x7
%
r8d
%
xmm0
%
xmm1
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
197
188
95
194
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
208
/
/
vcvtps2dq
%
ymm0
%
ymm2
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
210
1
/
/
vextracti128
0x1
%
ymm2
%
xmm2
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm0
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
4
/
/
vpinsrb
0x4
%
r10d
%
xmm0
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
5
/
/
vpinsrb
0x5
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
6
/
/
vpinsrb
0x6
%
r9d
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
195
121
32
208
7
/
/
vpinsrb
0x7
%
r8d
%
xmm0
%
xmm2
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
197
188
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
216
/
/
vmovd
%
eax
%
xmm3
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
97
32
216
1
/
/
vpinsrb
0x1
%
eax
%
xmm3
%
xmm3
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
97
32
216
2
/
/
vpinsrb
0x2
%
eax
%
xmm3
%
xmm3
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
227
97
32
216
3
/
/
vpinsrb
0x3
%
eax
%
xmm3
%
xmm3
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
97
32
216
4
/
/
vpinsrb
0x4
%
eax
%
xmm3
%
xmm3
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
97
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm3
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
121
32
216
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
194
125
49
194
/
/
vpmovzxbd
%
xmm10
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
77
25
3
0
/
/
vbroadcastss
0x3194d
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
226
125
49
201
/
/
vpmovzxbd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
226
125
49
210
/
/
vpmovzxbd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
227
97
32
216
7
/
/
vpinsrb
0x7
%
eax
%
xmm3
%
xmm3
.
byte
196
226
125
49
219
/
/
vpmovzxbd
%
xmm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_byte_tables_rgb_hsw
.
globl
_sk_byte_tables_rgb_hsw
FUNCTION
(
_sk_byte_tables_rgb_hsw
)
_sk_byte_tables_rgb_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
196
65
121
110
192
/
/
vmovd
%
r8d
%
xmm8
.
byte
196
66
125
88
192
/
/
vpbroadcastd
%
xmm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
52
95
208
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm10
.
byte
196
98
125
24
29
114
24
3
0
/
/
vbroadcastss
0x31872
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
44
93
211
/
/
vminps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
44
89
208
/
/
vmulps
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
125
91
210
/
/
vcvtps2dq
%
ymm10
%
ymm10
.
byte
196
65
249
126
208
/
/
vmovq
%
xmm10
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
193
121
110
193
/
/
vmovd
%
r9d
%
xmm0
.
byte
196
67
249
22
209
1
/
/
vpextrq
0x1
%
xmm10
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
121
32
224
1
/
/
vpinsrb
0x1
%
r8d
%
xmm0
%
xmm12
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
99
125
57
208
1
/
/
vextracti128
0x1
%
ymm10
%
xmm0
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
25
32
208
2
/
/
vpinsrb
0x2
%
r8d
%
xmm12
%
xmm10
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
3
/
/
vpinsrb
0x3
%
r9d
%
xmm10
%
xmm10
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
41
32
209
4
/
/
vpinsrb
0x4
%
r9d
%
xmm10
%
xmm10
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
41
32
192
5
/
/
vpinsrb
0x5
%
r8d
%
xmm10
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
121
32
192
6
/
/
vpinsrb
0x6
%
r8d
%
xmm0
%
xmm0
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
121
32
209
7
/
/
vpinsrb
0x7
%
r9d
%
xmm0
%
xmm10
.
byte
197
180
95
193
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
253
91
200
/
/
vcvtps2dq
%
ymm0
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm0
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
195
121
32
194
4
/
/
vpinsrb
0x4
%
r10d
%
xmm0
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
5
/
/
vpinsrb
0x5
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
6
/
/
vpinsrb
0x6
%
r9d
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
195
121
32
200
7
/
/
vpinsrb
0x7
%
r8d
%
xmm0
%
xmm1
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
180
95
194
/
/
vmaxps
%
ymm2
%
ymm9
%
ymm0
.
byte
196
193
124
93
195
/
/
vminps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
105
32
208
1
/
/
vpinsrb
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
2
/
/
vpinsrb
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
227
105
32
208
3
/
/
vpinsrb
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
4
/
/
vpinsrb
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
105
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm2
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
121
32
208
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
194
125
49
194
/
/
vpmovzxbd
%
xmm10
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
246
22
3
0
/
/
vbroadcastss
0x316f6
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
226
125
49
201
/
/
vpmovzxbd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
227
105
32
208
7
/
/
vpinsrb
0x7
%
eax
%
xmm2
%
xmm2
.
byte
196
226
125
49
210
/
/
vpmovzxbd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_r_hsw
.
globl
_sk_table_r_hsw
FUNCTION
(
_sk_table_r_hsw
)
_sk_table_r_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
66
125
88
192
/
/
vpbroadcastd
%
xmm8
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
21
41
22
3
0
/
/
vbroadcastss
0x31629
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
194
/
/
vminps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
128
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_g_hsw
.
globl
_sk_table_g_hsw
FUNCTION
(
_sk_table_g_hsw
)
_sk_table_g_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
66
125
88
192
/
/
vpbroadcastd
%
xmm8
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
98
125
24
21
222
21
3
0
/
/
vbroadcastss
0x315de
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
116
93
202
/
/
vminps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
253
91
201
/
/
vcvtps2dq
%
ymm1
%
ymm1
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
136
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm1
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
193
/
/
vmovaps
%
ymm8
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_b_hsw
.
globl
_sk_table_b_hsw
FUNCTION
(
_sk_table_b_hsw
)
_sk_table_b_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
66
125
88
192
/
/
vpbroadcastd
%
xmm8
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
21
147
21
3
0
/
/
vbroadcastss
0x31593
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
108
93
210
/
/
vminps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
253
91
210
/
/
vcvtps2dq
%
ymm2
%
ymm2
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
144
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm2
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
194
/
/
vmovaps
%
ymm8
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_a_hsw
.
globl
_sk_table_a_hsw
FUNCTION
(
_sk_table_a_hsw
)
_sk_table_a_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
66
125
88
192
/
/
vpbroadcastd
%
xmm8
%
ymm8
.
byte
196
65
124
91
200
/
/
vcvtdq2ps
%
ymm8
%
ymm9
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
219
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm3
.
byte
196
98
125
24
21
72
21
3
0
/
/
vbroadcastss
0x31548
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
100
93
218
/
/
vminps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
193
100
89
217
/
/
vmulps
%
ymm9
%
ymm3
%
ymm3
.
byte
197
253
91
219
/
/
vcvtps2dq
%
ymm3
%
ymm3
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
146
4
152
/
/
vgatherdps
%
ymm9
(
%
r8
%
ymm3
4
)
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
195
/
/
vmovaps
%
ymm8
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_r_hsw
.
globl
_sk_parametric_r_hsw
FUNCTION
(
_sk_parametric_r_hsw
)
_sk_parametric_r_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
124
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm0
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
196
66
125
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm0
%
ymm9
.
byte
196
98
125
24
88
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
66
125
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm0
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
36
194
208
0
/
/
vcmpeqps
%
ymm0
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
115
21
3
0
/
/
vbroadcastss
0x31573
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
98
125
24
61
110
21
3
0
/
/
vbroadcastss
0x3156e
(
%
rip
)
%
ymm15
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
223
/
/
vandps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
192
20
3
0
/
/
vbroadcastss
0x314c0
(
%
rip
)
%
ymm15
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
223
/
/
vorps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
86
21
3
0
/
/
vbroadcastss
0x31556
(
%
rip
)
%
ymm15
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
21
184
254
/
/
vfmadd231ps
%
ymm14
%
ymm13
%
ymm15
.
byte
196
98
125
24
45
76
21
3
0
/
/
vbroadcastss
0x3154c
(
%
rip
)
%
ymm13
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
66
37
172
239
/
/
vfnmadd213ps
%
ymm15
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
66
21
3
0
/
/
vbroadcastss
0x31542
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
56
21
3
0
/
/
vbroadcastss
0x31538
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
25
21
3
0
/
/
vbroadcastss
0x31519
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
15
21
3
0
/
/
vbroadcastss
0x3150f
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
66
29
172
235
/
/
vfnmadd213ps
%
ymm11
%
ymm12
%
ymm13
.
byte
196
98
125
24
29
5
21
3
0
/
/
vbroadcastss
0x31505
(
%
rip
)
%
ymm11
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
36
92
220
/
/
vsubps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
251
20
3
0
/
/
vbroadcastss
0x314fb
(
%
rip
)
%
ymm12
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
28
94
219
/
/
vdivps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
98
125
24
37
236
20
3
0
/
/
vbroadcastss
0x314ec
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
208
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
254
19
3
0
/
/
vbroadcastss
0x313fe
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_g_hsw
.
globl
_sk_parametric_g_hsw
FUNCTION
(
_sk_parametric_g_hsw
)
_sk_parametric_g_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
116
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
196
66
117
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm1
%
ymm9
.
byte
196
98
125
24
88
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
66
117
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm1
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
36
194
209
0
/
/
vcmpeqps
%
ymm1
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
65
20
3
0
/
/
vbroadcastss
0x31441
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
98
125
24
61
60
20
3
0
/
/
vbroadcastss
0x3143c
(
%
rip
)
%
ymm15
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
223
/
/
vandps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
142
19
3
0
/
/
vbroadcastss
0x3138e
(
%
rip
)
%
ymm15
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
223
/
/
vorps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
36
20
3
0
/
/
vbroadcastss
0x31424
(
%
rip
)
%
ymm15
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
21
184
254
/
/
vfmadd231ps
%
ymm14
%
ymm13
%
ymm15
.
byte
196
98
125
24
45
26
20
3
0
/
/
vbroadcastss
0x3141a
(
%
rip
)
%
ymm13
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
66
37
172
239
/
/
vfnmadd213ps
%
ymm15
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
16
20
3
0
/
/
vbroadcastss
0x31410
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
6
20
3
0
/
/
vbroadcastss
0x31406
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
231
19
3
0
/
/
vbroadcastss
0x313e7
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
221
19
3
0
/
/
vbroadcastss
0x313dd
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
66
29
172
235
/
/
vfnmadd213ps
%
ymm11
%
ymm12
%
ymm13
.
byte
196
98
125
24
29
211
19
3
0
/
/
vbroadcastss
0x313d3
(
%
rip
)
%
ymm11
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
36
92
220
/
/
vsubps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
201
19
3
0
/
/
vbroadcastss
0x313c9
(
%
rip
)
%
ymm12
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
28
94
219
/
/
vdivps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
98
125
24
37
186
19
3
0
/
/
vbroadcastss
0x313ba
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
209
160
/
/
vblendvps
%
ymm10
%
ymm1
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
98
125
24
5
204
18
3
0
/
/
vbroadcastss
0x312cc
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_b_hsw
.
globl
_sk_parametric_b_hsw
FUNCTION
(
_sk_parametric_b_hsw
)
_sk_parametric_b_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
108
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm2
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
196
66
109
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm2
%
ymm9
.
byte
196
98
125
24
88
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
66
109
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm2
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
36
194
210
0
/
/
vcmpeqps
%
ymm2
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
15
19
3
0
/
/
vbroadcastss
0x3130f
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
98
125
24
61
10
19
3
0
/
/
vbroadcastss
0x3130a
(
%
rip
)
%
ymm15
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
223
/
/
vandps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
92
18
3
0
/
/
vbroadcastss
0x3125c
(
%
rip
)
%
ymm15
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
223
/
/
vorps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
242
18
3
0
/
/
vbroadcastss
0x312f2
(
%
rip
)
%
ymm15
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
21
184
254
/
/
vfmadd231ps
%
ymm14
%
ymm13
%
ymm15
.
byte
196
98
125
24
45
232
18
3
0
/
/
vbroadcastss
0x312e8
(
%
rip
)
%
ymm13
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
66
37
172
239
/
/
vfnmadd213ps
%
ymm15
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
222
18
3
0
/
/
vbroadcastss
0x312de
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
212
18
3
0
/
/
vbroadcastss
0x312d4
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
181
18
3
0
/
/
vbroadcastss
0x312b5
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
171
18
3
0
/
/
vbroadcastss
0x312ab
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
66
29
172
235
/
/
vfnmadd213ps
%
ymm11
%
ymm12
%
ymm13
.
byte
196
98
125
24
29
161
18
3
0
/
/
vbroadcastss
0x312a1
(
%
rip
)
%
ymm11
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
36
92
220
/
/
vsubps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
151
18
3
0
/
/
vbroadcastss
0x31297
(
%
rip
)
%
ymm12
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
28
94
219
/
/
vdivps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
98
125
24
37
136
18
3
0
/
/
vbroadcastss
0x31288
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
210
160
/
/
vblendvps
%
ymm10
%
ymm2
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
154
17
3
0
/
/
vbroadcastss
0x3119a
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_a_hsw
.
globl
_sk_parametric_a_hsw
FUNCTION
(
_sk_parametric_a_hsw
)
_sk_parametric_a_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
100
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm3
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
196
66
101
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm3
%
ymm9
.
byte
196
98
125
24
88
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
66
101
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm3
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
36
194
211
0
/
/
vcmpeqps
%
ymm3
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
221
17
3
0
/
/
vbroadcastss
0x311dd
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
98
125
24
61
216
17
3
0
/
/
vbroadcastss
0x311d8
(
%
rip
)
%
ymm15
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
223
/
/
vandps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
42
17
3
0
/
/
vbroadcastss
0x3112a
(
%
rip
)
%
ymm15
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
223
/
/
vorps
%
ymm15
%
ymm11
%
ymm11
.
byte
196
98
125
24
61
192
17
3
0
/
/
vbroadcastss
0x311c0
(
%
rip
)
%
ymm15
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
21
184
254
/
/
vfmadd231ps
%
ymm14
%
ymm13
%
ymm15
.
byte
196
98
125
24
45
182
17
3
0
/
/
vbroadcastss
0x311b6
(
%
rip
)
%
ymm13
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
66
37
172
239
/
/
vfnmadd213ps
%
ymm15
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
172
17
3
0
/
/
vbroadcastss
0x311ac
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
162
17
3
0
/
/
vbroadcastss
0x311a2
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
131
17
3
0
/
/
vbroadcastss
0x31183
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
121
17
3
0
/
/
vbroadcastss
0x31179
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
66
29
172
235
/
/
vfnmadd213ps
%
ymm11
%
ymm12
%
ymm13
.
byte
196
98
125
24
29
111
17
3
0
/
/
vbroadcastss
0x3116f
(
%
rip
)
%
ymm11
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
36
92
220
/
/
vsubps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
101
17
3
0
/
/
vbroadcastss
0x31165
(
%
rip
)
%
ymm12
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
28
94
219
/
/
vdivps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
20
88
219
/
/
vaddps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
98
125
24
37
86
17
3
0
/
/
vbroadcastss
0x31156
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
211
160
/
/
vblendvps
%
ymm10
%
ymm3
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
219
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm3
.
byte
196
98
125
24
5
104
16
3
0
/
/
vbroadcastss
0x31068
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_hsw
.
globl
_sk_gamma_hsw
FUNCTION
(
_sk_gamma_hsw
)
_sk_gamma_hsw
:
.
byte
72
129
236
248
0
0
0
/
/
sub
0xf8
%
rsp
.
byte
197
252
17
188
36
192
0
0
0
/
/
vmovups
%
ymm7
0xc0
(
%
rsp
)
.
byte
197
252
17
180
36
160
0
0
0
/
/
vmovups
%
ymm6
0xa0
(
%
rsp
)
.
byte
197
252
17
172
36
128
0
0
0
/
/
vmovups
%
ymm5
0x80
(
%
rsp
)
.
byte
197
252
17
100
36
96
/
/
vmovups
%
ymm4
0x60
(
%
rsp
)
.
byte
197
252
17
92
36
64
/
/
vmovups
%
ymm3
0x40
(
%
rsp
)
.
byte
197
252
17
84
36
224
/
/
vmovups
%
ymm2
-
0x20
(
%
rsp
)
.
byte
197
252
17
12
36
/
/
vmovups
%
ymm1
(
%
rsp
)
.
byte
197
124
91
208
/
/
vcvtdq2ps
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
177
16
3
0
/
/
vbroadcastss
0x310b1
(
%
rip
)
%
ymm11
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
226
125
24
37
172
16
3
0
/
/
vbroadcastss
0x310ac
(
%
rip
)
%
ymm4
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
197
124
84
228
/
/
vandps
%
ymm4
%
ymm0
%
ymm12
.
byte
196
226
125
24
29
255
15
3
0
/
/
vbroadcastss
0x30fff
(
%
rip
)
%
ymm3
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
17
92
36
32
/
/
vmovups
%
ymm3
0x20
(
%
rsp
)
.
byte
197
28
86
227
/
/
vorps
%
ymm3
%
ymm12
%
ymm12
.
byte
196
98
125
24
45
144
16
3
0
/
/
vbroadcastss
0x31090
(
%
rip
)
%
ymm13
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
37
168
213
/
/
vfmadd213ps
%
ymm13
%
ymm11
%
ymm10
.
byte
197
124
91
241
/
/
vcvtdq2ps
%
ymm1
%
ymm14
.
byte
196
66
37
168
245
/
/
vfmadd213ps
%
ymm13
%
ymm11
%
ymm14
.
byte
197
124
91
250
/
/
vcvtdq2ps
%
ymm2
%
ymm15
.
byte
196
66
37
168
253
/
/
vfmadd213ps
%
ymm13
%
ymm11
%
ymm15
.
byte
196
226
125
24
45
116
16
3
0
/
/
vbroadcastss
0x31074
(
%
rip
)
%
ymm5
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
98
29
188
213
/
/
vfnmadd231ps
%
ymm5
%
ymm12
%
ymm10
.
byte
196
226
125
24
13
106
16
3
0
/
/
vbroadcastss
0x3106a
(
%
rip
)
%
ymm1
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
197
252
17
76
36
160
/
/
vmovups
%
ymm1
-
0x60
(
%
rsp
)
.
byte
197
28
88
225
/
/
vaddps
%
ymm1
%
ymm12
%
ymm12
.
byte
196
98
125
24
5
91
16
3
0
/
/
vbroadcastss
0x3105b
(
%
rip
)
%
ymm8
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
60
94
228
/
/
vdivps
%
ymm12
%
ymm8
%
ymm12
.
byte
197
124
17
68
36
128
/
/
vmovups
%
ymm8
-
0x80
(
%
rsp
)
.
byte
196
65
44
92
212
/
/
vsubps
%
ymm12
%
ymm10
%
ymm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
40
/
/
vbroadcastss
(
%
rax
)
%
ymm13
.
byte
196
65
44
89
213
/
/
vmulps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
195
125
8
250
1
/
/
vroundps
0x1
%
ymm10
%
ymm7
.
byte
197
172
92
255
/
/
vsubps
%
ymm7
%
ymm10
%
ymm7
.
byte
196
98
125
24
29
48
16
3
0
/
/
vbroadcastss
0x31030
(
%
rip
)
%
ymm11
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
24
37
38
16
3
0
/
/
vbroadcastss
0x31026
(
%
rip
)
%
ymm12
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
66
69
188
212
/
/
vfnmadd231ps
%
ymm12
%
ymm7
%
ymm10
.
byte
196
98
125
24
13
28
16
3
0
/
/
vbroadcastss
0x3101c
(
%
rip
)
%
ymm9
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
197
180
92
255
/
/
vsubps
%
ymm7
%
ymm9
%
ymm7
.
byte
196
226
125
24
13
19
16
3
0
/
/
vbroadcastss
0x31013
(
%
rip
)
%
ymm1
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
197
244
94
255
/
/
vdivps
%
ymm7
%
ymm1
%
ymm7
.
byte
197
172
88
255
/
/
vaddps
%
ymm7
%
ymm10
%
ymm7
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
124
194
242
0
/
/
vcmpeqps
%
ymm10
%
ymm0
%
ymm6
.
byte
196
226
125
24
29
251
15
3
0
/
/
vbroadcastss
0x30ffb
(
%
rip
)
%
ymm3
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
196
89
251
/
/
vmulps
%
ymm3
%
ymm7
%
ymm7
.
byte
197
253
91
255
/
/
vcvtps2dq
%
ymm7
%
ymm7
.
byte
196
195
69
74
194
96
/
/
vblendvps
%
ymm6
%
ymm10
%
ymm7
%
ymm0
.
byte
197
252
17
68
36
192
/
/
vmovups
%
ymm0
-
0x40
(
%
rsp
)
.
byte
197
252
16
4
36
/
/
vmovups
(
%
rsp
)
%
ymm0
.
byte
197
252
84
244
/
/
vandps
%
ymm4
%
ymm0
%
ymm6
.
byte
197
252
16
84
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm2
.
byte
197
204
86
242
/
/
vorps
%
ymm2
%
ymm6
%
ymm6
.
byte
196
98
77
188
245
/
/
vfnmadd231ps
%
ymm5
%
ymm6
%
ymm14
.
byte
197
252
16
124
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm7
.
byte
197
204
88
247
/
/
vaddps
%
ymm7
%
ymm6
%
ymm6
.
byte
197
188
94
246
/
/
vdivps
%
ymm6
%
ymm8
%
ymm6
.
byte
197
140
92
246
/
/
vsubps
%
ymm6
%
ymm14
%
ymm6
.
byte
196
193
76
89
245
/
/
vmulps
%
ymm13
%
ymm6
%
ymm6
.
byte
196
99
125
8
246
1
/
/
vroundps
0x1
%
ymm6
%
ymm14
.
byte
196
65
76
92
246
/
/
vsubps
%
ymm14
%
ymm6
%
ymm14
.
byte
196
193
76
88
243
/
/
vaddps
%
ymm11
%
ymm6
%
ymm6
.
byte
196
194
13
188
244
/
/
vfnmadd231ps
%
ymm12
%
ymm14
%
ymm6
.
byte
196
65
52
92
246
/
/
vsubps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
65
116
94
246
/
/
vdivps
%
ymm14
%
ymm1
%
ymm14
.
byte
196
193
76
88
246
/
/
vaddps
%
ymm14
%
ymm6
%
ymm6
.
byte
196
65
124
194
242
0
/
/
vcmpeqps
%
ymm10
%
ymm0
%
ymm14
.
byte
197
204
89
243
/
/
vmulps
%
ymm3
%
ymm6
%
ymm6
.
byte
197
253
91
246
/
/
vcvtps2dq
%
ymm6
%
ymm6
.
byte
196
195
77
74
242
224
/
/
vblendvps
%
ymm14
%
ymm10
%
ymm6
%
ymm6
.
byte
197
124
16
116
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm14
.
byte
197
12
84
196
/
/
vandps
%
ymm4
%
ymm14
%
ymm8
.
byte
197
60
86
194
/
/
vorps
%
ymm2
%
ymm8
%
ymm8
.
byte
196
98
61
188
253
/
/
vfnmadd231ps
%
ymm5
%
ymm8
%
ymm15
.
byte
197
60
88
199
/
/
vaddps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
252
16
68
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm0
.
byte
196
193
124
94
232
/
/
vdivps
%
ymm8
%
ymm0
%
ymm5
.
byte
197
132
92
237
/
/
vsubps
%
ymm5
%
ymm15
%
ymm5
.
byte
196
193
84
89
237
/
/
vmulps
%
ymm13
%
ymm5
%
ymm5
.
byte
196
99
125
8
197
1
/
/
vroundps
0x1
%
ymm5
%
ymm8
.
byte
196
65
84
92
192
/
/
vsubps
%
ymm8
%
ymm5
%
ymm8
.
byte
196
193
84
88
227
/
/
vaddps
%
ymm11
%
ymm5
%
ymm4
.
byte
196
194
61
188
228
/
/
vfnmadd231ps
%
ymm12
%
ymm8
%
ymm4
.
byte
196
193
52
92
208
/
/
vsubps
%
ymm8
%
ymm9
%
ymm2
.
byte
197
244
94
202
/
/
vdivps
%
ymm2
%
ymm1
%
ymm1
.
byte
197
220
88
201
/
/
vaddps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
244
89
195
/
/
vmulps
%
ymm3
%
ymm1
%
ymm0
.
byte
196
193
12
194
202
0
/
/
vcmpeqps
%
ymm10
%
ymm14
%
ymm1
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
195
125
74
210
16
/
/
vblendvps
%
ymm1
%
ymm10
%
ymm0
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
68
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
40
206
/
/
vmovaps
%
ymm6
%
ymm1
.
byte
197
252
16
92
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm3
.
byte
197
252
16
100
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm4
.
byte
197
252
16
172
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm5
.
byte
197
252
16
180
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm6
.
byte
197
252
16
188
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm7
.
byte
72
129
196
248
0
0
0
/
/
add
0xf8
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_dst_hsw
.
globl
_sk_gamma_dst_hsw
FUNCTION
(
_sk_gamma_dst_hsw
)
_sk_gamma_dst_hsw
:
.
byte
72
129
236
216
0
0
0
/
/
sub
0xd8
%
rsp
.
byte
197
252
17
188
36
160
0
0
0
/
/
vmovups
%
ymm7
0xa0
(
%
rsp
)
.
byte
197
252
17
116
36
128
/
/
vmovups
%
ymm6
-
0x80
(
%
rsp
)
.
byte
197
252
17
156
36
128
0
0
0
/
/
vmovups
%
ymm3
0x80
(
%
rsp
)
.
byte
197
252
17
84
36
96
/
/
vmovups
%
ymm2
0x60
(
%
rsp
)
.
byte
197
252
17
76
36
64
/
/
vmovups
%
ymm1
0x40
(
%
rsp
)
.
byte
197
252
17
68
36
32
/
/
vmovups
%
ymm0
0x20
(
%
rsp
)
.
byte
197
124
91
212
/
/
vcvtdq2ps
%
ymm4
%
ymm10
.
byte
196
98
125
24
29
125
14
3
0
/
/
vbroadcastss
0x30e7d
(
%
rip
)
%
ymm11
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
226
125
24
5
120
14
3
0
/
/
vbroadcastss
0x30e78
(
%
rip
)
%
ymm0
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
197
92
84
224
/
/
vandps
%
ymm0
%
ymm4
%
ymm12
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
252
40
220
/
/
vmovaps
%
ymm4
%
ymm3
.
byte
196
226
125
24
5
195
13
3
0
/
/
vbroadcastss
0x30dc3
(
%
rip
)
%
ymm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
17
4
36
/
/
vmovups
%
ymm0
(
%
rsp
)
.
byte
197
28
86
224
/
/
vorps
%
ymm0
%
ymm12
%
ymm12
.
byte
196
98
125
24
45
85
14
3
0
/
/
vbroadcastss
0x30e55
(
%
rip
)
%
ymm13
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
66
37
168
213
/
/
vfmadd213ps
%
ymm13
%
ymm11
%
ymm10
.
byte
197
252
40
197
/
/
vmovaps
%
ymm5
%
ymm0
.
byte
197
124
91
240
/
/
vcvtdq2ps
%
ymm0
%
ymm14
.
byte
196
66
37
168
245
/
/
vfmadd213ps
%
ymm13
%
ymm11
%
ymm14
.
byte
197
124
91
124
36
128
/
/
vcvtdq2ps
-
0x80
(
%
rsp
)
%
ymm15
.
byte
196
66
37
168
253
/
/
vfmadd213ps
%
ymm13
%
ymm11
%
ymm15
.
byte
196
226
125
24
45
51
14
3
0
/
/
vbroadcastss
0x30e33
(
%
rip
)
%
ymm5
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
98
29
188
213
/
/
vfnmadd231ps
%
ymm5
%
ymm12
%
ymm10
.
byte
196
226
125
24
13
41
14
3
0
/
/
vbroadcastss
0x30e29
(
%
rip
)
%
ymm1
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
197
252
17
76
36
192
/
/
vmovups
%
ymm1
-
0x40
(
%
rsp
)
.
byte
197
28
88
225
/
/
vaddps
%
ymm1
%
ymm12
%
ymm12
.
byte
196
98
125
24
13
26
14
3
0
/
/
vbroadcastss
0x30e1a
(
%
rip
)
%
ymm9
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
52
94
228
/
/
vdivps
%
ymm12
%
ymm9
%
ymm12
.
byte
197
124
17
76
36
160
/
/
vmovups
%
ymm9
-
0x60
(
%
rsp
)
.
byte
196
65
44
92
212
/
/
vsubps
%
ymm12
%
ymm10
%
ymm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
40
/
/
vbroadcastss
(
%
rax
)
%
ymm13
.
byte
196
65
44
89
213
/
/
vmulps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
195
125
8
250
1
/
/
vroundps
0x1
%
ymm10
%
ymm7
.
byte
197
172
92
255
/
/
vsubps
%
ymm7
%
ymm10
%
ymm7
.
byte
196
98
125
24
29
239
13
3
0
/
/
vbroadcastss
0x30def
(
%
rip
)
%
ymm11
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
24
37
229
13
3
0
/
/
vbroadcastss
0x30de5
(
%
rip
)
%
ymm12
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
66
69
188
212
/
/
vfnmadd231ps
%
ymm12
%
ymm7
%
ymm10
.
byte
196
98
125
24
5
219
13
3
0
/
/
vbroadcastss
0x30ddb
(
%
rip
)
%
ymm8
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
197
188
92
255
/
/
vsubps
%
ymm7
%
ymm8
%
ymm7
.
byte
196
226
125
24
13
210
13
3
0
/
/
vbroadcastss
0x30dd2
(
%
rip
)
%
ymm1
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
197
244
94
255
/
/
vdivps
%
ymm7
%
ymm1
%
ymm7
.
byte
197
172
88
255
/
/
vaddps
%
ymm7
%
ymm10
%
ymm7
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
100
194
242
0
/
/
vcmpeqps
%
ymm10
%
ymm3
%
ymm6
.
byte
196
226
125
24
29
186
13
3
0
/
/
vbroadcastss
0x30dba
(
%
rip
)
%
ymm3
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
196
89
251
/
/
vmulps
%
ymm3
%
ymm7
%
ymm7
.
byte
197
253
91
255
/
/
vcvtps2dq
%
ymm7
%
ymm7
.
byte
196
195
69
74
226
96
/
/
vblendvps
%
ymm6
%
ymm10
%
ymm7
%
ymm4
.
byte
197
252
17
100
36
224
/
/
vmovups
%
ymm4
-
0x20
(
%
rsp
)
.
byte
197
252
40
226
/
/
vmovaps
%
ymm2
%
ymm4
.
byte
197
252
84
244
/
/
vandps
%
ymm4
%
ymm0
%
ymm6
.
byte
197
252
16
20
36
/
/
vmovups
(
%
rsp
)
%
ymm2
.
byte
197
204
86
242
/
/
vorps
%
ymm2
%
ymm6
%
ymm6
.
byte
196
98
77
188
245
/
/
vfnmadd231ps
%
ymm5
%
ymm6
%
ymm14
.
byte
197
252
16
124
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm7
.
byte
197
204
88
247
/
/
vaddps
%
ymm7
%
ymm6
%
ymm6
.
byte
197
180
94
246
/
/
vdivps
%
ymm6
%
ymm9
%
ymm6
.
byte
197
140
92
246
/
/
vsubps
%
ymm6
%
ymm14
%
ymm6
.
byte
196
193
76
89
245
/
/
vmulps
%
ymm13
%
ymm6
%
ymm6
.
byte
196
99
125
8
246
1
/
/
vroundps
0x1
%
ymm6
%
ymm14
.
byte
196
65
76
92
246
/
/
vsubps
%
ymm14
%
ymm6
%
ymm14
.
byte
196
193
76
88
243
/
/
vaddps
%
ymm11
%
ymm6
%
ymm6
.
byte
196
194
13
188
244
/
/
vfnmadd231ps
%
ymm12
%
ymm14
%
ymm6
.
byte
196
65
60
92
246
/
/
vsubps
%
ymm14
%
ymm8
%
ymm14
.
byte
196
65
116
94
246
/
/
vdivps
%
ymm14
%
ymm1
%
ymm14
.
byte
196
193
76
88
246
/
/
vaddps
%
ymm14
%
ymm6
%
ymm6
.
byte
196
65
124
194
242
0
/
/
vcmpeqps
%
ymm10
%
ymm0
%
ymm14
.
byte
197
204
89
243
/
/
vmulps
%
ymm3
%
ymm6
%
ymm6
.
byte
197
253
91
246
/
/
vcvtps2dq
%
ymm6
%
ymm6
.
byte
196
67
77
74
242
224
/
/
vblendvps
%
ymm14
%
ymm10
%
ymm6
%
ymm14
.
byte
197
124
16
76
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm9
.
byte
197
180
84
244
/
/
vandps
%
ymm4
%
ymm9
%
ymm6
.
byte
197
204
86
242
/
/
vorps
%
ymm2
%
ymm6
%
ymm6
.
byte
196
98
77
188
253
/
/
vfnmadd231ps
%
ymm5
%
ymm6
%
ymm15
.
byte
197
204
88
247
/
/
vaddps
%
ymm7
%
ymm6
%
ymm6
.
byte
197
252
16
68
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm0
.
byte
197
252
94
238
/
/
vdivps
%
ymm6
%
ymm0
%
ymm5
.
byte
197
132
92
237
/
/
vsubps
%
ymm5
%
ymm15
%
ymm5
.
byte
196
193
84
89
237
/
/
vmulps
%
ymm13
%
ymm5
%
ymm5
.
byte
196
227
125
8
245
1
/
/
vroundps
0x1
%
ymm5
%
ymm6
.
byte
197
212
92
246
/
/
vsubps
%
ymm6
%
ymm5
%
ymm6
.
byte
196
193
84
88
227
/
/
vaddps
%
ymm11
%
ymm5
%
ymm4
.
byte
196
194
77
188
228
/
/
vfnmadd231ps
%
ymm12
%
ymm6
%
ymm4
.
byte
197
188
92
214
/
/
vsubps
%
ymm6
%
ymm8
%
ymm2
.
byte
197
244
94
202
/
/
vdivps
%
ymm2
%
ymm1
%
ymm1
.
byte
197
220
88
201
/
/
vaddps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
244
89
195
/
/
vmulps
%
ymm3
%
ymm1
%
ymm0
.
byte
196
193
52
194
202
0
/
/
vcmpeqps
%
ymm10
%
ymm9
%
ymm1
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
195
125
74
242
16
/
/
vblendvps
%
ymm1
%
ymm10
%
ymm0
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
68
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm0
.
byte
197
252
16
76
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm1
.
byte
197
252
16
84
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm2
.
byte
197
252
16
156
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm3
.
byte
197
252
16
100
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm4
.
byte
197
124
41
245
/
/
vmovaps
%
ymm14
%
ymm5
.
byte
197
252
16
188
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm7
.
byte
72
129
196
216
0
0
0
/
/
add
0xd8
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lab_to_xyz_hsw
.
globl
_sk_lab_to_xyz_hsw
FUNCTION
(
_sk_lab_to_xyz_hsw
)
_sk_lab_to_xyz_hsw
:
.
byte
196
98
125
24
5
165
12
3
0
/
/
vbroadcastss
0x30ca5
(
%
rip
)
%
ymm8
#
3c5c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x374
>
.
byte
196
98
125
24
13
4
12
3
0
/
/
vbroadcastss
0x30c04
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
98
125
24
21
151
12
3
0
/
/
vbroadcastss
0x30c97
(
%
rip
)
%
ymm10
#
3c5c4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x378
>
.
byte
196
194
53
168
202
/
/
vfmadd213ps
%
ymm10
%
ymm9
%
ymm1
.
byte
196
194
53
168
210
/
/
vfmadd213ps
%
ymm10
%
ymm9
%
ymm2
.
byte
196
98
125
24
13
136
12
3
0
/
/
vbroadcastss
0x30c88
(
%
rip
)
%
ymm9
#
3c5c8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x37c
>
.
byte
196
66
125
184
200
/
/
vfmadd231ps
%
ymm8
%
ymm0
%
ymm9
.
byte
196
226
125
24
5
126
12
3
0
/
/
vbroadcastss
0x30c7e
(
%
rip
)
%
ymm0
#
3c5cc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x380
>
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
98
125
24
5
117
12
3
0
/
/
vbroadcastss
0x30c75
(
%
rip
)
%
ymm8
#
3c5d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x384
>
.
byte
196
98
117
168
192
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm8
.
byte
196
98
125
24
13
107
12
3
0
/
/
vbroadcastss
0x30c6b
(
%
rip
)
%
ymm9
#
3c5d4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x388
>
.
byte
196
98
109
172
200
/
/
vfnmadd213ps
%
ymm0
%
ymm2
%
ymm9
.
byte
196
193
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm1
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
226
125
24
21
88
12
3
0
/
/
vbroadcastss
0x30c58
(
%
rip
)
%
ymm2
#
3c5d8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x38c
>
.
byte
197
108
194
209
1
/
/
vcmpltps
%
ymm1
%
ymm2
%
ymm10
.
byte
196
98
125
24
29
78
12
3
0
/
/
vbroadcastss
0x30c4e
(
%
rip
)
%
ymm11
#
3c5dc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x390
>
.
byte
196
65
60
88
195
/
/
vaddps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
98
125
24
37
68
12
3
0
/
/
vbroadcastss
0x30c44
(
%
rip
)
%
ymm12
#
3c5e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x394
>
.
byte
196
65
60
89
196
/
/
vmulps
%
ymm12
%
ymm8
%
ymm8
.
byte
196
99
61
74
193
160
/
/
vblendvps
%
ymm10
%
ymm1
%
ymm8
%
ymm8
.
byte
197
252
89
200
/
/
vmulps
%
ymm0
%
ymm0
%
ymm1
.
byte
197
252
89
201
/
/
vmulps
%
ymm1
%
ymm0
%
ymm1
.
byte
197
108
194
209
1
/
/
vcmpltps
%
ymm1
%
ymm2
%
ymm10
.
byte
196
193
124
88
195
/
/
vaddps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
196
/
/
vmulps
%
ymm12
%
ymm0
%
ymm0
.
byte
196
227
125
74
201
160
/
/
vblendvps
%
ymm10
%
ymm1
%
ymm0
%
ymm1
.
byte
196
193
52
89
193
/
/
vmulps
%
ymm9
%
ymm9
%
ymm0
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
236
194
208
1
/
/
vcmpltps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
65
52
88
203
/
/
vaddps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
52
89
204
/
/
vmulps
%
ymm12
%
ymm9
%
ymm9
.
byte
196
227
53
74
208
32
/
/
vblendvps
%
ymm2
%
ymm0
%
ymm9
%
ymm2
.
byte
196
226
125
24
5
249
11
3
0
/
/
vbroadcastss
0x30bf9
(
%
rip
)
%
ymm0
#
3c5e4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x398
>
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
240
11
3
0
/
/
vbroadcastss
0x30bf0
(
%
rip
)
%
ymm8
#
3c5e8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x39c
>
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_hsw
.
globl
_sk_load_a8_hsw
FUNCTION
(
_sk_load_a8_hsw
)
_sk_load_a8_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
52
/
/
jne
ba47
<
_sk_load_a8_hsw
+
0x46
>
.
byte
196
194
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
197
249
219
5
143
20
3
0
/
/
vpand
0x3148f
(
%
rip
)
%
xmm0
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
65
11
3
0
/
/
vbroadcastss
0x30b41
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
217
/
/
vmulps
%
ymm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
196
/
/
ja
ba19
<
_sk_load_a8_hsw
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
117
0
0
0
/
/
lea
0x75
(
%
rip
)
%
r9
#
bad4
<
_sk_load_a8_hsw
+
0xd3
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
235
166
/
/
jmp
ba19
<
_sk_load_a8_hsw
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
235
129
/
/
jmp
ba19
<
_sk_load_a8_hsw
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
233
70
255
255
255
/
/
jmpq
ba19
<
_sk_load_a8_hsw
+
0x18
>
.
byte
144
/
/
nop
.
byte
148
/
/
xchg
%
eax
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
174
255
255
255
159
/
/
ljmp
*
-
0x60000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
233
255
255
255
222
/
/
jmpq
ffffffffdf00bae4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffdefcf898
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
196
/
/
inc
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_a8_dst_hsw
.
globl
_sk_load_a8_dst_hsw
FUNCTION
(
_sk_load_a8_dst_hsw
)
_sk_load_a8_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
52
/
/
jne
bb36
<
_sk_load_a8_dst_hsw
+
0x46
>
.
byte
196
194
121
48
36
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
197
217
219
37
160
19
3
0
/
/
vpand
0x313a0
(
%
rip
)
%
xmm4
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
228
/
/
vpmovzxwd
%
xmm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
82
10
3
0
/
/
vbroadcastss
0x30a52
(
%
rip
)
%
ymm5
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
220
89
253
/
/
vmulps
%
ymm5
%
ymm4
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
196
/
/
ja
bb08
<
_sk_load_a8_dst_hsw
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
118
0
0
0
/
/
lea
0x76
(
%
rip
)
%
r9
#
bbc4
<
_sk_load_a8_dst_hsw
+
0xd4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
235
166
/
/
jmp
bb08
<
_sk_load_a8_dst_hsw
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
2
/
/
vpinsrw
0x2
%
eax
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
232
/
/
vmovd
%
eax
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
235
129
/
/
jmp
bb08
<
_sk_load_a8_dst_hsw
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
6
/
/
vpinsrw
0x6
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
5
/
/
vpinsrw
0x5
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
4
/
/
vpinsrw
0x4
%
eax
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
233
70
255
255
255
/
/
jmpq
bb08
<
_sk_load_a8_dst_hsw
+
0x18
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
147
/
/
xchg
%
eax
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
173
255
255
255
158
/
/
ljmp
*
-
0x61000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
221
/
/
callq
ffffffffde00bbd4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffddfcf988
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_a8_hsw
.
globl
_sk_gather_a8_hsw
FUNCTION
(
_sk_gather_a8_hsw
)
_sk_gather_a8_hsw
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
195
121
32
194
1
/
/
vpinsrb
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
121
32
192
3
/
/
vpinsrb
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
eax
.
byte
196
227
121
32
192
4
/
/
vpinsrb
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
eax
.
byte
196
227
121
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
eax
.
byte
196
227
121
32
192
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
4
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
eax
.
byte
196
227
121
32
192
7
/
/
vpinsrb
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
49
192
/
/
vpmovzxbd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
163
8
3
0
/
/
vbroadcastss
0x308a3
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
217
/
/
vmulps
%
ymm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_a8_hsw
.
globl
_sk_store_a8_hsw
FUNCTION
(
_sk_store_a8_hsw
)
_sk_store_a8_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
241
7
3
0
/
/
vbroadcastss
0x307f1
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
60
93
193
/
/
vminps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
15
8
3
0
/
/
vbroadcastss
0x3080f
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
197
57
103
192
/
/
vpackuswb
%
xmm0
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
bd41
<
_sk_store_a8_hsw
+
0x55
>
.
byte
196
65
121
214
4
16
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
bd3d
<
_sk_store_a8_hsw
+
0x51
>
.
byte
196
66
121
48
192
/
/
vpmovzxbw
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
86
0
0
0
/
/
lea
0x56
(
%
rip
)
%
r9
#
bdb0
<
_sk_store_a8_hsw
+
0xc4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
20
4
16
0
/
/
vpextrb
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
209
/
/
jmp
bd3d
<
_sk_store_a8_hsw
+
0x51
>
.
byte
196
67
121
20
68
16
2
4
/
/
vpextrb
0x4
%
xmm8
0x2
(
%
r8
%
rdx
1
)
.
byte
196
98
57
0
5
67
17
3
0
/
/
vpshufb
0x31143
(
%
rip
)
%
xmm8
%
xmm8
#
3cec0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc74
>
.
byte
196
67
121
21
4
16
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
183
/
/
jmp
bd3d
<
_sk_store_a8_hsw
+
0x51
>
.
byte
196
67
121
20
68
16
6
12
/
/
vpextrb
0xc
%
xmm8
0x6
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
5
10
/
/
vpextrb
0xa
%
xmm8
0x5
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
4
8
/
/
vpextrb
0x8
%
xmm8
0x4
(
%
r8
%
rdx
1
)
.
byte
196
98
57
0
5
41
17
3
0
/
/
vpshufb
0x31129
(
%
rip
)
%
xmm8
%
xmm8
#
3ced0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc84
>
.
byte
196
65
121
126
4
16
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
142
/
/
jmp
bd3d
<
_sk_store_a8_hsw
+
0x51
>
.
byte
144
/
/
nop
.
byte
179
255
/
/
mov
0xff
%
bl
.
byte
255
/
/
(
bad
)
.
byte
255
196
/
/
inc
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
255
255
255
238
/
/
mov
0xeeffffff
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
230
/
/
jmpq
*
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
255
/
/
fdivrp
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
214
/
/
callq
*
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_hsw
.
globl
_sk_load_g8_hsw
FUNCTION
(
_sk_load_g8_hsw
)
_sk_load_g8_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
57
/
/
jne
be17
<
_sk_load_g8_hsw
+
0x4b
>
.
byte
196
194
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
197
249
219
5
196
16
3
0
/
/
vpand
0x310c4
(
%
rip
)
%
xmm0
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
118
7
3
0
/
/
vbroadcastss
0x30776
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
239
6
3
0
/
/
vbroadcastss
0x306ef
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
191
/
/
ja
bde4
<
_sk_load_g8_hsw
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
121
0
0
0
/
/
lea
0x79
(
%
rip
)
%
r9
#
bea8
<
_sk_load_g8_hsw
+
0xdc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
235
161
/
/
jmp
bde4
<
_sk_load_g8_hsw
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
121
255
255
255
/
/
jmpq
bde4
<
_sk_load_g8_hsw
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
233
62
255
255
255
/
/
jmpq
bde4
<
_sk_load_g8_hsw
+
0x18
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
144
/
/
nop
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
170
255
255
255
155
/
/
ljmp
*
-
0x64000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
221
/
/
callq
ffffffffde00beb8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffddfcfc6c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_dst_hsw
.
globl
_sk_load_g8_dst_hsw
FUNCTION
(
_sk_load_g8_dst_hsw
)
_sk_load_g8_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
57
/
/
jne
bf0f
<
_sk_load_g8_dst_hsw
+
0x4b
>
.
byte
196
194
121
48
36
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
197
217
219
37
204
15
3
0
/
/
vpand
0x30fcc
(
%
rip
)
%
xmm4
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
125
51
228
/
/
vpmovzxwd
%
xmm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
126
6
3
0
/
/
vbroadcastss
0x3067e
(
%
rip
)
%
ymm5
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
220
89
229
/
/
vmulps
%
ymm5
%
ymm4
%
ymm4
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
247
5
3
0
/
/
vbroadcastss
0x305f7
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
236
/
/
vmovaps
%
ymm4
%
ymm5
.
byte
197
252
40
244
/
/
vmovaps
%
ymm4
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
191
/
/
ja
bedc
<
_sk_load_g8_dst_hsw
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
121
0
0
0
/
/
lea
0x79
(
%
rip
)
%
r9
#
bfa0
<
_sk_load_g8_dst_hsw
+
0xdc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
235
161
/
/
jmp
bedc
<
_sk_load_g8_dst_hsw
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
2
/
/
vpinsrw
0x2
%
eax
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
232
/
/
vmovd
%
eax
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
121
255
255
255
/
/
jmpq
bedc
<
_sk_load_g8_dst_hsw
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
6
/
/
vpinsrw
0x6
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
5
/
/
vpinsrw
0x5
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
4
/
/
vpinsrw
0x4
%
eax
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
233
62
255
255
255
/
/
jmpq
bedc
<
_sk_load_g8_dst_hsw
+
0x18
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
144
/
/
nop
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
170
255
255
255
155
/
/
ljmp
*
-
0x64000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
221
/
/
callq
ffffffffde00bfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffddfcfd64
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_g8_hsw
.
globl
_sk_gather_g8_hsw
FUNCTION
(
_sk_gather_g8_hsw
)
_sk_gather_g8_hsw
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
195
121
32
194
1
/
/
vpinsrb
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
121
32
192
3
/
/
vpinsrb
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
eax
.
byte
196
227
121
32
192
4
/
/
vpinsrb
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
eax
.
byte
196
227
121
32
192
5
/
/
vpinsrb
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
182
4
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
eax
.
byte
196
227
121
32
192
6
/
/
vpinsrb
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
4
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
eax
.
byte
196
227
121
32
192
7
/
/
vpinsrb
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
49
192
/
/
vpmovzxbd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
199
4
3
0
/
/
vbroadcastss
0x304c7
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
64
4
3
0
/
/
vbroadcastss
0x30440
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_hsw
.
globl
_sk_load_565_hsw
FUNCTION
(
_sk_load_565_hsw
)
_sk_load_565_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
114
/
/
jne
c154
<
_sk_load_565_hsw
+
0x87
>
.
byte
196
193
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
196
226
125
51
208
/
/
vpmovzxwd
%
xmm0
%
ymm2
.
byte
196
226
125
88
5
130
4
3
0
/
/
vpbroadcastd
0x30482
(
%
rip
)
%
ymm0
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
237
219
192
/
/
vpand
%
ymm0
%
ymm2
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
117
4
3
0
/
/
vbroadcastss
0x30475
(
%
rip
)
%
ymm1
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
88
13
108
4
3
0
/
/
vpbroadcastd
0x3046c
(
%
rip
)
%
ymm1
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
237
219
201
/
/
vpand
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
29
95
4
3
0
/
/
vbroadcastss
0x3045f
(
%
rip
)
%
ymm3
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
226
125
88
29
86
4
3
0
/
/
vpbroadcastd
0x30456
(
%
rip
)
%
ymm3
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
237
219
211
/
/
vpand
%
ymm3
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
226
125
24
29
73
4
3
0
/
/
vbroadcastss
0x30449
(
%
rip
)
%
ymm3
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
170
3
3
0
/
/
vbroadcastss
0x303aa
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
134
/
/
ja
c0e8
<
_sk_load_565_hsw
+
0x1b
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
100
0
0
0
/
/
lea
0x64
(
%
rip
)
%
r9
#
c1d0
<
_sk_load_565_hsw
+
0x103
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
101
255
255
255
/
/
jmpq
c0e8
<
_sk_load_565_hsw
+
0x1b
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
72
255
255
255
/
/
jmpq
c0e8
<
_sk_load_565_hsw
+
0x1b
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
233
27
255
255
255
/
/
jmpq
c0e8
<
_sk_load_565_hsw
+
0x1b
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
165
/
/
movsl
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
179
/
/
mov
0xb3ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
236
/
/
in
(
%
dx
)
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
228
/
/
jmpq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_565_dst_hsw
.
globl
_sk_load_565_dst_hsw
FUNCTION
(
_sk_load_565_dst_hsw
)
_sk_load_565_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
114
/
/
jne
c273
<
_sk_load_565_dst_hsw
+
0x87
>
.
byte
196
193
122
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
196
226
125
51
244
/
/
vpmovzxwd
%
xmm4
%
ymm6
.
byte
196
226
125
88
37
99
3
3
0
/
/
vpbroadcastd
0x30363
(
%
rip
)
%
ymm4
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
205
219
228
/
/
vpand
%
ymm4
%
ymm6
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
86
3
3
0
/
/
vbroadcastss
0x30356
(
%
rip
)
%
ymm5
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
197
220
89
229
/
/
vmulps
%
ymm5
%
ymm4
%
ymm4
.
byte
196
226
125
88
45
77
3
3
0
/
/
vpbroadcastd
0x3034d
(
%
rip
)
%
ymm5
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
205
219
237
/
/
vpand
%
ymm5
%
ymm6
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
226
125
24
61
64
3
3
0
/
/
vbroadcastss
0x30340
(
%
rip
)
%
ymm7
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
197
212
89
239
/
/
vmulps
%
ymm7
%
ymm5
%
ymm5
.
byte
196
226
125
88
61
55
3
3
0
/
/
vpbroadcastd
0x30337
(
%
rip
)
%
ymm7
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
205
219
247
/
/
vpand
%
ymm7
%
ymm6
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
226
125
24
61
42
3
3
0
/
/
vbroadcastss
0x3032a
(
%
rip
)
%
ymm7
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
197
204
89
247
/
/
vmulps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
139
2
3
0
/
/
vbroadcastss
0x3028b
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
134
/
/
ja
c207
<
_sk_load_565_dst_hsw
+
0x1b
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
97
0
0
0
/
/
lea
0x61
(
%
rip
)
%
r9
#
c2ec
<
_sk_load_565_dst_hsw
+
0x100
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
101
255
255
255
/
/
jmpq
c207
<
_sk_load_565_dst_hsw
+
0x1b
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
72
255
255
255
/
/
jmpq
c207
<
_sk_load_565_dst_hsw
+
0x1b
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
233
27
255
255
255
/
/
jmpq
c207
<
_sk_load_565_dst_hsw
+
0x1b
>
.
byte
168
255
/
/
test
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
182
255
255
255
239
/
/
pushq
-
0x10000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_565_hsw
.
globl
_sk_gather_565_hsw
FUNCTION
(
_sk_gather_565_hsw
)
_sk_gather_565_hsw
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
193
121
196
194
1
/
/
vpinsrw
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
196
193
121
196
193
2
/
/
vpinsrw
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
3
/
/
vpinsrw
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
eax
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
51
208
/
/
vpmovzxwd
%
xmm0
%
ymm2
.
byte
196
226
125
88
5
136
1
3
0
/
/
vpbroadcastd
0x30188
(
%
rip
)
%
ymm0
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
237
219
192
/
/
vpand
%
ymm0
%
ymm2
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
123
1
3
0
/
/
vbroadcastss
0x3017b
(
%
rip
)
%
ymm1
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
88
13
114
1
3
0
/
/
vpbroadcastd
0x30172
(
%
rip
)
%
ymm1
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
237
219
201
/
/
vpand
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
29
101
1
3
0
/
/
vbroadcastss
0x30165
(
%
rip
)
%
ymm3
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
226
125
88
29
92
1
3
0
/
/
vpbroadcastd
0x3015c
(
%
rip
)
%
ymm3
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
237
219
211
/
/
vpand
%
ymm3
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
226
125
24
29
79
1
3
0
/
/
vbroadcastss
0x3014f
(
%
rip
)
%
ymm3
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
176
0
3
0
/
/
vbroadcastss
0x300b0
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_565_hsw
.
globl
_sk_store_565_hsw
FUNCTION
(
_sk_store_565_hsw
)
_sk_store_565_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
133
0
3
0
/
/
vbroadcastss
0x30085
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
103
1
3
0
/
/
vbroadcastss
0x30167
(
%
rip
)
%
ymm11
#
3c5ec
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a0
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
193
53
114
241
11
/
/
vpslld
0xb
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
98
125
24
45
73
1
3
0
/
/
vbroadcastss
0x30149
(
%
rip
)
%
ymm13
#
3c5f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a4
>
.
byte
196
65
28
89
229
/
/
vmulps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
5
/
/
vpslld
0x5
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
194
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
c4ee
<
_sk_store_565_hsw
+
0x99
>
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
c4ea
<
_sk_store_565_hsw
+
0x95
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
66
0
0
0
/
/
lea
0x42
(
%
rip
)
%
r9
#
c544
<
_sk_store_565_hsw
+
0xef
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
214
/
/
jmp
c4ea
<
_sk_store_565_hsw
+
0x95
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
65
121
126
4
80
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
198
/
/
jmp
c4ea
<
_sk_store_565_hsw
+
0x95
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
65
121
214
4
80
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
166
/
/
jmp
c4ea
<
_sk_store_565_hsw
+
0x95
>
.
byte
199
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
248
/
/
clc
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
240
/
/
push
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
224
/
/
callq
ffffffffe100c55c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffe0fd0310
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_hsw
.
globl
_sk_load_4444_hsw
FUNCTION
(
_sk_load_4444_hsw
)
_sk_load_4444_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
138
0
0
0
/
/
jne
c603
<
_sk_load_4444_hsw
+
0xa3
>
.
byte
196
193
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
196
226
125
51
216
/
/
vpmovzxwd
%
xmm0
%
ymm3
.
byte
196
226
125
88
5
103
0
3
0
/
/
vpbroadcastd
0x30067
(
%
rip
)
%
ymm0
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
229
219
192
/
/
vpand
%
ymm0
%
ymm3
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
90
0
3
0
/
/
vbroadcastss
0x3005a
(
%
rip
)
%
ymm1
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
88
13
81
0
3
0
/
/
vpbroadcastd
0x30051
(
%
rip
)
%
ymm1
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
229
219
201
/
/
vpand
%
ymm1
%
ymm3
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
68
0
3
0
/
/
vbroadcastss
0x30044
(
%
rip
)
%
ymm2
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
197
244
89
202
/
/
vmulps
%
ymm2
%
ymm1
%
ymm1
.
byte
196
226
125
88
21
59
0
3
0
/
/
vpbroadcastd
0x3003b
(
%
rip
)
%
ymm2
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
229
219
210
/
/
vpand
%
ymm2
%
ymm3
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
46
0
3
0
/
/
vbroadcastss
0x3002e
(
%
rip
)
%
ymm8
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
98
125
88
5
36
0
3
0
/
/
vpbroadcastd
0x30024
(
%
rip
)
%
ymm8
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
196
193
101
219
216
/
/
vpand
%
ymm8
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
22
0
3
0
/
/
vbroadcastss
0x30016
(
%
rip
)
%
ymm8
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
106
255
255
255
/
/
ja
c57f
<
_sk_load_4444_hsw
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
97
0
0
0
/
/
lea
0x61
(
%
rip
)
%
r9
#
c680
<
_sk_load_4444_hsw
+
0x120
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
73
255
255
255
/
/
jmpq
c57f
<
_sk_load_4444_hsw
+
0x1f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
44
255
255
255
/
/
jmpq
c57f
<
_sk_load_4444_hsw
+
0x1f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
233
255
254
255
255
/
/
jmpq
c57f
<
_sk_load_4444_hsw
+
0x1f
>
.
byte
168
255
/
/
test
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
182
255
255
255
239
/
/
pushq
-
0x10000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_dst_hsw
.
globl
_sk_load_4444_dst_hsw
FUNCTION
(
_sk_load_4444_dst_hsw
)
_sk_load_4444_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
138
0
0
0
/
/
jne
c73f
<
_sk_load_4444_dst_hsw
+
0xa3
>
.
byte
196
193
122
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
196
226
125
51
252
/
/
vpmovzxwd
%
xmm4
%
ymm7
.
byte
196
226
125
88
37
43
255
2
0
/
/
vpbroadcastd
0x2ff2b
(
%
rip
)
%
ymm4
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
197
219
228
/
/
vpand
%
ymm4
%
ymm7
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
30
255
2
0
/
/
vbroadcastss
0x2ff1e
(
%
rip
)
%
ymm5
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
197
220
89
229
/
/
vmulps
%
ymm5
%
ymm4
%
ymm4
.
byte
196
226
125
88
45
21
255
2
0
/
/
vpbroadcastd
0x2ff15
(
%
rip
)
%
ymm5
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
197
219
237
/
/
vpand
%
ymm5
%
ymm7
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
226
125
24
53
8
255
2
0
/
/
vbroadcastss
0x2ff08
(
%
rip
)
%
ymm6
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
197
212
89
238
/
/
vmulps
%
ymm6
%
ymm5
%
ymm5
.
byte
196
226
125
88
53
255
254
2
0
/
/
vpbroadcastd
0x2feff
(
%
rip
)
%
ymm6
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
197
219
246
/
/
vpand
%
ymm6
%
ymm7
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
98
125
24
5
242
254
2
0
/
/
vbroadcastss
0x2fef2
(
%
rip
)
%
ymm8
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
196
98
125
88
5
232
254
2
0
/
/
vpbroadcastd
0x2fee8
(
%
rip
)
%
ymm8
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
196
193
69
219
248
/
/
vpand
%
ymm8
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
5
218
254
2
0
/
/
vbroadcastss
0x2feda
(
%
rip
)
%
ymm8
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
106
255
255
255
/
/
ja
c6bb
<
_sk_load_4444_dst_hsw
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
97
0
0
0
/
/
lea
0x61
(
%
rip
)
%
r9
#
c7bc
<
_sk_load_4444_dst_hsw
+
0x120
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
73
255
255
255
/
/
jmpq
c6bb
<
_sk_load_4444_dst_hsw
+
0x1f
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
44
255
255
255
/
/
jmpq
c6bb
<
_sk_load_4444_dst_hsw
+
0x1f
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
233
255
254
255
255
/
/
jmpq
c6bb
<
_sk_load_4444_dst_hsw
+
0x1f
>
.
byte
168
255
/
/
test
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
182
255
255
255
239
/
/
pushq
-
0x10000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_4444_hsw
.
globl
_sk_gather_4444_hsw
FUNCTION
(
_sk_gather_4444_hsw
)
_sk_gather_4444_hsw
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
196
193
121
110
195
/
/
vmovd
%
r11d
%
xmm0
.
byte
196
193
121
196
194
1
/
/
vpinsrw
0x1
%
r10d
%
xmm0
%
xmm0
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
196
193
121
196
193
2
/
/
vpinsrw
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
3
/
/
vpinsrw
0x3
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
67
15
183
4
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
eax
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
51
216
/
/
vpmovzxwd
%
xmm0
%
ymm3
.
byte
196
226
125
88
5
52
253
2
0
/
/
vpbroadcastd
0x2fd34
(
%
rip
)
%
ymm0
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
229
219
192
/
/
vpand
%
ymm0
%
ymm3
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
39
253
2
0
/
/
vbroadcastss
0x2fd27
(
%
rip
)
%
ymm1
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
88
13
30
253
2
0
/
/
vpbroadcastd
0x2fd1e
(
%
rip
)
%
ymm1
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
229
219
201
/
/
vpand
%
ymm1
%
ymm3
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
17
253
2
0
/
/
vbroadcastss
0x2fd11
(
%
rip
)
%
ymm2
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
197
244
89
202
/
/
vmulps
%
ymm2
%
ymm1
%
ymm1
.
byte
196
226
125
88
21
8
253
2
0
/
/
vpbroadcastd
0x2fd08
(
%
rip
)
%
ymm2
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
229
219
210
/
/
vpand
%
ymm2
%
ymm3
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
251
252
2
0
/
/
vbroadcastss
0x2fcfb
(
%
rip
)
%
ymm8
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
98
125
88
5
241
252
2
0
/
/
vpbroadcastd
0x2fcf1
(
%
rip
)
%
ymm8
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
196
193
101
219
216
/
/
vpand
%
ymm8
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
227
252
2
0
/
/
vbroadcastss
0x2fce3
(
%
rip
)
%
ymm8
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_4444_hsw
.
globl
_sk_store_4444_hsw
FUNCTION
(
_sk_store_4444_hsw
)
_sk_store_4444_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
157
251
2
0
/
/
vbroadcastss
0x2fb9d
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
167
252
2
0
/
/
vbroadcastss
0x2fca7
(
%
rip
)
%
ymm11
#
3c614
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c8
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
193
53
114
241
12
/
/
vpslld
0xc
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
8
/
/
vpslld
0x8
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
4
/
/
vpslld
0x4
%
ymm12
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
65
29
235
192
/
/
vpor
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
c9eb
<
_sk_store_4444_hsw
+
0xae
>
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
c9e7
<
_sk_store_4444_hsw
+
0xaa
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
69
0
0
0
/
/
lea
0x45
(
%
rip
)
%
r9
#
ca44
<
_sk_store_4444_hsw
+
0x107
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
214
/
/
jmp
c9e7
<
_sk_store_4444_hsw
+
0xaa
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
65
121
126
4
80
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
198
/
/
jmp
c9e7
<
_sk_store_4444_hsw
+
0xaa
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
65
121
214
4
80
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
166
/
/
jmp
c9e7
<
_sk_store_4444_hsw
+
0xaa
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
196
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
245
/
/
push
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
229
/
/
jmpq
*
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_hsw
.
globl
_sk_load_8888_hsw
FUNCTION
(
_sk_load_8888_hsw
)
_sk_load_8888_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
86
/
/
jne
cacc
<
_sk_load_8888_hsw
+
0x6c
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
197
229
219
5
156
253
2
0
/
/
vpand
0x2fd9c
(
%
rip
)
%
ymm3
%
ymm0
#
3c820
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x5d4
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
227
250
2
0
/
/
vbroadcastss
0x2fae3
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
226
101
0
13
161
253
2
0
/
/
vpshufb
0x2fda1
(
%
rip
)
%
ymm3
%
ymm1
#
3c840
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x5f4
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
226
101
0
21
175
253
2
0
/
/
vpshufb
0x2fdaf
(
%
rip
)
%
ymm3
%
ymm2
#
3c860
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x614
>
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
229
114
211
24
/
/
vpsrld
0x18
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
162
/
/
ja
ca7c
<
_sk_load_8888_hsw
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
136
0
0
0
/
/
lea
0x88
(
%
rip
)
%
r9
#
cb6c
<
_sk_load_8888_hsw
+
0x10c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
235
135
/
/
jmp
ca7c
<
_sk_load_8888_hsw
+
0x1c
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
4
/
/
vpblendd
0x4
%
ymm0
%
ymm1
%
ymm3
.
byte
196
193
122
126
4
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
96
255
255
255
/
/
jmpq
ca7c
<
_sk_load_8888_hsw
+
0x1c
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
17
255
255
255
/
/
jmpq
ca7c
<
_sk_load_8888_hsw
+
0x1c
>
.
byte
144
/
/
nop
.
byte
129
255
255
255
159
255
/
/
cmp
0xff9fffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
137
255
255
255
238
/
/
decl
-
0x11000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
176
255
/
/
mov
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_dst_hsw
.
globl
_sk_load_8888_dst_hsw
FUNCTION
(
_sk_load_8888_dst_hsw
)
_sk_load_8888_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
86
/
/
jne
cbf4
<
_sk_load_8888_dst_hsw
+
0x6c
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
197
197
219
37
212
252
2
0
/
/
vpand
0x2fcd4
(
%
rip
)
%
ymm7
%
ymm4
#
3c880
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x634
>
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
187
249
2
0
/
/
vbroadcastss
0x2f9bb
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
196
226
69
0
45
217
252
2
0
/
/
vpshufb
0x2fcd9
(
%
rip
)
%
ymm7
%
ymm5
#
3c8a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x654
>
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
84
89
232
/
/
vmulps
%
ymm8
%
ymm5
%
ymm5
.
byte
196
226
69
0
53
231
252
2
0
/
/
vpshufb
0x2fce7
(
%
rip
)
%
ymm7
%
ymm6
#
3c8c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x674
>
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
162
/
/
ja
cba4
<
_sk_load_8888_dst_hsw
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
136
0
0
0
/
/
lea
0x88
(
%
rip
)
%
r9
#
cc94
<
_sk_load_8888_dst_hsw
+
0x10c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
235
135
/
/
jmp
cba4
<
_sk_load_8888_dst_hsw
+
0x1c
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm7
.
byte
196
193
122
126
36
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
96
255
255
255
/
/
jmpq
cba4
<
_sk_load_8888_dst_hsw
+
0x1c
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
17
255
255
255
/
/
jmpq
cba4
<
_sk_load_8888_dst_hsw
+
0x1c
>
.
byte
144
/
/
nop
.
byte
129
255
255
255
159
255
/
/
cmp
0xff9fffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
137
255
255
255
238
/
/
decl
-
0x11000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
176
255
/
/
mov
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_8888_hsw
.
globl
_sk_gather_8888_hsw
FUNCTION
(
_sk_gather_8888_hsw
)
_sk_gather_8888_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
66
101
144
4
128
/
/
vpgatherdd
%
ymm3
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
197
189
219
5
217
251
2
0
/
/
vpand
0x2fbd9
(
%
rip
)
%
ymm8
%
ymm0
#
3c8e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x694
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
29
96
248
2
0
/
/
vbroadcastss
0x2f860
(
%
rip
)
%
ymm3
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
61
0
13
223
251
2
0
/
/
vpshufb
0x2fbdf
(
%
rip
)
%
ymm8
%
ymm1
#
3c900
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x6b4
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
226
61
0
21
238
251
2
0
/
/
vpshufb
0x2fbee
(
%
rip
)
%
ymm8
%
ymm2
#
3c920
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x6d4
>
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
193
61
114
208
24
/
/
vpsrld
0x18
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_8888_hsw
.
globl
_sk_store_8888_hsw
FUNCTION
(
_sk_store_8888_hsw
)
_sk_store_8888_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
140
247
2
0
/
/
vbroadcastss
0x2f78c
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
170
247
2
0
/
/
vbroadcastss
0x2f7aa
(
%
rip
)
%
ymm11
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
8
/
/
vpslld
0x8
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
16
/
/
vpslld
0x10
%
ymm12
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
29
235
192
/
/
vpor
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
cdf1
<
_sk_store_8888_hsw
+
0xa4
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
cded
<
_sk_store_8888_hsw
+
0xa0
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
83
0
0
0
/
/
lea
0x53
(
%
rip
)
%
r9
#
ce58
<
_sk_store_8888_hsw
+
0x10b
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
cded
<
_sk_store_8888_hsw
+
0xa0
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
199
/
/
jmp
cded
<
_sk_store_8888_hsw
+
0xa0
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
150
/
/
jmp
cded
<
_sk_store_8888_hsw
+
0xa0
>
.
byte
144
/
/
nop
.
byte
182
255
/
/
mov
0xff
%
dh
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
247
/
/
mov
0xf7ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
234
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
206
/
/
dec
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_hsw
.
globl
_sk_load_bgra_hsw
FUNCTION
(
_sk_load_bgra_hsw
)
_sk_load_bgra_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
86
/
/
jne
cee0
<
_sk_load_bgra_hsw
+
0x6c
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
197
229
219
5
168
250
2
0
/
/
vpand
0x2faa8
(
%
rip
)
%
ymm3
%
ymm0
#
3c940
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x6f4
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
207
246
2
0
/
/
vbroadcastss
0x2f6cf
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
208
/
/
vmulps
%
ymm8
%
ymm0
%
ymm2
.
byte
196
226
101
0
5
173
250
2
0
/
/
vpshufb
0x2faad
(
%
rip
)
%
ymm3
%
ymm0
#
3c960
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x714
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
200
/
/
vmulps
%
ymm8
%
ymm0
%
ymm1
.
byte
196
226
101
0
5
187
250
2
0
/
/
vpshufb
0x2fabb
(
%
rip
)
%
ymm3
%
ymm0
#
3c980
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x734
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
229
114
211
24
/
/
vpsrld
0x18
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
162
/
/
ja
ce90
<
_sk_load_bgra_hsw
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
136
0
0
0
/
/
lea
0x88
(
%
rip
)
%
r9
#
cf80
<
_sk_load_bgra_hsw
+
0x10c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
235
135
/
/
jmp
ce90
<
_sk_load_bgra_hsw
+
0x1c
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
4
/
/
vpblendd
0x4
%
ymm0
%
ymm1
%
ymm3
.
byte
196
193
122
126
4
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
96
255
255
255
/
/
jmpq
ce90
<
_sk_load_bgra_hsw
+
0x1c
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
17
255
255
255
/
/
jmpq
ce90
<
_sk_load_bgra_hsw
+
0x1c
>
.
byte
144
/
/
nop
.
byte
129
255
255
255
159
255
/
/
cmp
0xff9fffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
137
255
255
255
238
/
/
decl
-
0x11000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
176
255
/
/
mov
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_dst_hsw
.
globl
_sk_load_bgra_dst_hsw
FUNCTION
(
_sk_load_bgra_dst_hsw
)
_sk_load_bgra_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
86
/
/
jne
d008
<
_sk_load_bgra_dst_hsw
+
0x6c
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
197
197
219
37
224
249
2
0
/
/
vpand
0x2f9e0
(
%
rip
)
%
ymm7
%
ymm4
#
3c9a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x754
>
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
167
245
2
0
/
/
vbroadcastss
0x2f5a7
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
92
89
240
/
/
vmulps
%
ymm8
%
ymm4
%
ymm6
.
byte
196
226
69
0
37
229
249
2
0
/
/
vpshufb
0x2f9e5
(
%
rip
)
%
ymm7
%
ymm4
#
3c9c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x774
>
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
92
89
232
/
/
vmulps
%
ymm8
%
ymm4
%
ymm5
.
byte
196
226
69
0
37
243
249
2
0
/
/
vpshufb
0x2f9f3
(
%
rip
)
%
ymm7
%
ymm4
#
3c9e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x794
>
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
197
197
114
215
24
/
/
vpsrld
0x18
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
162
/
/
ja
cfb8
<
_sk_load_bgra_dst_hsw
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
136
0
0
0
/
/
lea
0x88
(
%
rip
)
%
r9
#
d0a8
<
_sk_load_bgra_dst_hsw
+
0x10c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
235
135
/
/
jmp
cfb8
<
_sk_load_bgra_dst_hsw
+
0x1c
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm7
.
byte
196
193
122
126
36
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
96
255
255
255
/
/
jmpq
cfb8
<
_sk_load_bgra_dst_hsw
+
0x1c
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
17
255
255
255
/
/
jmpq
cfb8
<
_sk_load_bgra_dst_hsw
+
0x1c
>
.
byte
144
/
/
nop
.
byte
129
255
255
255
159
255
/
/
cmp
0xff9fffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
137
255
255
255
238
/
/
decl
-
0x11000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
176
255
/
/
mov
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_bgra_hsw
.
globl
_sk_gather_bgra_hsw
FUNCTION
(
_sk_gather_bgra_hsw
)
_sk_gather_bgra_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
66
101
144
4
128
/
/
vpgatherdd
%
ymm3
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
197
189
219
5
229
248
2
0
/
/
vpand
0x2f8e5
(
%
rip
)
%
ymm8
%
ymm0
#
3ca00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x7b4
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
29
76
244
2
0
/
/
vbroadcastss
0x2f44c
(
%
rip
)
%
ymm3
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
211
/
/
vmulps
%
ymm3
%
ymm0
%
ymm2
.
byte
196
226
61
0
5
235
248
2
0
/
/
vpshufb
0x2f8eb
(
%
rip
)
%
ymm8
%
ymm0
#
3ca20
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x7d4
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
89
203
/
/
vmulps
%
ymm3
%
ymm0
%
ymm1
.
byte
196
226
61
0
5
250
248
2
0
/
/
vpshufb
0x2f8fa
(
%
rip
)
%
ymm8
%
ymm0
#
3ca40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x7f4
>
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
193
61
114
208
24
/
/
vpsrld
0x18
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_bgra_hsw
.
globl
_sk_store_bgra_hsw
FUNCTION
(
_sk_store_bgra_hsw
)
_sk_store_bgra_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
202
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
120
243
2
0
/
/
vbroadcastss
0x2f378
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
150
243
2
0
/
/
vbroadcastss
0x2f396
(
%
rip
)
%
ymm11
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
8
/
/
vpslld
0x8
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
224
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
16
/
/
vpslld
0x10
%
ymm12
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
24
/
/
vpslld
0x18
%
ymm8
%
ymm8
.
byte
196
65
29
235
192
/
/
vpor
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
d205
<
_sk_store_bgra_hsw
+
0xa4
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
d201
<
_sk_store_bgra_hsw
+
0xa0
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
83
0
0
0
/
/
lea
0x53
(
%
rip
)
%
r9
#
d26c
<
_sk_store_bgra_hsw
+
0x10b
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
d201
<
_sk_store_bgra_hsw
+
0xa0
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
199
/
/
jmp
d201
<
_sk_store_bgra_hsw
+
0xa0
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
150
/
/
jmp
d201
<
_sk_store_bgra_hsw
+
0xa0
>
.
byte
144
/
/
nop
.
byte
182
255
/
/
mov
0xff
%
dh
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
247
/
/
mov
0xf7ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
234
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
206
/
/
dec
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_1010102_hsw
.
globl
_sk_load_1010102_hsw
FUNCTION
(
_sk_load_1010102_hsw
)
_sk_load_1010102_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
100
/
/
jne
d302
<
_sk_load_1010102_hsw
+
0x7a
>
.
byte
196
193
126
111
28
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm3
.
byte
196
226
125
88
21
107
243
2
0
/
/
vpbroadcastd
0x2f36b
(
%
rip
)
%
ymm2
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
197
229
219
194
/
/
vpand
%
ymm2
%
ymm3
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
94
243
2
0
/
/
vbroadcastss
0x2f35e
(
%
rip
)
%
ymm8
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
245
114
211
10
/
/
vpsrld
0xa
%
ymm3
%
ymm1
.
byte
197
245
219
202
/
/
vpand
%
ymm2
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
181
114
211
20
/
/
vpsrld
0x14
%
ymm3
%
ymm9
.
byte
197
181
219
210
/
/
vpand
%
ymm2
%
ymm9
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
229
114
211
30
/
/
vpsrld
0x1e
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
111
242
2
0
/
/
vbroadcastss
0x2f26f
(
%
rip
)
%
ymm8
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
148
/
/
ja
d2a4
<
_sk_load_1010102_hsw
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
138
0
0
0
/
/
lea
0x8a
(
%
rip
)
%
r9
#
d3a4
<
_sk_load_1010102_hsw
+
0x11c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
28
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
118
255
255
255
/
/
jmpq
d2a4
<
_sk_load_1010102_hsw
+
0x1c
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
4
/
/
vpblendd
0x4
%
ymm0
%
ymm1
%
ymm3
.
byte
196
193
122
126
4
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
101
2
216
3
/
/
vpblendd
0x3
%
ymm0
%
ymm3
%
ymm3
.
byte
233
79
255
255
255
/
/
jmpq
d2a4
<
_sk_load_1010102_hsw
+
0x1c
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
216
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
227
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
227
101
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
196
193
122
111
4
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
227
125
2
219
240
/
/
vpblendd
0xf0
%
ymm3
%
ymm0
%
ymm3
.
byte
233
0
255
255
255
/
/
jmpq
d2a4
<
_sk_load_1010102_hsw
+
0x1c
>
.
byte
127
255
/
/
jg
d3a5
<
_sk_load_1010102_hsw
+
0x11d
>
.
byte
255
/
/
(
bad
)
.
byte
255
160
255
255
255
138
/
/
jmpq
*
-
0x75000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
177
255
/
/
mov
0xff
%
cl
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_1010102_dst_hsw
.
globl
_sk_load_1010102_dst_hsw
FUNCTION
(
_sk_load_1010102_dst_hsw
)
_sk_load_1010102_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
100
/
/
jne
d43a
<
_sk_load_1010102_dst_hsw
+
0x7a
>
.
byte
196
193
126
111
60
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm7
.
byte
196
226
125
88
53
51
242
2
0
/
/
vpbroadcastd
0x2f233
(
%
rip
)
%
ymm6
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
197
197
219
230
/
/
vpand
%
ymm6
%
ymm7
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
38
242
2
0
/
/
vbroadcastss
0x2f226
(
%
rip
)
%
ymm8
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
197
213
114
215
10
/
/
vpsrld
0xa
%
ymm7
%
ymm5
.
byte
197
213
219
238
/
/
vpand
%
ymm6
%
ymm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
84
89
232
/
/
vmulps
%
ymm8
%
ymm5
%
ymm5
.
byte
197
181
114
215
20
/
/
vpsrld
0x14
%
ymm7
%
ymm9
.
byte
197
181
219
246
/
/
vpand
%
ymm6
%
ymm9
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
197
197
114
215
30
/
/
vpsrld
0x1e
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
5
55
241
2
0
/
/
vbroadcastss
0x2f137
(
%
rip
)
%
ymm8
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
148
/
/
ja
d3dc
<
_sk_load_1010102_dst_hsw
+
0x1c
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
138
0
0
0
/
/
lea
0x8a
(
%
rip
)
%
r9
#
d4dc
<
_sk_load_1010102_dst_hsw
+
0x11c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
60
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
118
255
255
255
/
/
jmpq
d3dc
<
_sk_load_1010102_dst_hsw
+
0x1c
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm7
.
byte
196
193
122
126
36
144
/
/
vmovq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
69
2
252
3
/
/
vpblendd
0x3
%
ymm4
%
ymm7
%
ymm7
.
byte
233
79
255
255
255
/
/
jmpq
d3dc
<
_sk_load_1010102_dst_hsw
+
0x1c
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
252
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
227
125
57
252
1
/
/
vextracti128
0x1
%
ymm7
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
227
69
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm7
%
ymm7
.
byte
196
193
122
111
36
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
227
93
2
255
240
/
/
vpblendd
0xf0
%
ymm7
%
ymm4
%
ymm7
.
byte
233
0
255
255
255
/
/
jmpq
d3dc
<
_sk_load_1010102_dst_hsw
+
0x1c
>
.
byte
127
255
/
/
jg
d4dd
<
_sk_load_1010102_dst_hsw
+
0x11d
>
.
byte
255
/
/
(
bad
)
.
byte
255
160
255
255
255
138
/
/
jmpq
*
-
0x75000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
177
255
/
/
mov
0xff
%
cl
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_1010102_hsw
.
globl
_sk_gather_1010102_hsw
FUNCTION
(
_sk_gather_1010102_hsw
)
_sk_gather_1010102_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
196
66
101
144
4
128
/
/
vpgatherdd
%
ymm3
(
%
r8
%
ymm0
4
)
%
ymm8
.
byte
196
226
125
88
21
200
240
2
0
/
/
vpbroadcastd
0x2f0c8
(
%
rip
)
%
ymm2
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
197
189
219
194
/
/
vpand
%
ymm2
%
ymm8
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
29
187
240
2
0
/
/
vbroadcastss
0x2f0bb
(
%
rip
)
%
ymm3
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
193
117
114
208
10
/
/
vpsrld
0xa
%
ymm8
%
ymm1
.
byte
197
245
219
202
/
/
vpand
%
ymm2
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
193
53
114
208
20
/
/
vpsrld
0x14
%
ymm8
%
ymm9
.
byte
197
181
219
210
/
/
vpand
%
ymm2
%
ymm9
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
193
101
114
208
30
/
/
vpsrld
0x1e
%
ymm8
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
204
239
2
0
/
/
vbroadcastss
0x2efcc
(
%
rip
)
%
ymm8
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_1010102_hsw
.
globl
_sk_store_1010102_hsw
FUNCTION
(
_sk_store_1010102_hsw
)
_sk_store_1010102_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
52
239
2
0
/
/
vbroadcastss
0x2ef34
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
74
240
2
0
/
/
vbroadcastss
0x2f04a
(
%
rip
)
%
ymm11
#
3c620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d4
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
29
114
244
10
/
/
vpslld
0xa
%
ymm12
%
ymm12
.
byte
196
65
29
235
201
/
/
vpor
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
37
114
243
20
/
/
vpslld
0x14
%
ymm11
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
98
125
24
21
251
239
2
0
/
/
vbroadcastss
0x2effb
(
%
rip
)
%
ymm10
#
3c624
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d8
>
.
byte
196
65
60
89
194
/
/
vmulps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
61
114
240
30
/
/
vpslld
0x1e
%
ymm8
%
ymm8
.
byte
196
65
37
235
192
/
/
vpor
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
d652
<
_sk_store_1010102_hsw
+
0xad
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
d64e
<
_sk_store_1010102_hsw
+
0xa9
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
82
0
0
0
/
/
lea
0x52
(
%
rip
)
%
r9
#
d6b8
<
_sk_store_1010102_hsw
+
0x113
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
d64e
<
_sk_store_1010102_hsw
+
0xa9
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
199
/
/
jmp
d64e
<
_sk_store_1010102_hsw
+
0xa9
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
150
/
/
jmp
d64e
<
_sk_store_1010102_hsw
+
0xa9
>
.
byte
183
255
/
/
mov
0xff
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
248
/
/
mov
0xf8ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
235
255
/
/
jmp
d6c9
<
_sk_store_1010102_hsw
+
0x124
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
207
/
/
dec
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_f16_hsw
.
globl
_sk_load_f16_hsw
FUNCTION
(
_sk_load_f16_hsw
)
_sk_load_f16_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
d74f
<
_sk_load_f16_hsw
+
0x7b
>
.
byte
196
65
121
16
4
208
/
/
vmovupd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
193
121
16
84
208
16
/
/
vmovupd
0x10
(
%
r8
%
rdx
8
)
%
xmm2
.
byte
196
193
121
16
92
208
32
/
/
vmovupd
0x20
(
%
r8
%
rdx
8
)
%
xmm3
.
byte
196
65
122
111
76
208
48
/
/
vmovdqu
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
121
105
202
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm9
.
byte
197
241
97
211
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm2
.
byte
197
241
105
219
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm3
.
byte
197
185
108
194
/
/
vpunpcklqdq
%
xmm2
%
xmm8
%
xmm0
.
byte
196
226
125
19
192
/
/
vcvtph2ps
%
xmm0
%
ymm0
.
byte
197
185
109
202
/
/
vpunpckhqdq
%
xmm2
%
xmm8
%
xmm1
.
byte
196
226
125
19
201
/
/
vcvtph2ps
%
xmm1
%
ymm1
.
byte
197
177
108
211
/
/
vpunpcklqdq
%
xmm3
%
xmm9
%
xmm2
.
byte
196
226
125
19
210
/
/
vcvtph2ps
%
xmm2
%
ymm2
.
byte
197
177
109
219
/
/
vpunpckhqdq
%
xmm3
%
xmm9
%
xmm3
.
byte
196
226
125
19
219
/
/
vcvtph2ps
%
xmm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
123
16
4
208
/
/
vmovsd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
d7b5
<
_sk_load_f16_hsw
+
0xe1
>
.
byte
196
65
57
22
68
208
8
/
/
vmovhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
d7b5
<
_sk_load_f16_hsw
+
0xe1
>
.
byte
196
193
123
16
84
208
16
/
/
vmovsd
0x10
(
%
r8
%
rdx
8
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
d7c2
<
_sk_load_f16_hsw
+
0xee
>
.
byte
196
193
105
22
84
208
24
/
/
vmovhpd
0x18
(
%
r8
%
rdx
8
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
d7c2
<
_sk_load_f16_hsw
+
0xee
>
.
byte
196
193
123
16
92
208
32
/
/
vmovsd
0x20
(
%
r8
%
rdx
8
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
109
255
255
255
/
/
je
d705
<
_sk_load_f16_hsw
+
0x31
>
.
byte
196
193
97
22
92
208
40
/
/
vmovhpd
0x28
(
%
r8
%
rdx
8
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
92
255
255
255
/
/
jb
d705
<
_sk_load_f16_hsw
+
0x31
>
.
byte
196
65
122
126
76
208
48
/
/
vmovq
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
233
80
255
255
255
/
/
jmpq
d705
<
_sk_load_f16_hsw
+
0x31
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
67
255
255
255
/
/
jmpq
d705
<
_sk_load_f16_hsw
+
0x31
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
58
255
255
255
/
/
jmpq
d705
<
_sk_load_f16_hsw
+
0x31
>
HIDDEN
_sk_load_f16_dst_hsw
.
globl
_sk_load_f16_dst_hsw
FUNCTION
(
_sk_load_f16_dst_hsw
)
_sk_load_f16_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
d846
<
_sk_load_f16_dst_hsw
+
0x7b
>
.
byte
196
65
121
16
4
208
/
/
vmovupd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
193
121
16
116
208
16
/
/
vmovupd
0x10
(
%
r8
%
rdx
8
)
%
xmm6
.
byte
196
193
121
16
124
208
32
/
/
vmovupd
0x20
(
%
r8
%
rdx
8
)
%
xmm7
.
byte
196
65
122
111
76
208
48
/
/
vmovdqu
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
197
185
97
230
/
/
vpunpcklwd
%
xmm6
%
xmm8
%
xmm4
.
byte
197
185
105
246
/
/
vpunpckhwd
%
xmm6
%
xmm8
%
xmm6
.
byte
196
193
65
97
233
/
/
vpunpcklwd
%
xmm9
%
xmm7
%
xmm5
.
byte
196
193
65
105
249
/
/
vpunpckhwd
%
xmm9
%
xmm7
%
xmm7
.
byte
197
89
97
198
/
/
vpunpcklwd
%
xmm6
%
xmm4
%
xmm8
.
byte
197
89
105
206
/
/
vpunpckhwd
%
xmm6
%
xmm4
%
xmm9
.
byte
197
209
97
247
/
/
vpunpcklwd
%
xmm7
%
xmm5
%
xmm6
.
byte
197
209
105
255
/
/
vpunpckhwd
%
xmm7
%
xmm5
%
xmm7
.
byte
197
185
108
230
/
/
vpunpcklqdq
%
xmm6
%
xmm8
%
xmm4
.
byte
196
226
125
19
228
/
/
vcvtph2ps
%
xmm4
%
ymm4
.
byte
197
185
109
238
/
/
vpunpckhqdq
%
xmm6
%
xmm8
%
xmm5
.
byte
196
226
125
19
237
/
/
vcvtph2ps
%
xmm5
%
ymm5
.
byte
197
177
108
247
/
/
vpunpcklqdq
%
xmm7
%
xmm9
%
xmm6
.
byte
196
226
125
19
246
/
/
vcvtph2ps
%
xmm6
%
ymm6
.
byte
197
177
109
255
/
/
vpunpckhqdq
%
xmm7
%
xmm9
%
xmm7
.
byte
196
226
125
19
255
/
/
vcvtph2ps
%
xmm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
123
16
4
208
/
/
vmovsd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
d8ac
<
_sk_load_f16_dst_hsw
+
0xe1
>
.
byte
196
65
57
22
68
208
8
/
/
vmovhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
d8ac
<
_sk_load_f16_dst_hsw
+
0xe1
>
.
byte
196
193
123
16
116
208
16
/
/
vmovsd
0x10
(
%
r8
%
rdx
8
)
%
xmm6
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
d8b9
<
_sk_load_f16_dst_hsw
+
0xee
>
.
byte
196
193
73
22
116
208
24
/
/
vmovhpd
0x18
(
%
r8
%
rdx
8
)
%
xmm6
%
xmm6
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
d8b9
<
_sk_load_f16_dst_hsw
+
0xee
>
.
byte
196
193
123
16
124
208
32
/
/
vmovsd
0x20
(
%
r8
%
rdx
8
)
%
xmm7
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
109
255
255
255
/
/
je
d7fc
<
_sk_load_f16_dst_hsw
+
0x31
>
.
byte
196
193
65
22
124
208
40
/
/
vmovhpd
0x28
(
%
r8
%
rdx
8
)
%
xmm7
%
xmm7
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
92
255
255
255
/
/
jb
d7fc
<
_sk_load_f16_dst_hsw
+
0x31
>
.
byte
196
65
122
126
76
208
48
/
/
vmovq
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
233
80
255
255
255
/
/
jmpq
d7fc
<
_sk_load_f16_dst_hsw
+
0x31
>
.
byte
197
193
87
255
/
/
vxorpd
%
xmm7
%
xmm7
%
xmm7
.
byte
197
201
87
246
/
/
vxorpd
%
xmm6
%
xmm6
%
xmm6
.
byte
233
67
255
255
255
/
/
jmpq
d7fc
<
_sk_load_f16_dst_hsw
+
0x31
>
.
byte
197
193
87
255
/
/
vxorpd
%
xmm7
%
xmm7
%
xmm7
.
byte
233
58
255
255
255
/
/
jmpq
d7fc
<
_sk_load_f16_dst_hsw
+
0x31
>
HIDDEN
_sk_gather_f16_hsw
.
globl
_sk_gather_f16_hsw
FUNCTION
(
_sk_gather_f16_hsw
)
_sk_gather_f16_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
88
80
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
88
80
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm2
.
byte
197
237
254
211
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
125
88
80
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
109
64
201
/
/
vpmulld
%
ymm1
%
ymm2
%
ymm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
245
254
192
/
/
vpaddd
%
ymm0
%
ymm1
%
ymm0
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
196
194
245
144
20
192
/
/
vpgatherdq
%
ymm1
(
%
r8
%
xmm0
8
)
%
ymm2
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
196
66
229
144
4
192
/
/
vpgatherdq
%
ymm3
(
%
r8
%
xmm0
8
)
%
ymm8
.
byte
196
227
125
57
208
1
/
/
vextracti128
0x1
%
ymm2
%
xmm0
.
byte
196
99
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm1
.
byte
197
233
97
216
/
/
vpunpcklwd
%
xmm0
%
xmm2
%
xmm3
.
byte
197
233
105
192
/
/
vpunpckhwd
%
xmm0
%
xmm2
%
xmm0
.
byte
197
185
97
209
/
/
vpunpcklwd
%
xmm1
%
xmm8
%
xmm2
.
byte
197
185
105
201
/
/
vpunpckhwd
%
xmm1
%
xmm8
%
xmm1
.
byte
197
97
97
192
/
/
vpunpcklwd
%
xmm0
%
xmm3
%
xmm8
.
byte
197
97
105
200
/
/
vpunpckhwd
%
xmm0
%
xmm3
%
xmm9
.
byte
197
233
97
217
/
/
vpunpcklwd
%
xmm1
%
xmm2
%
xmm3
.
byte
197
105
105
209
/
/
vpunpckhwd
%
xmm1
%
xmm2
%
xmm10
.
byte
197
185
108
195
/
/
vpunpcklqdq
%
xmm3
%
xmm8
%
xmm0
.
byte
196
226
125
19
192
/
/
vcvtph2ps
%
xmm0
%
ymm0
.
byte
197
185
109
203
/
/
vpunpckhqdq
%
xmm3
%
xmm8
%
xmm1
.
byte
196
226
125
19
201
/
/
vcvtph2ps
%
xmm1
%
ymm1
.
byte
196
193
49
108
210
/
/
vpunpcklqdq
%
xmm10
%
xmm9
%
xmm2
.
byte
196
226
125
19
210
/
/
vcvtph2ps
%
xmm2
%
ymm2
.
byte
196
193
49
109
218
/
/
vpunpckhqdq
%
xmm10
%
xmm9
%
xmm3
.
byte
196
226
125
19
219
/
/
vcvtph2ps
%
xmm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_f16_hsw
.
globl
_sk_store_f16_hsw
FUNCTION
(
_sk_store_f16_hsw
)
_sk_store_f16_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
195
125
29
192
4
/
/
vcvtps2ph
0x4
%
ymm0
%
xmm8
.
byte
196
195
125
29
201
4
/
/
vcvtps2ph
0x4
%
ymm1
%
xmm9
.
byte
196
195
125
29
210
4
/
/
vcvtps2ph
0x4
%
ymm2
%
xmm10
.
byte
196
195
125
29
219
4
/
/
vcvtps2ph
0x4
%
ymm3
%
xmm11
.
byte
196
65
57
97
225
/
/
vpunpcklwd
%
xmm9
%
xmm8
%
xmm12
.
byte
196
65
57
105
193
/
/
vpunpckhwd
%
xmm9
%
xmm8
%
xmm8
.
byte
196
65
41
97
203
/
/
vpunpcklwd
%
xmm11
%
xmm10
%
xmm9
.
byte
196
65
41
105
235
/
/
vpunpckhwd
%
xmm11
%
xmm10
%
xmm13
.
byte
196
65
25
98
217
/
/
vpunpckldq
%
xmm9
%
xmm12
%
xmm11
.
byte
196
65
25
106
209
/
/
vpunpckhdq
%
xmm9
%
xmm12
%
xmm10
.
byte
196
65
57
98
205
/
/
vpunpckldq
%
xmm13
%
xmm8
%
xmm9
.
byte
196
65
57
106
197
/
/
vpunpckhdq
%
xmm13
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
31
/
/
jne
d9f0
<
_sk_store_f16_hsw
+
0x75
>
.
byte
196
65
122
127
28
208
/
/
vmovdqu
%
xmm11
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
84
208
16
/
/
vmovdqu
%
xmm10
0x10
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
76
208
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
68
208
48
/
/
vmovdqu
%
xmm8
0x30
(
%
r8
%
rdx
8
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
214
28
208
/
/
vmovq
%
xmm11
(
%
r8
%
rdx
8
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
d9ec
<
_sk_store_f16_hsw
+
0x71
>
.
byte
196
65
121
23
92
208
8
/
/
vmovhpd
%
xmm11
0x8
(
%
r8
%
rdx
8
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
d9ec
<
_sk_store_f16_hsw
+
0x71
>
.
byte
196
65
121
214
84
208
16
/
/
vmovq
%
xmm10
0x10
(
%
r8
%
rdx
8
)
.
byte
116
218
/
/
je
d9ec
<
_sk_store_f16_hsw
+
0x71
>
.
byte
196
65
121
23
84
208
24
/
/
vmovhpd
%
xmm10
0x18
(
%
r8
%
rdx
8
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
d9ec
<
_sk_store_f16_hsw
+
0x71
>
.
byte
196
65
121
214
76
208
32
/
/
vmovq
%
xmm9
0x20
(
%
r8
%
rdx
8
)
.
byte
116
196
/
/
je
d9ec
<
_sk_store_f16_hsw
+
0x71
>
.
byte
196
65
121
23
76
208
40
/
/
vmovhpd
%
xmm9
0x28
(
%
r8
%
rdx
8
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
183
/
/
jb
d9ec
<
_sk_store_f16_hsw
+
0x71
>
.
byte
196
65
121
214
68
208
48
/
/
vmovq
%
xmm8
0x30
(
%
r8
%
rdx
8
)
.
byte
235
174
/
/
jmp
d9ec
<
_sk_store_f16_hsw
+
0x71
>
HIDDEN
_sk_load_u16_be_hsw
.
globl
_sk_load_u16_be_hsw
FUNCTION
(
_sk_load_u16_be_hsw
)
_sk_load_u16_be_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
204
0
0
0
/
/
jne
db2b
<
_sk_load_u16_be_hsw
+
0xed
>
.
byte
196
1
121
16
4
65
/
/
vmovupd
(
%
r9
%
r8
2
)
%
xmm8
.
byte
196
129
121
16
84
65
16
/
/
vmovupd
0x10
(
%
r9
%
r8
2
)
%
xmm2
.
byte
196
129
121
16
92
65
32
/
/
vmovupd
0x20
(
%
r9
%
r8
2
)
%
xmm3
.
byte
196
1
122
111
76
65
48
/
/
vmovdqu
0x30
(
%
r9
%
r8
2
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
121
105
202
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm9
.
byte
197
241
97
211
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm2
.
byte
197
113
105
219
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm11
.
byte
197
185
108
194
/
/
vpunpcklqdq
%
xmm2
%
xmm8
%
xmm0
.
byte
197
241
113
240
8
/
/
vpsllw
0x8
%
xmm0
%
xmm1
.
byte
197
249
113
208
8
/
/
vpsrlw
0x8
%
xmm0
%
xmm0
.
byte
197
241
235
192
/
/
vpor
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
208
234
2
0
/
/
vbroadcastss
0x2ead0
(
%
rip
)
%
ymm10
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
185
109
202
/
/
vpunpckhqdq
%
xmm2
%
xmm8
%
xmm1
.
byte
197
233
113
241
8
/
/
vpsllw
0x8
%
xmm1
%
xmm2
.
byte
197
241
113
209
8
/
/
vpsrlw
0x8
%
xmm1
%
xmm1
.
byte
197
233
235
201
/
/
vpor
%
xmm1
%
xmm2
%
xmm1
.
byte
196
226
125
51
201
/
/
vpmovzxwd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
49
108
211
/
/
vpunpcklqdq
%
xmm11
%
xmm9
%
xmm2
.
byte
197
225
113
242
8
/
/
vpsllw
0x8
%
xmm2
%
xmm3
.
byte
197
233
113
210
8
/
/
vpsrlw
0x8
%
xmm2
%
xmm2
.
byte
197
225
235
210
/
/
vpor
%
xmm2
%
xmm3
%
xmm2
.
byte
196
226
125
51
210
/
/
vpmovzxwd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
210
/
/
vmulps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
49
109
219
/
/
vpunpckhqdq
%
xmm11
%
xmm9
%
xmm3
.
byte
197
185
113
243
8
/
/
vpsllw
0x8
%
xmm3
%
xmm8
.
byte
197
225
113
211
8
/
/
vpsrlw
0x8
%
xmm3
%
xmm3
.
byte
197
185
235
219
/
/
vpor
%
xmm3
%
xmm8
%
xmm3
.
byte
196
226
125
51
219
/
/
vpmovzxwd
%
xmm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
123
16
4
65
/
/
vmovsd
(
%
r9
%
r8
2
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
db91
<
_sk_load_u16_be_hsw
+
0x153
>
.
byte
196
1
57
22
68
65
8
/
/
vmovhpd
0x8
(
%
r9
%
r8
2
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
db91
<
_sk_load_u16_be_hsw
+
0x153
>
.
byte
196
129
123
16
84
65
16
/
/
vmovsd
0x10
(
%
r9
%
r8
2
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
db9e
<
_sk_load_u16_be_hsw
+
0x160
>
.
byte
196
129
105
22
84
65
24
/
/
vmovhpd
0x18
(
%
r9
%
r8
2
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
db9e
<
_sk_load_u16_be_hsw
+
0x160
>
.
byte
196
129
123
16
92
65
32
/
/
vmovsd
0x20
(
%
r9
%
r8
2
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
6
255
255
255
/
/
je
da7a
<
_sk_load_u16_be_hsw
+
0x3c
>
.
byte
196
129
97
22
92
65
40
/
/
vmovhpd
0x28
(
%
r9
%
r8
2
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
245
254
255
255
/
/
jb
da7a
<
_sk_load_u16_be_hsw
+
0x3c
>
.
byte
196
1
122
126
76
65
48
/
/
vmovq
0x30
(
%
r9
%
r8
2
)
%
xmm9
.
byte
233
233
254
255
255
/
/
jmpq
da7a
<
_sk_load_u16_be_hsw
+
0x3c
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
220
254
255
255
/
/
jmpq
da7a
<
_sk_load_u16_be_hsw
+
0x3c
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
211
254
255
255
/
/
jmpq
da7a
<
_sk_load_u16_be_hsw
+
0x3c
>
HIDDEN
_sk_load_rgb_u16_be_hsw
.
globl
_sk_load_rgb_u16_be_hsw
FUNCTION
(
_sk_load_rgb_u16_be_hsw
)
_sk_load_rgb_u16_be_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
205
0
0
0
/
/
jne
dc91
<
_sk_load_rgb_u16_be_hsw
+
0xea
>
.
byte
196
1
122
111
28
65
/
/
vmovdqu
(
%
r9
%
r8
2
)
%
xmm11
.
byte
196
129
122
111
92
65
12
/
/
vmovdqu
0xc
(
%
r9
%
r8
2
)
%
xmm3
.
byte
196
129
122
111
84
65
24
/
/
vmovdqu
0x18
(
%
r9
%
r8
2
)
%
xmm2
.
byte
196
129
122
111
68
65
32
/
/
vmovdqu
0x20
(
%
r9
%
r8
2
)
%
xmm0
.
byte
197
249
115
216
4
/
/
vpsrldq
0x4
%
xmm0
%
xmm0
.
byte
196
193
57
115
219
6
/
/
vpsrldq
0x6
%
xmm11
%
xmm8
.
byte
197
169
115
219
6
/
/
vpsrldq
0x6
%
xmm3
%
xmm10
.
byte
197
241
115
218
6
/
/
vpsrldq
0x6
%
xmm2
%
xmm1
.
byte
197
177
115
216
6
/
/
vpsrldq
0x6
%
xmm0
%
xmm9
.
byte
196
193
113
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm1
%
xmm1
.
byte
197
233
97
192
/
/
vpunpcklwd
%
xmm0
%
xmm2
%
xmm0
.
byte
196
193
57
97
210
/
/
vpunpcklwd
%
xmm10
%
xmm8
%
xmm2
.
byte
197
161
97
219
/
/
vpunpcklwd
%
xmm3
%
xmm11
%
xmm3
.
byte
197
97
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm3
%
xmm8
.
byte
197
225
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm3
%
xmm2
.
byte
197
249
97
217
/
/
vpunpcklwd
%
xmm1
%
xmm0
%
xmm3
.
byte
197
249
105
193
/
/
vpunpckhwd
%
xmm1
%
xmm0
%
xmm0
.
byte
197
233
108
208
/
/
vpunpcklqdq
%
xmm0
%
xmm2
%
xmm2
.
byte
197
185
108
195
/
/
vpunpcklqdq
%
xmm3
%
xmm8
%
xmm0
.
byte
197
241
113
240
8
/
/
vpsllw
0x8
%
xmm0
%
xmm1
.
byte
197
249
113
208
8
/
/
vpsrlw
0x8
%
xmm0
%
xmm0
.
byte
197
241
235
192
/
/
vpor
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
125
51
192
/
/
vpmovzxwd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
77
233
2
0
/
/
vbroadcastss
0x2e94d
(
%
rip
)
%
ymm9
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
185
109
203
/
/
vpunpckhqdq
%
xmm3
%
xmm8
%
xmm1
.
byte
197
225
113
241
8
/
/
vpsllw
0x8
%
xmm1
%
xmm3
.
byte
197
241
113
209
8
/
/
vpsrlw
0x8
%
xmm1
%
xmm1
.
byte
197
225
235
201
/
/
vpor
%
xmm1
%
xmm3
%
xmm1
.
byte
196
226
125
51
201
/
/
vpmovzxwd
%
xmm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
225
113
242
8
/
/
vpsllw
0x8
%
xmm2
%
xmm3
.
byte
197
233
113
210
8
/
/
vpsrlw
0x8
%
xmm2
%
xmm2
.
byte
197
225
235
210
/
/
vpor
%
xmm2
%
xmm3
%
xmm2
.
byte
196
226
125
51
210
/
/
vpmovzxwd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
109
232
2
0
/
/
vbroadcastss
0x2e86d
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
110
4
65
/
/
vmovd
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
92
65
4
2
/
/
vpinsrw
0x2
0x4
(
%
r9
%
r8
2
)
%
xmm0
%
xmm11
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
31
/
/
jne
dcc9
<
_sk_load_rgb_u16_be_hsw
+
0x122
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
233
48
255
255
255
/
/
jmpq
dbf9
<
_sk_load_rgb_u16_be_hsw
+
0x52
>
.
byte
196
129
121
110
68
65
6
/
/
vmovd
0x6
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
68
65
10
2
/
/
vpinsrw
0x2
0xa
(
%
r9
%
r8
2
)
%
xmm0
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
48
/
/
jb
dd13
<
_sk_load_rgb_u16_be_hsw
+
0x16c
>
.
byte
196
129
121
110
68
65
12
/
/
vmovd
0xc
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
92
65
16
2
/
/
vpinsrw
0x2
0x10
(
%
r9
%
r8
2
)
%
xmm0
%
xmm3
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
117
48
/
/
jne
dd2d
<
_sk_load_rgb_u16_be_hsw
+
0x186
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
233
230
254
255
255
/
/
jmpq
dbf9
<
_sk_load_rgb_u16_be_hsw
+
0x52
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
233
204
254
255
255
/
/
jmpq
dbf9
<
_sk_load_rgb_u16_be_hsw
+
0x52
>
.
byte
196
129
121
110
68
65
18
/
/
vmovd
0x12
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
84
65
22
2
/
/
vpinsrw
0x2
0x16
(
%
r9
%
r8
2
)
%
xmm0
%
xmm10
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
39
/
/
jb
dd6e
<
_sk_load_rgb_u16_be_hsw
+
0x1c7
>
.
byte
196
129
121
110
68
65
24
/
/
vmovd
0x18
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
84
65
28
2
/
/
vpinsrw
0x2
0x1c
(
%
r9
%
r8
2
)
%
xmm0
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
117
30
/
/
jne
dd7f
<
_sk_load_rgb_u16_be_hsw
+
0x1d8
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
233
139
254
255
255
/
/
jmpq
dbf9
<
_sk_load_rgb_u16_be_hsw
+
0x52
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
233
122
254
255
255
/
/
jmpq
dbf9
<
_sk_load_rgb_u16_be_hsw
+
0x52
>
.
byte
196
129
121
110
68
65
30
/
/
vmovd
0x1e
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
76
65
34
2
/
/
vpinsrw
0x2
0x22
(
%
r9
%
r8
2
)
%
xmm0
%
xmm1
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
25
/
/
jb
ddb2
<
_sk_load_rgb_u16_be_hsw
+
0x20b
>
.
byte
196
129
121
110
68
65
36
/
/
vmovd
0x24
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
68
65
40
2
/
/
vpinsrw
0x2
0x28
(
%
r9
%
r8
2
)
%
xmm0
%
xmm0
.
byte
233
71
254
255
255
/
/
jmpq
dbf9
<
_sk_load_rgb_u16_be_hsw
+
0x52
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
233
62
254
255
255
/
/
jmpq
dbf9
<
_sk_load_rgb_u16_be_hsw
+
0x52
>
HIDDEN
_sk_store_u16_be_hsw
.
globl
_sk_store_u16_be_hsw
FUNCTION
(
_sk_store_u16_be_hsw
)
_sk_store_u16_be_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
23
231
2
0
/
/
vbroadcastss
0x2e717
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
53
232
2
0
/
/
vbroadcastss
0x2e835
(
%
rip
)
%
ymm11
#
3c628
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
67
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm12
.
byte
196
66
49
43
204
/
/
vpackusdw
%
xmm12
%
xmm9
%
xmm9
.
byte
196
193
25
113
241
8
/
/
vpsllw
0x8
%
xmm9
%
xmm12
.
byte
196
193
49
113
209
8
/
/
vpsrlw
0x8
%
xmm9
%
xmm9
.
byte
196
65
25
235
201
/
/
vpor
%
xmm9
%
xmm12
%
xmm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
67
125
25
229
1
/
/
vextractf128
0x1
%
ymm12
%
xmm13
.
byte
196
66
25
43
229
/
/
vpackusdw
%
xmm13
%
xmm12
%
xmm12
.
byte
196
193
17
113
244
8
/
/
vpsllw
0x8
%
xmm12
%
xmm13
.
byte
196
193
25
113
212
8
/
/
vpsrlw
0x8
%
xmm12
%
xmm12
.
byte
196
65
17
235
228
/
/
vpor
%
xmm12
%
xmm13
%
xmm12
.
byte
197
60
95
234
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm13
.
byte
196
65
20
93
234
/
/
vminps
%
ymm10
%
ymm13
%
ymm13
.
byte
196
65
20
89
235
/
/
vmulps
%
ymm11
%
ymm13
%
ymm13
.
byte
196
65
125
91
237
/
/
vcvtps2dq
%
ymm13
%
ymm13
.
byte
196
67
125
25
238
1
/
/
vextractf128
0x1
%
ymm13
%
xmm14
.
byte
196
66
17
43
238
/
/
vpackusdw
%
xmm14
%
xmm13
%
xmm13
.
byte
196
193
9
113
245
8
/
/
vpsllw
0x8
%
xmm13
%
xmm14
.
byte
196
193
17
113
213
8
/
/
vpsrlw
0x8
%
xmm13
%
xmm13
.
byte
196
65
9
235
237
/
/
vpor
%
xmm13
%
xmm14
%
xmm13
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
67
125
25
194
1
/
/
vextractf128
0x1
%
ymm8
%
xmm10
.
byte
196
66
57
43
194
/
/
vpackusdw
%
xmm10
%
xmm8
%
xmm8
.
byte
196
193
41
113
240
8
/
/
vpsllw
0x8
%
xmm8
%
xmm10
.
byte
196
193
57
113
208
8
/
/
vpsrlw
0x8
%
xmm8
%
xmm8
.
byte
196
65
41
235
192
/
/
vpor
%
xmm8
%
xmm10
%
xmm8
.
byte
196
65
49
97
212
/
/
vpunpcklwd
%
xmm12
%
xmm9
%
xmm10
.
byte
196
65
49
105
228
/
/
vpunpckhwd
%
xmm12
%
xmm9
%
xmm12
.
byte
196
65
17
97
200
/
/
vpunpcklwd
%
xmm8
%
xmm13
%
xmm9
.
byte
196
65
17
105
192
/
/
vpunpckhwd
%
xmm8
%
xmm13
%
xmm8
.
byte
196
65
41
98
217
/
/
vpunpckldq
%
xmm9
%
xmm10
%
xmm11
.
byte
196
65
41
106
209
/
/
vpunpckhdq
%
xmm9
%
xmm10
%
xmm10
.
byte
196
65
25
98
200
/
/
vpunpckldq
%
xmm8
%
xmm12
%
xmm9
.
byte
196
65
25
106
192
/
/
vpunpckhdq
%
xmm8
%
xmm12
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
31
/
/
jne
def2
<
_sk_store_u16_be_hsw
+
0x137
>
.
byte
196
1
122
127
28
65
/
/
vmovdqu
%
xmm11
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
84
65
16
/
/
vmovdqu
%
xmm10
0x10
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
76
65
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
68
65
48
/
/
vmovdqu
%
xmm8
0x30
(
%
r9
%
r8
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
121
214
28
65
/
/
vmovq
%
xmm11
(
%
r9
%
r8
2
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
deee
<
_sk_store_u16_be_hsw
+
0x133
>
.
byte
196
1
121
23
92
65
8
/
/
vmovhpd
%
xmm11
0x8
(
%
r9
%
r8
2
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
deee
<
_sk_store_u16_be_hsw
+
0x133
>
.
byte
196
1
121
214
84
65
16
/
/
vmovq
%
xmm10
0x10
(
%
r9
%
r8
2
)
.
byte
116
218
/
/
je
deee
<
_sk_store_u16_be_hsw
+
0x133
>
.
byte
196
1
121
23
84
65
24
/
/
vmovhpd
%
xmm10
0x18
(
%
r9
%
r8
2
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
deee
<
_sk_store_u16_be_hsw
+
0x133
>
.
byte
196
1
121
214
76
65
32
/
/
vmovq
%
xmm9
0x20
(
%
r9
%
r8
2
)
.
byte
116
196
/
/
je
deee
<
_sk_store_u16_be_hsw
+
0x133
>
.
byte
196
1
121
23
76
65
40
/
/
vmovhpd
%
xmm9
0x28
(
%
r9
%
r8
2
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
183
/
/
jb
deee
<
_sk_store_u16_be_hsw
+
0x133
>
.
byte
196
1
121
214
68
65
48
/
/
vmovq
%
xmm8
0x30
(
%
r9
%
r8
2
)
.
byte
235
174
/
/
jmp
deee
<
_sk_store_u16_be_hsw
+
0x133
>
HIDDEN
_sk_load_f32_hsw
.
globl
_sk_load_f32_hsw
FUNCTION
(
_sk_load_f32_hsw
)
_sk_load_f32_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
253
87
192
/
/
vxorpd
%
ymm0
%
ymm0
%
ymm0
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
135
91
1
0
0
/
/
ja
e0ab
<
_sk_load_f32_hsw
+
0x16b
>
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
197
253
17
100
36
128
/
/
vmovupd
%
ymm4
-
0x80
(
%
rsp
)
.
byte
197
253
17
108
36
160
/
/
vmovupd
%
ymm5
-
0x60
(
%
rsp
)
.
byte
197
253
17
116
36
192
/
/
vmovupd
%
ymm6
-
0x40
(
%
rsp
)
.
byte
197
253
17
124
36
224
/
/
vmovupd
%
ymm7
-
0x20
(
%
rsp
)
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
76
141
21
50
1
0
0
/
/
lea
0x132
(
%
rip
)
%
r10
#
e0bc
<
_sk_load_f32_hsw
+
0x17c
>
.
byte
73
99
4
186
/
/
movslq
(
%
r10
%
rdi
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
197
221
87
228
/
/
vxorpd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
237
87
210
/
/
vxorpd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
37
87
219
/
/
vxorpd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
29
87
228
/
/
vxorpd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
13
87
246
/
/
vxorpd
%
ymm14
%
ymm14
%
ymm14
.
byte
197
205
87
246
/
/
vxorpd
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
5
87
255
/
/
vxorpd
%
ymm15
%
ymm15
%
ymm15
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
21
87
237
/
/
vxorpd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
16
68
129
112
/
/
vmovupd
0x70
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
6
192
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm0
.
byte
196
129
121
16
76
129
96
/
/
vmovupd
0x60
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
225
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm4
.
byte
196
129
121
16
76
129
80
/
/
vmovupd
0x50
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
209
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm2
.
byte
197
125
40
204
/
/
vmovapd
%
ymm4
%
ymm9
.
byte
196
129
121
16
76
129
64
/
/
vmovupd
0x40
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
99
125
6
193
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm8
.
byte
196
65
125
40
217
/
/
vmovapd
%
ymm9
%
ymm11
.
byte
197
125
40
226
/
/
vmovapd
%
ymm2
%
ymm12
.
byte
196
129
121
16
76
129
48
/
/
vmovupd
0x30
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
117
13
192
12
/
/
vblendpd
0xc
%
ymm0
%
ymm1
%
ymm0
.
byte
196
65
125
40
243
/
/
vmovapd
%
ymm11
%
ymm14
.
byte
197
125
41
230
/
/
vmovapd
%
ymm12
%
ymm6
.
byte
196
65
125
40
248
/
/
vmovapd
%
ymm8
%
ymm15
.
byte
196
129
121
16
76
129
32
/
/
vmovupd
0x20
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
195
117
13
206
12
/
/
vblendpd
0xc
%
ymm14
%
ymm1
%
ymm1
.
byte
197
125
40
238
/
/
vmovapd
%
ymm6
%
ymm13
.
byte
197
125
41
253
/
/
vmovapd
%
ymm15
%
ymm5
.
byte
196
129
121
16
84
129
16
/
/
vmovupd
0x10
(
%
r9
%
r8
4
)
%
xmm2
.
byte
196
67
109
13
213
12
/
/
vblendpd
0xc
%
ymm13
%
ymm2
%
ymm10
.
byte
197
253
40
217
/
/
vmovapd
%
ymm1
%
ymm3
.
byte
197
253
40
253
/
/
vmovapd
%
ymm5
%
ymm7
.
byte
196
129
121
16
12
129
/
/
vmovupd
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
117
13
207
12
/
/
vblendpd
0xc
%
ymm7
%
ymm1
%
ymm1
.
byte
197
252
16
124
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm7
.
byte
197
252
16
116
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
16
108
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm5
.
byte
197
252
16
100
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm4
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
196
193
116
20
210
/
/
vunpcklps
%
ymm10
%
ymm1
%
ymm2
.
byte
196
65
116
21
194
/
/
vunpckhps
%
ymm10
%
ymm1
%
ymm8
.
byte
197
228
20
200
/
/
vunpcklps
%
ymm0
%
ymm3
%
ymm1
.
byte
197
228
21
216
/
/
vunpckhps
%
ymm0
%
ymm3
%
ymm3
.
byte
197
237
20
193
/
/
vunpcklpd
%
ymm1
%
ymm2
%
ymm0
.
byte
197
237
21
201
/
/
vunpckhpd
%
ymm1
%
ymm2
%
ymm1
.
byte
197
189
20
211
/
/
vunpcklpd
%
ymm3
%
ymm8
%
ymm2
.
byte
197
189
21
219
/
/
vunpckhpd
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
235
203
/
/
jmp
e085
<
_sk_load_f32_hsw
+
0x145
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
27
255
/
/
sbb
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
161
255
255
255
140
/
/
jmpq
*
-
0x73000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
119
255
/
/
pushq
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
70
255
/
/
incl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
53
255
255
255
40
/
/
pushq
0x28ffffff
(
%
rip
)
#
2900e0d8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x28fd1e8c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_f32_dst_hsw
.
globl
_sk_load_f32_dst_hsw
FUNCTION
(
_sk_load_f32_dst_hsw
)
_sk_load_f32_dst_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
221
87
228
/
/
vxorpd
%
ymm4
%
ymm4
%
ymm4
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
135
91
1
0
0
/
/
ja
e247
<
_sk_load_f32_dst_hsw
+
0x16b
>
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
197
253
17
68
36
128
/
/
vmovupd
%
ymm0
-
0x80
(
%
rsp
)
.
byte
197
253
17
76
36
160
/
/
vmovupd
%
ymm1
-
0x60
(
%
rsp
)
.
byte
197
253
17
84
36
192
/
/
vmovupd
%
ymm2
-
0x40
(
%
rsp
)
.
byte
197
253
17
92
36
224
/
/
vmovupd
%
ymm3
-
0x20
(
%
rsp
)
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
76
141
21
50
1
0
0
/
/
lea
0x132
(
%
rip
)
%
r10
#
e258
<
_sk_load_f32_dst_hsw
+
0x17c
>
.
byte
73
99
4
186
/
/
movslq
(
%
r10
%
rdi
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
197
253
87
192
/
/
vxorpd
%
ymm0
%
ymm0
%
ymm0
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
205
87
246
/
/
vxorpd
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
37
87
219
/
/
vxorpd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
29
87
228
/
/
vxorpd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
13
87
246
/
/
vxorpd
%
ymm14
%
ymm14
%
ymm14
.
byte
197
237
87
210
/
/
vxorpd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
5
87
255
/
/
vxorpd
%
ymm15
%
ymm15
%
ymm15
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
65
21
87
237
/
/
vxorpd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
16
68
129
112
/
/
vmovupd
0x70
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
6
224
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm4
.
byte
196
129
121
16
68
129
96
/
/
vmovupd
0x60
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
6
192
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm0
.
byte
196
129
121
16
76
129
80
/
/
vmovupd
0x50
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
241
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm6
.
byte
197
125
40
200
/
/
vmovapd
%
ymm0
%
ymm9
.
byte
196
129
121
16
68
129
64
/
/
vmovupd
0x40
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
99
125
6
192
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
125
40
217
/
/
vmovapd
%
ymm9
%
ymm11
.
byte
197
125
40
230
/
/
vmovapd
%
ymm6
%
ymm12
.
byte
196
129
121
16
68
129
48
/
/
vmovupd
0x30
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
13
228
12
/
/
vblendpd
0xc
%
ymm4
%
ymm0
%
ymm4
.
byte
196
65
125
40
243
/
/
vmovapd
%
ymm11
%
ymm14
.
byte
197
125
41
226
/
/
vmovapd
%
ymm12
%
ymm2
.
byte
196
65
125
40
248
/
/
vmovapd
%
ymm8
%
ymm15
.
byte
196
129
121
16
68
129
32
/
/
vmovupd
0x20
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
195
125
13
238
12
/
/
vblendpd
0xc
%
ymm14
%
ymm0
%
ymm5
.
byte
197
125
40
234
/
/
vmovapd
%
ymm2
%
ymm13
.
byte
197
125
41
249
/
/
vmovapd
%
ymm15
%
ymm1
.
byte
196
129
121
16
68
129
16
/
/
vmovupd
0x10
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
67
125
13
213
12
/
/
vblendpd
0xc
%
ymm13
%
ymm0
%
ymm10
.
byte
197
253
40
253
/
/
vmovapd
%
ymm5
%
ymm7
.
byte
197
253
40
217
/
/
vmovapd
%
ymm1
%
ymm3
.
byte
196
129
121
16
4
129
/
/
vmovupd
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
13
235
12
/
/
vblendpd
0xc
%
ymm3
%
ymm0
%
ymm5
.
byte
197
252
16
92
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm3
.
byte
197
252
16
84
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm2
.
byte
197
252
16
76
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm1
.
byte
197
252
16
68
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm0
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
196
193
84
20
242
/
/
vunpcklps
%
ymm10
%
ymm5
%
ymm6
.
byte
196
65
84
21
194
/
/
vunpckhps
%
ymm10
%
ymm5
%
ymm8
.
byte
197
196
20
236
/
/
vunpcklps
%
ymm4
%
ymm7
%
ymm5
.
byte
197
196
21
252
/
/
vunpckhps
%
ymm4
%
ymm7
%
ymm7
.
byte
197
205
20
229
/
/
vunpcklpd
%
ymm5
%
ymm6
%
ymm4
.
byte
197
205
21
237
/
/
vunpckhpd
%
ymm5
%
ymm6
%
ymm5
.
byte
197
189
20
247
/
/
vunpcklpd
%
ymm7
%
ymm8
%
ymm6
.
byte
197
189
21
255
/
/
vunpckhpd
%
ymm7
%
ymm8
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
235
203
/
/
jmp
e221
<
_sk_load_f32_dst_hsw
+
0x145
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
27
255
/
/
sbb
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
161
255
255
255
140
/
/
jmpq
*
-
0x73000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
119
255
/
/
pushq
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
70
255
/
/
incl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
53
255
255
255
40
/
/
pushq
0x28ffffff
(
%
rip
)
#
2900e274
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x28fd2028
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_f32_hsw
.
globl
_sk_store_f32_hsw
FUNCTION
(
_sk_store_f32_hsw
)
_sk_store_f32_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
197
124
20
193
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm8
.
byte
197
124
21
217
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm11
.
byte
197
108
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm9
.
byte
197
108
21
227
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm12
.
byte
196
65
61
20
209
/
/
vunpcklpd
%
ymm9
%
ymm8
%
ymm10
.
byte
196
65
61
21
201
/
/
vunpckhpd
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
37
20
196
/
/
vunpcklpd
%
ymm12
%
ymm11
%
ymm8
.
byte
196
65
37
21
220
/
/
vunpckhpd
%
ymm12
%
ymm11
%
ymm11
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
55
/
/
jne
e2f1
<
_sk_store_f32_hsw
+
0x79
>
.
byte
196
67
45
24
225
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm12
.
byte
196
67
61
24
235
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm8
%
ymm13
.
byte
196
67
45
6
201
49
/
/
vperm2f128
0x31
%
ymm9
%
ymm10
%
ymm9
.
byte
196
67
61
6
195
49
/
/
vperm2f128
0x31
%
ymm11
%
ymm8
%
ymm8
.
byte
196
1
125
17
36
129
/
/
vmovupd
%
ymm12
(
%
r9
%
r8
4
)
.
byte
196
1
125
17
108
129
32
/
/
vmovupd
%
ymm13
0x20
(
%
r9
%
r8
4
)
.
byte
196
1
125
17
76
129
64
/
/
vmovupd
%
ymm9
0x40
(
%
r9
%
r8
4
)
.
byte
196
1
125
17
68
129
96
/
/
vmovupd
%
ymm8
0x60
(
%
r9
%
r8
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
121
17
20
129
/
/
vmovupd
%
xmm10
(
%
r9
%
r8
4
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
e2ed
<
_sk_store_f32_hsw
+
0x75
>
.
byte
196
1
121
17
76
129
16
/
/
vmovupd
%
xmm9
0x10
(
%
r9
%
r8
4
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
e2ed
<
_sk_store_f32_hsw
+
0x75
>
.
byte
196
1
121
17
68
129
32
/
/
vmovupd
%
xmm8
0x20
(
%
r9
%
r8
4
)
.
byte
116
218
/
/
je
e2ed
<
_sk_store_f32_hsw
+
0x75
>
.
byte
196
1
121
17
92
129
48
/
/
vmovupd
%
xmm11
0x30
(
%
r9
%
r8
4
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
e2ed
<
_sk_store_f32_hsw
+
0x75
>
.
byte
196
3
125
25
84
129
64
1
/
/
vextractf128
0x1
%
ymm10
0x40
(
%
r9
%
r8
4
)
.
byte
116
195
/
/
je
e2ed
<
_sk_store_f32_hsw
+
0x75
>
.
byte
196
3
125
25
76
129
80
1
/
/
vextractf128
0x1
%
ymm9
0x50
(
%
r9
%
r8
4
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
181
/
/
jb
e2ed
<
_sk_store_f32_hsw
+
0x75
>
.
byte
196
3
125
25
68
129
96
1
/
/
vextractf128
0x1
%
ymm8
0x60
(
%
r9
%
r8
4
)
.
byte
235
171
/
/
jmp
e2ed
<
_sk_store_f32_hsw
+
0x75
>
HIDDEN
_sk_repeat_x_hsw
.
globl
_sk_repeat_x_hsw
FUNCTION
(
_sk_repeat_x_hsw
)
_sk_repeat_x_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
197
60
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
67
125
8
192
1
/
/
vroundps
0x1
%
ymm8
%
ymm8
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
194
61
188
193
/
/
vfnmadd231ps
%
ymm9
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_y_hsw
.
globl
_sk_repeat_y_hsw
FUNCTION
(
_sk_repeat_y_hsw
)
_sk_repeat_y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
197
60
89
193
/
/
vmulps
%
ymm1
%
ymm8
%
ymm8
.
byte
196
67
125
8
192
1
/
/
vroundps
0x1
%
ymm8
%
ymm8
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
194
61
188
201
/
/
vfnmadd231ps
%
ymm9
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_hsw
.
globl
_sk_mirror_x_hsw
FUNCTION
(
_sk_mirror_x_hsw
)
_sk_mirror_x_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
72
4
/
/
vmovss
0x4
(
%
rax
)
%
xmm9
.
byte
196
66
125
24
208
/
/
vbroadcastss
%
xmm8
%
ymm10
.
byte
196
65
124
92
218
/
/
vsubps
%
ymm10
%
ymm0
%
ymm11
.
byte
196
193
58
88
192
/
/
vaddss
%
xmm8
%
xmm8
%
xmm0
.
byte
196
98
125
24
192
/
/
vbroadcastss
%
xmm0
%
ymm8
.
byte
197
178
89
5
79
225
2
0
/
/
vmulss
0x2e14f
(
%
rip
)
%
xmm9
%
xmm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
226
125
24
192
/
/
vbroadcastss
%
xmm0
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
196
227
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm0
.
byte
196
194
61
172
195
/
/
vfnmadd213ps
%
ymm11
%
ymm8
%
ymm0
.
byte
196
193
124
92
194
/
/
vsubps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_y_hsw
.
globl
_sk_mirror_y_hsw
FUNCTION
(
_sk_mirror_y_hsw
)
_sk_mirror_y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
72
4
/
/
vmovss
0x4
(
%
rax
)
%
xmm9
.
byte
196
66
125
24
208
/
/
vbroadcastss
%
xmm8
%
ymm10
.
byte
196
65
116
92
218
/
/
vsubps
%
ymm10
%
ymm1
%
ymm11
.
byte
196
193
58
88
200
/
/
vaddss
%
xmm8
%
xmm8
%
xmm1
.
byte
196
98
125
24
193
/
/
vbroadcastss
%
xmm1
%
ymm8
.
byte
197
178
89
13
254
224
2
0
/
/
vmulss
0x2e0fe
(
%
rip
)
%
xmm9
%
xmm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
226
125
24
201
/
/
vbroadcastss
%
xmm1
%
ymm1
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
196
227
125
8
201
1
/
/
vroundps
0x1
%
ymm1
%
ymm1
.
byte
196
194
61
172
203
/
/
vfnmadd213ps
%
ymm11
%
ymm8
%
ymm1
.
byte
196
193
116
92
202
/
/
vsubps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
193
/
/
vsubps
%
ymm1
%
ymm8
%
ymm8
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_x_1_hsw
.
globl
_sk_clamp_x_1_hsw
FUNCTION
(
_sk_clamp_x_1_hsw
)
_sk_clamp_x_1_hsw
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
198
224
2
0
/
/
vbroadcastss
0x2e0c6
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_hsw
.
globl
_sk_repeat_x_1_hsw
FUNCTION
(
_sk_repeat_x_1_hsw
)
_sk_repeat_x_1_hsw
:
.
byte
196
99
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
160
224
2
0
/
/
vbroadcastss
0x2e0a0
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_hsw
.
globl
_sk_mirror_x_1_hsw
FUNCTION
(
_sk_mirror_x_1_hsw
)
_sk_mirror_x_1_hsw
:
.
byte
196
98
125
24
5
166
224
2
0
/
/
vbroadcastss
0x2e0a6
(
%
rip
)
%
ymm8
#
3c514
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c8
>
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
124
224
2
0
/
/
vbroadcastss
0x2e07c
(
%
rip
)
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
124
89
201
/
/
vmulps
%
ymm9
%
ymm0
%
ymm9
.
byte
196
67
125
8
201
1
/
/
vroundps
0x1
%
ymm9
%
ymm9
.
byte
196
65
52
88
201
/
/
vaddps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
193
124
92
193
/
/
vsubps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
200
/
/
vsubps
%
ymm0
%
ymm8
%
ymm9
.
byte
197
180
84
192
/
/
vandps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
76
224
2
0
/
/
vbroadcastss
0x2e04c
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_hsw
.
globl
_sk_decal_x_hsw
FUNCTION
(
_sk_decal_x_hsw
)
_sk_decal_x_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
200
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
80
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm10
.
byte
196
65
124
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm0
%
ymm10
.
byte
196
65
44
84
201
/
/
vandps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_hsw
.
globl
_sk_decal_y_hsw
FUNCTION
(
_sk_decal_y_hsw
)
_sk_decal_y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
201
2
/
/
vcmpleps
%
ymm1
%
ymm8
%
ymm9
.
byte
196
98
125
24
80
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm10
.
byte
196
65
116
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm1
%
ymm10
.
byte
196
65
44
84
201
/
/
vandps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_hsw
.
globl
_sk_decal_x_and_y_hsw
FUNCTION
(
_sk_decal_x_and_y_hsw
)
_sk_decal_x_and_y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
200
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
67
125
25
202
1
/
/
vextractf128
0x1
%
ymm9
%
xmm10
.
byte
196
65
49
99
202
/
/
vpacksswb
%
xmm10
%
xmm9
%
xmm9
.
byte
196
98
125
24
80
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm10
.
byte
196
65
124
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm0
%
ymm10
.
byte
196
67
125
25
211
1
/
/
vextractf128
0x1
%
ymm10
%
xmm11
.
byte
196
65
41
99
211
/
/
vpacksswb
%
xmm11
%
xmm10
%
xmm10
.
byte
197
60
194
217
2
/
/
vcmpleps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
67
125
25
220
1
/
/
vextractf128
0x1
%
ymm11
%
xmm12
.
byte
196
65
33
99
220
/
/
vpacksswb
%
xmm12
%
xmm11
%
xmm11
.
byte
196
65
33
219
201
/
/
vpand
%
xmm9
%
xmm11
%
xmm9
.
byte
196
65
49
219
202
/
/
vpand
%
xmm10
%
xmm9
%
xmm9
.
byte
196
98
125
24
80
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm10
.
byte
196
65
116
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm1
%
ymm10
.
byte
196
67
125
25
211
1
/
/
vextractf128
0x1
%
ymm10
%
xmm11
.
byte
196
65
41
99
211
/
/
vpacksswb
%
xmm11
%
xmm10
%
xmm10
.
byte
196
65
49
219
202
/
/
vpand
%
xmm10
%
xmm9
%
xmm9
.
byte
196
66
125
51
201
/
/
vpmovzxwd
%
xmm9
%
ymm9
.
byte
196
193
53
114
241
31
/
/
vpslld
0x1f
%
ymm9
%
ymm9
.
byte
196
193
53
114
225
31
/
/
vpsrad
0x1f
%
ymm9
%
ymm9
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_hsw
.
globl
_sk_check_decal_mask_hsw
FUNCTION
(
_sk_check_decal_mask_hsw
)
_sk_check_decal_mask_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
0
/
/
vmovups
(
%
rax
)
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
84
210
/
/
vandps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
84
219
/
/
vandps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminance_to_alpha_hsw
.
globl
_sk_luminance_to_alpha_hsw
FUNCTION
(
_sk_luminance_to_alpha_hsw
)
_sk_luminance_to_alpha_hsw
:
.
byte
196
226
125
24
29
108
224
2
0
/
/
vbroadcastss
0x2e06c
(
%
rip
)
%
ymm3
#
3c630
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e4
>
.
byte
196
98
125
24
5
95
224
2
0
/
/
vbroadcastss
0x2e05f
(
%
rip
)
%
ymm8
#
3c62c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e0
>
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
226
125
184
203
/
/
vfmadd231ps
%
ymm3
%
ymm0
%
ymm1
.
byte
196
226
125
24
29
84
224
2
0
/
/
vbroadcastss
0x2e054
(
%
rip
)
%
ymm3
#
3c634
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e8
>
.
byte
196
226
109
168
217
/
/
vfmadd213ps
%
ymm1
%
ymm2
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_hsw
.
globl
_sk_matrix_translate_hsw
FUNCTION
(
_sk_matrix_translate_hsw
)
_sk_matrix_translate_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
197
188
88
201
/
/
vaddps
%
ymm1
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_hsw
.
globl
_sk_matrix_scale_translate_hsw
FUNCTION
(
_sk_matrix_scale_translate_hsw
)
_sk_matrix_scale_translate_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm9
.
byte
196
194
61
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm0
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
194
61
168
201
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_hsw
.
globl
_sk_matrix_2x3_hsw
FUNCTION
(
_sk_matrix_2x3_hsw
)
_sk_matrix_2x3_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
66
117
184
194
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm8
.
byte
196
66
125
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
72
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm9
.
byte
196
66
117
184
203
/
/
vfmadd231ps
%
ymm11
%
ymm1
%
ymm9
.
byte
196
66
125
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_3x4_hsw
.
globl
_sk_matrix_3x4_hsw
FUNCTION
(
_sk_matrix_3x4_hsw
)
_sk_matrix_3x4_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
64
36
/
/
vbroadcastss
0x24
(
%
rax
)
%
ymm8
.
byte
196
66
109
184
195
/
/
vfmadd231ps
%
ymm11
%
ymm2
%
ymm8
.
byte
196
66
117
184
194
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm8
.
byte
196
66
125
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
72
40
/
/
vbroadcastss
0x28
(
%
rax
)
%
ymm9
.
byte
196
66
109
184
204
/
/
vfmadd231ps
%
ymm12
%
ymm2
%
ymm9
.
byte
196
66
117
184
203
/
/
vfmadd231ps
%
ymm11
%
ymm1
%
ymm9
.
byte
196
66
125
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm9
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
80
44
/
/
vbroadcastss
0x2c
(
%
rax
)
%
ymm10
.
byte
196
66
109
184
213
/
/
vfmadd231ps
%
ymm13
%
ymm2
%
ymm10
.
byte
196
66
117
184
212
/
/
vfmadd231ps
%
ymm12
%
ymm1
%
ymm10
.
byte
196
66
125
184
211
/
/
vfmadd231ps
%
ymm11
%
ymm0
%
ymm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
197
124
41
210
/
/
vmovaps
%
ymm10
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x5_hsw
.
globl
_sk_matrix_4x5_hsw
FUNCTION
(
_sk_matrix_4x5_hsw
)
_sk_matrix_4x5_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
48
/
/
vbroadcastss
0x30
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
64
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm8
.
byte
196
66
101
184
196
/
/
vfmadd231ps
%
ymm12
%
ymm3
%
ymm8
.
byte
196
66
109
184
195
/
/
vfmadd231ps
%
ymm11
%
ymm2
%
ymm8
.
byte
196
66
117
184
194
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm8
.
byte
196
66
125
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
36
/
/
vbroadcastss
0x24
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
52
/
/
vbroadcastss
0x34
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
72
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm9
.
byte
196
66
101
184
205
/
/
vfmadd231ps
%
ymm13
%
ymm3
%
ymm9
.
byte
196
66
109
184
204
/
/
vfmadd231ps
%
ymm12
%
ymm2
%
ymm9
.
byte
196
66
117
184
203
/
/
vfmadd231ps
%
ymm11
%
ymm1
%
ymm9
.
byte
196
66
125
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm9
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
40
/
/
vbroadcastss
0x28
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
112
56
/
/
vbroadcastss
0x38
(
%
rax
)
%
ymm14
.
byte
196
98
125
24
80
72
/
/
vbroadcastss
0x48
(
%
rax
)
%
ymm10
.
byte
196
66
101
184
214
/
/
vfmadd231ps
%
ymm14
%
ymm3
%
ymm10
.
byte
196
66
109
184
213
/
/
vfmadd231ps
%
ymm13
%
ymm2
%
ymm10
.
byte
196
66
117
184
212
/
/
vfmadd231ps
%
ymm12
%
ymm1
%
ymm10
.
byte
196
66
125
184
211
/
/
vfmadd231ps
%
ymm11
%
ymm0
%
ymm10
.
byte
196
98
125
24
96
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
112
44
/
/
vbroadcastss
0x2c
(
%
rax
)
%
ymm14
.
byte
196
98
125
24
120
60
/
/
vbroadcastss
0x3c
(
%
rax
)
%
ymm15
.
byte
196
98
125
24
88
76
/
/
vbroadcastss
0x4c
(
%
rax
)
%
ymm11
.
byte
196
66
101
184
223
/
/
vfmadd231ps
%
ymm15
%
ymm3
%
ymm11
.
byte
196
66
109
184
222
/
/
vfmadd231ps
%
ymm14
%
ymm2
%
ymm11
.
byte
196
66
117
184
221
/
/
vfmadd231ps
%
ymm13
%
ymm1
%
ymm11
.
byte
196
66
125
184
220
/
/
vfmadd231ps
%
ymm12
%
ymm0
%
ymm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
197
124
41
210
/
/
vmovaps
%
ymm10
%
ymm2
.
byte
197
124
41
219
/
/
vmovaps
%
ymm11
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x3_hsw
.
globl
_sk_matrix_4x3_hsw
FUNCTION
(
_sk_matrix_4x3_hsw
)
_sk_matrix_4x3_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
64
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm8
.
byte
196
98
117
184
195
/
/
vfmadd231ps
%
ymm3
%
ymm1
%
ymm8
.
byte
196
98
125
184
194
/
/
vfmadd231ps
%
ymm2
%
ymm0
%
ymm8
.
byte
196
226
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
72
36
/
/
vbroadcastss
0x24
(
%
rax
)
%
ymm9
.
byte
196
98
117
184
203
/
/
vfmadd231ps
%
ymm3
%
ymm1
%
ymm9
.
byte
196
98
125
184
202
/
/
vfmadd231ps
%
ymm2
%
ymm0
%
ymm9
.
byte
196
226
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
196
226
125
24
80
40
/
/
vbroadcastss
0x28
(
%
rax
)
%
ymm2
.
byte
196
194
117
184
210
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm2
.
byte
196
226
125
184
211
/
/
vfmadd231ps
%
ymm3
%
ymm0
%
ymm2
.
byte
196
98
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm11
.
byte
196
226
125
24
88
44
/
/
vbroadcastss
0x2c
(
%
rax
)
%
ymm3
.
byte
196
194
117
184
219
/
/
vfmadd231ps
%
ymm11
%
ymm1
%
ymm3
.
byte
196
194
125
184
218
/
/
vfmadd231ps
%
ymm10
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_hsw
.
globl
_sk_matrix_perspective_hsw
FUNCTION
(
_sk_matrix_perspective_hsw
)
_sk_matrix_perspective_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
66
117
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm1
%
ymm10
.
byte
196
66
125
184
208
/
/
vfmadd231ps
%
ymm8
%
ymm0
%
ymm10
.
byte
196
98
125
24
64
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
66
117
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm1
%
ymm11
.
byte
196
66
125
184
216
/
/
vfmadd231ps
%
ymm8
%
ymm0
%
ymm11
.
byte
196
98
125
24
64
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
96
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm12
.
byte
196
66
117
184
225
/
/
vfmadd231ps
%
ymm9
%
ymm1
%
ymm12
.
byte
196
66
125
184
224
/
/
vfmadd231ps
%
ymm8
%
ymm0
%
ymm12
.
byte
196
193
124
83
204
/
/
vrcpps
%
ymm12
%
ymm1
.
byte
197
172
89
193
/
/
vmulps
%
ymm1
%
ymm10
%
ymm0
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_hsw
.
globl
_sk_evenly_spaced_gradient_hsw
FUNCTION
(
_sk_evenly_spaced_gradient_hsw
)
_sk_evenly_spaced_gradient_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
8
/
/
mov
(
%
rax
)
%
r9
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
77
137
202
/
/
mov
%
r9
%
r10
.
byte
73
255
202
/
/
dec
%
r10
.
byte
120
7
/
/
js
e8d8
<
_sk_evenly_spaced_gradient_hsw
+
0x18
>
.
byte
196
193
242
42
202
/
/
vcvtsi2ss
%
r10
%
xmm1
%
xmm1
.
byte
235
22
/
/
jmp
e8ee
<
_sk_evenly_spaced_gradient_hsw
+
0x2e
>
.
byte
77
137
211
/
/
mov
%
r10
%
r11
.
byte
73
209
235
/
/
shr
%
r11
.
byte
65
131
226
1
/
/
and
0x1
%
r10d
.
byte
77
9
218
/
/
or
%
r11
%
r10
.
byte
196
193
242
42
202
/
/
vcvtsi2ss
%
r10
%
xmm1
%
xmm1
.
byte
197
242
88
201
/
/
vaddss
%
xmm1
%
xmm1
%
xmm1
.
byte
196
226
125
24
201
/
/
vbroadcastss
%
xmm1
%
ymm1
.
byte
197
244
89
200
/
/
vmulps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
126
91
225
/
/
vcvttps2dq
%
ymm1
%
ymm12
.
byte
73
131
249
8
/
/
cmp
0x8
%
r9
.
byte
119
73
/
/
ja
e94a
<
_sk_evenly_spaced_gradient_hsw
+
0x8a
>
.
byte
196
66
29
22
0
/
/
vpermps
(
%
r8
)
%
ymm12
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
66
29
22
8
/
/
vpermps
(
%
r8
)
%
ymm12
%
ymm9
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
196
194
29
22
8
/
/
vpermps
(
%
r8
)
%
ymm12
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
196
66
29
22
24
/
/
vpermps
(
%
r8
)
%
ymm12
%
ymm11
.
byte
196
194
29
22
17
/
/
vpermps
(
%
r9
)
%
ymm12
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
196
66
29
22
40
/
/
vpermps
(
%
r8
)
%
ymm12
%
ymm13
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
194
29
22
24
/
/
vpermps
(
%
r8
)
%
ymm12
%
ymm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
98
29
22
16
/
/
vpermps
(
%
rax
)
%
ymm12
%
ymm10
.
byte
233
147
0
0
0
/
/
jmpq
e9dd
<
_sk_evenly_spaced_gradient_hsw
+
0x11d
>
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
2
117
146
4
160
/
/
vgatherdps
%
ymm1
(
%
r8
%
ymm12
4
)
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
2
117
146
12
160
/
/
vgatherdps
%
ymm1
(
%
r8
%
ymm12
4
)
%
ymm9
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
130
109
146
12
160
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm12
4
)
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
2
109
146
28
160
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm12
4
)
%
ymm11
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
130
101
146
20
161
/
/
vgatherdps
%
ymm3
(
%
r9
%
ymm12
4
)
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
196
2
101
146
44
160
/
/
vgatherdps
%
ymm3
(
%
r8
%
ymm12
4
)
%
ymm13
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
130
5
146
28
160
/
/
vgatherdps
%
ymm15
(
%
r8
%
ymm12
4
)
%
ymm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
34
13
146
20
160
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm12
4
)
%
ymm10
.
byte
196
66
125
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
194
125
168
203
/
/
vfmadd213ps
%
ymm11
%
ymm0
%
ymm1
.
byte
196
194
125
168
213
/
/
vfmadd213ps
%
ymm13
%
ymm0
%
ymm2
.
byte
196
194
125
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_hsw
.
globl
_sk_gradient_hsw
FUNCTION
(
_sk_gradient_hsw
)
_sk_gradient_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
73
131
248
1
/
/
cmp
0x1
%
r8
.
byte
15
134
220
0
0
0
/
/
jbe
eae4
<
_sk_gradient_hsw
+
0xeb
>
.
byte
76
139
72
72
/
/
mov
0x48
(
%
rax
)
%
r9
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
65
186
1
0
0
0
/
/
mov
0x1
%
r10d
.
byte
196
226
125
24
21
225
218
2
0
/
/
vbroadcastss
0x2dae1
(
%
rip
)
%
ymm2
#
3c500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b4
>
.
byte
196
65
53
239
201
/
/
vpxor
%
ymm9
%
ymm9
%
ymm9
.
byte
196
130
125
24
28
145
/
/
vbroadcastss
(
%
r9
%
r10
4
)
%
ymm3
.
byte
197
228
194
216
2
/
/
vcmpleps
%
ymm0
%
ymm3
%
ymm3
.
byte
196
227
117
74
218
48
/
/
vblendvps
%
ymm3
%
ymm2
%
ymm1
%
ymm3
.
byte
197
53
254
203
/
/
vpaddd
%
ymm3
%
ymm9
%
ymm9
.
byte
73
255
194
/
/
inc
%
r10
.
byte
77
57
208
/
/
cmp
%
r10
%
r8
.
byte
117
227
/
/
jne
ea24
<
_sk_gradient_hsw
+
0x2b
>
.
byte
76
139
72
8
/
/
mov
0x8
(
%
rax
)
%
r9
.
byte
73
131
248
8
/
/
cmp
0x8
%
r8
.
byte
15
134
158
0
0
0
/
/
jbe
eaed
<
_sk_gradient_hsw
+
0xf4
>
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
2
117
146
4
137
/
/
vgatherdps
%
ymm1
(
%
r9
%
ymm9
4
)
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
2
117
146
20
136
/
/
vgatherdps
%
ymm1
(
%
r8
%
ymm9
4
)
%
ymm10
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
130
109
146
12
136
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm9
4
)
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
2
109
146
36
136
/
/
vgatherdps
%
ymm2
(
%
r8
%
ymm9
4
)
%
ymm12
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
130
101
146
20
137
/
/
vgatherdps
%
ymm3
(
%
r9
%
ymm9
4
)
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
2
101
146
44
136
/
/
vgatherdps
%
ymm3
(
%
r8
%
ymm9
4
)
%
ymm13
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
130
5
146
28
136
/
/
vgatherdps
%
ymm15
(
%
r8
%
ymm9
4
)
%
ymm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
34
13
146
28
136
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm9
4
)
%
ymm11
.
byte
235
77
/
/
jmp
eb31
<
_sk_gradient_hsw
+
0x138
>
.
byte
76
139
72
8
/
/
mov
0x8
(
%
rax
)
%
r9
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
53
22
1
/
/
vpermps
(
%
r9
)
%
ymm9
%
ymm8
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
66
53
22
16
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm10
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
196
194
53
22
8
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm1
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
196
66
53
22
32
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm12
.
byte
196
194
53
22
17
/
/
vpermps
(
%
r9
)
%
ymm9
%
ymm2
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
196
66
53
22
40
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm13
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
194
53
22
24
/
/
vpermps
(
%
r8
)
%
ymm9
%
ymm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
98
53
22
24
/
/
vpermps
(
%
rax
)
%
ymm9
%
ymm11
.
byte
196
66
125
168
194
/
/
vfmadd213ps
%
ymm10
%
ymm0
%
ymm8
.
byte
196
194
125
168
204
/
/
vfmadd213ps
%
ymm12
%
ymm0
%
ymm1
.
byte
196
194
125
168
213
/
/
vfmadd213ps
%
ymm13
%
ymm0
%
ymm2
.
byte
196
194
125
168
219
/
/
vfmadd213ps
%
ymm11
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_hsw
.
globl
_sk_evenly_spaced_2_stop_gradient_hsw
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_hsw
)
_sk_evenly_spaced_2_stop_gradient_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm1
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
98
125
184
193
/
/
vfmadd231ps
%
ymm1
%
ymm0
%
ymm8
.
byte
196
226
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
72
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm1
.
byte
196
226
125
184
202
/
/
vfmadd231ps
%
ymm2
%
ymm0
%
ymm1
.
byte
196
226
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm3
.
byte
196
226
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm2
.
byte
196
226
125
184
211
/
/
vfmadd231ps
%
ymm3
%
ymm0
%
ymm2
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
226
125
24
88
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm3
.
byte
196
194
125
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_hsw
.
globl
_sk_xy_to_unit_angle_hsw
FUNCTION
(
_sk_xy_to_unit_angle_hsw
)
_sk_xy_to_unit_angle_hsw
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
200
/
/
vsubps
%
ymm0
%
ymm8
%
ymm9
.
byte
197
52
84
200
/
/
vandps
%
ymm0
%
ymm9
%
ymm9
.
byte
197
60
92
209
/
/
vsubps
%
ymm1
%
ymm8
%
ymm10
.
byte
197
44
84
209
/
/
vandps
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
52
93
218
/
/
vminps
%
ymm10
%
ymm9
%
ymm11
.
byte
196
65
52
95
226
/
/
vmaxps
%
ymm10
%
ymm9
%
ymm12
.
byte
196
65
36
94
220
/
/
vdivps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
36
89
227
/
/
vmulps
%
ymm11
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
108
218
2
0
/
/
vbroadcastss
0x2da6c
(
%
rip
)
%
ymm13
#
3c638
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ec
>
.
byte
196
98
125
24
53
103
218
2
0
/
/
vbroadcastss
0x2da67
(
%
rip
)
%
ymm14
#
3c63c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f0
>
.
byte
196
66
29
184
245
/
/
vfmadd231ps
%
ymm13
%
ymm12
%
ymm14
.
byte
196
98
125
24
45
93
218
2
0
/
/
vbroadcastss
0x2da5d
(
%
rip
)
%
ymm13
#
3c640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f4
>
.
byte
196
66
29
184
238
/
/
vfmadd231ps
%
ymm14
%
ymm12
%
ymm13
.
byte
196
98
125
24
53
83
218
2
0
/
/
vbroadcastss
0x2da53
(
%
rip
)
%
ymm14
#
3c644
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f8
>
.
byte
196
66
29
184
245
/
/
vfmadd231ps
%
ymm13
%
ymm12
%
ymm14
.
byte
196
65
36
89
222
/
/
vmulps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
65
52
194
202
1
/
/
vcmpltps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
62
218
2
0
/
/
vbroadcastss
0x2da3e
(
%
rip
)
%
ymm10
#
3c648
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3fc
>
.
byte
196
65
44
92
211
/
/
vsubps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
37
74
202
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm11
%
ymm9
.
byte
196
193
124
194
192
1
/
/
vcmpltps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
212
216
2
0
/
/
vbroadcastss
0x2d8d4
(
%
rip
)
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
44
92
209
/
/
vsubps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
196
65
116
194
200
1
/
/
vcmpltps
%
ymm8
%
ymm1
%
ymm9
.
byte
196
98
125
24
21
190
216
2
0
/
/
vbroadcastss
0x2d8be
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
44
92
208
/
/
vsubps
%
ymm0
%
ymm10
%
ymm10
.
byte
196
195
125
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm0
%
ymm0
.
byte
196
65
124
194
200
3
/
/
vcmpunordps
%
ymm8
%
ymm0
%
ymm9
.
byte
196
195
125
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_hsw
.
globl
_sk_xy_to_radius_hsw
FUNCTION
(
_sk_xy_to_radius_hsw
)
_sk_xy_to_radius_hsw
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
184
192
/
/
vfmadd231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
193
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_negate_x_hsw
.
globl
_sk_negate_x_hsw
FUNCTION
(
_sk_negate_x_hsw
)
_sk_negate_x_hsw
:
.
byte
196
98
125
24
5
217
217
2
0
/
/
vbroadcastss
0x2d9d9
(
%
rip
)
%
ymm8
#
3c64c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x400
>
.
byte
196
193
124
87
192
/
/
vxorps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_strip_hsw
.
globl
_sk_xy_to_2pt_conical_strip_hsw
FUNCTION
(
_sk_xy_to_2pt_conical_strip_hsw
)
_sk_xy_to_2pt_conical_strip_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm8
.
byte
196
98
117
188
193
/
/
vfnmadd231ps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_focal_on_circle_hsw
.
globl
_sk_xy_to_2pt_conical_focal_on_circle_hsw
FUNCTION
(
_sk_xy_to_2pt_conical_focal_on_circle_hsw
)
_sk_xy_to_2pt_conical_focal_on_circle_hsw
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
197
60
94
192
/
/
vdivps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_well_behaved_hsw
.
globl
_sk_xy_to_2pt_conical_well_behaved_hsw
FUNCTION
(
_sk_xy_to_2pt_conical_well_behaved_hsw
)
_sk_xy_to_2pt_conical_well_behaved_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
184
192
/
/
vfmadd231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
196
194
53
172
192
/
/
vfnmadd213ps
%
ymm8
%
ymm9
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_greater_hsw
.
globl
_sk_xy_to_2pt_conical_greater_hsw
FUNCTION
(
_sk_xy_to_2pt_conical_greater_hsw
)
_sk_xy_to_2pt_conical_greater_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
186
192
/
/
vfmsub231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
196
194
53
172
192
/
/
vfnmadd213ps
%
ymm8
%
ymm9
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_smaller_hsw
.
globl
_sk_xy_to_2pt_conical_smaller_hsw
FUNCTION
(
_sk_xy_to_2pt_conical_smaller_hsw
)
_sk_xy_to_2pt_conical_smaller_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
98
125
186
192
/
/
vfmsub231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
196
194
53
174
192
/
/
vfnmsub213ps
%
ymm8
%
ymm9
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_compensate_focal_hsw
.
globl
_sk_alter_2pt_conical_compensate_focal_hsw
FUNCTION
(
_sk_alter_2pt_conical_compensate_focal_hsw
)
_sk_alter_2pt_conical_compensate_focal_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_unswap_hsw
.
globl
_sk_alter_2pt_conical_unswap_hsw
FUNCTION
(
_sk_alter_2pt_conical_unswap_hsw
)
_sk_alter_2pt_conical_unswap_hsw
:
.
byte
196
98
125
24
5
224
215
2
0
/
/
vbroadcastss
0x2d7e0
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_nan_hsw
.
globl
_sk_mask_2pt_conical_nan_hsw
FUNCTION
(
_sk_mask_2pt_conical_nan_hsw
)
_sk_mask_2pt_conical_nan_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
124
194
193
3
/
/
vcmpunordps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
67
125
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm0
%
ymm8
.
byte
196
193
124
194
193
7
/
/
vcmpordps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_degenerates_hsw
.
globl
_sk_mask_2pt_conical_degenerates_hsw
FUNCTION
(
_sk_mask_2pt_conical_degenerates_hsw
)
_sk_mask_2pt_conical_degenerates_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
52
194
192
5
/
/
vcmpnltps
%
ymm0
%
ymm9
%
ymm8
.
byte
196
67
125
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm0
%
ymm8
.
byte
197
180
194
192
1
/
/
vcmpltps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_apply_vector_mask_hsw
.
globl
_sk_apply_vector_mask_hsw
FUNCTION
(
_sk_apply_vector_mask_hsw
)
_sk_apply_vector_mask_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
0
/
/
vmovups
(
%
rax
)
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
84
210
/
/
vandps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
84
219
/
/
vandps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_save_xy_hsw
.
globl
_sk_save_xy_hsw
FUNCTION
(
_sk_save_xy_hsw
)
_sk_save_xy_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
81
215
2
0
/
/
vbroadcastss
0x2d751
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
124
88
200
/
/
vaddps
%
ymm8
%
ymm0
%
ymm9
.
byte
196
67
125
8
209
1
/
/
vroundps
0x1
%
ymm9
%
ymm10
.
byte
196
65
52
92
202
/
/
vsubps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
116
88
192
/
/
vaddps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
67
125
8
208
1
/
/
vroundps
0x1
%
ymm8
%
ymm10
.
byte
196
65
60
92
194
/
/
vsubps
%
ymm10
%
ymm8
%
ymm8
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
197
252
17
72
64
/
/
vmovups
%
ymm1
0x40
(
%
rax
)
.
byte
197
124
17
136
128
0
0
0
/
/
vmovups
%
ymm9
0x80
(
%
rax
)
.
byte
197
124
17
128
192
0
0
0
/
/
vmovups
%
ymm8
0xc0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_accumulate_hsw
.
globl
_sk_accumulate_hsw
FUNCTION
(
_sk_accumulate_hsw
)
_sk_accumulate_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
128
0
1
0
0
/
/
vmovups
0x100
(
%
rax
)
%
ymm8
.
byte
197
60
89
128
64
1
0
0
/
/
vmulps
0x140
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
226
61
184
224
/
/
vfmadd231ps
%
ymm0
%
ymm8
%
ymm4
.
byte
196
226
61
184
233
/
/
vfmadd231ps
%
ymm1
%
ymm8
%
ymm5
.
byte
196
226
61
184
242
/
/
vfmadd231ps
%
ymm2
%
ymm8
%
ymm6
.
byte
196
98
101
168
199
/
/
vfmadd213ps
%
ymm7
%
ymm3
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
199
/
/
vmovaps
%
ymm8
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_nx_hsw
.
globl
_sk_bilinear_nx_hsw
FUNCTION
(
_sk_bilinear_nx_hsw
)
_sk_bilinear_nx_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
51
216
2
0
/
/
vbroadcastss
0x2d833
(
%
rip
)
%
ymm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
210
214
2
0
/
/
vbroadcastss
0x2d6d2
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_px_hsw
.
globl
_sk_bilinear_px_hsw
FUNCTION
(
_sk_bilinear_px_hsw
)
_sk_bilinear_px_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
175
214
2
0
/
/
vbroadcastss
0x2d6af
(
%
rip
)
%
ymm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
124
16
128
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_ny_hsw
.
globl
_sk_bilinear_ny_hsw
FUNCTION
(
_sk_bilinear_ny_hsw
)
_sk_bilinear_ny_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
228
215
2
0
/
/
vbroadcastss
0x2d7e4
(
%
rip
)
%
ymm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
130
214
2
0
/
/
vbroadcastss
0x2d682
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_py_hsw
.
globl
_sk_bilinear_py_hsw
FUNCTION
(
_sk_bilinear_py_hsw
)
_sk_bilinear_py_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
95
214
2
0
/
/
vbroadcastss
0x2d65f
(
%
rip
)
%
ymm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
197
124
16
128
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3x_hsw
.
globl
_sk_bicubic_n3x_hsw
FUNCTION
(
_sk_bicubic_n3x_hsw
)
_sk_bicubic_n3x_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
151
215
2
0
/
/
vbroadcastss
0x2d797
(
%
rip
)
%
ymm0
#
3c654
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x408
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
50
214
2
0
/
/
vbroadcastss
0x2d632
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
144
214
2
0
/
/
vbroadcastss
0x2d690
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
98
125
24
29
111
215
2
0
/
/
vbroadcastss
0x2d76f
(
%
rip
)
%
ymm11
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
66
61
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm8
%
ymm11
.
byte
196
65
36
89
193
/
/
vmulps
%
ymm9
%
ymm11
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1x_hsw
.
globl
_sk_bicubic_n1x_hsw
FUNCTION
(
_sk_bicubic_n1x_hsw
)
_sk_bicubic_n1x_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
70
215
2
0
/
/
vbroadcastss
0x2d746
(
%
rip
)
%
ymm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
229
213
2
0
/
/
vbroadcastss
0x2d5e5
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
56
215
2
0
/
/
vbroadcastss
0x2d738
(
%
rip
)
%
ymm9
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
98
125
24
21
43
215
2
0
/
/
vbroadcastss
0x2d72b
(
%
rip
)
%
ymm10
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
66
61
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm10
.
byte
196
98
125
24
13
185
213
2
0
/
/
vbroadcastss
0x2d5b9
(
%
rip
)
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
66
61
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
23
215
2
0
/
/
vbroadcastss
0x2d717
(
%
rip
)
%
ymm10
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
66
61
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm8
%
ymm10
.
byte
197
124
17
144
0
1
0
0
/
/
vmovups
%
ymm10
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1x_hsw
.
globl
_sk_bicubic_p1x_hsw
FUNCTION
(
_sk_bicubic_p1x_hsw
)
_sk_bicubic_p1x_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
143
213
2
0
/
/
vbroadcastss
0x2d58f
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
188
88
0
/
/
vaddps
(
%
rax
)
%
ymm8
%
ymm0
.
byte
197
124
16
136
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
21
226
214
2
0
/
/
vbroadcastss
0x2d6e2
(
%
rip
)
%
ymm10
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
98
125
24
29
213
214
2
0
/
/
vbroadcastss
0x2d6d5
(
%
rip
)
%
ymm11
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
66
53
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm9
%
ymm11
.
byte
196
66
53
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm9
%
ymm11
.
byte
196
98
125
24
5
202
214
2
0
/
/
vbroadcastss
0x2d6ca
(
%
rip
)
%
ymm8
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
66
53
184
195
/
/
vfmadd231ps
%
ymm11
%
ymm9
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3x_hsw
.
globl
_sk_bicubic_p3x_hsw
FUNCTION
(
_sk_bicubic_p3x_hsw
)
_sk_bicubic_p3x_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
170
214
2
0
/
/
vbroadcastss
0x2d6aa
(
%
rip
)
%
ymm0
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
124
16
128
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
160
213
2
0
/
/
vbroadcastss
0x2d5a0
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
98
125
24
29
127
214
2
0
/
/
vbroadcastss
0x2d67f
(
%
rip
)
%
ymm11
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
66
61
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm8
%
ymm11
.
byte
196
65
52
89
195
/
/
vmulps
%
ymm11
%
ymm9
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3y_hsw
.
globl
_sk_bicubic_n3y_hsw
FUNCTION
(
_sk_bicubic_n3y_hsw
)
_sk_bicubic_n3y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
90
214
2
0
/
/
vbroadcastss
0x2d65a
(
%
rip
)
%
ymm1
#
3c654
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x408
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
244
212
2
0
/
/
vbroadcastss
0x2d4f4
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
82
213
2
0
/
/
vbroadcastss
0x2d552
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
98
125
24
29
49
214
2
0
/
/
vbroadcastss
0x2d631
(
%
rip
)
%
ymm11
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
66
61
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm8
%
ymm11
.
byte
196
65
36
89
193
/
/
vmulps
%
ymm9
%
ymm11
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1y_hsw
.
globl
_sk_bicubic_n1y_hsw
FUNCTION
(
_sk_bicubic_n1y_hsw
)
_sk_bicubic_n1y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
8
214
2
0
/
/
vbroadcastss
0x2d608
(
%
rip
)
%
ymm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
166
212
2
0
/
/
vbroadcastss
0x2d4a6
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
249
213
2
0
/
/
vbroadcastss
0x2d5f9
(
%
rip
)
%
ymm9
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
98
125
24
21
236
213
2
0
/
/
vbroadcastss
0x2d5ec
(
%
rip
)
%
ymm10
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
66
61
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm10
.
byte
196
98
125
24
13
122
212
2
0
/
/
vbroadcastss
0x2d47a
(
%
rip
)
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
66
61
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
216
213
2
0
/
/
vbroadcastss
0x2d5d8
(
%
rip
)
%
ymm10
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
66
61
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm8
%
ymm10
.
byte
197
124
17
144
64
1
0
0
/
/
vmovups
%
ymm10
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1y_hsw
.
globl
_sk_bicubic_p1y_hsw
FUNCTION
(
_sk_bicubic_p1y_hsw
)
_sk_bicubic_p1y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
80
212
2
0
/
/
vbroadcastss
0x2d450
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
188
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm8
%
ymm1
.
byte
197
124
16
136
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
21
162
213
2
0
/
/
vbroadcastss
0x2d5a2
(
%
rip
)
%
ymm10
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
98
125
24
29
149
213
2
0
/
/
vbroadcastss
0x2d595
(
%
rip
)
%
ymm11
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
66
53
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm9
%
ymm11
.
byte
196
66
53
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm9
%
ymm11
.
byte
196
98
125
24
5
138
213
2
0
/
/
vbroadcastss
0x2d58a
(
%
rip
)
%
ymm8
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
66
53
184
195
/
/
vfmadd231ps
%
ymm11
%
ymm9
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3y_hsw
.
globl
_sk_bicubic_p3y_hsw
FUNCTION
(
_sk_bicubic_p3y_hsw
)
_sk_bicubic_p3y_hsw
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
106
213
2
0
/
/
vbroadcastss
0x2d56a
(
%
rip
)
%
ymm1
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
197
124
16
128
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
95
212
2
0
/
/
vbroadcastss
0x2d45f
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
98
125
24
29
62
213
2
0
/
/
vbroadcastss
0x2d53e
(
%
rip
)
%
ymm11
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
66
61
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm8
%
ymm11
.
byte
196
65
52
89
195
/
/
vmulps
%
ymm11
%
ymm9
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_callback_hsw
.
globl
_sk_callback_hsw
FUNCTION
(
_sk_callback_hsw
)
_sk_callback_hsw
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
228
224
/
/
and
0xffffffffffffffe0
%
rsp
.
byte
72
129
236
160
0
0
0
/
/
sub
0xa0
%
rsp
.
byte
197
252
41
124
36
96
/
/
vmovaps
%
ymm7
0x60
(
%
rsp
)
.
byte
197
252
41
116
36
64
/
/
vmovaps
%
ymm6
0x40
(
%
rsp
)
.
byte
197
252
41
108
36
32
/
/
vmovaps
%
ymm5
0x20
(
%
rsp
)
.
byte
197
252
41
36
36
/
/
vmovaps
%
ymm4
(
%
rsp
)
.
byte
73
137
206
/
/
mov
%
rcx
%
r14
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
73
137
253
/
/
mov
%
rdi
%
r13
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
137
195
/
/
mov
%
rax
%
rbx
.
byte
73
137
244
/
/
mov
%
rsi
%
r12
.
byte
197
252
20
225
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm4
.
byte
197
252
21
193
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
236
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm1
.
byte
197
236
21
211
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
221
20
217
/
/
vunpcklpd
%
ymm1
%
ymm4
%
ymm3
.
byte
197
221
21
201
/
/
vunpckhpd
%
ymm1
%
ymm4
%
ymm1
.
byte
197
253
20
226
/
/
vunpcklpd
%
ymm2
%
ymm0
%
ymm4
.
byte
197
253
21
194
/
/
vunpckhpd
%
ymm2
%
ymm0
%
ymm0
.
byte
196
227
101
24
209
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm2
.
byte
196
227
93
24
232
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm4
%
ymm5
.
byte
196
227
101
6
201
49
/
/
vperm2f128
0x31
%
ymm1
%
ymm3
%
ymm1
.
byte
196
227
93
6
192
49
/
/
vperm2f128
0x31
%
ymm0
%
ymm4
%
ymm0
.
byte
197
253
17
83
8
/
/
vmovupd
%
ymm2
0x8
(
%
rbx
)
.
byte
197
253
17
107
40
/
/
vmovupd
%
ymm5
0x28
(
%
rbx
)
.
byte
197
253
17
75
72
/
/
vmovupd
%
ymm1
0x48
(
%
rbx
)
.
byte
197
253
17
67
104
/
/
vmovupd
%
ymm0
0x68
(
%
rbx
)
.
byte
77
133
237
/
/
test
%
r13
%
r13
.
byte
190
8
0
0
0
/
/
mov
0x8
%
esi
.
byte
65
15
69
245
/
/
cmovne
%
r13d
%
esi
.
byte
72
137
223
/
/
mov
%
rbx
%
rdi
.
byte
197
248
119
/
/
vzeroupper
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
72
139
131
8
1
0
0
/
/
mov
0x108
(
%
rbx
)
%
rax
.
byte
197
248
16
0
/
/
vmovups
(
%
rax
)
%
xmm0
.
byte
197
248
16
72
16
/
/
vmovups
0x10
(
%
rax
)
%
xmm1
.
byte
197
248
16
80
32
/
/
vmovups
0x20
(
%
rax
)
%
xmm2
.
byte
197
248
16
88
48
/
/
vmovups
0x30
(
%
rax
)
%
xmm3
.
byte
196
227
101
24
88
112
1
/
/
vinsertf128
0x1
0x70
(
%
rax
)
%
ymm3
%
ymm3
.
byte
196
227
109
24
80
96
1
/
/
vinsertf128
0x1
0x60
(
%
rax
)
%
ymm2
%
ymm2
.
byte
196
227
117
24
72
80
1
/
/
vinsertf128
0x1
0x50
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
227
125
24
64
64
1
/
/
vinsertf128
0x1
0x40
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
252
20
225
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm4
.
byte
197
252
21
233
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm5
.
byte
197
236
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm1
.
byte
197
236
21
219
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm3
.
byte
197
221
20
193
/
/
vunpcklpd
%
ymm1
%
ymm4
%
ymm0
.
byte
197
221
21
201
/
/
vunpckhpd
%
ymm1
%
ymm4
%
ymm1
.
byte
197
213
20
211
/
/
vunpcklpd
%
ymm3
%
ymm5
%
ymm2
.
byte
197
213
21
219
/
/
vunpckhpd
%
ymm3
%
ymm5
%
ymm3
.
byte
76
137
230
/
/
mov
%
r12
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
137
239
/
/
mov
%
r13
%
rdi
.
byte
76
137
250
/
/
mov
%
r15
%
rdx
.
byte
76
137
241
/
/
mov
%
r14
%
rcx
.
byte
197
252
40
36
36
/
/
vmovaps
(
%
rsp
)
%
ymm4
.
byte
197
252
40
108
36
32
/
/
vmovaps
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
40
116
36
64
/
/
vmovaps
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
40
124
36
96
/
/
vmovaps
0x60
(
%
rsp
)
%
ymm7
.
byte
72
141
101
216
/
/
lea
-
0x28
(
%
rbp
)
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_3D_hsw
.
globl
_sk_clut_3D_hsw
FUNCTION
(
_sk_clut_3D_hsw
)
_sk_clut_3D_hsw
:
.
byte
72
129
236
56
3
0
0
/
/
sub
0x338
%
rsp
.
byte
197
252
17
188
36
0
3
0
0
/
/
vmovups
%
ymm7
0x300
(
%
rsp
)
.
byte
197
252
17
180
36
224
2
0
0
/
/
vmovups
%
ymm6
0x2e0
(
%
rsp
)
.
byte
197
252
17
172
36
192
2
0
0
/
/
vmovups
%
ymm5
0x2c0
(
%
rsp
)
.
byte
197
252
17
164
36
160
2
0
0
/
/
vmovups
%
ymm4
0x2a0
(
%
rsp
)
.
byte
197
252
17
156
36
128
2
0
0
/
/
vmovups
%
ymm3
0x280
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
217
/
/
vmovd
%
r9d
%
xmm3
.
byte
196
226
125
88
219
/
/
vpbroadcastd
%
xmm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
254
91
234
/
/
vcvttps2dq
%
ymm2
%
ymm5
.
byte
197
252
40
242
/
/
vmovaps
%
ymm2
%
ymm6
.
byte
197
252
17
180
36
192
0
0
0
/
/
vmovups
%
ymm6
0xc0
(
%
rsp
)
.
byte
196
193
121
110
208
/
/
vmovd
%
r8d
%
xmm2
.
byte
196
193
121
110
216
/
/
vmovd
%
r8d
%
xmm3
.
byte
196
226
125
88
219
/
/
vpbroadcastd
%
xmm3
%
ymm3
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
225
/
/
vmovd
%
r9d
%
xmm4
.
byte
196
226
125
88
228
/
/
vpbroadcastd
%
xmm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
197
220
89
201
/
/
vmulps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
254
91
225
/
/
vcvttps2dq
%
ymm1
%
ymm4
.
byte
197
254
127
164
36
96
2
0
0
/
/
vmovdqu
%
ymm4
0x260
(
%
rsp
)
.
byte
197
124
40
201
/
/
vmovaps
%
ymm1
%
ymm9
.
byte
197
124
17
140
36
224
1
0
0
/
/
vmovups
%
ymm9
0x1e0
(
%
rsp
)
.
byte
196
98
101
64
228
/
/
vpmulld
%
ymm4
%
ymm3
%
ymm12
.
byte
197
157
254
205
/
/
vpaddd
%
ymm5
%
ymm12
%
ymm1
.
byte
197
125
111
197
/
/
vmovdqa
%
ymm5
%
ymm8
.
byte
197
126
127
132
36
128
1
0
0
/
/
vmovdqu
%
ymm8
0x180
(
%
rsp
)
.
byte
196
193
121
110
224
/
/
vmovd
%
r8d
%
xmm4
.
byte
196
226
93
64
210
/
/
vpmulld
%
ymm2
%
ymm4
%
ymm2
.
byte
196
226
125
88
226
/
/
vpbroadcastd
%
xmm2
%
ymm4
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
196
193
121
110
208
/
/
vmovd
%
r8d
%
xmm2
.
byte
196
226
125
88
210
/
/
vpbroadcastd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
197
236
89
192
/
/
vmulps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
254
91
208
/
/
vcvttps2dq
%
ymm0
%
ymm2
.
byte
197
254
127
148
36
64
2
0
0
/
/
vmovdqu
%
ymm2
0x240
(
%
rsp
)
.
byte
197
124
40
208
/
/
vmovaps
%
ymm0
%
ymm10
.
byte
197
124
17
148
36
192
1
0
0
/
/
vmovups
%
ymm10
0x1c0
(
%
rsp
)
.
byte
196
98
93
64
218
/
/
vpmulld
%
ymm2
%
ymm4
%
ymm11
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
197
165
254
193
/
/
vpaddd
%
ymm1
%
ymm11
%
ymm0
.
byte
196
98
125
88
61
0
211
2
0
/
/
vpbroadcastd
0x2d300
(
%
rip
)
%
ymm15
#
3c66c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x420
>
.
byte
196
194
125
64
199
/
/
vpmulld
%
ymm15
%
ymm0
%
ymm0
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
109
146
44
128
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm0
4
)
%
ymm5
.
byte
197
252
17
172
36
32
2
0
0
/
/
vmovups
%
ymm5
0x220
(
%
rsp
)
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
196
193
125
250
213
/
/
vpsubd
%
ymm13
%
ymm0
%
ymm2
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
226
85
146
60
144
/
/
vgatherdps
%
ymm5
(
%
rax
%
ymm2
4
)
%
ymm7
.
byte
197
252
17
188
36
0
2
0
0
/
/
vmovups
%
ymm7
0x200
(
%
rsp
)
.
byte
196
226
125
88
61
86
209
2
0
/
/
vpbroadcastd
0x2d156
(
%
rip
)
%
ymm7
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
197
253
254
199
/
/
vpaddd
%
ymm7
%
ymm0
%
ymm0
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
226
85
146
20
128
/
/
vgatherdps
%
ymm5
(
%
rax
%
ymm0
4
)
%
ymm2
.
byte
197
252
17
148
36
32
1
0
0
/
/
vmovups
%
ymm2
0x120
(
%
rsp
)
.
byte
196
226
125
24
5
146
210
2
0
/
/
vbroadcastss
0x2d292
(
%
rip
)
%
ymm0
#
3c668
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x41c
>
.
byte
197
172
88
232
/
/
vaddps
%
ymm0
%
ymm10
%
ymm5
.
byte
197
254
91
237
/
/
vcvttps2dq
%
ymm5
%
ymm5
.
byte
196
226
93
64
213
/
/
vpmulld
%
ymm5
%
ymm4
%
ymm2
.
byte
197
237
254
201
/
/
vpaddd
%
ymm1
%
ymm2
%
ymm1
.
byte
196
194
117
64
207
/
/
vpmulld
%
ymm15
%
ymm1
%
ymm1
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
93
146
44
136
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm1
4
)
%
ymm5
.
byte
197
252
17
172
36
160
1
0
0
/
/
vmovups
%
ymm5
0x1a0
(
%
rsp
)
.
byte
196
193
117
250
229
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm4
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
98
85
146
20
160
/
/
vgatherdps
%
ymm5
(
%
rax
%
ymm4
4
)
%
ymm10
.
byte
197
124
17
148
36
96
1
0
0
/
/
vmovups
%
ymm10
0x160
(
%
rsp
)
.
byte
197
245
254
207
/
/
vpaddd
%
ymm7
%
ymm1
%
ymm1
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
93
146
44
136
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm1
4
)
%
ymm5
.
byte
197
252
17
172
36
64
1
0
0
/
/
vmovups
%
ymm5
0x140
(
%
rsp
)
.
byte
197
180
88
200
/
/
vaddps
%
ymm0
%
ymm9
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
98
101
64
201
/
/
vpmulld
%
ymm1
%
ymm3
%
ymm9
.
byte
196
193
53
254
200
/
/
vpaddd
%
ymm8
%
ymm9
%
ymm1
.
byte
196
193
117
254
219
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm3
.
byte
196
194
101
64
223
/
/
vpmulld
%
ymm15
%
ymm3
%
ymm3
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
93
146
44
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm5
.
byte
197
252
17
172
36
0
1
0
0
/
/
vmovups
%
ymm5
0x100
(
%
rsp
)
.
byte
196
193
101
250
229
/
/
vpsubd
%
ymm13
%
ymm3
%
ymm4
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
98
85
146
4
160
/
/
vgatherdps
%
ymm5
(
%
rax
%
ymm4
4
)
%
ymm8
.
byte
197
124
17
132
36
224
0
0
0
/
/
vmovups
%
ymm8
0xe0
(
%
rsp
)
.
byte
197
229
254
223
/
/
vpaddd
%
ymm7
%
ymm3
%
ymm3
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
93
146
44
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm5
.
byte
197
252
17
172
36
160
0
0
0
/
/
vmovups
%
ymm5
0xa0
(
%
rsp
)
.
byte
197
237
254
201
/
/
vpaddd
%
ymm1
%
ymm2
%
ymm1
.
byte
196
194
117
64
207
/
/
vpmulld
%
ymm15
%
ymm1
%
ymm1
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
101
146
36
136
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm1
4
)
%
ymm4
.
byte
197
252
17
164
36
128
0
0
0
/
/
vmovups
%
ymm4
0x80
(
%
rsp
)
.
byte
196
193
117
250
221
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm3
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
93
146
44
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm5
.
byte
197
252
17
108
36
96
/
/
vmovups
%
ymm5
0x60
(
%
rsp
)
.
byte
197
245
254
207
/
/
vpaddd
%
ymm7
%
ymm1
%
ymm1
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
101
146
36
136
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm1
4
)
%
ymm4
.
byte
197
252
17
100
36
64
/
/
vmovups
%
ymm4
0x40
(
%
rsp
)
.
byte
197
204
88
192
/
/
vaddps
%
ymm0
%
ymm6
%
ymm0
.
byte
197
126
91
208
/
/
vcvttps2dq
%
ymm0
%
ymm10
.
byte
196
193
29
254
218
/
/
vpaddd
%
ymm10
%
ymm12
%
ymm3
.
byte
197
165
254
195
/
/
vpaddd
%
ymm3
%
ymm11
%
ymm0
.
byte
196
194
125
64
199
/
/
vpmulld
%
ymm15
%
ymm0
%
ymm0
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
226
117
146
36
128
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm0
4
)
%
ymm4
.
byte
197
252
17
100
36
32
/
/
vmovups
%
ymm4
0x20
(
%
rsp
)
.
byte
196
193
125
250
237
/
/
vpsubd
%
ymm13
%
ymm0
%
ymm5
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
205
118
246
/
/
vpcmpeqd
%
ymm6
%
ymm6
%
ymm6
.
byte
196
226
77
146
12
168
/
/
vgatherdps
%
ymm6
(
%
rax
%
ymm5
4
)
%
ymm1
.
byte
197
252
17
12
36
/
/
vmovups
%
ymm1
(
%
rsp
)
.
byte
197
253
254
239
/
/
vpaddd
%
ymm7
%
ymm0
%
ymm5
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
205
118
246
/
/
vpcmpeqd
%
ymm6
%
ymm6
%
ymm6
.
byte
196
226
77
146
4
168
/
/
vgatherdps
%
ymm6
(
%
rax
%
ymm5
4
)
%
ymm0
.
byte
197
252
17
68
36
224
/
/
vmovups
%
ymm0
-
0x20
(
%
rsp
)
.
byte
197
237
254
219
/
/
vpaddd
%
ymm3
%
ymm2
%
ymm3
.
byte
196
194
101
64
223
/
/
vpmulld
%
ymm15
%
ymm3
%
ymm3
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
226
85
146
4
152
/
/
vgatherdps
%
ymm5
(
%
rax
%
ymm3
4
)
%
ymm0
.
byte
197
252
17
68
36
192
/
/
vmovups
%
ymm0
-
0x40
(
%
rsp
)
.
byte
196
193
101
250
197
/
/
vpsubd
%
ymm13
%
ymm3
%
ymm0
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
226
13
146
36
128
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm0
4
)
%
ymm4
.
byte
197
229
254
199
/
/
vpaddd
%
ymm7
%
ymm3
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
226
13
146
12
128
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm0
4
)
%
ymm1
.
byte
197
252
17
76
36
160
/
/
vmovups
%
ymm1
-
0x60
(
%
rsp
)
.
byte
196
193
53
254
202
/
/
vpaddd
%
ymm10
%
ymm9
%
ymm1
.
byte
196
65
117
254
211
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm10
.
byte
196
66
45
64
247
/
/
vpmulld
%
ymm15
%
ymm10
%
ymm14
.
byte
196
65
45
239
210
/
/
vpxor
%
ymm10
%
ymm10
%
ymm10
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
34
29
146
20
176
/
/
vgatherdps
%
ymm12
(
%
rax
%
ymm14
4
)
%
ymm10
.
byte
196
65
13
250
229
/
/
vpsubd
%
ymm13
%
ymm14
%
ymm12
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
196
65
37
118
219
/
/
vpcmpeqd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
162
37
146
4
160
/
/
vgatherdps
%
ymm11
(
%
rax
%
ymm12
4
)
%
ymm0
.
byte
197
252
17
68
36
128
/
/
vmovups
%
ymm0
-
0x80
(
%
rsp
)
.
byte
197
13
254
223
/
/
vpaddd
%
ymm7
%
ymm14
%
ymm11
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
34
13
146
4
152
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm11
4
)
%
ymm8
.
byte
197
237
254
201
/
/
vpaddd
%
ymm1
%
ymm2
%
ymm1
.
byte
196
194
117
64
207
/
/
vpmulld
%
ymm15
%
ymm1
%
ymm1
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
13
118
246
/
/
vpcmpeqd
%
ymm14
%
ymm14
%
ymm14
.
byte
196
98
13
146
28
136
/
/
vgatherdps
%
ymm14
(
%
rax
%
ymm1
4
)
%
ymm11
.
byte
196
65
117
250
245
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm14
.
byte
196
65
5
239
255
/
/
vpxor
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
34
53
146
60
176
/
/
vgatherdps
%
ymm9
(
%
rax
%
ymm14
4
)
%
ymm15
.
byte
197
245
254
207
/
/
vpaddd
%
ymm7
%
ymm1
%
ymm1
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
196
226
21
146
60
136
/
/
vgatherdps
%
ymm13
(
%
rax
%
ymm1
4
)
%
ymm7
.
byte
197
252
91
140
36
64
2
0
0
/
/
vcvtdq2ps
0x240
(
%
rsp
)
%
ymm1
.
byte
197
252
16
132
36
192
1
0
0
/
/
vmovups
0x1c0
(
%
rsp
)
%
ymm0
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
197
252
16
132
36
32
2
0
0
/
/
vmovups
0x220
(
%
rsp
)
%
ymm0
.
byte
197
252
16
156
36
160
1
0
0
/
/
vmovups
0x1a0
(
%
rsp
)
%
ymm3
.
byte
197
100
92
200
/
/
vsubps
%
ymm0
%
ymm3
%
ymm9
.
byte
196
98
117
168
200
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm9
.
byte
197
252
16
132
36
0
2
0
0
/
/
vmovups
0x200
(
%
rsp
)
%
ymm0
.
byte
197
252
16
156
36
96
1
0
0
/
/
vmovups
0x160
(
%
rsp
)
%
ymm3
.
byte
197
100
92
232
/
/
vsubps
%
ymm0
%
ymm3
%
ymm13
.
byte
196
98
117
168
232
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm13
.
byte
197
252
16
148
36
32
1
0
0
/
/
vmovups
0x120
(
%
rsp
)
%
ymm2
.
byte
197
252
16
132
36
64
1
0
0
/
/
vmovups
0x140
(
%
rsp
)
%
ymm0
.
byte
197
124
92
242
/
/
vsubps
%
ymm2
%
ymm0
%
ymm14
.
byte
196
98
117
168
242
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm14
.
byte
197
252
16
132
36
0
1
0
0
/
/
vmovups
0x100
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm2
.
byte
197
108
92
224
/
/
vsubps
%
ymm0
%
ymm2
%
ymm12
.
byte
196
98
117
168
224
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm12
.
byte
197
252
16
132
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
84
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm2
.
byte
197
236
92
232
/
/
vsubps
%
ymm0
%
ymm2
%
ymm5
.
byte
196
226
117
168
232
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm5
.
byte
197
252
16
132
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
84
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm2
.
byte
197
236
92
240
/
/
vsubps
%
ymm0
%
ymm2
%
ymm6
.
byte
196
226
117
168
240
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm6
.
byte
197
252
16
84
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm2
.
byte
197
252
16
68
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
117
168
194
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm0
.
byte
197
252
16
28
36
/
/
vmovups
(
%
rsp
)
%
ymm3
.
byte
197
220
92
211
/
/
vsubps
%
ymm3
%
ymm4
%
ymm2
.
byte
196
226
117
168
211
/
/
vfmadd213ps
%
ymm3
%
ymm1
%
ymm2
.
byte
197
252
16
100
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm4
.
byte
197
252
16
92
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm3
.
byte
197
228
92
220
/
/
vsubps
%
ymm4
%
ymm3
%
ymm3
.
byte
196
226
117
168
220
/
/
vfmadd213ps
%
ymm4
%
ymm1
%
ymm3
.
byte
196
65
36
92
218
/
/
vsubps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
66
117
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm1
%
ymm11
.
byte
197
252
16
100
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm4
.
byte
197
4
92
212
/
/
vsubps
%
ymm4
%
ymm15
%
ymm10
.
byte
196
98
117
168
212
/
/
vfmadd213ps
%
ymm4
%
ymm1
%
ymm10
.
byte
196
193
68
92
248
/
/
vsubps
%
ymm8
%
ymm7
%
ymm7
.
byte
196
194
117
168
248
/
/
vfmadd213ps
%
ymm8
%
ymm1
%
ymm7
.
byte
197
252
91
140
36
96
2
0
0
/
/
vcvtdq2ps
0x260
(
%
rsp
)
%
ymm1
.
byte
197
124
16
132
36
224
1
0
0
/
/
vmovups
0x1e0
(
%
rsp
)
%
ymm8
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
193
28
92
225
/
/
vsubps
%
ymm9
%
ymm12
%
ymm4
.
byte
196
194
117
168
225
/
/
vfmadd213ps
%
ymm9
%
ymm1
%
ymm4
.
byte
196
193
84
92
237
/
/
vsubps
%
ymm13
%
ymm5
%
ymm5
.
byte
196
194
117
168
237
/
/
vfmadd213ps
%
ymm13
%
ymm1
%
ymm5
.
byte
196
193
76
92
246
/
/
vsubps
%
ymm14
%
ymm6
%
ymm6
.
byte
196
194
117
168
246
/
/
vfmadd213ps
%
ymm14
%
ymm1
%
ymm6
.
byte
197
36
92
192
/
/
vsubps
%
ymm0
%
ymm11
%
ymm8
.
byte
196
98
117
168
192
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm8
.
byte
197
44
92
202
/
/
vsubps
%
ymm2
%
ymm10
%
ymm9
.
byte
196
98
117
168
202
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm9
.
byte
197
196
92
211
/
/
vsubps
%
ymm3
%
ymm7
%
ymm2
.
byte
196
226
117
168
211
/
/
vfmadd213ps
%
ymm3
%
ymm1
%
ymm2
.
byte
197
252
91
132
36
128
1
0
0
/
/
vcvtdq2ps
0x180
(
%
rsp
)
%
ymm0
.
byte
197
252
16
140
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm1
.
byte
197
244
92
216
/
/
vsubps
%
ymm0
%
ymm1
%
ymm3
.
byte
197
188
92
196
/
/
vsubps
%
ymm4
%
ymm8
%
ymm0
.
byte
196
226
101
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
180
92
205
/
/
vsubps
%
ymm5
%
ymm9
%
ymm1
.
byte
196
226
101
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
226
101
168
214
/
/
vfmadd213ps
%
ymm6
%
ymm3
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
156
36
128
2
0
0
/
/
vmovups
0x280
(
%
rsp
)
%
ymm3
.
byte
197
252
16
164
36
160
2
0
0
/
/
vmovups
0x2a0
(
%
rsp
)
%
ymm4
.
byte
197
252
16
172
36
192
2
0
0
/
/
vmovups
0x2c0
(
%
rsp
)
%
ymm5
.
byte
197
252
16
180
36
224
2
0
0
/
/
vmovups
0x2e0
(
%
rsp
)
%
ymm6
.
byte
197
252
16
188
36
0
3
0
0
/
/
vmovups
0x300
(
%
rsp
)
%
ymm7
.
byte
72
129
196
56
3
0
0
/
/
add
0x338
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_4D_hsw
.
globl
_sk_clut_4D_hsw
FUNCTION
(
_sk_clut_4D_hsw
)
_sk_clut_4D_hsw
:
.
byte
72
129
236
248
6
0
0
/
/
sub
0x6f8
%
rsp
.
byte
197
252
17
188
36
192
6
0
0
/
/
vmovups
%
ymm7
0x6c0
(
%
rsp
)
.
byte
197
252
17
180
36
160
6
0
0
/
/
vmovups
%
ymm6
0x6a0
(
%
rsp
)
.
byte
197
252
17
172
36
128
6
0
0
/
/
vmovups
%
ymm5
0x680
(
%
rsp
)
.
byte
197
252
17
164
36
96
6
0
0
/
/
vmovups
%
ymm4
0x660
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
20
/
/
mov
0x14
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
225
/
/
vmovd
%
r9d
%
xmm4
.
byte
196
226
125
88
228
/
/
vpbroadcastd
%
xmm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
197
220
89
219
/
/
vmulps
%
ymm3
%
ymm4
%
ymm3
.
byte
197
252
17
156
36
32
1
0
0
/
/
vmovups
%
ymm3
0x120
(
%
rsp
)
.
byte
197
254
91
243
/
/
vcvttps2dq
%
ymm3
%
ymm6
.
byte
196
193
121
110
216
/
/
vmovd
%
r8d
%
xmm3
.
byte
196
193
121
110
224
/
/
vmovd
%
r8d
%
xmm4
.
byte
196
98
125
88
252
/
/
vpbroadcastd
%
xmm4
%
ymm15
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
233
/
/
vmovd
%
r9d
%
xmm5
.
byte
196
226
125
88
237
/
/
vpbroadcastd
%
xmm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
197
212
89
210
/
/
vmulps
%
ymm2
%
ymm5
%
ymm2
.
byte
197
254
91
234
/
/
vcvttps2dq
%
ymm2
%
ymm5
.
byte
197
254
127
172
36
64
6
0
0
/
/
vmovdqu
%
ymm5
0x640
(
%
rsp
)
.
byte
197
124
40
242
/
/
vmovaps
%
ymm2
%
ymm14
.
byte
197
124
17
180
36
224
5
0
0
/
/
vmovups
%
ymm14
0x5e0
(
%
rsp
)
.
byte
196
226
5
64
213
/
/
vpmulld
%
ymm5
%
ymm15
%
ymm2
.
byte
197
254
127
84
36
160
/
/
vmovdqu
%
ymm2
-
0x60
(
%
rsp
)
.
byte
197
237
254
230
/
/
vpaddd
%
ymm6
%
ymm2
%
ymm4
.
byte
197
125
111
230
/
/
vmovdqa
%
ymm6
%
ymm12
.
byte
197
126
127
164
36
192
5
0
0
/
/
vmovdqu
%
ymm12
0x5c0
(
%
rsp
)
.
byte
196
193
121
110
232
/
/
vmovd
%
r8d
%
xmm5
.
byte
196
226
85
64
235
/
/
vpmulld
%
ymm3
%
ymm5
%
ymm5
.
byte
196
226
125
88
221
/
/
vpbroadcastd
%
xmm5
%
ymm3
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
241
/
/
vmovd
%
r9d
%
xmm6
.
byte
196
226
125
88
246
/
/
vpbroadcastd
%
xmm6
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
197
204
89
241
/
/
vmulps
%
ymm1
%
ymm6
%
ymm6
.
byte
196
193
121
110
200
/
/
vmovd
%
r8d
%
xmm1
.
byte
196
226
117
64
205
/
/
vpmulld
%
ymm5
%
ymm1
%
ymm1
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
196
193
121
110
232
/
/
vmovd
%
r8d
%
xmm5
.
byte
196
226
125
88
237
/
/
vpbroadcastd
%
xmm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
197
212
89
232
/
/
vmulps
%
ymm0
%
ymm5
%
ymm5
.
byte
197
254
91
198
/
/
vcvttps2dq
%
ymm6
%
ymm0
.
byte
197
254
127
132
36
32
6
0
0
/
/
vmovdqu
%
ymm0
0x620
(
%
rsp
)
.
byte
197
252
17
180
36
0
6
0
0
/
/
vmovups
%
ymm6
0x600
(
%
rsp
)
.
byte
196
226
125
88
209
/
/
vpbroadcastd
%
xmm1
%
ymm2
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
196
226
101
64
192
/
/
vpmulld
%
ymm0
%
ymm3
%
ymm0
.
byte
197
254
127
68
36
128
/
/
vmovdqu
%
ymm0
-
0x80
(
%
rsp
)
.
byte
197
253
254
252
/
/
vpaddd
%
ymm4
%
ymm0
%
ymm7
.
byte
197
254
91
197
/
/
vcvttps2dq
%
ymm5
%
ymm0
.
byte
197
252
40
205
/
/
vmovaps
%
ymm5
%
ymm1
.
byte
197
252
17
140
36
160
0
0
0
/
/
vmovups
%
ymm1
0xa0
(
%
rsp
)
.
byte
197
254
127
132
36
0
1
0
0
/
/
vmovdqu
%
ymm0
0x100
(
%
rsp
)
.
byte
196
98
109
64
200
/
/
vpmulld
%
ymm0
%
ymm2
%
ymm9
.
byte
197
181
254
199
/
/
vpaddd
%
ymm7
%
ymm9
%
ymm0
.
byte
196
98
125
88
21
43
205
2
0
/
/
vpbroadcastd
0x2cd2b
(
%
rip
)
%
ymm10
#
3c66c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x420
>
.
byte
196
194
125
64
194
/
/
vpmulld
%
ymm10
%
ymm0
%
ymm0
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
98
85
146
4
128
/
/
vgatherdps
%
ymm5
(
%
rax
%
ymm0
4
)
%
ymm8
.
byte
197
124
17
132
36
224
0
0
0
/
/
vmovups
%
ymm8
0xe0
(
%
rsp
)
.
byte
196
65
21
118
237
/
/
vpcmpeqd
%
ymm13
%
ymm13
%
ymm13
.
byte
196
193
125
250
237
/
/
vpsubd
%
ymm13
%
ymm0
%
ymm5
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
98
61
146
28
168
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm5
4
)
%
ymm11
.
byte
197
124
17
156
36
192
0
0
0
/
/
vmovups
%
ymm11
0xc0
(
%
rsp
)
.
byte
196
98
125
88
29
126
203
2
0
/
/
vpbroadcastd
0x2cb7e
(
%
rip
)
%
ymm11
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
196
193
125
254
195
/
/
vpaddd
%
ymm11
%
ymm0
%
ymm0
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
226
61
146
44
128
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm0
4
)
%
ymm5
.
byte
197
252
17
108
36
32
/
/
vmovups
%
ymm5
0x20
(
%
rsp
)
.
byte
196
226
125
24
5
187
204
2
0
/
/
vbroadcastss
0x2ccbb
(
%
rip
)
%
ymm0
#
3c668
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x41c
>
.
byte
197
116
88
192
/
/
vaddps
%
ymm0
%
ymm1
%
ymm8
.
byte
196
65
126
91
192
/
/
vcvttps2dq
%
ymm8
%
ymm8
.
byte
196
194
109
64
232
/
/
vpmulld
%
ymm8
%
ymm2
%
ymm5
.
byte
197
213
254
207
/
/
vpaddd
%
ymm7
%
ymm5
%
ymm1
.
byte
196
194
117
64
202
/
/
vpmulld
%
ymm10
%
ymm1
%
ymm1
.
byte
197
197
118
255
/
/
vpcmpeqd
%
ymm7
%
ymm7
%
ymm7
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
69
146
20
136
/
/
vgatherdps
%
ymm7
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
148
36
128
0
0
0
/
/
vmovups
%
ymm2
0x80
(
%
rsp
)
.
byte
196
193
117
250
253
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm7
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
61
146
20
184
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm7
4
)
%
ymm2
.
byte
197
252
17
84
36
96
/
/
vmovups
%
ymm2
0x60
(
%
rsp
)
.
byte
196
193
117
254
203
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
197
118
255
/
/
vpcmpeqd
%
ymm7
%
ymm7
%
ymm7
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
69
146
20
136
/
/
vgatherdps
%
ymm7
(
%
rax
%
ymm1
4
)
%
ymm2
.
byte
197
252
17
84
36
64
/
/
vmovups
%
ymm2
0x40
(
%
rsp
)
.
byte
197
204
88
200
/
/
vaddps
%
ymm0
%
ymm6
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
101
64
241
/
/
vpmulld
%
ymm1
%
ymm3
%
ymm6
.
byte
197
205
254
204
/
/
vpaddd
%
ymm4
%
ymm6
%
ymm1
.
byte
196
193
117
254
209
/
/
vpaddd
%
ymm9
%
ymm1
%
ymm2
.
byte
196
194
109
64
210
/
/
vpmulld
%
ymm10
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
96
5
0
0
/
/
vmovups
%
ymm4
0x560
(
%
rsp
)
.
byte
196
193
109
250
221
/
/
vpsubd
%
ymm13
%
ymm2
%
ymm3
.
byte
197
197
118
255
/
/
vpcmpeqd
%
ymm7
%
ymm7
%
ymm7
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
69
146
36
152
/
/
vgatherdps
%
ymm7
(
%
rax
%
ymm3
4
)
%
ymm4
.
byte
197
252
17
36
36
/
/
vmovups
%
ymm4
(
%
rsp
)
.
byte
196
193
109
254
211
/
/
vpaddd
%
ymm11
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
100
36
224
/
/
vmovups
%
ymm4
-
0x20
(
%
rsp
)
.
byte
197
213
254
201
/
/
vpaddd
%
ymm1
%
ymm5
%
ymm1
.
byte
196
194
117
64
202
/
/
vpmulld
%
ymm10
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
92
36
192
/
/
vmovups
%
ymm3
-
0x40
(
%
rsp
)
.
byte
196
193
117
250
213
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
160
5
0
0
/
/
vmovups
%
ymm4
0x5a0
(
%
rsp
)
.
byte
196
193
117
254
203
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
128
5
0
0
/
/
vmovups
%
ymm3
0x580
(
%
rsp
)
.
byte
197
140
88
200
/
/
vaddps
%
ymm0
%
ymm14
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
98
5
64
249
/
/
vpmulld
%
ymm1
%
ymm15
%
ymm15
.
byte
196
193
5
254
204
/
/
vpaddd
%
ymm12
%
ymm15
%
ymm1
.
byte
197
126
111
68
36
128
/
/
vmovdqu
-
0x80
(
%
rsp
)
%
ymm8
.
byte
196
193
117
254
208
/
/
vpaddd
%
ymm8
%
ymm1
%
ymm2
.
byte
196
193
109
254
217
/
/
vpaddd
%
ymm9
%
ymm2
%
ymm3
.
byte
196
194
101
64
218
/
/
vpmulld
%
ymm10
%
ymm3
%
ymm3
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
196
226
93
146
60
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm7
.
byte
197
252
17
188
36
96
3
0
0
/
/
vmovups
%
ymm7
0x360
(
%
rsp
)
.
byte
196
193
101
250
229
/
/
vpsubd
%
ymm13
%
ymm3
%
ymm4
.
byte
197
197
118
255
/
/
vpcmpeqd
%
ymm7
%
ymm7
%
ymm7
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
98
69
146
52
160
/
/
vgatherdps
%
ymm7
(
%
rax
%
ymm4
4
)
%
ymm14
.
byte
197
124
17
180
36
64
5
0
0
/
/
vmovups
%
ymm14
0x540
(
%
rsp
)
.
byte
196
193
101
254
219
/
/
vpaddd
%
ymm11
%
ymm3
%
ymm3
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
196
226
93
146
60
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm7
.
byte
197
252
17
188
36
192
4
0
0
/
/
vmovups
%
ymm7
0x4c0
(
%
rsp
)
.
byte
197
213
254
210
/
/
vpaddd
%
ymm2
%
ymm5
%
ymm2
.
byte
196
194
109
64
210
/
/
vpmulld
%
ymm10
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
32
5
0
0
/
/
vmovups
%
ymm4
0x520
(
%
rsp
)
.
byte
196
193
109
250
221
/
/
vpsubd
%
ymm13
%
ymm2
%
ymm3
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
98
93
146
52
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm14
.
byte
197
124
17
180
36
0
5
0
0
/
/
vmovups
%
ymm14
0x500
(
%
rsp
)
.
byte
196
193
109
254
211
/
/
vpaddd
%
ymm11
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
224
4
0
0
/
/
vmovups
%
ymm4
0x4e0
(
%
rsp
)
.
byte
197
205
254
201
/
/
vpaddd
%
ymm1
%
ymm6
%
ymm1
.
byte
196
193
117
254
209
/
/
vpaddd
%
ymm9
%
ymm1
%
ymm2
.
byte
196
194
109
64
210
/
/
vpmulld
%
ymm10
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
160
4
0
0
/
/
vmovups
%
ymm4
0x4a0
(
%
rsp
)
.
byte
196
193
109
250
221
/
/
vpsubd
%
ymm13
%
ymm2
%
ymm3
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
196
226
93
146
60
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm7
.
byte
197
252
17
188
36
128
4
0
0
/
/
vmovups
%
ymm7
0x480
(
%
rsp
)
.
byte
196
193
109
254
211
/
/
vpaddd
%
ymm11
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
0
3
0
0
/
/
vmovups
%
ymm4
0x300
(
%
rsp
)
.
byte
197
213
254
201
/
/
vpaddd
%
ymm1
%
ymm5
%
ymm1
.
byte
196
194
117
64
202
/
/
vpmulld
%
ymm10
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
96
4
0
0
/
/
vmovups
%
ymm3
0x460
(
%
rsp
)
.
byte
196
193
117
250
213
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
64
4
0
0
/
/
vmovups
%
ymm4
0x440
(
%
rsp
)
.
byte
196
193
117
254
203
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
32
4
0
0
/
/
vmovups
%
ymm3
0x420
(
%
rsp
)
.
byte
197
252
88
132
36
32
1
0
0
/
/
vaddps
0x120
(
%
rsp
)
%
ymm0
%
ymm0
.
byte
197
254
91
248
/
/
vcvttps2dq
%
ymm0
%
ymm7
.
byte
197
197
254
68
36
160
/
/
vpaddd
-
0x60
(
%
rsp
)
%
ymm7
%
ymm0
.
byte
197
189
254
200
/
/
vpaddd
%
ymm0
%
ymm8
%
ymm1
.
byte
197
181
254
209
/
/
vpaddd
%
ymm1
%
ymm9
%
ymm2
.
byte
196
194
109
64
210
/
/
vpmulld
%
ymm10
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
100
36
160
/
/
vmovups
%
ymm4
-
0x60
(
%
rsp
)
.
byte
196
193
109
250
221
/
/
vpsubd
%
ymm13
%
ymm2
%
ymm3
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
98
93
146
52
152
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm3
4
)
%
ymm14
.
byte
197
124
17
180
36
0
4
0
0
/
/
vmovups
%
ymm14
0x400
(
%
rsp
)
.
byte
196
193
109
254
211
/
/
vpaddd
%
ymm11
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
224
3
0
0
/
/
vmovups
%
ymm4
0x3e0
(
%
rsp
)
.
byte
197
213
254
201
/
/
vpaddd
%
ymm1
%
ymm5
%
ymm1
.
byte
196
194
117
64
202
/
/
vpmulld
%
ymm10
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
192
3
0
0
/
/
vmovups
%
ymm3
0x3c0
(
%
rsp
)
.
byte
196
193
117
250
213
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
160
3
0
0
/
/
vmovups
%
ymm4
0x3a0
(
%
rsp
)
.
byte
196
193
117
254
203
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
128
3
0
0
/
/
vmovups
%
ymm3
0x380
(
%
rsp
)
.
byte
197
205
254
192
/
/
vpaddd
%
ymm0
%
ymm6
%
ymm0
.
byte
196
193
125
254
201
/
/
vpaddd
%
ymm9
%
ymm0
%
ymm1
.
byte
196
194
117
64
202
/
/
vpmulld
%
ymm10
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
32
3
0
0
/
/
vmovups
%
ymm3
0x320
(
%
rsp
)
.
byte
196
193
117
250
213
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
164
36
64
3
0
0
/
/
vmovups
%
ymm4
0x340
(
%
rsp
)
.
byte
196
193
117
254
203
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
128
2
0
0
/
/
vmovups
%
ymm3
0x280
(
%
rsp
)
.
byte
197
213
254
192
/
/
vpaddd
%
ymm0
%
ymm5
%
ymm0
.
byte
196
194
125
64
194
/
/
vpmulld
%
ymm10
%
ymm0
%
ymm0
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
117
146
20
128
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm0
4
)
%
ymm2
.
byte
197
252
17
148
36
224
2
0
0
/
/
vmovups
%
ymm2
0x2e0
(
%
rsp
)
.
byte
196
193
125
250
205
/
/
vpsubd
%
ymm13
%
ymm0
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
192
2
0
0
/
/
vmovups
%
ymm3
0x2c0
(
%
rsp
)
.
byte
196
193
125
254
195
/
/
vpaddd
%
ymm11
%
ymm0
%
ymm0
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
226
117
146
20
128
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm0
4
)
%
ymm2
.
byte
197
252
17
148
36
160
2
0
0
/
/
vmovups
%
ymm2
0x2a0
(
%
rsp
)
.
byte
197
133
254
199
/
/
vpaddd
%
ymm7
%
ymm15
%
ymm0
.
byte
196
193
125
254
200
/
/
vpaddd
%
ymm8
%
ymm0
%
ymm1
.
byte
196
193
117
254
209
/
/
vpaddd
%
ymm9
%
ymm1
%
ymm2
.
byte
196
194
109
64
210
/
/
vpmulld
%
ymm10
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
101
146
36
144
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm2
4
)
%
ymm4
.
byte
197
252
17
100
36
128
/
/
vmovups
%
ymm4
-
0x80
(
%
rsp
)
.
byte
196
193
109
250
221
/
/
vpsubd
%
ymm13
%
ymm2
%
ymm3
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
226
61
146
36
152
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm3
4
)
%
ymm4
.
byte
197
252
17
164
36
96
2
0
0
/
/
vmovups
%
ymm4
0x260
(
%
rsp
)
.
byte
196
193
109
254
211
/
/
vpaddd
%
ymm11
%
ymm2
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
61
146
28
144
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm2
4
)
%
ymm3
.
byte
197
252
17
156
36
224
1
0
0
/
/
vmovups
%
ymm3
0x1e0
(
%
rsp
)
.
byte
197
213
254
201
/
/
vpaddd
%
ymm1
%
ymm5
%
ymm1
.
byte
196
194
117
64
202
/
/
vpmulld
%
ymm10
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
64
2
0
0
/
/
vmovups
%
ymm3
0x240
(
%
rsp
)
.
byte
196
193
117
250
213
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm2
.
byte
196
65
61
118
192
/
/
vpcmpeqd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
61
146
28
144
/
/
vgatherdps
%
ymm8
(
%
rax
%
ymm2
4
)
%
ymm3
.
byte
197
252
17
156
36
32
2
0
0
/
/
vmovups
%
ymm3
0x220
(
%
rsp
)
.
byte
196
193
117
254
203
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
226
109
146
28
136
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm1
4
)
%
ymm3
.
byte
197
252
17
156
36
0
2
0
0
/
/
vmovups
%
ymm3
0x200
(
%
rsp
)
.
byte
197
205
254
216
/
/
vpaddd
%
ymm0
%
ymm6
%
ymm3
.
byte
196
193
101
254
193
/
/
vpaddd
%
ymm9
%
ymm3
%
ymm0
.
byte
196
194
125
64
194
/
/
vpmulld
%
ymm10
%
ymm0
%
ymm0
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
226
109
146
12
128
/
/
vgatherdps
%
ymm2
(
%
rax
%
ymm0
4
)
%
ymm1
.
byte
197
252
17
140
36
192
1
0
0
/
/
vmovups
%
ymm1
0x1c0
(
%
rsp
)
.
byte
196
193
125
250
229
/
/
vpsubd
%
ymm13
%
ymm0
%
ymm4
.
byte
197
245
118
201
/
/
vpcmpeqd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
29
239
228
/
/
vpxor
%
ymm12
%
ymm12
%
ymm12
.
byte
196
98
117
146
36
160
/
/
vgatherdps
%
ymm1
(
%
rax
%
ymm4
4
)
%
ymm12
.
byte
196
193
125
254
203
/
/
vpaddd
%
ymm11
%
ymm0
%
ymm1
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
196
226
93
146
4
136
/
/
vgatherdps
%
ymm4
(
%
rax
%
ymm1
4
)
%
ymm0
.
byte
197
252
17
132
36
160
1
0
0
/
/
vmovups
%
ymm0
0x1a0
(
%
rsp
)
.
byte
197
213
254
203
/
/
vpaddd
%
ymm3
%
ymm5
%
ymm1
.
byte
196
194
117
64
202
/
/
vpmulld
%
ymm10
%
ymm1
%
ymm1
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
196
226
101
146
4
136
/
/
vgatherdps
%
ymm3
(
%
rax
%
ymm1
4
)
%
ymm0
.
byte
197
252
17
132
36
128
1
0
0
/
/
vmovups
%
ymm0
0x180
(
%
rsp
)
.
byte
196
193
117
250
221
/
/
vpsubd
%
ymm13
%
ymm1
%
ymm3
.
byte
196
65
45
118
210
/
/
vpcmpeqd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
196
226
45
146
4
152
/
/
vgatherdps
%
ymm10
(
%
rax
%
ymm3
4
)
%
ymm0
.
byte
197
252
17
132
36
96
1
0
0
/
/
vmovups
%
ymm0
0x160
(
%
rsp
)
.
byte
196
193
117
254
203
/
/
vpaddd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
196
226
21
146
4
136
/
/
vgatherdps
%
ymm13
(
%
rax
%
ymm1
4
)
%
ymm0
.
byte
197
252
17
132
36
64
1
0
0
/
/
vmovups
%
ymm0
0x140
(
%
rsp
)
.
byte
197
252
91
140
36
0
1
0
0
/
/
vcvtdq2ps
0x100
(
%
rsp
)
%
ymm1
.
byte
197
252
16
132
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm0
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
197
252
16
132
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm2
.
byte
197
108
92
248
/
/
vsubps
%
ymm0
%
ymm2
%
ymm15
.
byte
196
98
117
168
248
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm15
.
byte
197
252
16
132
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
84
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm2
.
byte
197
108
92
240
/
/
vsubps
%
ymm0
%
ymm2
%
ymm14
.
byte
196
98
117
168
240
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm14
.
byte
197
252
16
108
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
16
68
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
92
197
/
/
vsubps
%
ymm5
%
ymm0
%
ymm0
.
byte
196
226
117
168
197
/
/
vfmadd213ps
%
ymm5
%
ymm1
%
ymm0
.
byte
197
252
17
132
36
0
1
0
0
/
/
vmovups
%
ymm0
0x100
(
%
rsp
)
.
byte
197
252
16
68
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
96
5
0
0
/
/
vmovups
0x560
(
%
rsp
)
%
ymm2
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
117
168
194
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm0
.
byte
197
252
17
132
36
224
0
0
0
/
/
vmovups
%
ymm0
0xe0
(
%
rsp
)
.
byte
197
252
16
4
36
/
/
vmovups
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
160
5
0
0
/
/
vmovups
0x5a0
(
%
rsp
)
%
ymm2
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
117
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm2
.
byte
197
252
17
148
36
192
0
0
0
/
/
vmovups
%
ymm2
0xc0
(
%
rsp
)
.
byte
197
252
16
68
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
128
5
0
0
/
/
vmovups
0x580
(
%
rsp
)
%
ymm2
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
117
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm2
.
byte
197
252
17
148
36
160
0
0
0
/
/
vmovups
%
ymm2
0xa0
(
%
rsp
)
.
byte
197
252
16
132
36
32
5
0
0
/
/
vmovups
0x520
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
96
3
0
0
/
/
vmovups
0x360
(
%
rsp
)
%
ymm2
.
byte
197
124
92
210
/
/
vsubps
%
ymm2
%
ymm0
%
ymm10
.
byte
196
98
117
168
210
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm10
.
byte
197
252
16
132
36
64
5
0
0
/
/
vmovups
0x540
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
0
5
0
0
/
/
vmovups
0x500
(
%
rsp
)
%
ymm2
.
byte
197
108
92
216
/
/
vsubps
%
ymm0
%
ymm2
%
ymm11
.
byte
196
98
117
168
216
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm11
.
byte
197
252
16
132
36
224
4
0
0
/
/
vmovups
0x4e0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
192
4
0
0
/
/
vmovups
0x4c0
(
%
rsp
)
%
ymm2
.
byte
197
124
92
202
/
/
vsubps
%
ymm2
%
ymm0
%
ymm9
.
byte
196
98
117
168
202
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm9
.
byte
197
252
16
132
36
160
4
0
0
/
/
vmovups
0x4a0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
96
4
0
0
/
/
vmovups
0x460
(
%
rsp
)
%
ymm2
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
117
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm2
.
byte
197
252
17
148
36
128
0
0
0
/
/
vmovups
%
ymm2
0x80
(
%
rsp
)
.
byte
197
252
16
132
36
128
4
0
0
/
/
vmovups
0x480
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
64
4
0
0
/
/
vmovups
0x440
(
%
rsp
)
%
ymm2
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
117
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm2
.
byte
197
252
17
84
36
96
/
/
vmovups
%
ymm2
0x60
(
%
rsp
)
.
byte
197
252
16
132
36
32
4
0
0
/
/
vmovups
0x420
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
0
3
0
0
/
/
vmovups
0x300
(
%
rsp
)
%
ymm2
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
117
168
194
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm0
.
byte
197
252
17
68
36
64
/
/
vmovups
%
ymm0
0x40
(
%
rsp
)
.
byte
197
252
16
68
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
192
3
0
0
/
/
vmovups
0x3c0
(
%
rsp
)
%
ymm2
.
byte
197
236
92
248
/
/
vsubps
%
ymm0
%
ymm2
%
ymm7
.
byte
196
226
117
168
248
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm7
.
byte
197
252
16
132
36
0
4
0
0
/
/
vmovups
0x400
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
160
3
0
0
/
/
vmovups
0x3a0
(
%
rsp
)
%
ymm2
.
byte
197
108
92
192
/
/
vsubps
%
ymm0
%
ymm2
%
ymm8
.
byte
196
98
117
168
192
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm8
.
byte
197
252
16
132
36
224
3
0
0
/
/
vmovups
0x3e0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
128
3
0
0
/
/
vmovups
0x380
(
%
rsp
)
%
ymm2
.
byte
197
236
92
240
/
/
vsubps
%
ymm0
%
ymm2
%
ymm6
.
byte
196
226
117
168
240
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm6
.
byte
197
252
16
132
36
32
3
0
0
/
/
vmovups
0x320
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
224
2
0
0
/
/
vmovups
0x2e0
(
%
rsp
)
%
ymm2
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
117
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm2
.
byte
197
252
17
84
36
32
/
/
vmovups
%
ymm2
0x20
(
%
rsp
)
.
byte
197
252
16
132
36
64
3
0
0
/
/
vmovups
0x340
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
192
2
0
0
/
/
vmovups
0x2c0
(
%
rsp
)
%
ymm2
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
117
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm2
.
byte
197
252
17
20
36
/
/
vmovups
%
ymm2
(
%
rsp
)
.
byte
197
252
16
132
36
160
2
0
0
/
/
vmovups
0x2a0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
128
2
0
0
/
/
vmovups
0x280
(
%
rsp
)
%
ymm2
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
117
168
194
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm0
.
byte
197
252
17
68
36
224
/
/
vmovups
%
ymm0
-
0x20
(
%
rsp
)
.
byte
197
252
16
68
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm0
.
byte
197
252
16
148
36
64
2
0
0
/
/
vmovups
0x240
(
%
rsp
)
%
ymm2
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
117
168
208
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm2
.
byte
197
252
16
132
36
96
2
0
0
/
/
vmovups
0x260
(
%
rsp
)
%
ymm0
.
byte
197
252
16
156
36
32
2
0
0
/
/
vmovups
0x220
(
%
rsp
)
%
ymm3
.
byte
197
228
92
216
/
/
vsubps
%
ymm0
%
ymm3
%
ymm3
.
byte
196
226
117
168
216
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm3
.
byte
197
252
16
132
36
0
2
0
0
/
/
vmovups
0x200
(
%
rsp
)
%
ymm0
.
byte
197
252
16
172
36
224
1
0
0
/
/
vmovups
0x1e0
(
%
rsp
)
%
ymm5
.
byte
197
252
92
229
/
/
vsubps
%
ymm5
%
ymm0
%
ymm4
.
byte
196
226
117
168
229
/
/
vfmadd213ps
%
ymm5
%
ymm1
%
ymm4
.
byte
197
252
16
132
36
192
1
0
0
/
/
vmovups
0x1c0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
172
36
128
1
0
0
/
/
vmovups
0x180
(
%
rsp
)
%
ymm5
.
byte
197
84
92
232
/
/
vsubps
%
ymm0
%
ymm5
%
ymm13
.
byte
196
98
117
168
232
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm13
.
byte
197
252
16
132
36
96
1
0
0
/
/
vmovups
0x160
(
%
rsp
)
%
ymm0
.
byte
196
193
124
92
196
/
/
vsubps
%
ymm12
%
ymm0
%
ymm0
.
byte
196
194
117
168
196
/
/
vfmadd213ps
%
ymm12
%
ymm1
%
ymm0
.
byte
197
252
17
68
36
192
/
/
vmovups
%
ymm0
-
0x40
(
%
rsp
)
.
byte
197
252
16
132
36
160
1
0
0
/
/
vmovups
0x1a0
(
%
rsp
)
%
ymm0
.
byte
197
252
16
172
36
64
1
0
0
/
/
vmovups
0x140
(
%
rsp
)
%
ymm5
.
byte
197
84
92
224
/
/
vsubps
%
ymm0
%
ymm5
%
ymm12
.
byte
196
98
117
168
224
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm12
.
byte
197
252
91
140
36
32
6
0
0
/
/
vcvtdq2ps
0x620
(
%
rsp
)
%
ymm1
.
byte
197
252
16
132
36
0
6
0
0
/
/
vmovups
0x600
(
%
rsp
)
%
ymm0
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
197
252
16
132
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm0
.
byte
196
193
124
92
199
/
/
vsubps
%
ymm15
%
ymm0
%
ymm0
.
byte
196
194
117
168
199
/
/
vfmadd213ps
%
ymm15
%
ymm1
%
ymm0
.
byte
197
252
16
172
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm5
.
byte
196
65
84
92
254
/
/
vsubps
%
ymm14
%
ymm5
%
ymm15
.
byte
196
66
117
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm1
%
ymm15
.
byte
197
252
16
172
36
0
1
0
0
/
/
vmovups
0x100
(
%
rsp
)
%
ymm5
.
byte
197
124
16
180
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm14
.
byte
197
12
92
245
/
/
vsubps
%
ymm5
%
ymm14
%
ymm14
.
byte
196
98
117
168
245
/
/
vfmadd213ps
%
ymm5
%
ymm1
%
ymm14
.
byte
197
252
16
172
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm5
.
byte
196
193
84
92
234
/
/
vsubps
%
ymm10
%
ymm5
%
ymm5
.
byte
196
194
117
168
234
/
/
vfmadd213ps
%
ymm10
%
ymm1
%
ymm5
.
byte
197
124
16
84
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm10
.
byte
196
65
44
92
211
/
/
vsubps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
66
117
168
211
/
/
vfmadd213ps
%
ymm11
%
ymm1
%
ymm10
.
byte
197
124
16
92
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm11
.
byte
196
65
36
92
217
/
/
vsubps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
66
117
168
217
/
/
vfmadd213ps
%
ymm9
%
ymm1
%
ymm11
.
byte
197
124
16
76
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm9
.
byte
197
52
92
207
/
/
vsubps
%
ymm7
%
ymm9
%
ymm9
.
byte
196
98
117
168
207
/
/
vfmadd213ps
%
ymm7
%
ymm1
%
ymm9
.
byte
197
252
16
60
36
/
/
vmovups
(
%
rsp
)
%
ymm7
.
byte
196
193
68
92
248
/
/
vsubps
%
ymm8
%
ymm7
%
ymm7
.
byte
196
194
117
168
248
/
/
vfmadd213ps
%
ymm8
%
ymm1
%
ymm7
.
byte
197
124
16
68
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm8
.
byte
197
60
92
198
/
/
vsubps
%
ymm6
%
ymm8
%
ymm8
.
byte
196
98
117
168
198
/
/
vfmadd213ps
%
ymm6
%
ymm1
%
ymm8
.
byte
197
148
92
242
/
/
vsubps
%
ymm2
%
ymm13
%
ymm6
.
byte
196
226
117
168
242
/
/
vfmadd213ps
%
ymm2
%
ymm1
%
ymm6
.
byte
197
252
16
84
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm2
.
byte
197
236
92
211
/
/
vsubps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
226
117
168
211
/
/
vfmadd213ps
%
ymm3
%
ymm1
%
ymm2
.
byte
197
156
92
220
/
/
vsubps
%
ymm4
%
ymm12
%
ymm3
.
byte
196
226
117
168
220
/
/
vfmadd213ps
%
ymm4
%
ymm1
%
ymm3
.
byte
197
252
91
140
36
64
6
0
0
/
/
vcvtdq2ps
0x640
(
%
rsp
)
%
ymm1
.
byte
197
252
16
164
36
224
5
0
0
/
/
vmovups
0x5e0
(
%
rsp
)
%
ymm4
.
byte
197
220
92
201
/
/
vsubps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
212
92
224
/
/
vsubps
%
ymm0
%
ymm5
%
ymm4
.
byte
196
226
117
168
224
/
/
vfmadd213ps
%
ymm0
%
ymm1
%
ymm4
.
byte
196
193
44
92
239
/
/
vsubps
%
ymm15
%
ymm10
%
ymm5
.
byte
196
194
117
168
239
/
/
vfmadd213ps
%
ymm15
%
ymm1
%
ymm5
.
byte
196
65
36
92
214
/
/
vsubps
%
ymm14
%
ymm11
%
ymm10
.
byte
196
66
117
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm1
%
ymm10
.
byte
196
193
76
92
193
/
/
vsubps
%
ymm9
%
ymm6
%
ymm0
.
byte
196
194
117
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm1
%
ymm0
.
byte
197
236
92
215
/
/
vsubps
%
ymm7
%
ymm2
%
ymm2
.
byte
196
226
117
168
215
/
/
vfmadd213ps
%
ymm7
%
ymm1
%
ymm2
.
byte
196
193
100
92
216
/
/
vsubps
%
ymm8
%
ymm3
%
ymm3
.
byte
196
194
117
168
216
/
/
vfmadd213ps
%
ymm8
%
ymm1
%
ymm3
.
byte
197
252
91
140
36
192
5
0
0
/
/
vcvtdq2ps
0x5c0
(
%
rsp
)
%
ymm1
.
byte
197
252
16
180
36
32
1
0
0
/
/
vmovups
0x120
(
%
rsp
)
%
ymm6
.
byte
197
204
92
241
/
/
vsubps
%
ymm1
%
ymm6
%
ymm6
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
226
77
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm6
%
ymm0
.
byte
197
236
92
205
/
/
vsubps
%
ymm5
%
ymm2
%
ymm1
.
byte
196
226
77
168
205
/
/
vfmadd213ps
%
ymm5
%
ymm6
%
ymm1
.
byte
196
193
100
92
210
/
/
vsubps
%
ymm10
%
ymm3
%
ymm2
.
byte
196
194
77
168
210
/
/
vfmadd213ps
%
ymm10
%
ymm6
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
161
193
2
0
/
/
vbroadcastss
0x2c1a1
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
16
164
36
96
6
0
0
/
/
vmovups
0x660
(
%
rsp
)
%
ymm4
.
byte
197
252
16
172
36
128
6
0
0
/
/
vmovups
0x680
(
%
rsp
)
%
ymm5
.
byte
197
252
16
180
36
160
6
0
0
/
/
vmovups
0x6a0
(
%
rsp
)
%
ymm6
.
byte
197
252
16
188
36
192
6
0
0
/
/
vmovups
0x6c0
(
%
rsp
)
%
ymm7
.
byte
72
129
196
248
6
0
0
/
/
add
0x6f8
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gauss_a_to_rgba_hsw
.
globl
_sk_gauss_a_to_rgba_hsw
FUNCTION
(
_sk_gauss_a_to_rgba_hsw
)
_sk_gauss_a_to_rgba_hsw
:
.
byte
196
226
125
24
5
227
194
2
0
/
/
vbroadcastss
0x2c2e3
(
%
rip
)
%
ymm0
#
3c674
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x428
>
.
byte
196
226
125
24
13
214
194
2
0
/
/
vbroadcastss
0x2c2d6
(
%
rip
)
%
ymm1
#
3c670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x424
>
.
byte
196
226
101
168
200
/
/
vfmadd213ps
%
ymm0
%
ymm3
%
ymm1
.
byte
196
226
125
24
5
208
194
2
0
/
/
vbroadcastss
0x2c2d0
(
%
rip
)
%
ymm0
#
3c678
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x42c
>
.
byte
196
226
101
184
193
/
/
vfmadd231ps
%
ymm1
%
ymm3
%
ymm0
.
byte
196
226
125
24
13
198
194
2
0
/
/
vbroadcastss
0x2c2c6
(
%
rip
)
%
ymm1
#
3c67c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x430
>
.
byte
196
226
101
184
200
/
/
vfmadd231ps
%
ymm0
%
ymm3
%
ymm1
.
byte
196
226
125
24
5
188
194
2
0
/
/
vbroadcastss
0x2c2bc
(
%
rip
)
%
ymm0
#
3c680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x434
>
.
byte
196
226
101
184
193
/
/
vfmadd231ps
%
ymm1
%
ymm3
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
252
40
216
/
/
vmovaps
%
ymm0
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilerp_clamp_8888_hsw
.
globl
_sk_bilerp_clamp_8888_hsw
FUNCTION
(
_sk_bilerp_clamp_8888_hsw
)
_sk_bilerp_clamp_8888_hsw
:
.
byte
72
129
236
248
0
0
0
/
/
sub
0xf8
%
rsp
.
byte
197
252
17
60
36
/
/
vmovups
%
ymm7
(
%
rsp
)
.
byte
197
252
17
116
36
224
/
/
vmovups
%
ymm6
-
0x20
(
%
rsp
)
.
byte
197
252
17
108
36
192
/
/
vmovups
%
ymm5
-
0x40
(
%
rsp
)
.
byte
197
252
17
100
36
160
/
/
vmovups
%
ymm4
-
0x60
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
37
246
192
2
0
/
/
vbroadcastss
0x2c0f6
(
%
rip
)
%
ymm4
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
88
212
/
/
vaddps
%
ymm4
%
ymm0
%
ymm2
.
byte
196
227
125
8
218
1
/
/
vroundps
0x1
%
ymm2
%
ymm3
.
byte
197
108
92
235
/
/
vsubps
%
ymm3
%
ymm2
%
ymm13
.
byte
197
252
17
140
36
160
0
0
0
/
/
vmovups
%
ymm1
0xa0
(
%
rsp
)
.
byte
197
244
88
204
/
/
vaddps
%
ymm4
%
ymm1
%
ymm1
.
byte
196
227
125
8
209
1
/
/
vroundps
0x1
%
ymm1
%
ymm2
.
byte
197
244
92
210
/
/
vsubps
%
ymm2
%
ymm1
%
ymm2
.
byte
196
226
125
24
13
204
192
2
0
/
/
vbroadcastss
0x2c0cc
(
%
rip
)
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
116
92
221
/
/
vsubps
%
ymm13
%
ymm1
%
ymm3
.
byte
197
252
17
156
36
192
0
0
0
/
/
vmovups
%
ymm3
0xc0
(
%
rsp
)
.
byte
197
252
17
148
36
128
0
0
0
/
/
vmovups
%
ymm2
0x80
(
%
rsp
)
.
byte
197
244
92
202
/
/
vsubps
%
ymm2
%
ymm1
%
ymm1
.
byte
197
252
17
76
36
32
/
/
vmovups
%
ymm1
0x20
(
%
rsp
)
.
byte
196
226
125
88
72
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
117
254
242
/
/
vpaddd
%
ymm2
%
ymm1
%
ymm14
.
byte
196
226
125
88
72
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm1
.
byte
197
245
254
202
/
/
vpaddd
%
ymm2
%
ymm1
%
ymm1
.
byte
197
254
127
76
36
96
/
/
vmovdqu
%
ymm1
0x60
(
%
rsp
)
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
196
226
125
24
72
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm1
.
byte
197
252
17
76
36
64
/
/
vmovups
%
ymm1
0x40
(
%
rsp
)
.
byte
197
250
16
13
202
193
2
0
/
/
vmovss
0x2c1ca
(
%
rip
)
%
xmm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
226
125
24
61
229
192
2
0
/
/
vbroadcastss
0x2c0e5
(
%
rip
)
%
ymm7
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
248
40
233
/
/
vmovaps
%
xmm1
%
xmm5
.
byte
196
226
125
24
205
/
/
vbroadcastss
%
xmm5
%
ymm1
.
byte
197
244
88
140
36
160
0
0
0
/
/
vaddps
0xa0
(
%
rsp
)
%
ymm1
%
ymm1
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
220
95
201
/
/
vmaxps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
248
41
108
36
144
/
/
vmovaps
%
xmm5
-
0x70
(
%
rsp
)
.
byte
197
248
46
45
195
193
2
0
/
/
vucomiss
0x2c1c3
(
%
rip
)
%
xmm5
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
197
124
16
164
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm12
.
byte
119
6
/
/
ja
104da
<
_sk_bilerp_clamp_8888_hsw
+
0x101
>
.
byte
197
124
16
100
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm12
.
byte
197
244
93
76
36
96
/
/
vminps
0x60
(
%
rsp
)
%
ymm1
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
226
117
64
100
36
64
/
/
vpmulld
0x40
(
%
rsp
)
%
ymm1
%
ymm4
.
byte
197
250
16
53
93
193
2
0
/
/
vmovss
0x2c15d
(
%
rip
)
%
xmm6
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
226
125
24
206
/
/
vbroadcastss
%
xmm6
%
ymm1
.
byte
197
244
88
200
/
/
vaddps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
212
95
201
/
/
vmaxps
%
ymm1
%
ymm5
%
ymm1
.
byte
196
193
116
93
206
/
/
vminps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
221
254
201
/
/
vpaddd
%
ymm1
%
ymm4
%
ymm1
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
65
45
239
210
/
/
vpxor
%
ymm10
%
ymm10
%
ymm10
.
byte
196
66
85
144
20
136
/
/
vpgatherdd
%
ymm5
(
%
r8
%
ymm1
4
)
%
ymm10
.
byte
197
173
219
13
56
197
2
0
/
/
vpand
0x2c538
(
%
rip
)
%
ymm10
%
ymm1
#
3ca60
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x814
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
116
89
223
/
/
vmulps
%
ymm7
%
ymm1
%
ymm11
.
byte
196
226
45
0
13
71
197
2
0
/
/
vpshufb
0x2c547
(
%
rip
)
%
ymm10
%
ymm1
#
3ca80
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x834
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
244
89
239
/
/
vmulps
%
ymm7
%
ymm1
%
ymm5
.
byte
196
226
45
0
13
86
197
2
0
/
/
vpshufb
0x2c556
(
%
rip
)
%
ymm10
%
ymm1
#
3caa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x854
>
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
196
193
45
114
210
24
/
/
vpsrld
0x18
%
ymm10
%
ymm10
.
byte
196
65
124
91
210
/
/
vcvtdq2ps
%
ymm10
%
ymm10
.
byte
197
44
89
255
/
/
vmulps
%
ymm7
%
ymm10
%
ymm15
.
byte
197
248
46
53
35
193
2
0
/
/
vucomiss
0x2c123
(
%
rip
)
%
xmm6
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
196
65
124
40
213
/
/
vmovaps
%
ymm13
%
ymm10
.
byte
119
9
/
/
ja
10579
<
_sk_bilerp_clamp_8888_hsw
+
0x1a0
>
.
byte
197
124
16
148
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
66
45
184
203
/
/
vfmadd231ps
%
ymm11
%
ymm10
%
ymm9
.
byte
196
98
45
184
197
/
/
vfmadd231ps
%
ymm5
%
ymm10
%
ymm8
.
byte
196
226
45
184
209
/
/
vfmadd231ps
%
ymm1
%
ymm10
%
ymm2
.
byte
196
194
45
184
223
/
/
vfmadd231ps
%
ymm15
%
ymm10
%
ymm3
.
byte
197
202
88
53
98
191
2
0
/
/
vaddss
0x2bf62
(
%
rip
)
%
xmm6
%
xmm6
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
250
16
13
86
191
2
0
/
/
vmovss
0x2bf56
(
%
rip
)
%
xmm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
248
46
206
/
/
vucomiss
%
xmm6
%
xmm1
.
byte
15
131
71
255
255
255
/
/
jae
104f3
<
_sk_bilerp_clamp_8888_hsw
+
0x11a
>
.
byte
197
248
40
108
36
144
/
/
vmovaps
-
0x70
(
%
rsp
)
%
xmm5
.
byte
197
210
88
45
66
191
2
0
/
/
vaddss
0x2bf42
(
%
rip
)
%
xmm5
%
xmm5
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
250
16
13
54
191
2
0
/
/
vmovss
0x2bf36
(
%
rip
)
%
xmm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
248
46
205
/
/
vucomiss
%
xmm5
%
xmm1
.
byte
15
131
217
254
255
255
/
/
jae
104a5
<
_sk_bilerp_clamp_8888_hsw
+
0xcc
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
200
/
/
vmovaps
%
ymm9
%
ymm0
.
byte
197
124
41
193
/
/
vmovaps
%
ymm8
%
ymm1
.
byte
197
252
16
100
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm6
.
byte
197
252
16
60
36
/
/
vmovups
(
%
rsp
)
%
ymm7
.
byte
72
129
196
248
0
0
0
/
/
add
0xf8
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
144
/
/
nop
HIDDEN
_sk_start_pipeline_avx
.
globl
_sk_start_pipeline_avx
FUNCTION
(
_sk_start_pipeline_avx
)
_sk_start_pipeline_avx
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
151
0
0
0
/
/
jae
106c2
<
_sk_start_pipeline_avx
+
0xca
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
8
/
/
lea
0x8
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
119
67
/
/
ja
10684
<
_sk_start_pipeline_avx
+
0x8c
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
8
/
/
lea
0x8
(
%
r12
)
%
rdx
.
byte
73
131
196
16
/
/
add
0x10
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
193
/
/
jbe
10645
<
_sk_start_pipeline_avx
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
41
/
/
je
106b5
<
_sk_start_pipeline_avx
+
0xbd
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
255
195
/
/
inc
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
15
133
117
255
255
255
/
/
jne
10637
<
_sk_start_pipeline_avx
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
197
248
119
/
/
vzeroupper
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_avx
.
globl
_sk_just_return_avx
FUNCTION
(
_sk_just_return_avx
)
_sk_just_return_avx
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_avx
.
globl
_sk_seed_shader_avx
FUNCTION
(
_sk_seed_shader_avx
)
_sk_seed_shader_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
249
110
194
/
/
vmovd
%
edx
%
xmm0
.
byte
197
249
112
192
0
/
/
vpshufd
0x0
%
xmm0
%
xmm0
.
byte
196
227
125
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
249
110
201
/
/
vmovd
%
ecx
%
xmm1
.
byte
197
249
112
201
0
/
/
vpshufd
0x0
%
xmm1
%
xmm1
.
byte
196
227
117
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
238
189
2
0
/
/
vbroadcastss
0x2bdee
(
%
rip
)
%
ymm2
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
244
88
202
/
/
vaddps
%
ymm2
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
21
227
189
2
0
/
/
vbroadcastss
0x2bde3
(
%
rip
)
%
ymm2
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dither_avx
.
globl
_sk_dither_avx
FUNCTION
(
_sk_dither_avx
)
_sk_dither_avx
:
.
byte
197
121
110
194
/
/
vmovd
%
edx
%
xmm8
.
byte
196
65
121
112
192
0
/
/
vpshufd
0x0
%
xmm8
%
xmm8
.
byte
197
57
254
13
159
199
2
0
/
/
vpaddd
0x2c79f
(
%
rip
)
%
xmm8
%
xmm9
#
3cee0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc94
>
.
byte
197
57
254
5
167
199
2
0
/
/
vpaddd
0x2c7a7
(
%
rip
)
%
xmm8
%
xmm8
#
3cef0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xca4
>
.
byte
196
67
53
24
208
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm10
.
byte
197
121
110
217
/
/
vmovd
%
ecx
%
xmm11
.
byte
196
65
121
112
219
0
/
/
vpshufd
0x0
%
xmm11
%
xmm11
.
byte
196
67
37
24
219
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm11
%
ymm11
.
byte
196
65
36
87
218
/
/
vxorps
%
ymm10
%
ymm11
%
ymm11
.
byte
196
98
125
24
21
147
189
2
0
/
/
vbroadcastss
0x2bd93
(
%
rip
)
%
ymm10
#
3c500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b4
>
.
byte
196
65
36
84
210
/
/
vandps
%
ymm10
%
ymm11
%
ymm10
.
byte
196
193
25
114
242
5
/
/
vpslld
0x5
%
xmm10
%
xmm12
.
byte
196
67
125
25
210
1
/
/
vextractf128
0x1
%
ymm10
%
xmm10
.
byte
196
193
41
114
242
5
/
/
vpslld
0x5
%
xmm10
%
xmm10
.
byte
196
67
29
24
210
1
/
/
vinsertf128
0x1
%
xmm10
%
ymm12
%
ymm10
.
byte
197
123
18
37
190
207
2
0
/
/
vmovddup
0x2cfbe
(
%
rip
)
%
xmm12
#
3d750
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1504
>
.
byte
196
65
49
219
236
/
/
vpand
%
xmm12
%
xmm9
%
xmm13
.
byte
196
193
17
114
245
4
/
/
vpslld
0x4
%
xmm13
%
xmm13
.
byte
196
65
57
219
228
/
/
vpand
%
xmm12
%
xmm8
%
xmm12
.
byte
196
193
25
114
244
4
/
/
vpslld
0x4
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
196
98
125
24
45
81
189
2
0
/
/
vbroadcastss
0x2bd51
(
%
rip
)
%
ymm13
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
196
65
36
84
237
/
/
vandps
%
ymm13
%
ymm11
%
ymm13
.
byte
196
193
9
114
245
2
/
/
vpslld
0x2
%
xmm13
%
xmm14
.
byte
196
67
125
25
237
1
/
/
vextractf128
0x1
%
ymm13
%
xmm13
.
byte
196
193
17
114
245
2
/
/
vpslld
0x2
%
xmm13
%
xmm13
.
byte
196
67
13
24
237
1
/
/
vinsertf128
0x1
%
xmm13
%
ymm14
%
ymm13
.
byte
197
123
18
53
124
207
2
0
/
/
vmovddup
0x2cf7c
(
%
rip
)
%
xmm14
#
3d758
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x150c
>
.
byte
196
65
49
219
254
/
/
vpand
%
xmm14
%
xmm9
%
xmm15
.
byte
196
65
1
254
255
/
/
vpaddd
%
xmm15
%
xmm15
%
xmm15
.
byte
196
65
57
219
246
/
/
vpand
%
xmm14
%
xmm8
%
xmm14
.
byte
196
65
9
254
246
/
/
vpaddd
%
xmm14
%
xmm14
%
xmm14
.
byte
196
67
5
24
246
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm15
%
ymm14
.
byte
196
65
12
86
228
/
/
vorps
%
ymm12
%
ymm14
%
ymm12
.
byte
196
98
125
24
53
0
189
2
0
/
/
vbroadcastss
0x2bd00
(
%
rip
)
%
ymm14
#
3c504
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b8
>
.
byte
196
65
36
84
222
/
/
vandps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
193
9
114
211
1
/
/
vpsrld
0x1
%
xmm11
%
xmm14
.
byte
196
67
125
25
219
1
/
/
vextractf128
0x1
%
ymm11
%
xmm11
.
byte
196
193
33
114
211
1
/
/
vpsrld
0x1
%
xmm11
%
xmm11
.
byte
196
67
13
24
219
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm14
%
ymm11
.
byte
196
65
20
86
219
/
/
vorps
%
ymm11
%
ymm13
%
ymm11
.
byte
197
123
18
45
50
207
2
0
/
/
vmovddup
0x2cf32
(
%
rip
)
%
xmm13
#
3d760
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1514
>
.
byte
196
65
49
219
205
/
/
vpand
%
xmm13
%
xmm9
%
xmm9
.
byte
196
65
57
219
197
/
/
vpand
%
xmm13
%
xmm8
%
xmm8
.
byte
196
193
49
114
209
2
/
/
vpsrld
0x2
%
xmm9
%
xmm9
.
byte
196
193
57
114
208
2
/
/
vpsrld
0x2
%
xmm8
%
xmm8
.
byte
196
67
53
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
28
86
192
/
/
vorps
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
60
86
194
/
/
vorps
%
ymm10
%
ymm8
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
86
195
/
/
vorps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
163
188
2
0
/
/
vbroadcastss
0x2bca3
(
%
rip
)
%
ymm9
#
3c50c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c0
>
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
153
188
2
0
/
/
vbroadcastss
0x2bc99
(
%
rip
)
%
ymm9
#
3c510
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c4
>
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
88
201
/
/
vaddps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
88
210
/
/
vaddps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
252
93
195
/
/
vminps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
244
93
203
/
/
vminps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
236
93
211
/
/
vminps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
188
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_avx
.
globl
_sk_uniform_color_avx
FUNCTION
(
_sk_uniform_color_avx
)
_sk_uniform_color_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm0
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_avx
.
globl
_sk_black_color_avx
FUNCTION
(
_sk_black_color_avx
)
_sk_black_color_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
33
188
2
0
/
/
vbroadcastss
0x2bc21
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_avx
.
globl
_sk_white_color_avx
FUNCTION
(
_sk_white_color_avx
)
_sk_white_color_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
8
188
2
0
/
/
vbroadcastss
0x2bc08
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
252
40
216
/
/
vmovaps
%
ymm0
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_rgba_avx
.
globl
_sk_load_rgba_avx
FUNCTION
(
_sk_load_rgba_avx
)
_sk_load_rgba_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
0
/
/
vmovups
(
%
rax
)
%
ymm0
.
byte
197
252
16
72
32
/
/
vmovups
0x20
(
%
rax
)
%
ymm1
.
byte
197
252
16
80
64
/
/
vmovups
0x40
(
%
rax
)
%
ymm2
.
byte
197
252
16
88
96
/
/
vmovups
0x60
(
%
rax
)
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_rgba_avx
.
globl
_sk_store_rgba_avx
FUNCTION
(
_sk_store_rgba_avx
)
_sk_store_rgba_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
197
252
17
72
32
/
/
vmovups
%
ymm1
0x20
(
%
rax
)
.
byte
197
252
17
80
64
/
/
vmovups
%
ymm2
0x40
(
%
rax
)
.
byte
197
252
17
88
96
/
/
vmovups
%
ymm3
0x60
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_avx
.
globl
_sk_clear_avx
FUNCTION
(
_sk_clear_avx
)
_sk_clear_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_avx
.
globl
_sk_srcatop_avx
FUNCTION
(
_sk_srcatop_avx
)
_sk_srcatop_avx
:
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
167
187
2
0
/
/
vbroadcastss
0x2bba7
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
204
/
/
vmulps
%
ymm4
%
ymm8
%
ymm9
.
byte
197
180
88
192
/
/
vaddps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
60
89
205
/
/
vmulps
%
ymm5
%
ymm8
%
ymm9
.
byte
197
180
88
201
/
/
vaddps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
60
89
206
/
/
vmulps
%
ymm6
%
ymm8
%
ymm9
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
196
193
100
88
216
/
/
vaddps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_avx
.
globl
_sk_dstatop_avx
FUNCTION
(
_sk_dstatop_avx
)
_sk_dstatop_avx
:
.
byte
197
100
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm8
.
byte
196
98
125
24
13
101
187
2
0
/
/
vbroadcastss
0x2bb65
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
207
/
/
vsubps
%
ymm7
%
ymm9
%
ymm9
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
100
89
197
/
/
vmulps
%
ymm5
%
ymm3
%
ymm8
.
byte
197
180
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
188
88
201
/
/
vaddps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
100
89
198
/
/
vmulps
%
ymm6
%
ymm3
%
ymm8
.
byte
197
180
89
210
/
/
vmulps
%
ymm2
%
ymm9
%
ymm2
.
byte
197
188
88
210
/
/
vaddps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
100
89
199
/
/
vmulps
%
ymm7
%
ymm3
%
ymm8
.
byte
197
180
89
219
/
/
vmulps
%
ymm3
%
ymm9
%
ymm3
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_avx
.
globl
_sk_srcin_avx
FUNCTION
(
_sk_srcin_avx
)
_sk_srcin_avx
:
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_avx
.
globl
_sk_dstin_avx
FUNCTION
(
_sk_dstin_avx
)
_sk_dstin_avx
:
.
byte
197
228
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
228
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
228
89
214
/
/
vmulps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_avx
.
globl
_sk_srcout_avx
FUNCTION
(
_sk_srcout_avx
)
_sk_srcout_avx
:
.
byte
196
98
125
24
5
0
187
2
0
/
/
vbroadcastss
0x2bb00
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_avx
.
globl
_sk_dstout_avx
FUNCTION
(
_sk_dstout_avx
)
_sk_dstout_avx
:
.
byte
196
226
125
24
5
223
186
2
0
/
/
vbroadcastss
0x2badf
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
92
219
/
/
vsubps
%
ymm3
%
ymm0
%
ymm3
.
byte
197
228
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm0
.
byte
197
228
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm1
.
byte
197
228
89
214
/
/
vmulps
%
ymm6
%
ymm3
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_avx
.
globl
_sk_srcover_avx
FUNCTION
(
_sk_srcover_avx
)
_sk_srcover_avx
:
.
byte
196
98
125
24
5
190
186
2
0
/
/
vbroadcastss
0x2babe
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
204
/
/
vmulps
%
ymm4
%
ymm8
%
ymm9
.
byte
197
180
88
192
/
/
vaddps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
60
89
205
/
/
vmulps
%
ymm5
%
ymm8
%
ymm9
.
byte
197
180
88
201
/
/
vaddps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
60
89
206
/
/
vmulps
%
ymm6
%
ymm8
%
ymm9
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_avx
.
globl
_sk_dstover_avx
FUNCTION
(
_sk_dstover_avx
)
_sk_dstover_avx
:
.
byte
196
98
125
24
5
141
186
2
0
/
/
vbroadcastss
0x2ba8d
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
199
/
/
vsubps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
244
88
205
/
/
vaddps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
236
88
214
/
/
vaddps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_avx
.
globl
_sk_modulate_avx
FUNCTION
(
_sk_modulate_avx
)
_sk_modulate_avx
:
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_avx
.
globl
_sk_multiply_avx
FUNCTION
(
_sk_multiply_avx
)
_sk_multiply_avx
:
.
byte
196
98
125
24
5
72
186
2
0
/
/
vbroadcastss
0x2ba48
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
52
89
208
/
/
vmulps
%
ymm0
%
ymm9
%
ymm10
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
220
/
/
vmulps
%
ymm4
%
ymm8
%
ymm11
.
byte
196
65
36
88
210
/
/
vaddps
%
ymm10
%
ymm11
%
ymm10
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
193
124
88
194
/
/
vaddps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
52
89
209
/
/
vmulps
%
ymm1
%
ymm9
%
ymm10
.
byte
197
60
89
221
/
/
vmulps
%
ymm5
%
ymm8
%
ymm11
.
byte
196
65
36
88
210
/
/
vaddps
%
ymm10
%
ymm11
%
ymm10
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
193
116
88
202
/
/
vaddps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
52
89
210
/
/
vmulps
%
ymm2
%
ymm9
%
ymm10
.
byte
197
60
89
222
/
/
vmulps
%
ymm6
%
ymm8
%
ymm11
.
byte
196
65
36
88
210
/
/
vaddps
%
ymm10
%
ymm11
%
ymm10
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
193
108
88
210
/
/
vaddps
%
ymm10
%
ymm2
%
ymm2
.
byte
197
52
89
203
/
/
vmulps
%
ymm3
%
ymm9
%
ymm9
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
88
216
/
/
vaddps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__avx
.
globl
_sk_plus__avx
FUNCTION
(
_sk_plus__avx
)
_sk_plus__avx
:
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
215
185
2
0
/
/
vbroadcastss
0x2b9d7
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
244
88
205
/
/
vaddps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
236
88
214
/
/
vaddps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_avx
.
globl
_sk_screen_avx
FUNCTION
(
_sk_screen_avx
)
_sk_screen_avx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
100
88
199
/
/
vaddps
%
ymm7
%
ymm3
%
ymm8
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
197
188
92
219
/
/
vsubps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__avx
.
globl
_sk_xor__avx
FUNCTION
(
_sk_xor__avx
)
_sk_xor__avx
:
.
byte
196
98
125
24
5
118
185
2
0
/
/
vbroadcastss
0x2b976
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
212
/
/
vmulps
%
ymm4
%
ymm8
%
ymm10
.
byte
197
172
88
192
/
/
vaddps
%
ymm0
%
ymm10
%
ymm0
.
byte
197
180
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
60
89
213
/
/
vmulps
%
ymm5
%
ymm8
%
ymm10
.
byte
197
172
88
201
/
/
vaddps
%
ymm1
%
ymm10
%
ymm1
.
byte
197
180
89
210
/
/
vmulps
%
ymm2
%
ymm9
%
ymm2
.
byte
197
60
89
214
/
/
vmulps
%
ymm6
%
ymm8
%
ymm10
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
180
89
219
/
/
vmulps
%
ymm3
%
ymm9
%
ymm3
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_avx
.
globl
_sk_darken_avx
FUNCTION
(
_sk_darken_avx
)
_sk_darken_avx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
95
193
/
/
vmaxps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
95
201
/
/
vmaxps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
95
209
/
/
vmaxps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
242
184
2
0
/
/
vbroadcastss
0x2b8f2
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_avx
.
globl
_sk_lighten_avx
FUNCTION
(
_sk_lighten_avx
)
_sk_lighten_avx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
93
193
/
/
vminps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
93
201
/
/
vminps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
93
209
/
/
vminps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
154
184
2
0
/
/
vbroadcastss
0x2b89a
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_avx
.
globl
_sk_difference_avx
FUNCTION
(
_sk_difference_avx
)
_sk_difference_avx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
124
93
193
/
/
vminps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm1
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
116
93
201
/
/
vminps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
215
/
/
vmulps
%
ymm7
%
ymm2
%
ymm2
.
byte
197
100
89
206
/
/
vmulps
%
ymm6
%
ymm3
%
ymm9
.
byte
196
193
108
93
209
/
/
vminps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
54
184
2
0
/
/
vbroadcastss
0x2b836
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_avx
.
globl
_sk_exclusion_avx
FUNCTION
(
_sk_exclusion_avx
)
_sk_exclusion_avx
:
.
byte
197
124
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm8
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
116
88
197
/
/
vaddps
%
ymm5
%
ymm1
%
ymm8
.
byte
197
244
89
205
/
/
vmulps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
108
88
198
/
/
vaddps
%
ymm6
%
ymm2
%
ymm8
.
byte
197
236
89
214
/
/
vmulps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
237
183
2
0
/
/
vbroadcastss
0x2b7ed
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colorburn_avx
.
globl
_sk_colorburn_avx
FUNCTION
(
_sk_colorburn_avx
)
_sk_colorburn_avx
:
.
byte
196
98
125
24
5
212
183
2
0
/
/
vbroadcastss
0x2b7d4
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
52
89
216
/
/
vmulps
%
ymm0
%
ymm9
%
ymm11
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
65
124
194
226
0
/
/
vcmpeqps
%
ymm10
%
ymm0
%
ymm12
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
68
92
236
/
/
vsubps
%
ymm4
%
ymm7
%
ymm13
.
byte
197
20
89
235
/
/
vmulps
%
ymm3
%
ymm13
%
ymm13
.
byte
197
252
83
192
/
/
vrcpps
%
ymm0
%
ymm0
.
byte
197
148
89
192
/
/
vmulps
%
ymm0
%
ymm13
%
ymm0
.
byte
197
60
89
236
/
/
vmulps
%
ymm4
%
ymm8
%
ymm13
.
byte
197
196
93
192
/
/
vminps
%
ymm0
%
ymm7
%
ymm0
.
byte
197
196
92
192
/
/
vsubps
%
ymm0
%
ymm7
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
164
88
192
/
/
vaddps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
148
88
192
/
/
vaddps
%
ymm0
%
ymm13
%
ymm0
.
byte
196
195
125
74
197
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm0
%
ymm0
.
byte
197
92
194
231
0
/
/
vcmpeqps
%
ymm7
%
ymm4
%
ymm12
.
byte
197
36
88
220
/
/
vaddps
%
ymm4
%
ymm11
%
ymm11
.
byte
196
195
125
74
195
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm0
%
ymm0
.
byte
197
52
89
217
/
/
vmulps
%
ymm1
%
ymm9
%
ymm11
.
byte
196
65
116
194
226
0
/
/
vcmpeqps
%
ymm10
%
ymm1
%
ymm12
.
byte
197
68
92
237
/
/
vsubps
%
ymm5
%
ymm7
%
ymm13
.
byte
197
20
89
235
/
/
vmulps
%
ymm3
%
ymm13
%
ymm13
.
byte
197
252
83
201
/
/
vrcpps
%
ymm1
%
ymm1
.
byte
197
148
89
201
/
/
vmulps
%
ymm1
%
ymm13
%
ymm1
.
byte
197
60
89
237
/
/
vmulps
%
ymm5
%
ymm8
%
ymm13
.
byte
197
196
93
201
/
/
vminps
%
ymm1
%
ymm7
%
ymm1
.
byte
197
196
92
201
/
/
vsubps
%
ymm1
%
ymm7
%
ymm1
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
148
88
201
/
/
vaddps
%
ymm1
%
ymm13
%
ymm1
.
byte
196
195
117
74
205
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm1
%
ymm1
.
byte
197
84
194
231
0
/
/
vcmpeqps
%
ymm7
%
ymm5
%
ymm12
.
byte
197
36
88
221
/
/
vaddps
%
ymm5
%
ymm11
%
ymm11
.
byte
196
195
117
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm1
%
ymm1
.
byte
197
52
89
202
/
/
vmulps
%
ymm2
%
ymm9
%
ymm9
.
byte
196
65
108
194
210
0
/
/
vcmpeqps
%
ymm10
%
ymm2
%
ymm10
.
byte
197
68
92
222
/
/
vsubps
%
ymm6
%
ymm7
%
ymm11
.
byte
197
36
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm11
.
byte
197
252
83
210
/
/
vrcpps
%
ymm2
%
ymm2
.
byte
197
164
89
210
/
/
vmulps
%
ymm2
%
ymm11
%
ymm2
.
byte
197
60
89
222
/
/
vmulps
%
ymm6
%
ymm8
%
ymm11
.
byte
197
196
93
210
/
/
vminps
%
ymm2
%
ymm7
%
ymm2
.
byte
197
196
92
210
/
/
vsubps
%
ymm2
%
ymm7
%
ymm2
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
197
164
88
210
/
/
vaddps
%
ymm2
%
ymm11
%
ymm2
.
byte
196
195
109
74
211
160
/
/
vblendvps
%
ymm10
%
ymm11
%
ymm2
%
ymm2
.
byte
197
76
194
215
0
/
/
vcmpeqps
%
ymm7
%
ymm6
%
ymm10
.
byte
197
52
88
206
/
/
vaddps
%
ymm6
%
ymm9
%
ymm9
.
byte
196
195
109
74
209
160
/
/
vblendvps
%
ymm10
%
ymm9
%
ymm2
%
ymm2
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colordodge_avx
.
globl
_sk_colordodge_avx
FUNCTION
(
_sk_colordodge_avx
)
_sk_colordodge_avx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
216
182
2
0
/
/
vbroadcastss
0x2b6d8
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
215
/
/
vsubps
%
ymm7
%
ymm9
%
ymm10
.
byte
197
44
89
216
/
/
vmulps
%
ymm0
%
ymm10
%
ymm11
.
byte
197
52
92
203
/
/
vsubps
%
ymm3
%
ymm9
%
ymm9
.
byte
197
100
89
228
/
/
vmulps
%
ymm4
%
ymm3
%
ymm12
.
byte
197
100
92
232
/
/
vsubps
%
ymm0
%
ymm3
%
ymm13
.
byte
196
65
124
83
237
/
/
vrcpps
%
ymm13
%
ymm13
.
byte
196
65
28
89
229
/
/
vmulps
%
ymm13
%
ymm12
%
ymm12
.
byte
197
52
89
236
/
/
vmulps
%
ymm4
%
ymm9
%
ymm13
.
byte
196
65
68
93
228
/
/
vminps
%
ymm12
%
ymm7
%
ymm12
.
byte
197
28
89
227
/
/
vmulps
%
ymm3
%
ymm12
%
ymm12
.
byte
196
65
36
88
228
/
/
vaddps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
65
20
88
228
/
/
vaddps
%
ymm12
%
ymm13
%
ymm12
.
byte
197
20
88
232
/
/
vaddps
%
ymm0
%
ymm13
%
ymm13
.
byte
197
252
194
195
0
/
/
vcmpeqps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
195
29
74
197
0
/
/
vblendvps
%
ymm0
%
ymm13
%
ymm12
%
ymm0
.
byte
196
65
92
194
224
0
/
/
vcmpeqps
%
ymm8
%
ymm4
%
ymm12
.
byte
196
195
125
74
195
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm0
%
ymm0
.
byte
197
44
89
217
/
/
vmulps
%
ymm1
%
ymm10
%
ymm11
.
byte
197
100
89
229
/
/
vmulps
%
ymm5
%
ymm3
%
ymm12
.
byte
197
100
92
233
/
/
vsubps
%
ymm1
%
ymm3
%
ymm13
.
byte
196
65
124
83
237
/
/
vrcpps
%
ymm13
%
ymm13
.
byte
196
65
28
89
229
/
/
vmulps
%
ymm13
%
ymm12
%
ymm12
.
byte
197
52
89
237
/
/
vmulps
%
ymm5
%
ymm9
%
ymm13
.
byte
196
65
68
93
228
/
/
vminps
%
ymm12
%
ymm7
%
ymm12
.
byte
197
28
89
227
/
/
vmulps
%
ymm3
%
ymm12
%
ymm12
.
byte
196
65
36
88
228
/
/
vaddps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
65
20
88
228
/
/
vaddps
%
ymm12
%
ymm13
%
ymm12
.
byte
197
20
88
233
/
/
vaddps
%
ymm1
%
ymm13
%
ymm13
.
byte
197
244
194
203
0
/
/
vcmpeqps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
195
29
74
205
16
/
/
vblendvps
%
ymm1
%
ymm13
%
ymm12
%
ymm1
.
byte
196
65
84
194
224
0
/
/
vcmpeqps
%
ymm8
%
ymm5
%
ymm12
.
byte
196
195
117
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm1
%
ymm1
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
197
100
92
226
/
/
vsubps
%
ymm2
%
ymm3
%
ymm12
.
byte
196
65
124
83
228
/
/
vrcpps
%
ymm12
%
ymm12
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
52
89
230
/
/
vmulps
%
ymm6
%
ymm9
%
ymm12
.
byte
196
65
68
93
219
/
/
vminps
%
ymm11
%
ymm7
%
ymm11
.
byte
197
36
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm11
.
byte
196
65
44
88
219
/
/
vaddps
%
ymm11
%
ymm10
%
ymm11
.
byte
196
65
28
88
219
/
/
vaddps
%
ymm11
%
ymm12
%
ymm11
.
byte
197
28
88
226
/
/
vaddps
%
ymm2
%
ymm12
%
ymm12
.
byte
197
236
194
211
0
/
/
vcmpeqps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
195
37
74
212
32
/
/
vblendvps
%
ymm2
%
ymm12
%
ymm11
%
ymm2
.
byte
196
65
76
194
192
0
/
/
vcmpeqps
%
ymm8
%
ymm6
%
ymm8
.
byte
196
195
109
74
210
128
/
/
vblendvps
%
ymm8
%
ymm10
%
ymm2
%
ymm2
.
byte
197
52
89
199
/
/
vmulps
%
ymm7
%
ymm9
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_avx
.
globl
_sk_hardlight_avx
FUNCTION
(
_sk_hardlight_avx
)
_sk_hardlight_avx
:
.
byte
196
98
125
24
5
227
181
2
0
/
/
vbroadcastss
0x2b5e3
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
215
/
/
vsubps
%
ymm7
%
ymm8
%
ymm10
.
byte
197
44
89
200
/
/
vmulps
%
ymm0
%
ymm10
%
ymm9
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
220
/
/
vmulps
%
ymm4
%
ymm8
%
ymm11
.
byte
196
65
36
88
217
/
/
vaddps
%
ymm9
%
ymm11
%
ymm11
.
byte
197
124
88
200
/
/
vaddps
%
ymm0
%
ymm0
%
ymm9
.
byte
197
52
194
227
2
/
/
vcmpleps
%
ymm3
%
ymm9
%
ymm12
.
byte
197
124
89
204
/
/
vmulps
%
ymm4
%
ymm0
%
ymm9
.
byte
196
65
52
88
233
/
/
vaddps
%
ymm9
%
ymm9
%
ymm13
.
byte
197
100
89
207
/
/
vmulps
%
ymm7
%
ymm3
%
ymm9
.
byte
197
68
92
244
/
/
vsubps
%
ymm4
%
ymm7
%
ymm14
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
180
92
192
/
/
vsubps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
197
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm0
%
ymm0
.
byte
197
164
88
192
/
/
vaddps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
44
89
217
/
/
vmulps
%
ymm1
%
ymm10
%
ymm11
.
byte
197
60
89
229
/
/
vmulps
%
ymm5
%
ymm8
%
ymm12
.
byte
196
65
28
88
219
/
/
vaddps
%
ymm11
%
ymm12
%
ymm11
.
byte
197
116
88
225
/
/
vaddps
%
ymm1
%
ymm1
%
ymm12
.
byte
197
28
194
227
2
/
/
vcmpleps
%
ymm3
%
ymm12
%
ymm12
.
byte
197
116
89
237
/
/
vmulps
%
ymm5
%
ymm1
%
ymm13
.
byte
196
65
20
88
237
/
/
vaddps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
68
92
245
/
/
vsubps
%
ymm5
%
ymm7
%
ymm14
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
116
89
206
/
/
vmulps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
180
92
201
/
/
vsubps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
195
117
74
205
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm1
%
ymm1
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
197
60
89
222
/
/
vmulps
%
ymm6
%
ymm8
%
ymm11
.
byte
196
65
36
88
210
/
/
vaddps
%
ymm10
%
ymm11
%
ymm10
.
byte
197
108
88
218
/
/
vaddps
%
ymm2
%
ymm2
%
ymm11
.
byte
197
36
194
219
2
/
/
vcmpleps
%
ymm3
%
ymm11
%
ymm11
.
byte
197
108
89
230
/
/
vmulps
%
ymm6
%
ymm2
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
68
92
238
/
/
vsubps
%
ymm6
%
ymm7
%
ymm13
.
byte
197
228
92
210
/
/
vsubps
%
ymm2
%
ymm3
%
ymm2
.
byte
196
193
108
89
213
/
/
vmulps
%
ymm13
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
180
92
210
/
/
vsubps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
195
109
74
212
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm2
%
ymm2
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_avx
.
globl
_sk_overlay_avx
FUNCTION
(
_sk_overlay_avx
)
_sk_overlay_avx
:
.
byte
196
98
125
24
5
8
181
2
0
/
/
vbroadcastss
0x2b508
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
215
/
/
vsubps
%
ymm7
%
ymm8
%
ymm10
.
byte
197
44
89
200
/
/
vmulps
%
ymm0
%
ymm10
%
ymm9
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
220
/
/
vmulps
%
ymm4
%
ymm8
%
ymm11
.
byte
196
65
36
88
217
/
/
vaddps
%
ymm9
%
ymm11
%
ymm11
.
byte
197
92
88
204
/
/
vaddps
%
ymm4
%
ymm4
%
ymm9
.
byte
197
52
194
231
2
/
/
vcmpleps
%
ymm7
%
ymm9
%
ymm12
.
byte
197
124
89
204
/
/
vmulps
%
ymm4
%
ymm0
%
ymm9
.
byte
196
65
52
88
233
/
/
vaddps
%
ymm9
%
ymm9
%
ymm13
.
byte
197
100
89
207
/
/
vmulps
%
ymm7
%
ymm3
%
ymm9
.
byte
197
68
92
244
/
/
vsubps
%
ymm4
%
ymm7
%
ymm14
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
252
88
192
/
/
vaddps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
180
92
192
/
/
vsubps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
197
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm0
%
ymm0
.
byte
197
164
88
192
/
/
vaddps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
44
89
217
/
/
vmulps
%
ymm1
%
ymm10
%
ymm11
.
byte
197
60
89
229
/
/
vmulps
%
ymm5
%
ymm8
%
ymm12
.
byte
196
65
28
88
219
/
/
vaddps
%
ymm11
%
ymm12
%
ymm11
.
byte
197
84
88
229
/
/
vaddps
%
ymm5
%
ymm5
%
ymm12
.
byte
197
28
194
231
2
/
/
vcmpleps
%
ymm7
%
ymm12
%
ymm12
.
byte
197
116
89
237
/
/
vmulps
%
ymm5
%
ymm1
%
ymm13
.
byte
196
65
20
88
237
/
/
vaddps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
68
92
245
/
/
vsubps
%
ymm5
%
ymm7
%
ymm14
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
116
89
206
/
/
vmulps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
244
88
201
/
/
vaddps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
180
92
201
/
/
vsubps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
195
117
74
205
192
/
/
vblendvps
%
ymm12
%
ymm13
%
ymm1
%
ymm1
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
197
60
89
222
/
/
vmulps
%
ymm6
%
ymm8
%
ymm11
.
byte
196
65
36
88
210
/
/
vaddps
%
ymm10
%
ymm11
%
ymm10
.
byte
197
76
88
222
/
/
vaddps
%
ymm6
%
ymm6
%
ymm11
.
byte
197
36
194
223
2
/
/
vcmpleps
%
ymm7
%
ymm11
%
ymm11
.
byte
197
108
89
230
/
/
vmulps
%
ymm6
%
ymm2
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
68
92
238
/
/
vsubps
%
ymm6
%
ymm7
%
ymm13
.
byte
197
228
92
210
/
/
vsubps
%
ymm2
%
ymm3
%
ymm2
.
byte
196
193
108
89
213
/
/
vmulps
%
ymm13
%
ymm2
%
ymm2
.
byte
197
236
88
210
/
/
vaddps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
180
92
210
/
/
vsubps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
195
109
74
212
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm2
%
ymm2
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_softlight_avx
.
globl
_sk_softlight_avx
FUNCTION
(
_sk_softlight_avx
)
_sk_softlight_avx
:
.
byte
197
252
17
84
36
200
/
/
vmovups
%
ymm2
-
0x38
(
%
rsp
)
.
byte
197
252
40
209
/
/
vmovaps
%
ymm1
%
ymm2
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
52
194
215
1
/
/
vcmpltps
%
ymm7
%
ymm9
%
ymm10
.
byte
197
92
94
199
/
/
vdivps
%
ymm7
%
ymm4
%
ymm8
.
byte
196
67
53
74
224
160
/
/
vblendvps
%
ymm10
%
ymm8
%
ymm9
%
ymm12
.
byte
196
65
28
88
196
/
/
vaddps
%
ymm12
%
ymm12
%
ymm8
.
byte
196
65
60
88
192
/
/
vaddps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
60
89
216
/
/
vmulps
%
ymm8
%
ymm8
%
ymm11
.
byte
196
65
60
88
195
/
/
vaddps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
98
125
24
29
15
180
2
0
/
/
vbroadcastss
0x2b40f
(
%
rip
)
%
ymm11
#
3c514
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c8
>
.
byte
196
65
28
88
235
/
/
vaddps
%
ymm11
%
ymm12
%
ymm13
.
byte
196
65
20
89
192
/
/
vmulps
%
ymm8
%
ymm13
%
ymm8
.
byte
196
98
125
24
45
0
180
2
0
/
/
vbroadcastss
0x2b400
(
%
rip
)
%
ymm13
#
3c518
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2cc
>
.
byte
196
65
28
89
245
/
/
vmulps
%
ymm13
%
ymm12
%
ymm14
.
byte
196
65
12
88
192
/
/
vaddps
%
ymm8
%
ymm14
%
ymm8
.
byte
196
65
124
82
244
/
/
vrsqrtps
%
ymm12
%
ymm14
.
byte
196
65
124
83
246
/
/
vrcpps
%
ymm14
%
ymm14
.
byte
196
65
12
92
244
/
/
vsubps
%
ymm12
%
ymm14
%
ymm14
.
byte
197
92
88
252
/
/
vaddps
%
ymm4
%
ymm4
%
ymm15
.
byte
196
65
4
88
255
/
/
vaddps
%
ymm15
%
ymm15
%
ymm15
.
byte
197
4
194
255
2
/
/
vcmpleps
%
ymm7
%
ymm15
%
ymm15
.
byte
196
67
13
74
240
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm14
%
ymm14
.
byte
197
116
88
249
/
/
vaddps
%
ymm1
%
ymm1
%
ymm15
.
byte
196
98
125
24
5
170
179
2
0
/
/
vbroadcastss
0x2b3aa
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
60
92
228
/
/
vsubps
%
ymm12
%
ymm8
%
ymm12
.
byte
197
132
92
195
/
/
vsubps
%
ymm3
%
ymm15
%
ymm0
.
byte
196
65
124
89
228
/
/
vmulps
%
ymm12
%
ymm0
%
ymm12
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
196
193
124
89
198
/
/
vmulps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
100
89
244
/
/
vmulps
%
ymm4
%
ymm3
%
ymm14
.
byte
197
140
88
192
/
/
vaddps
%
ymm0
%
ymm14
%
ymm0
.
byte
197
28
88
227
/
/
vaddps
%
ymm3
%
ymm12
%
ymm12
.
byte
197
28
89
228
/
/
vmulps
%
ymm4
%
ymm12
%
ymm12
.
byte
197
4
194
243
2
/
/
vcmpleps
%
ymm3
%
ymm15
%
ymm14
.
byte
196
195
125
74
196
224
/
/
vblendvps
%
ymm14
%
ymm12
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
168
/
/
vmovups
%
ymm0
-
0x58
(
%
rsp
)
.
byte
197
212
94
199
/
/
vdivps
%
ymm7
%
ymm5
%
ymm0
.
byte
196
227
53
74
192
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm9
%
ymm0
.
byte
197
124
88
240
/
/
vaddps
%
ymm0
%
ymm0
%
ymm14
.
byte
196
65
12
88
246
/
/
vaddps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
12
89
254
/
/
vmulps
%
ymm14
%
ymm14
%
ymm15
.
byte
196
65
12
88
247
/
/
vaddps
%
ymm15
%
ymm14
%
ymm14
.
byte
196
65
124
88
251
/
/
vaddps
%
ymm11
%
ymm0
%
ymm15
.
byte
196
65
4
89
246
/
/
vmulps
%
ymm14
%
ymm15
%
ymm14
.
byte
196
65
124
89
253
/
/
vmulps
%
ymm13
%
ymm0
%
ymm15
.
byte
196
65
4
88
246
/
/
vaddps
%
ymm14
%
ymm15
%
ymm14
.
byte
197
124
82
248
/
/
vrsqrtps
%
ymm0
%
ymm15
.
byte
196
65
124
83
255
/
/
vrcpps
%
ymm15
%
ymm15
.
byte
197
4
92
248
/
/
vsubps
%
ymm0
%
ymm15
%
ymm15
.
byte
197
84
88
229
/
/
vaddps
%
ymm5
%
ymm5
%
ymm12
.
byte
196
65
28
88
228
/
/
vaddps
%
ymm12
%
ymm12
%
ymm12
.
byte
197
28
194
231
2
/
/
vcmpleps
%
ymm7
%
ymm12
%
ymm12
.
byte
196
67
5
74
230
192
/
/
vblendvps
%
ymm12
%
ymm14
%
ymm15
%
ymm12
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
108
88
242
/
/
vaddps
%
ymm2
%
ymm2
%
ymm14
.
byte
197
12
92
251
/
/
vsubps
%
ymm3
%
ymm14
%
ymm15
.
byte
197
132
89
192
/
/
vmulps
%
ymm0
%
ymm15
%
ymm0
.
byte
197
4
89
255
/
/
vmulps
%
ymm7
%
ymm15
%
ymm15
.
byte
196
65
4
89
228
/
/
vmulps
%
ymm12
%
ymm15
%
ymm12
.
byte
197
100
89
253
/
/
vmulps
%
ymm5
%
ymm3
%
ymm15
.
byte
196
65
4
88
228
/
/
vaddps
%
ymm12
%
ymm15
%
ymm12
.
byte
197
252
88
195
/
/
vaddps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
252
89
197
/
/
vmulps
%
ymm5
%
ymm0
%
ymm0
.
byte
197
12
194
243
2
/
/
vcmpleps
%
ymm3
%
ymm14
%
ymm14
.
byte
196
99
29
74
240
224
/
/
vblendvps
%
ymm14
%
ymm0
%
ymm12
%
ymm14
.
byte
197
204
94
199
/
/
vdivps
%
ymm7
%
ymm6
%
ymm0
.
byte
196
227
53
74
192
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm9
%
ymm0
.
byte
197
124
88
200
/
/
vaddps
%
ymm0
%
ymm0
%
ymm9
.
byte
196
65
52
88
201
/
/
vaddps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
52
89
209
/
/
vmulps
%
ymm9
%
ymm9
%
ymm10
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
124
88
211
/
/
vaddps
%
ymm11
%
ymm0
%
ymm10
.
byte
196
65
44
89
201
/
/
vmulps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
65
124
89
213
/
/
vmulps
%
ymm13
%
ymm0
%
ymm10
.
byte
196
65
44
88
201
/
/
vaddps
%
ymm9
%
ymm10
%
ymm9
.
byte
197
124
82
208
/
/
vrsqrtps
%
ymm0
%
ymm10
.
byte
196
65
124
83
210
/
/
vrcpps
%
ymm10
%
ymm10
.
byte
197
44
92
208
/
/
vsubps
%
ymm0
%
ymm10
%
ymm10
.
byte
197
76
88
222
/
/
vaddps
%
ymm6
%
ymm6
%
ymm11
.
byte
196
65
36
88
219
/
/
vaddps
%
ymm11
%
ymm11
%
ymm11
.
byte
197
36
194
223
2
/
/
vcmpleps
%
ymm7
%
ymm11
%
ymm11
.
byte
196
67
45
74
201
176
/
/
vblendvps
%
ymm11
%
ymm9
%
ymm10
%
ymm9
.
byte
197
124
16
100
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm12
.
byte
196
65
28
88
212
/
/
vaddps
%
ymm12
%
ymm12
%
ymm10
.
byte
197
44
92
219
/
/
vsubps
%
ymm3
%
ymm10
%
ymm11
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
36
89
223
/
/
vmulps
%
ymm7
%
ymm11
%
ymm11
.
byte
196
65
36
89
201
/
/
vmulps
%
ymm9
%
ymm11
%
ymm9
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
196
65
36
88
201
/
/
vaddps
%
ymm9
%
ymm11
%
ymm9
.
byte
197
252
88
195
/
/
vaddps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
252
89
198
/
/
vmulps
%
ymm6
%
ymm0
%
ymm0
.
byte
197
44
194
211
2
/
/
vcmpleps
%
ymm3
%
ymm10
%
ymm10
.
byte
196
99
53
74
200
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm9
%
ymm9
.
byte
197
60
92
215
/
/
vsubps
%
ymm7
%
ymm8
%
ymm10
.
byte
197
172
89
193
/
/
vmulps
%
ymm1
%
ymm10
%
ymm0
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
188
89
204
/
/
vmulps
%
ymm4
%
ymm8
%
ymm1
.
byte
197
244
88
192
/
/
vaddps
%
ymm0
%
ymm1
%
ymm0
.
byte
197
252
88
68
36
168
/
/
vaddps
-
0x58
(
%
rsp
)
%
ymm0
%
ymm0
.
byte
197
172
89
202
/
/
vmulps
%
ymm2
%
ymm10
%
ymm1
.
byte
197
188
89
213
/
/
vmulps
%
ymm5
%
ymm8
%
ymm2
.
byte
197
236
88
201
/
/
vaddps
%
ymm1
%
ymm2
%
ymm1
.
byte
196
193
116
88
206
/
/
vaddps
%
ymm14
%
ymm1
%
ymm1
.
byte
196
193
44
89
212
/
/
vmulps
%
ymm12
%
ymm10
%
ymm2
.
byte
197
60
89
214
/
/
vmulps
%
ymm6
%
ymm8
%
ymm10
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
196
193
108
88
209
/
/
vaddps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
60
89
199
/
/
vmulps
%
ymm7
%
ymm8
%
ymm8
.
byte
197
188
88
219
/
/
vaddps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hue_avx
.
globl
_sk_hue_avx
FUNCTION
(
_sk_hue_avx
)
_sk_hue_avx
:
.
byte
197
252
17
84
36
200
/
/
vmovups
%
ymm2
-
0x38
(
%
rsp
)
.
byte
197
124
40
193
/
/
vmovaps
%
ymm1
%
ymm8
.
byte
197
124
17
68
36
168
/
/
vmovups
%
ymm8
-
0x58
(
%
rsp
)
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
116
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm9
.
byte
197
60
89
211
/
/
vmulps
%
ymm3
%
ymm8
%
ymm10
.
byte
197
108
89
219
/
/
vmulps
%
ymm3
%
ymm2
%
ymm11
.
byte
197
84
95
198
/
/
vmaxps
%
ymm6
%
ymm5
%
ymm8
.
byte
196
65
92
95
192
/
/
vmaxps
%
ymm8
%
ymm4
%
ymm8
.
byte
197
84
93
230
/
/
vminps
%
ymm6
%
ymm5
%
ymm12
.
byte
196
65
92
93
228
/
/
vminps
%
ymm12
%
ymm4
%
ymm12
.
byte
196
65
60
92
196
/
/
vsubps
%
ymm12
%
ymm8
%
ymm8
.
byte
197
60
89
227
/
/
vmulps
%
ymm3
%
ymm8
%
ymm12
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
232
/
/
vminps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
44
95
195
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
95
192
/
/
vmaxps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
65
60
92
245
/
/
vsubps
%
ymm13
%
ymm8
%
ymm14
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
12
194
248
0
/
/
vcmpeqps
%
ymm8
%
ymm14
%
ymm15
.
byte
196
65
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm9
.
byte
196
65
28
89
201
/
/
vmulps
%
ymm9
%
ymm12
%
ymm9
.
byte
196
65
52
94
206
/
/
vdivps
%
ymm14
%
ymm9
%
ymm9
.
byte
196
67
53
74
200
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm9
%
ymm9
.
byte
196
65
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
67
45
74
208
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
67
37
74
224
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm11
%
ymm12
.
byte
196
98
125
24
53
141
177
2
0
/
/
vbroadcastss
0x2b18d
(
%
rip
)
%
ymm14
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
92
89
222
/
/
vmulps
%
ymm14
%
ymm4
%
ymm11
.
byte
196
98
125
24
61
131
177
2
0
/
/
vbroadcastss
0x2b183
(
%
rip
)
%
ymm15
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
239
/
/
vmulps
%
ymm15
%
ymm5
%
ymm13
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
226
125
24
5
116
177
2
0
/
/
vbroadcastss
0x2b174
(
%
rip
)
%
ymm0
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
197
76
89
232
/
/
vmulps
%
ymm0
%
ymm6
%
ymm13
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
65
52
89
238
/
/
vmulps
%
ymm14
%
ymm9
%
ymm13
.
byte
196
193
44
89
215
/
/
vmulps
%
ymm15
%
ymm10
%
ymm2
.
byte
197
148
88
210
/
/
vaddps
%
ymm2
%
ymm13
%
ymm2
.
byte
197
28
89
232
/
/
vmulps
%
ymm0
%
ymm12
%
ymm13
.
byte
196
193
108
88
213
/
/
vaddps
%
ymm13
%
ymm2
%
ymm2
.
byte
197
36
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm11
.
byte
197
164
92
210
/
/
vsubps
%
ymm2
%
ymm11
%
ymm2
.
byte
197
52
88
202
/
/
vaddps
%
ymm2
%
ymm9
%
ymm9
.
byte
197
44
88
218
/
/
vaddps
%
ymm2
%
ymm10
%
ymm11
.
byte
197
28
88
226
/
/
vaddps
%
ymm2
%
ymm12
%
ymm12
.
byte
196
193
36
93
212
/
/
vminps
%
ymm12
%
ymm11
%
ymm2
.
byte
197
52
93
234
/
/
vminps
%
ymm2
%
ymm9
%
ymm13
.
byte
196
193
52
89
214
/
/
vmulps
%
ymm14
%
ymm9
%
ymm2
.
byte
196
65
36
89
215
/
/
vmulps
%
ymm15
%
ymm11
%
ymm10
.
byte
196
193
108
88
210
/
/
vaddps
%
ymm10
%
ymm2
%
ymm2
.
byte
197
156
89
192
/
/
vmulps
%
ymm0
%
ymm12
%
ymm0
.
byte
197
124
88
210
/
/
vaddps
%
ymm2
%
ymm0
%
ymm10
.
byte
196
193
52
92
194
/
/
vsubps
%
ymm10
%
ymm9
%
ymm0
.
byte
197
172
89
192
/
/
vmulps
%
ymm0
%
ymm10
%
ymm0
.
byte
196
193
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm2
.
byte
197
252
94
194
/
/
vdivps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
65
36
92
242
/
/
vsubps
%
ymm10
%
ymm11
%
ymm14
.
byte
196
65
44
89
246
/
/
vmulps
%
ymm14
%
ymm10
%
ymm14
.
byte
197
12
94
242
/
/
vdivps
%
ymm2
%
ymm14
%
ymm14
.
byte
196
65
28
92
250
/
/
vsubps
%
ymm10
%
ymm12
%
ymm15
.
byte
196
65
44
89
255
/
/
vmulps
%
ymm15
%
ymm10
%
ymm15
.
byte
197
132
94
210
/
/
vdivps
%
ymm2
%
ymm15
%
ymm2
.
byte
196
65
60
194
237
2
/
/
vcmpleps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
44
88
246
/
/
vaddps
%
ymm14
%
ymm10
%
ymm14
.
byte
196
67
13
74
243
208
/
/
vblendvps
%
ymm13
%
ymm11
%
ymm14
%
ymm14
.
byte
196
65
36
95
220
/
/
vmaxps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
196
195
109
74
212
208
/
/
vblendvps
%
ymm13
%
ymm12
%
ymm2
%
ymm2
.
byte
197
172
88
192
/
/
vaddps
%
ymm0
%
ymm10
%
ymm0
.
byte
196
195
125
74
193
208
/
/
vblendvps
%
ymm13
%
ymm9
%
ymm0
%
ymm0
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
52
95
203
/
/
vmaxps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
124
92
218
/
/
vsubps
%
ymm10
%
ymm0
%
ymm11
.
byte
196
65
28
92
234
/
/
vsubps
%
ymm10
%
ymm12
%
ymm13
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
194
249
1
/
/
vcmpltps
%
ymm9
%
ymm12
%
ymm15
.
byte
196
65
52
92
202
/
/
vsubps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
36
94
217
/
/
vdivps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
44
88
219
/
/
vaddps
%
ymm11
%
ymm10
%
ymm11
.
byte
196
195
125
74
195
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm0
%
ymm0
.
byte
196
65
12
92
218
/
/
vsubps
%
ymm10
%
ymm14
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
217
/
/
vdivps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
44
88
219
/
/
vaddps
%
ymm11
%
ymm10
%
ymm11
.
byte
196
67
13
74
219
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
108
92
242
/
/
vsubps
%
ymm10
%
ymm2
%
ymm14
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
20
94
201
/
/
vdivps
%
ymm9
%
ymm13
%
ymm9
.
byte
196
65
44
88
201
/
/
vaddps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
36
95
208
/
/
vmaxps
%
ymm8
%
ymm11
%
ymm10
.
byte
196
195
109
74
209
240
/
/
vblendvps
%
ymm15
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
108
95
208
/
/
vmaxps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
33
176
2
0
/
/
vbroadcastss
0x2b021
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
180
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
220
/
/
vmulps
%
ymm4
%
ymm8
%
ymm11
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
244
88
192
/
/
vaddps
%
ymm0
%
ymm1
%
ymm0
.
byte
197
180
89
76
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm9
%
ymm1
.
byte
197
60
89
221
/
/
vmulps
%
ymm5
%
ymm8
%
ymm11
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
196
193
116
88
202
/
/
vaddps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
52
89
76
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm9
%
ymm9
.
byte
197
60
89
198
/
/
vmulps
%
ymm6
%
ymm8
%
ymm8
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
188
88
210
/
/
vaddps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_saturation_avx
.
globl
_sk_saturation_avx
FUNCTION
(
_sk_saturation_avx
)
_sk_saturation_avx
:
.
byte
197
124
40
193
/
/
vmovaps
%
ymm1
%
ymm8
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
100
89
204
/
/
vmulps
%
ymm4
%
ymm3
%
ymm9
.
byte
197
100
89
213
/
/
vmulps
%
ymm5
%
ymm3
%
ymm10
.
byte
197
100
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm11
.
byte
197
252
17
84
36
200
/
/
vmovups
%
ymm2
-
0x38
(
%
rsp
)
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
252
17
68
36
168
/
/
vmovups
%
ymm0
-
0x58
(
%
rsp
)
.
byte
197
124
95
194
/
/
vmaxps
%
ymm2
%
ymm0
%
ymm8
.
byte
196
65
116
95
192
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm8
.
byte
197
124
93
226
/
/
vminps
%
ymm2
%
ymm0
%
ymm12
.
byte
196
65
116
93
228
/
/
vminps
%
ymm12
%
ymm1
%
ymm12
.
byte
196
65
60
92
196
/
/
vsubps
%
ymm12
%
ymm8
%
ymm8
.
byte
197
60
89
231
/
/
vmulps
%
ymm7
%
ymm8
%
ymm12
.
byte
196
65
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
93
232
/
/
vminps
%
ymm8
%
ymm9
%
ymm13
.
byte
196
65
44
95
195
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
65
52
95
192
/
/
vmaxps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
65
60
92
245
/
/
vsubps
%
ymm13
%
ymm8
%
ymm14
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
12
194
248
0
/
/
vcmpeqps
%
ymm8
%
ymm14
%
ymm15
.
byte
196
65
52
92
205
/
/
vsubps
%
ymm13
%
ymm9
%
ymm9
.
byte
196
65
28
89
201
/
/
vmulps
%
ymm9
%
ymm12
%
ymm9
.
byte
196
65
52
94
206
/
/
vdivps
%
ymm14
%
ymm9
%
ymm9
.
byte
196
67
53
74
200
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm9
%
ymm9
.
byte
196
65
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
28
89
210
/
/
vmulps
%
ymm10
%
ymm12
%
ymm10
.
byte
196
65
44
94
214
/
/
vdivps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
67
45
74
208
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
36
94
222
/
/
vdivps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
67
37
74
224
240
/
/
vblendvps
%
ymm15
%
ymm8
%
ymm11
%
ymm12
.
byte
196
98
125
24
53
75
175
2
0
/
/
vbroadcastss
0x2af4b
(
%
rip
)
%
ymm14
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
92
89
222
/
/
vmulps
%
ymm14
%
ymm4
%
ymm11
.
byte
196
98
125
24
61
65
175
2
0
/
/
vbroadcastss
0x2af41
(
%
rip
)
%
ymm15
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
239
/
/
vmulps
%
ymm15
%
ymm5
%
ymm13
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
226
125
24
5
50
175
2
0
/
/
vbroadcastss
0x2af32
(
%
rip
)
%
ymm0
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
197
76
89
232
/
/
vmulps
%
ymm0
%
ymm6
%
ymm13
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
65
52
89
238
/
/
vmulps
%
ymm14
%
ymm9
%
ymm13
.
byte
196
193
44
89
215
/
/
vmulps
%
ymm15
%
ymm10
%
ymm2
.
byte
197
148
88
210
/
/
vaddps
%
ymm2
%
ymm13
%
ymm2
.
byte
197
28
89
232
/
/
vmulps
%
ymm0
%
ymm12
%
ymm13
.
byte
196
193
108
88
213
/
/
vaddps
%
ymm13
%
ymm2
%
ymm2
.
byte
197
36
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm11
.
byte
197
164
92
210
/
/
vsubps
%
ymm2
%
ymm11
%
ymm2
.
byte
197
52
88
202
/
/
vaddps
%
ymm2
%
ymm9
%
ymm9
.
byte
197
44
88
218
/
/
vaddps
%
ymm2
%
ymm10
%
ymm11
.
byte
197
28
88
226
/
/
vaddps
%
ymm2
%
ymm12
%
ymm12
.
byte
196
193
36
93
212
/
/
vminps
%
ymm12
%
ymm11
%
ymm2
.
byte
197
52
93
234
/
/
vminps
%
ymm2
%
ymm9
%
ymm13
.
byte
196
193
52
89
214
/
/
vmulps
%
ymm14
%
ymm9
%
ymm2
.
byte
196
65
36
89
215
/
/
vmulps
%
ymm15
%
ymm11
%
ymm10
.
byte
196
193
108
88
210
/
/
vaddps
%
ymm10
%
ymm2
%
ymm2
.
byte
197
156
89
192
/
/
vmulps
%
ymm0
%
ymm12
%
ymm0
.
byte
197
124
88
210
/
/
vaddps
%
ymm2
%
ymm0
%
ymm10
.
byte
196
193
52
92
194
/
/
vsubps
%
ymm10
%
ymm9
%
ymm0
.
byte
197
172
89
192
/
/
vmulps
%
ymm0
%
ymm10
%
ymm0
.
byte
196
193
44
92
213
/
/
vsubps
%
ymm13
%
ymm10
%
ymm2
.
byte
197
252
94
194
/
/
vdivps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
65
36
92
242
/
/
vsubps
%
ymm10
%
ymm11
%
ymm14
.
byte
196
65
44
89
246
/
/
vmulps
%
ymm14
%
ymm10
%
ymm14
.
byte
197
12
94
242
/
/
vdivps
%
ymm2
%
ymm14
%
ymm14
.
byte
196
65
28
92
250
/
/
vsubps
%
ymm10
%
ymm12
%
ymm15
.
byte
196
65
44
89
255
/
/
vmulps
%
ymm15
%
ymm10
%
ymm15
.
byte
197
132
94
210
/
/
vdivps
%
ymm2
%
ymm15
%
ymm2
.
byte
196
65
60
194
237
2
/
/
vcmpleps
%
ymm13
%
ymm8
%
ymm13
.
byte
196
65
44
88
246
/
/
vaddps
%
ymm14
%
ymm10
%
ymm14
.
byte
196
67
13
74
243
208
/
/
vblendvps
%
ymm13
%
ymm11
%
ymm14
%
ymm14
.
byte
196
65
36
95
220
/
/
vmaxps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
172
88
210
/
/
vaddps
%
ymm2
%
ymm10
%
ymm2
.
byte
196
195
109
74
212
208
/
/
vblendvps
%
ymm13
%
ymm12
%
ymm2
%
ymm2
.
byte
197
172
88
192
/
/
vaddps
%
ymm0
%
ymm10
%
ymm0
.
byte
196
195
125
74
193
208
/
/
vblendvps
%
ymm13
%
ymm9
%
ymm0
%
ymm0
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
52
95
203
/
/
vmaxps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
124
92
218
/
/
vsubps
%
ymm10
%
ymm0
%
ymm11
.
byte
196
65
28
92
234
/
/
vsubps
%
ymm10
%
ymm12
%
ymm13
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
194
249
1
/
/
vcmpltps
%
ymm9
%
ymm12
%
ymm15
.
byte
196
65
52
92
202
/
/
vsubps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
36
94
217
/
/
vdivps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
44
88
219
/
/
vaddps
%
ymm11
%
ymm10
%
ymm11
.
byte
196
195
125
74
195
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm0
%
ymm0
.
byte
196
65
12
92
218
/
/
vsubps
%
ymm10
%
ymm14
%
ymm11
.
byte
196
65
20
89
219
/
/
vmulps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
36
94
217
/
/
vdivps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
44
88
219
/
/
vaddps
%
ymm11
%
ymm10
%
ymm11
.
byte
196
67
13
74
219
240
/
/
vblendvps
%
ymm15
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
108
92
242
/
/
vsubps
%
ymm10
%
ymm2
%
ymm14
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
20
94
201
/
/
vdivps
%
ymm9
%
ymm13
%
ymm9
.
byte
196
65
44
88
201
/
/
vaddps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
36
95
208
/
/
vmaxps
%
ymm8
%
ymm11
%
ymm10
.
byte
196
195
109
74
209
240
/
/
vblendvps
%
ymm15
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
108
95
208
/
/
vmaxps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
223
173
2
0
/
/
vbroadcastss
0x2addf
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
207
/
/
vsubps
%
ymm7
%
ymm8
%
ymm9
.
byte
197
180
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
60
92
195
/
/
vsubps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
60
89
220
/
/
vmulps
%
ymm4
%
ymm8
%
ymm11
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
244
88
192
/
/
vaddps
%
ymm0
%
ymm1
%
ymm0
.
byte
197
180
89
76
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm9
%
ymm1
.
byte
197
60
89
221
/
/
vmulps
%
ymm5
%
ymm8
%
ymm11
.
byte
197
164
88
201
/
/
vaddps
%
ymm1
%
ymm11
%
ymm1
.
byte
196
193
116
88
202
/
/
vaddps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
52
89
76
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm9
%
ymm9
.
byte
197
60
89
198
/
/
vmulps
%
ymm6
%
ymm8
%
ymm8
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
188
88
210
/
/
vaddps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_color_avx
.
globl
_sk_color_avx
FUNCTION
(
_sk_color_avx
)
_sk_color_avx
:
.
byte
197
252
17
84
36
136
/
/
vmovups
%
ymm2
-
0x78
(
%
rsp
)
.
byte
197
252
17
76
36
200
/
/
vmovups
%
ymm1
-
0x38
(
%
rsp
)
.
byte
197
252
17
68
36
168
/
/
vmovups
%
ymm0
-
0x58
(
%
rsp
)
.
byte
197
124
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm8
.
byte
197
116
89
207
/
/
vmulps
%
ymm7
%
ymm1
%
ymm9
.
byte
196
98
125
24
45
145
173
2
0
/
/
vbroadcastss
0x2ad91
(
%
rip
)
%
ymm13
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
92
89
213
/
/
vmulps
%
ymm13
%
ymm4
%
ymm10
.
byte
196
98
125
24
53
135
173
2
0
/
/
vbroadcastss
0x2ad87
(
%
rip
)
%
ymm14
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
84
89
222
/
/
vmulps
%
ymm14
%
ymm5
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
24
61
120
173
2
0
/
/
vbroadcastss
0x2ad78
(
%
rip
)
%
ymm15
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
65
76
89
223
/
/
vmulps
%
ymm15
%
ymm6
%
ymm11
.
byte
196
193
44
88
195
/
/
vaddps
%
ymm11
%
ymm10
%
ymm0
.
byte
196
65
60
89
221
/
/
vmulps
%
ymm13
%
ymm8
%
ymm11
.
byte
196
65
52
89
230
/
/
vmulps
%
ymm14
%
ymm9
%
ymm12
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
108
89
231
/
/
vmulps
%
ymm7
%
ymm2
%
ymm12
.
byte
196
65
28
89
215
/
/
vmulps
%
ymm15
%
ymm12
%
ymm10
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
193
124
92
194
/
/
vsubps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
60
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
52
88
208
/
/
vaddps
%
ymm0
%
ymm9
%
ymm10
.
byte
197
28
88
216
/
/
vaddps
%
ymm0
%
ymm12
%
ymm11
.
byte
196
193
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm0
.
byte
197
60
93
224
/
/
vminps
%
ymm0
%
ymm8
%
ymm12
.
byte
196
193
60
89
197
/
/
vmulps
%
ymm13
%
ymm8
%
ymm0
.
byte
196
65
44
89
206
/
/
vmulps
%
ymm14
%
ymm10
%
ymm9
.
byte
196
193
124
88
193
/
/
vaddps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
65
36
89
207
/
/
vmulps
%
ymm15
%
ymm11
%
ymm9
.
byte
197
52
88
200
/
/
vaddps
%
ymm0
%
ymm9
%
ymm9
.
byte
196
193
60
92
193
/
/
vsubps
%
ymm9
%
ymm8
%
ymm0
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
65
52
92
236
/
/
vsubps
%
ymm12
%
ymm9
%
ymm13
.
byte
196
193
124
94
197
/
/
vdivps
%
ymm13
%
ymm0
%
ymm0
.
byte
196
65
44
92
241
/
/
vsubps
%
ymm9
%
ymm10
%
ymm14
.
byte
196
65
52
89
246
/
/
vmulps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
65
12
94
245
/
/
vdivps
%
ymm13
%
ymm14
%
ymm14
.
byte
196
65
36
92
249
/
/
vsubps
%
ymm9
%
ymm11
%
ymm15
.
byte
196
65
52
89
255
/
/
vmulps
%
ymm15
%
ymm9
%
ymm15
.
byte
196
65
4
94
237
/
/
vdivps
%
ymm13
%
ymm15
%
ymm13
.
byte
196
65
4
87
255
/
/
vxorps
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
4
194
228
2
/
/
vcmpleps
%
ymm12
%
ymm15
%
ymm12
.
byte
196
65
52
88
246
/
/
vaddps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
67
13
74
242
192
/
/
vblendvps
%
ymm12
%
ymm10
%
ymm14
%
ymm14
.
byte
196
65
44
95
211
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
52
88
237
/
/
vaddps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
219
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm13
%
ymm11
.
byte
197
180
88
192
/
/
vaddps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
200
192
/
/
vblendvps
%
ymm12
%
ymm8
%
ymm0
%
ymm1
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
60
95
194
/
/
vmaxps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
116
92
209
/
/
vsubps
%
ymm9
%
ymm1
%
ymm10
.
byte
196
65
28
92
233
/
/
vsubps
%
ymm9
%
ymm12
%
ymm13
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
28
194
192
1
/
/
vcmpltps
%
ymm8
%
ymm12
%
ymm0
.
byte
196
65
60
92
193
/
/
vsubps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
94
208
/
/
vdivps
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
52
88
210
/
/
vaddps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
195
117
74
202
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
12
92
209
/
/
vsubps
%
ymm9
%
ymm14
%
ymm10
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
44
94
208
/
/
vdivps
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
52
88
210
/
/
vaddps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
67
13
74
210
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm14
%
ymm10
.
byte
196
65
36
92
241
/
/
vsubps
%
ymm9
%
ymm11
%
ymm14
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
20
94
192
/
/
vdivps
%
ymm8
%
ymm13
%
ymm8
.
byte
196
65
52
88
192
/
/
vaddps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
193
116
95
207
/
/
vmaxps
%
ymm15
%
ymm1
%
ymm1
.
byte
196
65
44
95
207
/
/
vmaxps
%
ymm15
%
ymm10
%
ymm9
.
byte
196
195
37
74
192
0
/
/
vblendvps
%
ymm0
%
ymm8
%
ymm11
%
ymm0
.
byte
196
65
124
95
199
/
/
vmaxps
%
ymm15
%
ymm0
%
ymm8
.
byte
196
226
125
24
5
19
172
2
0
/
/
vbroadcastss
0x2ac13
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
124
92
215
/
/
vsubps
%
ymm7
%
ymm0
%
ymm10
.
byte
197
172
89
84
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm10
%
ymm2
.
byte
197
124
92
219
/
/
vsubps
%
ymm3
%
ymm0
%
ymm11
.
byte
197
164
89
196
/
/
vmulps
%
ymm4
%
ymm11
%
ymm0
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
172
89
76
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm10
%
ymm1
.
byte
197
164
89
213
/
/
vmulps
%
ymm5
%
ymm11
%
ymm2
.
byte
197
236
88
201
/
/
vaddps
%
ymm1
%
ymm2
%
ymm1
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
172
89
84
36
136
/
/
vmulps
-
0x78
(
%
rsp
)
%
ymm10
%
ymm2
.
byte
197
36
89
206
/
/
vmulps
%
ymm6
%
ymm11
%
ymm9
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
193
108
88
208
/
/
vaddps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminosity_avx
.
globl
_sk_luminosity_avx
FUNCTION
(
_sk_luminosity_avx
)
_sk_luminosity_avx
:
.
byte
197
124
40
226
/
/
vmovaps
%
ymm2
%
ymm12
.
byte
197
124
17
100
36
168
/
/
vmovups
%
ymm12
-
0x58
(
%
rsp
)
.
byte
197
252
17
76
36
200
/
/
vmovups
%
ymm1
-
0x38
(
%
rsp
)
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
100
89
196
/
/
vmulps
%
ymm4
%
ymm3
%
ymm8
.
byte
197
100
89
205
/
/
vmulps
%
ymm5
%
ymm3
%
ymm9
.
byte
196
98
125
24
45
193
171
2
0
/
/
vbroadcastss
0x2abc1
(
%
rip
)
%
ymm13
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
108
89
213
/
/
vmulps
%
ymm13
%
ymm2
%
ymm10
.
byte
196
98
125
24
53
183
171
2
0
/
/
vbroadcastss
0x2abb7
(
%
rip
)
%
ymm14
#
3c520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d4
>
.
byte
196
65
116
89
222
/
/
vmulps
%
ymm14
%
ymm1
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
24
61
168
171
2
0
/
/
vbroadcastss
0x2aba8
(
%
rip
)
%
ymm15
#
3c524
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d8
>
.
byte
196
65
28
89
223
/
/
vmulps
%
ymm15
%
ymm12
%
ymm11
.
byte
196
193
44
88
195
/
/
vaddps
%
ymm11
%
ymm10
%
ymm0
.
byte
196
65
60
89
221
/
/
vmulps
%
ymm13
%
ymm8
%
ymm11
.
byte
196
65
52
89
230
/
/
vmulps
%
ymm14
%
ymm9
%
ymm12
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
100
89
230
/
/
vmulps
%
ymm6
%
ymm3
%
ymm12
.
byte
196
65
28
89
215
/
/
vmulps
%
ymm15
%
ymm12
%
ymm10
.
byte
196
65
36
88
210
/
/
vaddps
%
ymm10
%
ymm11
%
ymm10
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
196
193
124
92
194
/
/
vsubps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
60
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
52
88
208
/
/
vaddps
%
ymm0
%
ymm9
%
ymm10
.
byte
197
28
88
216
/
/
vaddps
%
ymm0
%
ymm12
%
ymm11
.
byte
196
193
44
93
195
/
/
vminps
%
ymm11
%
ymm10
%
ymm0
.
byte
197
60
93
224
/
/
vminps
%
ymm0
%
ymm8
%
ymm12
.
byte
196
193
60
89
197
/
/
vmulps
%
ymm13
%
ymm8
%
ymm0
.
byte
196
65
44
89
206
/
/
vmulps
%
ymm14
%
ymm10
%
ymm9
.
byte
196
193
124
88
193
/
/
vaddps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
65
36
89
207
/
/
vmulps
%
ymm15
%
ymm11
%
ymm9
.
byte
197
52
88
200
/
/
vaddps
%
ymm0
%
ymm9
%
ymm9
.
byte
196
193
60
92
193
/
/
vsubps
%
ymm9
%
ymm8
%
ymm0
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
65
52
92
236
/
/
vsubps
%
ymm12
%
ymm9
%
ymm13
.
byte
196
193
124
94
197
/
/
vdivps
%
ymm13
%
ymm0
%
ymm0
.
byte
196
65
44
92
241
/
/
vsubps
%
ymm9
%
ymm10
%
ymm14
.
byte
196
65
52
89
246
/
/
vmulps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
65
12
94
245
/
/
vdivps
%
ymm13
%
ymm14
%
ymm14
.
byte
196
65
36
92
249
/
/
vsubps
%
ymm9
%
ymm11
%
ymm15
.
byte
196
65
52
89
255
/
/
vmulps
%
ymm15
%
ymm9
%
ymm15
.
byte
196
65
4
94
237
/
/
vdivps
%
ymm13
%
ymm15
%
ymm13
.
byte
196
65
4
87
255
/
/
vxorps
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
4
194
228
2
/
/
vcmpleps
%
ymm12
%
ymm15
%
ymm12
.
byte
196
65
52
88
246
/
/
vaddps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
67
13
74
242
192
/
/
vblendvps
%
ymm12
%
ymm10
%
ymm14
%
ymm14
.
byte
196
65
44
95
211
/
/
vmaxps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
52
88
237
/
/
vaddps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
219
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm13
%
ymm11
.
byte
197
180
88
192
/
/
vaddps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
195
125
74
200
192
/
/
vblendvps
%
ymm12
%
ymm8
%
ymm0
%
ymm1
.
byte
197
100
89
231
/
/
vmulps
%
ymm7
%
ymm3
%
ymm12
.
byte
196
65
60
95
194
/
/
vmaxps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
116
92
209
/
/
vsubps
%
ymm9
%
ymm1
%
ymm10
.
byte
196
65
28
92
233
/
/
vsubps
%
ymm9
%
ymm12
%
ymm13
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
193
28
194
192
1
/
/
vcmpltps
%
ymm8
%
ymm12
%
ymm0
.
byte
196
65
60
92
193
/
/
vsubps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
94
208
/
/
vdivps
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
52
88
210
/
/
vaddps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
195
117
74
202
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
12
92
209
/
/
vsubps
%
ymm9
%
ymm14
%
ymm10
.
byte
196
65
20
89
210
/
/
vmulps
%
ymm10
%
ymm13
%
ymm10
.
byte
196
65
44
94
208
/
/
vdivps
%
ymm8
%
ymm10
%
ymm10
.
byte
196
65
52
88
210
/
/
vaddps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
67
13
74
210
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm14
%
ymm10
.
byte
196
65
36
92
241
/
/
vsubps
%
ymm9
%
ymm11
%
ymm14
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
20
94
192
/
/
vdivps
%
ymm8
%
ymm13
%
ymm8
.
byte
196
65
52
88
192
/
/
vaddps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
193
116
95
207
/
/
vmaxps
%
ymm15
%
ymm1
%
ymm1
.
byte
196
65
44
95
207
/
/
vmaxps
%
ymm15
%
ymm10
%
ymm9
.
byte
196
195
37
74
192
0
/
/
vblendvps
%
ymm0
%
ymm8
%
ymm11
%
ymm0
.
byte
196
65
124
95
199
/
/
vmaxps
%
ymm15
%
ymm0
%
ymm8
.
byte
196
226
125
24
5
67
170
2
0
/
/
vbroadcastss
0x2aa43
(
%
rip
)
%
ymm0
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
124
92
215
/
/
vsubps
%
ymm7
%
ymm0
%
ymm10
.
byte
197
172
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm2
.
byte
197
124
92
219
/
/
vsubps
%
ymm3
%
ymm0
%
ymm11
.
byte
197
164
89
196
/
/
vmulps
%
ymm4
%
ymm11
%
ymm0
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
172
89
76
36
200
/
/
vmulps
-
0x38
(
%
rsp
)
%
ymm10
%
ymm1
.
byte
197
164
89
213
/
/
vmulps
%
ymm5
%
ymm11
%
ymm2
.
byte
197
236
88
201
/
/
vaddps
%
ymm1
%
ymm2
%
ymm1
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
172
89
84
36
168
/
/
vmulps
-
0x58
(
%
rsp
)
%
ymm10
%
ymm2
.
byte
197
36
89
206
/
/
vmulps
%
ymm6
%
ymm11
%
ymm9
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
193
108
88
208
/
/
vaddps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
92
220
/
/
vsubps
%
ymm12
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_avx
.
globl
_sk_srcover_rgba_8888_avx
FUNCTION
(
_sk_srcover_rgba_8888_avx
)
_sk_srcover_rgba_8888_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
87
1
0
0
/
/
jne
11c75
<
_sk_srcover_rgba_8888_avx
+
0x171
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
197
125
40
5
180
175
2
0
/
/
vmovapd
0x2afb4
(
%
rip
)
%
ymm8
#
3cae0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x894
>
.
byte
196
193
53
84
224
/
/
vandpd
%
ymm8
%
ymm9
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
81
114
209
8
/
/
vpsrld
0x8
%
xmm9
%
xmm5
.
byte
196
99
125
25
207
1
/
/
vextractf128
0x1
%
ymm9
%
xmm7
.
byte
197
201
114
215
8
/
/
vpsrld
0x8
%
xmm7
%
xmm6
.
byte
196
227
85
24
238
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm5
%
ymm5
.
byte
196
193
85
84
232
/
/
vandpd
%
ymm8
%
ymm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
41
114
209
16
/
/
vpsrld
0x10
%
xmm9
%
xmm10
.
byte
197
201
114
215
16
/
/
vpsrld
0x10
%
xmm7
%
xmm6
.
byte
196
227
45
24
246
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm10
%
ymm6
.
byte
196
193
77
84
240
/
/
vandpd
%
ymm8
%
ymm6
%
ymm6
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
193
49
114
209
24
/
/
vpsrld
0x18
%
xmm9
%
xmm9
.
byte
197
193
114
215
24
/
/
vpsrld
0x18
%
xmm7
%
xmm7
.
byte
196
227
53
24
255
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm9
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
13
106
169
2
0
/
/
vbroadcastss
0x2a96a
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
211
/
/
vsubps
%
ymm3
%
ymm9
%
ymm10
.
byte
196
98
125
24
13
137
169
2
0
/
/
vbroadcastss
0x2a989
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
44
89
220
/
/
vmulps
%
ymm4
%
ymm10
%
ymm11
.
byte
196
193
124
88
195
/
/
vaddps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
44
89
221
/
/
vmulps
%
ymm5
%
ymm10
%
ymm11
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
44
89
222
/
/
vmulps
%
ymm6
%
ymm10
%
ymm11
.
byte
196
193
108
88
211
/
/
vaddps
%
ymm11
%
ymm2
%
ymm2
.
byte
196
193
100
89
217
/
/
vmulps
%
ymm9
%
ymm3
%
ymm3
.
byte
197
44
89
215
/
/
vmulps
%
ymm7
%
ymm10
%
ymm10
.
byte
196
193
100
88
218
/
/
vaddps
%
ymm10
%
ymm3
%
ymm3
.
byte
197
60
95
208
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm10
.
byte
196
65
44
93
209
/
/
vminps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
125
91
210
/
/
vcvtps2dq
%
ymm10
%
ymm10
.
byte
197
60
95
217
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
65
36
93
217
/
/
vminps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
25
114
243
8
/
/
vpslld
0x8
%
xmm11
%
xmm12
.
byte
196
67
125
25
219
1
/
/
vextractf128
0x1
%
ymm11
%
xmm11
.
byte
196
193
33
114
243
8
/
/
vpslld
0x8
%
xmm11
%
xmm11
.
byte
196
67
29
24
219
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm12
%
ymm11
.
byte
196
65
37
86
210
/
/
vorpd
%
ymm10
%
ymm11
%
ymm10
.
byte
197
60
95
218
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm11
.
byte
196
65
36
93
217
/
/
vminps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
25
114
243
16
/
/
vpslld
0x10
%
xmm11
%
xmm12
.
byte
196
67
125
25
219
1
/
/
vextractf128
0x1
%
ymm11
%
xmm11
.
byte
196
193
33
114
243
16
/
/
vpslld
0x10
%
xmm11
%
xmm11
.
byte
196
67
29
24
219
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm12
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
193
/
/
vminps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
49
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm9
.
byte
196
67
125
25
192
1
/
/
vextractf128
0x1
%
ymm8
%
xmm8
.
byte
196
193
57
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm8
.
byte
196
67
53
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
37
86
192
/
/
vorpd
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
45
86
192
/
/
vorpd
%
ymm8
%
ymm10
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
59
/
/
jne
11ca6
<
_sk_srcover_rgba_8888_avx
+
0x1a2
>
.
byte
196
65
124
17
4
144
/
/
vmovups
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
156
254
255
255
/
/
ja
11b24
<
_sk_srcover_rgba_8888_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
254
0
0
0
/
/
lea
0xfe
(
%
rip
)
%
r9
#
11d90
<
_sk_srcover_rgba_8888_avx
+
0x28c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
126
254
255
255
/
/
jmpq
11b24
<
_sk_srcover_rgba_8888_avx
+
0x20
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
193
/
/
ja
11c71
<
_sk_srcover_rgba_8888_avx
+
0x16d
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
242
0
0
0
/
/
lea
0xf2
(
%
rip
)
%
r9
#
11dac
<
_sk_srcover_rgba_8888_avx
+
0x2a8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
166
/
/
jmp
11c71
<
_sk_srcover_rgba_8888_avx
+
0x16d
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
4
/
/
vblendps
0x4
%
ymm4
%
ymm5
%
ymm9
.
byte
196
193
123
16
36
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
99
53
13
204
1
/
/
vblendpd
0x1
%
ymm4
%
ymm9
%
ymm9
.
byte
233
50
254
255
255
/
/
jmpq
11b24
<
_sk_srcover_rgba_8888_avx
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
196
227
125
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm0
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
64
/
/
vblendps
0x40
%
ymm4
%
ymm5
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
193
121
16
36
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
67
93
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm4
%
ymm9
.
byte
233
221
253
255
255
/
/
jmpq
11b24
<
_sk_srcover_rgba_8888_avx
+
0x20
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
23
255
255
255
/
/
jmpq
11c71
<
_sk_srcover_rgba_8888_avx
+
0x16d
>
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
121
17
4
144
/
/
vmovupd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
227
254
255
255
/
/
jmpq
11c71
<
_sk_srcover_rgba_8888_avx
+
0x16d
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
11
255
/
/
or
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
81
255
/
/
callq
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
59
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
166
255
255
255
146
/
/
jmpq
*
-
0x6d000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
126
255
/
/
jle
11da5
<
_sk_srcover_rgba_8888_avx
+
0x2a1
>
.
byte
255
/
/
(
bad
)
.
byte
255
98
255
/
/
jmpq
*
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
23
/
/
callq
*
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
163
255
255
255
155
/
/
jmpq
*
-
0x64000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
215
/
/
callq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
202
/
/
dec
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
255
255
255
174
/
/
mov
0xaeffffff
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_srcover_bgra_8888_avx
.
globl
_sk_srcover_bgra_8888_avx
FUNCTION
(
_sk_srcover_bgra_8888_avx
)
_sk_srcover_bgra_8888_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
87
1
0
0
/
/
jne
11f39
<
_sk_srcover_bgra_8888_avx
+
0x171
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
197
125
40
5
16
173
2
0
/
/
vmovapd
0x2ad10
(
%
rip
)
%
ymm8
#
3cb00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x8b4
>
.
byte
196
193
53
84
232
/
/
vandpd
%
ymm8
%
ymm9
%
ymm5
.
byte
197
252
91
245
/
/
vcvtdq2ps
%
ymm5
%
ymm6
.
byte
196
193
81
114
209
8
/
/
vpsrld
0x8
%
xmm9
%
xmm5
.
byte
196
99
125
25
207
1
/
/
vextractf128
0x1
%
ymm9
%
xmm7
.
byte
197
217
114
215
8
/
/
vpsrld
0x8
%
xmm7
%
xmm4
.
byte
196
227
85
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
196
193
93
84
224
/
/
vandpd
%
ymm8
%
ymm4
%
ymm4
.
byte
197
252
91
236
/
/
vcvtdq2ps
%
ymm4
%
ymm5
.
byte
196
193
41
114
209
16
/
/
vpsrld
0x10
%
xmm9
%
xmm10
.
byte
197
217
114
215
16
/
/
vpsrld
0x10
%
xmm7
%
xmm4
.
byte
196
227
45
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm10
%
ymm4
.
byte
196
193
93
84
224
/
/
vandpd
%
ymm8
%
ymm4
%
ymm4
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
49
114
209
24
/
/
vpsrld
0x18
%
xmm9
%
xmm9
.
byte
197
193
114
215
24
/
/
vpsrld
0x18
%
xmm7
%
xmm7
.
byte
196
227
53
24
255
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm9
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
13
166
166
2
0
/
/
vbroadcastss
0x2a6a6
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
52
92
211
/
/
vsubps
%
ymm3
%
ymm9
%
ymm10
.
byte
196
98
125
24
13
197
166
2
0
/
/
vbroadcastss
0x2a6c5
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
44
89
220
/
/
vmulps
%
ymm4
%
ymm10
%
ymm11
.
byte
196
193
124
88
195
/
/
vaddps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
44
89
221
/
/
vmulps
%
ymm5
%
ymm10
%
ymm11
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
197
44
89
222
/
/
vmulps
%
ymm6
%
ymm10
%
ymm11
.
byte
196
193
108
88
211
/
/
vaddps
%
ymm11
%
ymm2
%
ymm2
.
byte
196
193
100
89
217
/
/
vmulps
%
ymm9
%
ymm3
%
ymm3
.
byte
197
44
89
215
/
/
vmulps
%
ymm7
%
ymm10
%
ymm10
.
byte
196
193
100
88
218
/
/
vaddps
%
ymm10
%
ymm3
%
ymm3
.
byte
197
60
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm10
.
byte
196
65
44
93
209
/
/
vminps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
65
125
91
210
/
/
vcvtps2dq
%
ymm10
%
ymm10
.
byte
197
60
95
217
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
65
36
93
217
/
/
vminps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
25
114
243
8
/
/
vpslld
0x8
%
xmm11
%
xmm12
.
byte
196
67
125
25
219
1
/
/
vextractf128
0x1
%
ymm11
%
xmm11
.
byte
196
193
33
114
243
8
/
/
vpslld
0x8
%
xmm11
%
xmm11
.
byte
196
67
29
24
219
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm12
%
ymm11
.
byte
196
65
37
86
210
/
/
vorpd
%
ymm10
%
ymm11
%
ymm10
.
byte
197
60
95
216
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm11
.
byte
196
65
36
93
217
/
/
vminps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
25
114
243
16
/
/
vpslld
0x10
%
xmm11
%
xmm12
.
byte
196
67
125
25
219
1
/
/
vextractf128
0x1
%
ymm11
%
xmm11
.
byte
196
193
33
114
243
16
/
/
vpslld
0x10
%
xmm11
%
xmm11
.
byte
196
67
29
24
219
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm12
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
193
/
/
vminps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
49
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm9
.
byte
196
67
125
25
192
1
/
/
vextractf128
0x1
%
ymm8
%
xmm8
.
byte
196
193
57
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm8
.
byte
196
67
53
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
37
86
192
/
/
vorpd
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
45
86
192
/
/
vorpd
%
ymm8
%
ymm10
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
59
/
/
jne
11f6a
<
_sk_srcover_bgra_8888_avx
+
0x1a2
>
.
byte
196
65
124
17
4
144
/
/
vmovups
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
156
254
255
255
/
/
ja
11de8
<
_sk_srcover_bgra_8888_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
254
0
0
0
/
/
lea
0xfe
(
%
rip
)
%
r9
#
12054
<
_sk_srcover_bgra_8888_avx
+
0x28c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
126
254
255
255
/
/
jmpq
11de8
<
_sk_srcover_bgra_8888_avx
+
0x20
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
193
/
/
ja
11f35
<
_sk_srcover_bgra_8888_avx
+
0x16d
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
242
0
0
0
/
/
lea
0xf2
(
%
rip
)
%
r9
#
12070
<
_sk_srcover_bgra_8888_avx
+
0x2a8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
166
/
/
jmp
11f35
<
_sk_srcover_bgra_8888_avx
+
0x16d
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
4
/
/
vblendps
0x4
%
ymm4
%
ymm5
%
ymm9
.
byte
196
193
123
16
36
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
99
53
13
204
1
/
/
vblendpd
0x1
%
ymm4
%
ymm9
%
ymm9
.
byte
233
50
254
255
255
/
/
jmpq
11de8
<
_sk_srcover_bgra_8888_avx
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
196
227
125
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm0
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
64
/
/
vblendps
0x40
%
ymm4
%
ymm5
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
193
121
16
36
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
67
93
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm4
%
ymm9
.
byte
233
221
253
255
255
/
/
jmpq
11de8
<
_sk_srcover_bgra_8888_avx
+
0x20
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
23
255
255
255
/
/
jmpq
11f35
<
_sk_srcover_bgra_8888_avx
+
0x16d
>
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
121
17
4
144
/
/
vmovupd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
227
254
255
255
/
/
jmpq
11f35
<
_sk_srcover_bgra_8888_avx
+
0x16d
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
11
255
/
/
or
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
81
255
/
/
callq
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
59
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
166
255
255
255
146
/
/
jmpq
*
-
0x6d000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
126
255
/
/
jle
12069
<
_sk_srcover_bgra_8888_avx
+
0x2a1
>
.
byte
255
/
/
(
bad
)
.
byte
255
98
255
/
/
jmpq
*
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
23
/
/
callq
*
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
163
255
255
255
155
/
/
jmpq
*
-
0x64000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
215
/
/
callq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
202
/
/
dec
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
255
255
255
174
/
/
mov
0xaeffffff
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_clamp_0_avx
.
globl
_sk_clamp_0_avx
FUNCTION
(
_sk_clamp_0_avx
)
_sk_clamp_0_avx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
95
200
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
108
95
208
/
/
vmaxps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
95
216
/
/
vmaxps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_1_avx
.
globl
_sk_clamp_1_avx
FUNCTION
(
_sk_clamp_1_avx
)
_sk_clamp_1_avx
:
.
byte
196
98
125
24
5
74
164
2
0
/
/
vbroadcastss
0x2a44a
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_avx
.
globl
_sk_clamp_a_avx
FUNCTION
(
_sk_clamp_a_avx
)
_sk_clamp_a_avx
:
.
byte
196
98
125
24
5
41
164
2
0
/
/
vbroadcastss
0x2a429
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
252
93
195
/
/
vminps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
244
93
203
/
/
vminps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
236
93
211
/
/
vminps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_avx
.
globl
_sk_clamp_a_dst_avx
FUNCTION
(
_sk_clamp_a_dst_avx
)
_sk_clamp_a_dst_avx
:
.
byte
196
98
125
24
5
11
164
2
0
/
/
vbroadcastss
0x2a40b
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
68
93
248
/
/
vminps
%
ymm8
%
ymm7
%
ymm7
.
byte
197
220
93
231
/
/
vminps
%
ymm7
%
ymm4
%
ymm4
.
byte
197
212
93
239
/
/
vminps
%
ymm7
%
ymm5
%
ymm5
.
byte
197
204
93
247
/
/
vminps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_avx
.
globl
_sk_set_rgb_avx
FUNCTION
(
_sk_set_rgb_avx
)
_sk_set_rgb_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm0
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_avx
.
globl
_sk_swap_rb_avx
FUNCTION
(
_sk_swap_rb_avx
)
_sk_swap_rb_avx
:
.
byte
197
124
40
192
/
/
vmovaps
%
ymm0
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
194
/
/
vmovaps
%
ymm2
%
ymm0
.
byte
197
124
41
194
/
/
vmovaps
%
ymm8
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_avx
.
globl
_sk_invert_avx
FUNCTION
(
_sk_invert_avx
)
_sk_invert_avx
:
.
byte
196
98
125
24
5
198
163
2
0
/
/
vbroadcastss
0x2a3c6
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
92
201
/
/
vsubps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
92
210
/
/
vsubps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
92
219
/
/
vsubps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_avx
.
globl
_sk_move_src_dst_avx
FUNCTION
(
_sk_move_src_dst_avx
)
_sk_move_src_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
224
/
/
vmovaps
%
ymm0
%
ymm4
.
byte
197
252
40
233
/
/
vmovaps
%
ymm1
%
ymm5
.
byte
197
252
40
242
/
/
vmovaps
%
ymm2
%
ymm6
.
byte
197
252
40
251
/
/
vmovaps
%
ymm3
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_avx
.
globl
_sk_move_dst_src_avx
FUNCTION
(
_sk_move_dst_src_avx
)
_sk_move_dst_src_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
196
/
/
vmovaps
%
ymm4
%
ymm0
.
byte
197
252
40
205
/
/
vmovaps
%
ymm5
%
ymm1
.
byte
197
252
40
214
/
/
vmovaps
%
ymm6
%
ymm2
.
byte
197
252
40
223
/
/
vmovaps
%
ymm7
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_avx
.
globl
_sk_premul_avx
FUNCTION
(
_sk_premul_avx
)
_sk_premul_avx
:
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_avx
.
globl
_sk_premul_dst_avx
FUNCTION
(
_sk_premul_dst_avx
)
_sk_premul_dst_avx
:
.
byte
197
220
89
231
/
/
vmulps
%
ymm7
%
ymm4
%
ymm4
.
byte
197
212
89
239
/
/
vmulps
%
ymm7
%
ymm5
%
ymm5
.
byte
197
204
89
247
/
/
vmulps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_unpremul_avx
.
globl
_sk_unpremul_avx
FUNCTION
(
_sk_unpremul_avx
)
_sk_unpremul_avx
:
.
byte
196
98
125
24
5
97
163
2
0
/
/
vbroadcastss
0x2a361
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
94
195
/
/
vdivps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
132
163
2
0
/
/
vbroadcastss
0x2a384
(
%
rip
)
%
ymm9
#
3c52c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e0
>
.
byte
196
65
60
194
201
1
/
/
vcmpltps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
45
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm10
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_avx
.
globl
_sk_force_opaque_avx
FUNCTION
(
_sk_force_opaque_avx
)
_sk_force_opaque_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
40
163
2
0
/
/
vbroadcastss
0x2a328
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_avx
.
globl
_sk_force_opaque_dst_avx
FUNCTION
(
_sk_force_opaque_dst_avx
)
_sk_force_opaque_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
27
163
2
0
/
/
vbroadcastss
0x2a31b
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_avx
.
globl
_sk_from_srgb_avx
FUNCTION
(
_sk_from_srgb_avx
)
_sk_from_srgb_avx
:
.
byte
196
98
125
24
5
68
163
2
0
/
/
vbroadcastss
0x2a344
(
%
rip
)
%
ymm8
#
3c530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e4
>
.
byte
196
65
124
89
200
/
/
vmulps
%
ymm8
%
ymm0
%
ymm9
.
byte
197
124
89
208
/
/
vmulps
%
ymm0
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
30
163
2
0
/
/
vbroadcastss
0x2a31e
(
%
rip
)
%
ymm11
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
124
89
227
/
/
vmulps
%
ymm11
%
ymm0
%
ymm12
.
byte
196
98
125
24
45
40
163
2
0
/
/
vbroadcastss
0x2a328
(
%
rip
)
%
ymm13
#
3c534
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e8
>
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
65
44
89
212
/
/
vmulps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
98
125
24
37
25
163
2
0
/
/
vbroadcastss
0x2a319
(
%
rip
)
%
ymm12
#
3c538
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ec
>
.
byte
196
65
44
88
212
/
/
vaddps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
98
125
24
53
15
163
2
0
/
/
vbroadcastss
0x2a30f
(
%
rip
)
%
ymm14
#
3c53c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f0
>
.
byte
196
193
124
194
198
1
/
/
vcmpltps
%
ymm14
%
ymm0
%
ymm0
.
byte
196
195
45
74
193
0
/
/
vblendvps
%
ymm0
%
ymm9
%
ymm10
%
ymm0
.
byte
196
65
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm9
.
byte
197
116
89
209
/
/
vmulps
%
ymm1
%
ymm1
%
ymm10
.
byte
196
65
116
89
251
/
/
vmulps
%
ymm11
%
ymm1
%
ymm15
.
byte
196
65
4
88
253
/
/
vaddps
%
ymm13
%
ymm15
%
ymm15
.
byte
196
65
44
89
215
/
/
vmulps
%
ymm15
%
ymm10
%
ymm10
.
byte
196
65
44
88
212
/
/
vaddps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
193
116
194
206
1
/
/
vcmpltps
%
ymm14
%
ymm1
%
ymm1
.
byte
196
195
45
74
201
16
/
/
vblendvps
%
ymm1
%
ymm9
%
ymm10
%
ymm1
.
byte
196
65
108
89
192
/
/
vmulps
%
ymm8
%
ymm2
%
ymm8
.
byte
197
108
89
202
/
/
vmulps
%
ymm2
%
ymm2
%
ymm9
.
byte
196
65
108
89
211
/
/
vmulps
%
ymm11
%
ymm2
%
ymm10
.
byte
196
65
44
88
213
/
/
vaddps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
52
88
204
/
/
vaddps
%
ymm12
%
ymm9
%
ymm9
.
byte
196
193
108
194
214
1
/
/
vcmpltps
%
ymm14
%
ymm2
%
ymm2
.
byte
196
195
53
74
208
32
/
/
vblendvps
%
ymm2
%
ymm8
%
ymm9
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_dst_avx
.
globl
_sk_from_srgb_dst_avx
FUNCTION
(
_sk_from_srgb_dst_avx
)
_sk_from_srgb_dst_avx
:
.
byte
196
98
125
24
5
152
162
2
0
/
/
vbroadcastss
0x2a298
(
%
rip
)
%
ymm8
#
3c530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e4
>
.
byte
196
65
92
89
200
/
/
vmulps
%
ymm8
%
ymm4
%
ymm9
.
byte
197
92
89
212
/
/
vmulps
%
ymm4
%
ymm4
%
ymm10
.
byte
196
98
125
24
29
114
162
2
0
/
/
vbroadcastss
0x2a272
(
%
rip
)
%
ymm11
#
3c51c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2d0
>
.
byte
196
65
92
89
227
/
/
vmulps
%
ymm11
%
ymm4
%
ymm12
.
byte
196
98
125
24
45
124
162
2
0
/
/
vbroadcastss
0x2a27c
(
%
rip
)
%
ymm13
#
3c534
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2e8
>
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
65
44
89
212
/
/
vmulps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
98
125
24
37
109
162
2
0
/
/
vbroadcastss
0x2a26d
(
%
rip
)
%
ymm12
#
3c538
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ec
>
.
byte
196
65
44
88
212
/
/
vaddps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
98
125
24
53
99
162
2
0
/
/
vbroadcastss
0x2a263
(
%
rip
)
%
ymm14
#
3c53c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f0
>
.
byte
196
193
92
194
230
1
/
/
vcmpltps
%
ymm14
%
ymm4
%
ymm4
.
byte
196
195
45
74
225
64
/
/
vblendvps
%
ymm4
%
ymm9
%
ymm10
%
ymm4
.
byte
196
65
84
89
200
/
/
vmulps
%
ymm8
%
ymm5
%
ymm9
.
byte
197
84
89
213
/
/
vmulps
%
ymm5
%
ymm5
%
ymm10
.
byte
196
65
84
89
251
/
/
vmulps
%
ymm11
%
ymm5
%
ymm15
.
byte
196
65
4
88
253
/
/
vaddps
%
ymm13
%
ymm15
%
ymm15
.
byte
196
65
44
89
215
/
/
vmulps
%
ymm15
%
ymm10
%
ymm10
.
byte
196
65
44
88
212
/
/
vaddps
%
ymm12
%
ymm10
%
ymm10
.
byte
196
193
84
194
238
1
/
/
vcmpltps
%
ymm14
%
ymm5
%
ymm5
.
byte
196
195
45
74
233
80
/
/
vblendvps
%
ymm5
%
ymm9
%
ymm10
%
ymm5
.
byte
196
65
76
89
192
/
/
vmulps
%
ymm8
%
ymm6
%
ymm8
.
byte
197
76
89
206
/
/
vmulps
%
ymm6
%
ymm6
%
ymm9
.
byte
196
65
76
89
211
/
/
vmulps
%
ymm11
%
ymm6
%
ymm10
.
byte
196
65
44
88
213
/
/
vaddps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
52
88
204
/
/
vaddps
%
ymm12
%
ymm9
%
ymm9
.
byte
196
193
76
194
246
1
/
/
vcmpltps
%
ymm14
%
ymm6
%
ymm6
.
byte
196
195
53
74
240
96
/
/
vblendvps
%
ymm6
%
ymm8
%
ymm9
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_to_srgb_avx
.
globl
_sk_to_srgb_avx
FUNCTION
(
_sk_to_srgb_avx
)
_sk_to_srgb_avx
:
.
byte
197
124
82
200
/
/
vrsqrtps
%
ymm0
%
ymm9
.
byte
196
98
125
24
5
248
161
2
0
/
/
vbroadcastss
0x2a1f8
(
%
rip
)
%
ymm8
#
3c540
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f4
>
.
byte
196
65
124
89
208
/
/
vmulps
%
ymm8
%
ymm0
%
ymm10
.
byte
196
98
125
24
29
242
161
2
0
/
/
vbroadcastss
0x2a1f2
(
%
rip
)
%
ymm11
#
3c548
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2fc
>
.
byte
196
65
52
89
227
/
/
vmulps
%
ymm11
%
ymm9
%
ymm12
.
byte
196
98
125
24
45
224
161
2
0
/
/
vbroadcastss
0x2a1e0
(
%
rip
)
%
ymm13
#
3c544
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2f8
>
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
65
52
89
228
/
/
vmulps
%
ymm12
%
ymm9
%
ymm12
.
byte
196
98
125
24
53
13
163
2
0
/
/
vbroadcastss
0x2a30d
(
%
rip
)
%
ymm14
#
3c684
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x438
>
.
byte
196
65
28
88
230
/
/
vaddps
%
ymm14
%
ymm12
%
ymm12
.
byte
196
98
125
24
61
3
163
2
0
/
/
vbroadcastss
0x2a303
(
%
rip
)
%
ymm15
#
3c688
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x43c
>
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
124
83
201
/
/
vrcpps
%
ymm9
%
ymm9
.
byte
196
65
52
89
204
/
/
vmulps
%
ymm12
%
ymm9
%
ymm9
.
byte
196
98
125
24
37
183
161
2
0
/
/
vbroadcastss
0x2a1b7
(
%
rip
)
%
ymm12
#
3c554
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x308
>
.
byte
196
193
124
194
196
1
/
/
vcmpltps
%
ymm12
%
ymm0
%
ymm0
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
197
124
82
201
/
/
vrsqrtps
%
ymm1
%
ymm9
.
byte
196
65
52
89
211
/
/
vmulps
%
ymm11
%
ymm9
%
ymm10
.
byte
196
65
44
88
213
/
/
vaddps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
52
89
210
/
/
vmulps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
65
44
88
214
/
/
vaddps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
124
83
201
/
/
vrcpps
%
ymm9
%
ymm9
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
116
89
208
/
/
vmulps
%
ymm8
%
ymm1
%
ymm10
.
byte
196
193
116
194
204
1
/
/
vcmpltps
%
ymm12
%
ymm1
%
ymm1
.
byte
196
195
53
74
202
16
/
/
vblendvps
%
ymm1
%
ymm10
%
ymm9
%
ymm1
.
byte
197
124
82
202
/
/
vrsqrtps
%
ymm2
%
ymm9
.
byte
196
65
52
89
211
/
/
vmulps
%
ymm11
%
ymm9
%
ymm10
.
byte
196
65
44
88
213
/
/
vaddps
%
ymm13
%
ymm10
%
ymm10
.
byte
196
65
52
89
210
/
/
vmulps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
65
44
88
214
/
/
vaddps
%
ymm14
%
ymm10
%
ymm10
.
byte
196
65
52
88
207
/
/
vaddps
%
ymm15
%
ymm9
%
ymm9
.
byte
196
65
124
83
201
/
/
vrcpps
%
ymm9
%
ymm9
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
108
89
192
/
/
vmulps
%
ymm8
%
ymm2
%
ymm8
.
byte
196
193
108
194
212
1
/
/
vcmpltps
%
ymm12
%
ymm2
%
ymm2
.
byte
196
195
53
74
208
32
/
/
vblendvps
%
ymm2
%
ymm8
%
ymm9
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_rgb_to_hsl_avx
.
globl
_sk_rgb_to_hsl_avx
FUNCTION
(
_sk_rgb_to_hsl_avx
)
_sk_rgb_to_hsl_avx
:
.
byte
197
116
95
194
/
/
vmaxps
%
ymm2
%
ymm1
%
ymm8
.
byte
196
65
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm8
.
byte
197
116
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm9
.
byte
196
65
124
93
201
/
/
vminps
%
ymm9
%
ymm0
%
ymm9
.
byte
196
65
60
92
209
/
/
vsubps
%
ymm9
%
ymm8
%
ymm10
.
byte
196
98
125
24
29
191
160
2
0
/
/
vbroadcastss
0x2a0bf
(
%
rip
)
%
ymm11
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
36
94
218
/
/
vdivps
%
ymm10
%
ymm11
%
ymm11
.
byte
197
116
92
226
/
/
vsubps
%
ymm2
%
ymm1
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
197
116
194
234
1
/
/
vcmpltps
%
ymm2
%
ymm1
%
ymm13
.
byte
197
60
194
241
0
/
/
vcmpeqps
%
ymm1
%
ymm8
%
ymm14
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
193
108
89
211
/
/
vmulps
%
ymm11
%
ymm2
%
ymm2
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
196
193
116
89
203
/
/
vmulps
%
ymm11
%
ymm1
%
ymm1
.
byte
196
98
125
24
29
240
160
2
0
/
/
vbroadcastss
0x2a0f0
(
%
rip
)
%
ymm11
#
3c560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x314
>
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
196
98
125
24
29
222
160
2
0
/
/
vbroadcastss
0x2a0de
(
%
rip
)
%
ymm11
#
3c55c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x310
>
.
byte
196
193
108
88
211
/
/
vaddps
%
ymm11
%
ymm2
%
ymm2
.
byte
196
227
117
74
202
224
/
/
vblendvps
%
ymm14
%
ymm2
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
198
160
2
0
/
/
vbroadcastss
0x2a0c6
(
%
rip
)
%
ymm2
#
3c558
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30c
>
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
227
13
74
210
208
/
/
vblendvps
%
ymm13
%
ymm2
%
ymm14
%
ymm2
.
byte
197
188
194
192
0
/
/
vcmpeqps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
193
108
88
212
/
/
vaddps
%
ymm12
%
ymm2
%
ymm2
.
byte
196
227
117
74
194
0
/
/
vblendvps
%
ymm0
%
ymm2
%
ymm1
%
ymm0
.
byte
196
193
60
88
201
/
/
vaddps
%
ymm9
%
ymm8
%
ymm1
.
byte
196
98
125
24
37
61
160
2
0
/
/
vbroadcastss
0x2a03d
(
%
rip
)
%
ymm12
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
116
89
212
/
/
vmulps
%
ymm12
%
ymm1
%
ymm2
.
byte
197
28
194
226
1
/
/
vcmpltps
%
ymm2
%
ymm12
%
ymm12
.
byte
196
65
36
92
216
/
/
vsubps
%
ymm8
%
ymm11
%
ymm11
.
byte
196
65
36
92
217
/
/
vsubps
%
ymm9
%
ymm11
%
ymm11
.
byte
196
195
117
74
203
192
/
/
vblendvps
%
ymm12
%
ymm11
%
ymm1
%
ymm1
.
byte
196
65
60
194
193
0
/
/
vcmpeqps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
172
94
201
/
/
vdivps
%
ymm1
%
ymm10
%
ymm1
.
byte
196
195
125
74
198
128
/
/
vblendvps
%
ymm8
%
ymm14
%
ymm0
%
ymm0
.
byte
196
195
117
74
206
128
/
/
vblendvps
%
ymm8
%
ymm14
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
112
160
2
0
/
/
vbroadcastss
0x2a070
(
%
rip
)
%
ymm8
#
3c564
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x318
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hsl_to_rgb_avx
.
globl
_sk_hsl_to_rgb_avx
FUNCTION
(
_sk_hsl_to_rgb_avx
)
_sk_hsl_to_rgb_avx
:
.
byte
72
131
236
56
/
/
sub
0x38
%
rsp
.
byte
197
252
17
60
36
/
/
vmovups
%
ymm7
(
%
rsp
)
.
byte
197
252
17
116
36
224
/
/
vmovups
%
ymm6
-
0x20
(
%
rsp
)
.
byte
197
252
17
108
36
192
/
/
vmovups
%
ymm5
-
0x40
(
%
rsp
)
.
byte
197
252
17
100
36
160
/
/
vmovups
%
ymm4
-
0x60
(
%
rsp
)
.
byte
197
252
17
92
36
128
/
/
vmovups
%
ymm3
-
0x80
(
%
rsp
)
.
byte
197
252
40
225
/
/
vmovaps
%
ymm1
%
ymm4
.
byte
197
252
40
216
/
/
vmovaps
%
ymm0
%
ymm3
.
byte
196
98
125
24
5
201
159
2
0
/
/
vbroadcastss
0x29fc9
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
60
194
202
2
/
/
vcmpleps
%
ymm2
%
ymm8
%
ymm9
.
byte
197
92
89
210
/
/
vmulps
%
ymm2
%
ymm4
%
ymm10
.
byte
196
65
92
92
218
/
/
vsubps
%
ymm10
%
ymm4
%
ymm11
.
byte
196
67
45
74
203
144
/
/
vblendvps
%
ymm9
%
ymm11
%
ymm10
%
ymm9
.
byte
197
52
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm10
.
byte
197
108
88
202
/
/
vaddps
%
ymm2
%
ymm2
%
ymm9
.
byte
196
65
52
92
202
/
/
vsubps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
15
160
2
0
/
/
vbroadcastss
0x2a00f
(
%
rip
)
%
ymm11
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
65
100
88
219
/
/
vaddps
%
ymm11
%
ymm3
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
252
/
/
vsubps
%
ymm12
%
ymm11
%
ymm15
.
byte
196
65
44
92
217
/
/
vsubps
%
ymm9
%
ymm10
%
ymm11
.
byte
196
98
125
24
37
225
159
2
0
/
/
vbroadcastss
0x29fe1
(
%
rip
)
%
ymm12
#
3c558
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30c
>
.
byte
196
193
4
89
196
/
/
vmulps
%
ymm12
%
ymm15
%
ymm0
.
byte
196
98
125
24
45
219
159
2
0
/
/
vbroadcastss
0x29fdb
(
%
rip
)
%
ymm13
#
3c560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x314
>
.
byte
197
20
92
240
/
/
vsubps
%
ymm0
%
ymm13
%
ymm14
.
byte
196
65
36
89
246
/
/
vmulps
%
ymm14
%
ymm11
%
ymm14
.
byte
196
65
52
88
246
/
/
vaddps
%
ymm14
%
ymm9
%
ymm14
.
byte
196
226
125
24
13
208
159
2
0
/
/
vbroadcastss
0x29fd0
(
%
rip
)
%
ymm1
#
3c56c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x320
>
.
byte
196
193
116
194
255
2
/
/
vcmpleps
%
ymm15
%
ymm1
%
ymm7
.
byte
196
195
13
74
249
112
/
/
vblendvps
%
ymm7
%
ymm9
%
ymm14
%
ymm7
.
byte
196
65
60
194
247
2
/
/
vcmpleps
%
ymm15
%
ymm8
%
ymm14
.
byte
196
227
45
74
255
224
/
/
vblendvps
%
ymm14
%
ymm7
%
ymm10
%
ymm7
.
byte
196
98
125
24
53
167
159
2
0
/
/
vbroadcastss
0x29fa7
(
%
rip
)
%
ymm14
#
3c564
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x318
>
.
byte
196
65
12
194
255
2
/
/
vcmpleps
%
ymm15
%
ymm14
%
ymm15
.
byte
196
193
124
89
195
/
/
vmulps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
180
88
192
/
/
vaddps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
99
125
74
255
240
/
/
vblendvps
%
ymm15
%
ymm7
%
ymm0
%
ymm15
.
byte
196
227
125
8
195
1
/
/
vroundps
0x1
%
ymm3
%
ymm0
.
byte
197
228
92
192
/
/
vsubps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
89
252
/
/
vmulps
%
ymm12
%
ymm0
%
ymm7
.
byte
197
148
92
247
/
/
vsubps
%
ymm7
%
ymm13
%
ymm6
.
byte
197
164
89
246
/
/
vmulps
%
ymm6
%
ymm11
%
ymm6
.
byte
197
180
88
246
/
/
vaddps
%
ymm6
%
ymm9
%
ymm6
.
byte
197
244
194
232
2
/
/
vcmpleps
%
ymm0
%
ymm1
%
ymm5
.
byte
196
195
77
74
233
80
/
/
vblendvps
%
ymm5
%
ymm9
%
ymm6
%
ymm5
.
byte
197
188
194
240
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm6
.
byte
196
227
45
74
237
96
/
/
vblendvps
%
ymm6
%
ymm5
%
ymm10
%
ymm5
.
byte
197
140
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm14
%
ymm0
.
byte
197
164
89
247
/
/
vmulps
%
ymm7
%
ymm11
%
ymm6
.
byte
197
180
88
246
/
/
vaddps
%
ymm6
%
ymm9
%
ymm6
.
byte
196
227
77
74
237
0
/
/
vblendvps
%
ymm0
%
ymm5
%
ymm6
%
ymm5
.
byte
196
226
125
24
5
81
159
2
0
/
/
vbroadcastss
0x29f51
(
%
rip
)
%
ymm0
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
197
228
88
192
/
/
vaddps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
227
125
8
216
1
/
/
vroundps
0x1
%
ymm0
%
ymm3
.
byte
197
252
92
195
/
/
vsubps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
244
194
200
2
/
/
vcmpleps
%
ymm0
%
ymm1
%
ymm1
.
byte
196
193
124
89
220
/
/
vmulps
%
ymm12
%
ymm0
%
ymm3
.
byte
197
148
92
243
/
/
vsubps
%
ymm3
%
ymm13
%
ymm6
.
byte
197
164
89
246
/
/
vmulps
%
ymm6
%
ymm11
%
ymm6
.
byte
197
180
88
246
/
/
vaddps
%
ymm6
%
ymm9
%
ymm6
.
byte
196
195
77
74
201
16
/
/
vblendvps
%
ymm1
%
ymm9
%
ymm6
%
ymm1
.
byte
197
188
194
240
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm6
.
byte
196
227
45
74
201
96
/
/
vblendvps
%
ymm6
%
ymm1
%
ymm10
%
ymm1
.
byte
197
140
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm14
%
ymm0
.
byte
197
164
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm3
.
byte
197
180
88
219
/
/
vaddps
%
ymm3
%
ymm9
%
ymm3
.
byte
196
227
101
74
217
0
/
/
vblendvps
%
ymm0
%
ymm1
%
ymm3
%
ymm3
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
220
194
224
0
/
/
vcmpeqps
%
ymm0
%
ymm4
%
ymm4
.
byte
196
227
5
74
194
64
/
/
vblendvps
%
ymm4
%
ymm2
%
ymm15
%
ymm0
.
byte
196
227
85
74
202
64
/
/
vblendvps
%
ymm4
%
ymm2
%
ymm5
%
ymm1
.
byte
196
227
101
74
210
64
/
/
vblendvps
%
ymm4
%
ymm2
%
ymm3
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
92
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm3
.
byte
197
252
16
100
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm6
.
byte
197
252
16
60
36
/
/
vmovups
(
%
rsp
)
%
ymm7
.
byte
72
131
196
56
/
/
add
0x38
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_avx
.
globl
_sk_scale_1_float_avx
FUNCTION
(
_sk_scale_1_float_avx
)
_sk_scale_1_float_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_avx
.
globl
_sk_scale_u8_avx
FUNCTION
(
_sk_scale_u8_avx
)
_sk_scale_u8_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
75
/
/
jne
1271f
<
_sk_scale_u8_avx
+
0x5d
>
.
byte
196
66
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
197
57
219
5
206
167
2
0
/
/
vpand
0x2a7ce
(
%
rip
)
%
xmm8
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
66
121
51
200
/
/
vpmovzxwd
%
xmm8
%
xmm9
.
byte
196
65
121
112
192
78
/
/
vpshufd
0x4e
%
xmm8
%
xmm8
.
byte
196
66
121
51
192
/
/
vpmovzxwd
%
xmm8
%
xmm8
.
byte
196
67
53
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
110
158
2
0
/
/
vbroadcastss
0x29e6e
(
%
rip
)
%
ymm9
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
89
219
/
/
vmulps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
172
/
/
ja
126da
<
_sk_scale_u8_avx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
124
0
0
0
/
/
lea
0x7c
(
%
rip
)
%
r9
#
127b4
<
_sk_scale_u8_avx
+
0xf2
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
235
142
/
/
jmp
126da
<
_sk_scale_u8_avx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
200
/
/
vmovd
%
eax
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
57
14
193
3
/
/
vpblendw
0x3
%
xmm9
%
xmm8
%
xmm8
.
byte
233
101
255
255
255
/
/
jmpq
126da
<
_sk_scale_u8_avx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
49
14
192
240
/
/
vpblendw
0xf0
%
xmm8
%
xmm9
%
xmm8
.
byte
233
41
255
255
255
/
/
jmpq
126da
<
_sk_scale_u8_avx
+
0x18
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
141
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
168
255
255
255
152
/
/
ljmp
*
-
0x67000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_scale_565_avx
.
globl
_sk_scale_565_avx
FUNCTION
(
_sk_scale_565_avx
)
_sk_scale_565_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
178
0
0
0
/
/
jne
1289b
<
_sk_scale_565_avx
+
0xcb
>
.
byte
196
65
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
196
66
121
51
200
/
/
vpmovzxwd
%
xmm8
%
xmm9
.
byte
196
65
121
112
192
78
/
/
vpshufd
0x4e
%
xmm8
%
xmm8
.
byte
196
66
121
51
192
/
/
vpmovzxwd
%
xmm8
%
xmm8
.
byte
196
67
53
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
98
125
24
13
106
157
2
0
/
/
vbroadcastss
0x29d6a
(
%
rip
)
%
ymm9
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
196
65
60
84
201
/
/
vandps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
124
91
201
/
/
vcvtdq2ps
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
91
157
2
0
/
/
vbroadcastss
0x29d5b
(
%
rip
)
%
ymm10
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
81
157
2
0
/
/
vbroadcastss
0x29d51
(
%
rip
)
%
ymm10
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
196
65
60
84
210
/
/
vandps
%
ymm10
%
ymm8
%
ymm10
.
byte
196
65
124
91
210
/
/
vcvtdq2ps
%
ymm10
%
ymm10
.
byte
196
98
125
24
29
66
157
2
0
/
/
vbroadcastss
0x29d42
(
%
rip
)
%
ymm11
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
196
65
44
89
211
/
/
vmulps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
24
29
56
157
2
0
/
/
vbroadcastss
0x29d38
(
%
rip
)
%
ymm11
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
196
65
60
84
195
/
/
vandps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
29
41
157
2
0
/
/
vbroadcastss
0x29d29
(
%
rip
)
%
ymm11
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
197
100
194
223
1
/
/
vcmpltps
%
ymm7
%
ymm3
%
ymm11
.
byte
196
65
44
93
224
/
/
vminps
%
ymm8
%
ymm10
%
ymm12
.
byte
196
65
52
93
228
/
/
vminps
%
ymm12
%
ymm9
%
ymm12
.
byte
196
65
44
95
232
/
/
vmaxps
%
ymm8
%
ymm10
%
ymm13
.
byte
196
65
52
95
237
/
/
vmaxps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
220
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
172
89
201
/
/
vmulps
%
ymm1
%
ymm10
%
ymm1
.
byte
197
188
89
210
/
/
vmulps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
164
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
65
255
255
255
/
/
ja
127ef
<
_sk_scale_565_avx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
100
0
0
0
/
/
lea
0x64
(
%
rip
)
%
r9
#
1291c
<
_sk_scale_565_avx
+
0x14c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
32
255
255
255
/
/
jmpq
127ef
<
_sk_scale_565_avx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
57
14
193
3
/
/
vpblendw
0x3
%
xmm9
%
xmm8
%
xmm8
.
byte
233
2
255
255
255
/
/
jmpq
127ef
<
_sk_scale_565_avx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
49
14
192
240
/
/
vpblendw
0xf0
%
xmm8
%
xmm9
%
xmm8
.
byte
233
212
254
255
255
/
/
jmpq
127ef
<
_sk_scale_565_avx
+
0x1f
>
.
byte
144
/
/
nop
.
byte
165
/
/
movsl
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
192
/
/
inc
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
179
255
255
255
238
/
/
pushq
-
0x11000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
230
/
/
jmpq
*
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
255
/
/
fdivrp
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_1_float_avx
.
globl
_sk_lerp_1_float_avx
FUNCTION
(
_sk_lerp_1_float_avx
)
_sk_lerp_1_float_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
244
88
205
/
/
vaddps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
236
88
214
/
/
vaddps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_u8_avx
.
globl
_sk_lerp_u8_avx
FUNCTION
(
_sk_lerp_u8_avx
)
_sk_lerp_u8_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
111
/
/
jne
129f8
<
_sk_lerp_u8_avx
+
0x81
>
.
byte
196
66
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
197
57
219
5
25
165
2
0
/
/
vpand
0x2a519
(
%
rip
)
%
xmm8
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
66
121
51
200
/
/
vpmovzxwd
%
xmm8
%
xmm9
.
byte
196
65
121
112
192
78
/
/
vpshufd
0x4e
%
xmm8
%
xmm8
.
byte
196
66
121
51
192
/
/
vpmovzxwd
%
xmm8
%
xmm8
.
byte
196
67
53
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
185
155
2
0
/
/
vbroadcastss
0x29bb9
(
%
rip
)
%
ymm9
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
244
88
205
/
/
vaddps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
236
88
214
/
/
vaddps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
136
/
/
ja
1298f
<
_sk_lerp_u8_avx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
127
0
0
0
/
/
lea
0x7f
(
%
rip
)
%
r9
#
12a90
<
_sk_lerp_u8_avx
+
0x119
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
103
255
255
255
/
/
jmpq
1298f
<
_sk_lerp_u8_avx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
200
/
/
vmovd
%
eax
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
57
14
193
3
/
/
vpblendw
0x3
%
xmm9
%
xmm8
%
xmm8
.
byte
233
62
255
255
255
/
/
jmpq
1298f
<
_sk_lerp_u8_avx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
197
57
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm8
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
57
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
66
121
48
201
/
/
vpmovzxbw
%
xmm9
%
xmm9
.
byte
196
67
49
14
192
240
/
/
vpblendw
0xf0
%
xmm8
%
xmm9
%
xmm8
.
byte
233
2
255
255
255
/
/
jmpq
1298f
<
_sk_lerp_u8_avx
+
0x18
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
138
255
/
/
mov
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
168
255
255
255
152
/
/
ljmp
*
-
0x67000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_565_avx
.
globl
_sk_lerp_565_avx
FUNCTION
(
_sk_lerp_565_avx
)
_sk_lerp_565_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
214
0
0
0
/
/
jne
12b9b
<
_sk_lerp_565_avx
+
0xef
>
.
byte
196
65
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
196
66
121
51
200
/
/
vpmovzxwd
%
xmm8
%
xmm9
.
byte
196
65
121
112
192
78
/
/
vpshufd
0x4e
%
xmm8
%
xmm8
.
byte
196
66
121
51
192
/
/
vpmovzxwd
%
xmm8
%
xmm8
.
byte
196
67
53
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
98
125
24
13
142
154
2
0
/
/
vbroadcastss
0x29a8e
(
%
rip
)
%
ymm9
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
196
65
60
84
201
/
/
vandps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
124
91
201
/
/
vcvtdq2ps
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
127
154
2
0
/
/
vbroadcastss
0x29a7f
(
%
rip
)
%
ymm10
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
196
65
52
89
202
/
/
vmulps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
117
154
2
0
/
/
vbroadcastss
0x29a75
(
%
rip
)
%
ymm10
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
196
65
60
84
210
/
/
vandps
%
ymm10
%
ymm8
%
ymm10
.
byte
196
65
124
91
210
/
/
vcvtdq2ps
%
ymm10
%
ymm10
.
byte
196
98
125
24
29
102
154
2
0
/
/
vbroadcastss
0x29a66
(
%
rip
)
%
ymm11
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
196
65
44
89
211
/
/
vmulps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
24
29
92
154
2
0
/
/
vbroadcastss
0x29a5c
(
%
rip
)
%
ymm11
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
196
65
60
84
195
/
/
vandps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
98
125
24
29
77
154
2
0
/
/
vbroadcastss
0x29a4d
(
%
rip
)
%
ymm11
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
197
100
194
223
1
/
/
vcmpltps
%
ymm7
%
ymm3
%
ymm11
.
byte
196
65
44
93
224
/
/
vminps
%
ymm8
%
ymm10
%
ymm12
.
byte
196
65
52
93
228
/
/
vminps
%
ymm12
%
ymm9
%
ymm12
.
byte
196
65
44
95
232
/
/
vmaxps
%
ymm8
%
ymm10
%
ymm13
.
byte
196
65
52
95
237
/
/
vmaxps
%
ymm13
%
ymm9
%
ymm13
.
byte
196
67
21
74
220
176
/
/
vblendvps
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
244
92
205
/
/
vsubps
%
ymm5
%
ymm1
%
ymm1
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
244
88
205
/
/
vaddps
%
ymm5
%
ymm1
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
236
88
214
/
/
vaddps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
228
92
223
/
/
vsubps
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
100
89
219
/
/
vmulps
%
ymm11
%
ymm3
%
ymm3
.
byte
197
228
88
223
/
/
vaddps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
29
255
255
255
/
/
ja
12acb
<
_sk_lerp_565_avx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
100
0
0
0
/
/
lea
0x64
(
%
rip
)
%
r9
#
12c1c
<
_sk_lerp_565_avx
+
0x170
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
252
254
255
255
/
/
jmpq
12acb
<
_sk_lerp_565_avx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
57
14
193
3
/
/
vpblendw
0x3
%
xmm9
%
xmm8
%
xmm8
.
byte
233
222
254
255
255
/
/
jmpq
12acb
<
_sk_lerp_565_avx
+
0x1f
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
57
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
49
14
192
240
/
/
vpblendw
0xf0
%
xmm8
%
xmm9
%
xmm8
.
byte
233
176
254
255
255
/
/
jmpq
12acb
<
_sk_lerp_565_avx
+
0x1f
>
.
byte
144
/
/
nop
.
byte
165
/
/
movsl
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
192
/
/
inc
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
179
255
255
255
238
/
/
pushq
-
0x11000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
230
/
/
jmpq
*
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
255
/
/
fdivrp
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_tables_avx
.
globl
_sk_load_tables_avx
FUNCTION
(
_sk_load_tables_avx
)
_sk_load_tables_avx
:
.
byte
197
252
17
124
36
200
/
/
vmovups
%
ymm7
-
0x38
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
4
2
0
0
/
/
jne
12e50
<
_sk_load_tables_avx
+
0x218
>
.
byte
196
65
125
16
20
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm10
.
byte
197
125
40
13
198
158
2
0
/
/
vmovapd
0x29ec6
(
%
rip
)
%
ymm9
#
3cb20
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x8d4
>
.
byte
196
193
45
84
201
/
/
vandpd
%
ymm9
%
ymm10
%
ymm1
.
byte
196
227
125
25
200
1
/
/
vextractf128
0x1
%
ymm1
%
xmm0
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
80
8
/
/
mov
0x8
(
%
rax
)
%
r10
.
byte
196
129
122
16
20
138
/
/
vmovss
(
%
r10
%
r9
4
)
%
xmm2
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
131
105
33
4
130
16
/
/
vinsertps
0x10
(
%
r10
%
r8
4
)
%
xmm2
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
196
131
121
33
4
130
32
/
/
vinsertps
0x20
(
%
r10
%
r8
4
)
%
xmm0
%
xmm0
.
byte
196
193
249
126
200
/
/
vmovq
%
xmm1
%
r8
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
3
121
33
4
138
48
/
/
vinsertps
0x30
(
%
r10
%
r9
4
)
%
xmm0
%
xmm8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
196
129
122
16
20
138
/
/
vmovss
(
%
r10
%
r9
4
)
%
xmm2
.
byte
196
195
249
22
201
1
/
/
vpextrq
0x1
%
xmm1
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
131
105
33
12
130
16
/
/
vinsertps
0x10
(
%
r10
%
r8
4
)
%
xmm2
%
xmm1
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
131
113
33
12
130
32
/
/
vinsertps
0x20
(
%
r10
%
r8
4
)
%
xmm1
%
xmm1
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
196
3
113
33
36
138
48
/
/
vinsertps
0x30
(
%
r10
%
r9
4
)
%
xmm1
%
xmm12
.
byte
196
193
105
114
210
8
/
/
vpsrld
0x8
%
xmm10
%
xmm2
.
byte
196
67
125
25
213
1
/
/
vextractf128
0x1
%
ymm10
%
xmm13
.
byte
196
193
121
114
213
8
/
/
vpsrld
0x8
%
xmm13
%
xmm0
.
byte
196
227
109
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm0
.
byte
196
193
125
84
209
/
/
vandpd
%
ymm9
%
ymm0
%
ymm2
.
byte
196
227
125
25
208
1
/
/
vextractf128
0x1
%
ymm2
%
xmm0
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
196
129
122
16
12
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
3
113
33
52
136
16
/
/
vinsertps
0x10
(
%
r8
%
r9
4
)
%
xmm1
%
xmm14
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
196
129
122
16
28
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm3
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
129
122
16
12
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm1
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
196
129
122
16
4
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
3
121
33
28
136
16
/
/
vinsertps
0x10
(
%
r8
%
r9
4
)
%
xmm0
%
xmm11
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
196
1
122
16
60
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm15
.
byte
196
195
29
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm12
%
ymm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
9
33
219
32
/
/
vinsertps
0x20
%
xmm3
%
xmm14
%
xmm3
.
byte
196
227
97
33
249
48
/
/
vinsertps
0x30
%
xmm1
%
xmm3
%
xmm7
.
byte
196
1
122
16
52
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm14
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
196
193
97
114
210
16
/
/
vpsrld
0x10
%
xmm10
%
xmm3
.
byte
196
193
105
114
213
16
/
/
vpsrld
0x10
%
xmm13
%
xmm2
.
byte
196
227
101
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm2
.
byte
196
65
109
84
201
/
/
vandpd
%
ymm9
%
ymm2
%
ymm9
.
byte
196
99
125
25
202
1
/
/
vextractf128
0x1
%
ymm9
%
xmm2
.
byte
196
193
249
126
208
/
/
vmovq
%
xmm2
%
r8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
196
193
122
16
28
129
/
/
vmovss
(
%
r9
%
rax
4
)
%
xmm3
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
3
97
33
36
129
16
/
/
vinsertps
0x10
(
%
r9
%
r8
4
)
%
xmm3
%
xmm12
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
196
193
122
16
28
129
/
/
vmovss
(
%
r9
%
rax
4
)
%
xmm3
.
byte
196
65
249
126
200
/
/
vmovq
%
xmm9
%
r8
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
129
122
16
20
145
/
/
vmovss
(
%
r9
%
r10
4
)
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
196
193
122
16
12
129
/
/
vmovss
(
%
r9
%
rax
4
)
%
xmm1
.
byte
196
67
249
22
202
1
/
/
vpextrq
0x1
%
xmm9
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
3
113
33
12
129
16
/
/
vinsertps
0x10
(
%
r9
%
r8
4
)
%
xmm1
%
xmm9
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
196
65
122
16
4
129
/
/
vmovss
(
%
r9
%
rax
4
)
%
xmm8
.
byte
196
195
33
33
207
32
/
/
vinsertps
0x20
%
xmm15
%
xmm11
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
1
122
16
28
145
/
/
vmovss
(
%
r9
%
r10
4
)
%
xmm11
.
byte
196
195
113
33
206
48
/
/
vinsertps
0x30
%
xmm14
%
xmm1
%
xmm1
.
byte
196
227
117
24
207
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm1
%
ymm1
.
byte
196
227
25
33
219
32
/
/
vinsertps
0x20
%
xmm3
%
xmm12
%
xmm3
.
byte
196
227
97
33
210
48
/
/
vinsertps
0x30
%
xmm2
%
xmm3
%
xmm2
.
byte
196
195
49
33
216
32
/
/
vinsertps
0x20
%
xmm8
%
xmm9
%
xmm3
.
byte
196
195
97
33
219
48
/
/
vinsertps
0x30
%
xmm11
%
xmm3
%
xmm3
.
byte
196
227
101
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm2
.
byte
196
193
97
114
210
24
/
/
vpsrld
0x18
%
xmm10
%
xmm3
.
byte
196
193
65
114
213
24
/
/
vpsrld
0x18
%
xmm13
%
xmm7
.
byte
196
227
101
24
223
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
226
125
24
61
50
151
2
0
/
/
vbroadcastss
0x29732
(
%
rip
)
%
ymm7
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
228
89
223
/
/
vmulps
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
124
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
137
249
/
/
mov
%
edi
%
r9d
.
byte
65
128
225
7
/
/
and
0x7
%
r9b
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
65
254
201
/
/
dec
%
r9b
.
byte
65
128
249
6
/
/
cmp
0x6
%
r9b
.
byte
15
135
233
253
255
255
/
/
ja
12c52
<
_sk_load_tables_avx
+
0x1a
>
.
byte
69
15
182
201
/
/
movzbl
%
r9b
%
r9d
.
byte
76
141
21
148
0
0
0
/
/
lea
0x94
(
%
rip
)
%
r10
#
12f08
<
_sk_load_tables_avx
+
0x2d0
>
.
byte
79
99
12
138
/
/
movslq
(
%
r10
%
r9
4
)
%
r9
.
byte
77
1
209
/
/
add
%
r10
%
r9
.
byte
65
255
225
/
/
jmpq
*
%
r9
.
byte
196
65
122
16
20
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm10
.
byte
233
201
253
255
255
/
/
jmpq
12c52
<
_sk_load_tables_avx
+
0x1a
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
208
4
/
/
vblendps
0x4
%
ymm0
%
ymm1
%
ymm10
.
byte
196
193
123
16
4
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
99
45
13
208
1
/
/
vblendpd
0x1
%
ymm0
%
ymm10
%
ymm10
.
byte
233
162
253
255
255
/
/
jmpq
12c52
<
_sk_load_tables_avx
+
0x1a
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
196
227
125
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
208
64
/
/
vblendps
0x40
%
ymm0
%
ymm1
%
ymm10
.
byte
196
99
125
25
208
1
/
/
vextractf128
0x1
%
ymm10
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
45
24
208
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm10
%
ymm10
.
byte
196
99
125
25
208
1
/
/
vextractf128
0x1
%
ymm10
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
45
24
208
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm10
%
ymm10
.
byte
196
193
121
16
4
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
67
125
13
210
12
/
/
vblendpd
0xc
%
ymm10
%
ymm0
%
ymm10
.
byte
233
77
253
255
255
/
/
jmpq
12c52
<
_sk_load_tables_avx
+
0x1a
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
118
255
/
/
jbe
12f09
<
_sk_load_tables_avx
+
0x2d1
>
.
byte
255
/
/
(
bad
)
.
byte
255
151
255
255
255
129
/
/
callq
*
-
0x7e000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
236
/
/
in
(
%
dx
)
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
196
/
/
inc
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
168
255
/
/
test
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_tables_u16_be_avx
.
globl
_sk_load_tables_u16_be_avx
FUNCTION
(
_sk_load_tables_u16_be_avx
)
_sk_load_tables_u16_be_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
197
252
17
124
36
200
/
/
vmovups
%
ymm7
-
0x38
(
%
rsp
)
.
byte
197
252
17
116
36
168
/
/
vmovups
%
ymm6
-
0x58
(
%
rsp
)
.
byte
15
133
75
2
0
0
/
/
jne
13191
<
_sk_load_tables_u16_be_avx
+
0x26d
>
.
byte
196
1
121
16
4
72
/
/
vmovupd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
129
121
16
84
72
16
/
/
vmovupd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
196
129
121
16
92
72
32
/
/
vmovupd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
1
122
111
76
72
48
/
/
vmovdqu
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
202
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm9
.
byte
197
121
105
194
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
241
97
195
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm0
.
byte
197
113
105
227
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm12
.
byte
197
177
108
208
/
/
vpunpcklqdq
%
xmm0
%
xmm9
%
xmm2
.
byte
197
49
109
232
/
/
vpunpckhqdq
%
xmm0
%
xmm9
%
xmm13
.
byte
196
65
57
108
212
/
/
vpunpcklqdq
%
xmm12
%
xmm8
%
xmm10
.
byte
197
121
111
29
24
159
2
0
/
/
vmovdqa
0x29f18
(
%
rip
)
%
xmm11
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
193
105
219
195
/
/
vpand
%
xmm11
%
xmm2
%
xmm0
.
byte
197
249
112
208
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm2
.
byte
196
226
121
51
210
/
/
vpmovzxwd
%
xmm2
%
xmm2
.
byte
196
193
249
126
208
/
/
vmovq
%
xmm2
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
80
8
/
/
mov
0x8
(
%
rax
)
%
r10
.
byte
196
129
122
16
28
138
/
/
vmovss
(
%
r10
%
r9
4
)
%
xmm3
.
byte
196
195
249
22
209
1
/
/
vpextrq
0x1
%
xmm2
%
r9
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
196
3
97
33
12
2
16
/
/
vinsertps
0x10
(
%
r10
%
r8
1
)
%
xmm3
%
xmm9
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
196
129
122
16
28
130
/
/
vmovss
(
%
r10
%
r8
4
)
%
xmm3
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
129
122
16
12
10
/
/
vmovss
(
%
r10
%
r9
1
)
%
xmm1
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
196
129
122
16
20
138
/
/
vmovss
(
%
r10
%
r9
4
)
%
xmm2
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
196
131
105
33
4
2
16
/
/
vinsertps
0x10
(
%
r10
%
r8
1
)
%
xmm2
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
131
121
33
20
130
32
/
/
vinsertps
0x20
(
%
r10
%
r8
4
)
%
xmm0
%
xmm2
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
196
227
49
33
195
32
/
/
vinsertps
0x20
%
xmm3
%
xmm9
%
xmm0
.
byte
196
99
121
33
241
48
/
/
vinsertps
0x30
%
xmm1
%
xmm0
%
xmm14
.
byte
196
3
105
33
12
10
48
/
/
vinsertps
0x30
(
%
r10
%
r9
1
)
%
xmm2
%
xmm9
.
byte
196
193
17
219
203
/
/
vpand
%
xmm11
%
xmm13
%
xmm1
.
byte
197
249
112
209
78
/
/
vpshufd
0x4e
%
xmm1
%
xmm2
.
byte
196
226
121
51
210
/
/
vpmovzxwd
%
xmm2
%
xmm2
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
196
129
122
16
28
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm3
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
226
121
51
193
/
/
vpmovzxwd
%
xmm1
%
xmm0
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
3
97
33
60
8
16
/
/
vinsertps
0x10
(
%
r8
%
r9
1
)
%
xmm3
%
xmm15
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
196
129
122
16
20
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
73
193
234
30
/
/
shr
0x1e
%
r10
.
byte
196
129
122
16
28
16
/
/
vmovss
(
%
r8
%
r10
1
)
%
xmm3
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
196
129
122
16
12
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
131
113
33
4
8
16
/
/
vinsertps
0x10
(
%
r8
%
r9
1
)
%
xmm1
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
30
/
/
shr
0x1e
%
r10
.
byte
196
131
121
33
4
136
32
/
/
vinsertps
0x20
(
%
r8
%
r9
4
)
%
xmm0
%
xmm0
.
byte
196
3
121
33
44
16
48
/
/
vinsertps
0x30
(
%
r8
%
r10
1
)
%
xmm0
%
xmm13
.
byte
76
139
80
24
/
/
mov
0x18
(
%
rax
)
%
r10
.
byte
196
193
41
219
195
/
/
vpand
%
xmm11
%
xmm10
%
xmm0
.
byte
197
249
112
200
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm1
.
byte
196
226
121
51
201
/
/
vpmovzxwd
%
xmm1
%
xmm1
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
196
193
122
16
12
130
/
/
vmovss
(
%
r10
%
rax
4
)
%
xmm1
.
byte
196
195
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
r11
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
3
113
33
20
10
16
/
/
vinsertps
0x10
(
%
r10
%
r9
1
)
%
xmm1
%
xmm10
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
196
65
122
16
28
130
/
/
vmovss
(
%
r10
%
rax
4
)
%
xmm11
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
196
129
122
16
60
2
/
/
vmovss
(
%
r10
%
r8
1
)
%
xmm7
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
196
193
122
16
4
130
/
/
vmovss
(
%
r10
%
rax
4
)
%
xmm0
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
131
121
33
4
10
16
/
/
vinsertps
0x10
(
%
r10
%
r9
1
)
%
xmm0
%
xmm0
.
byte
68
137
216
/
/
mov
%
r11d
%
eax
.
byte
196
195
121
33
4
130
32
/
/
vinsertps
0x20
(
%
r10
%
rax
4
)
%
xmm0
%
xmm0
.
byte
73
193
235
30
/
/
shr
0x1e
%
r11
.
byte
196
131
121
33
52
26
48
/
/
vinsertps
0x30
(
%
r10
%
r11
1
)
%
xmm0
%
xmm6
.
byte
196
195
53
24
198
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm9
%
ymm0
.
byte
196
227
1
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm15
%
xmm1
.
byte
196
227
113
33
203
48
/
/
vinsertps
0x30
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
21
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm13
%
ymm1
.
byte
196
195
41
33
211
32
/
/
vinsertps
0x20
%
xmm11
%
xmm10
%
xmm2
.
byte
196
227
105
33
215
48
/
/
vinsertps
0x30
%
xmm7
%
xmm2
%
xmm2
.
byte
196
227
77
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm6
%
ymm2
.
byte
196
193
57
109
220
/
/
vpunpckhqdq
%
xmm12
%
xmm8
%
xmm3
.
byte
197
201
113
243
8
/
/
vpsllw
0x8
%
xmm3
%
xmm6
.
byte
197
225
113
211
8
/
/
vpsrlw
0x8
%
xmm3
%
xmm3
.
byte
197
201
235
219
/
/
vpor
%
xmm3
%
xmm6
%
xmm3
.
byte
196
226
121
51
243
/
/
vpmovzxwd
%
xmm3
%
xmm6
.
byte
197
249
112
219
78
/
/
vpshufd
0x4e
%
xmm3
%
xmm3
.
byte
196
226
121
51
219
/
/
vpmovzxwd
%
xmm3
%
xmm3
.
byte
196
227
77
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm6
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
226
125
24
53
19
148
2
0
/
/
vbroadcastss
0x29413
(
%
rip
)
%
ymm6
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
197
228
89
222
/
/
vmulps
%
ymm6
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
116
36
168
/
/
vmovups
-
0x58
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
123
16
4
72
/
/
vmovsd
(
%
r8
%
r9
2
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
131f7
<
_sk_load_tables_u16_be_avx
+
0x2d3
>
.
byte
196
1
57
22
68
72
8
/
/
vmovhpd
0x8
(
%
r8
%
r9
2
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
131f7
<
_sk_load_tables_u16_be_avx
+
0x2d3
>
.
byte
196
129
123
16
84
72
16
/
/
vmovsd
0x10
(
%
r8
%
r9
2
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
13204
<
_sk_load_tables_u16_be_avx
+
0x2e0
>
.
byte
196
129
105
22
84
72
24
/
/
vmovhpd
0x18
(
%
r8
%
r9
2
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
13204
<
_sk_load_tables_u16_be_avx
+
0x2e0
>
.
byte
196
129
123
16
92
72
32
/
/
vmovsd
0x20
(
%
r8
%
r9
2
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
135
253
255
255
/
/
je
12f61
<
_sk_load_tables_u16_be_avx
+
0x3d
>
.
byte
196
129
97
22
92
72
40
/
/
vmovhpd
0x28
(
%
r8
%
r9
2
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
118
253
255
255
/
/
jb
12f61
<
_sk_load_tables_u16_be_avx
+
0x3d
>
.
byte
196
1
122
126
76
72
48
/
/
vmovq
0x30
(
%
r8
%
r9
2
)
%
xmm9
.
byte
233
106
253
255
255
/
/
jmpq
12f61
<
_sk_load_tables_u16_be_avx
+
0x3d
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
93
253
255
255
/
/
jmpq
12f61
<
_sk_load_tables_u16_be_avx
+
0x3d
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
84
253
255
255
/
/
jmpq
12f61
<
_sk_load_tables_u16_be_avx
+
0x3d
>
HIDDEN
_sk_load_tables_rgb_u16_be_avx
.
globl
_sk_load_tables_rgb_u16_be_avx
FUNCTION
(
_sk_load_tables_rgb_u16_be_avx
)
_sk_load_tables_rgb_u16_be_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
197
252
17
124
36
200
/
/
vmovups
%
ymm7
-
0x38
(
%
rsp
)
.
byte
197
252
17
116
36
168
/
/
vmovups
%
ymm6
-
0x58
(
%
rsp
)
.
byte
15
133
54
2
0
0
/
/
jne
13461
<
_sk_load_tables_rgb_u16_be_avx
+
0x254
>
.
byte
196
1
122
111
28
72
/
/
vmovdqu
(
%
r8
%
r9
2
)
%
xmm11
.
byte
196
129
122
111
92
72
12
/
/
vmovdqu
0xc
(
%
r8
%
r9
2
)
%
xmm3
.
byte
196
129
122
111
84
72
24
/
/
vmovdqu
0x18
(
%
r8
%
r9
2
)
%
xmm2
.
byte
196
129
122
111
68
72
32
/
/
vmovdqu
0x20
(
%
r8
%
r9
2
)
%
xmm0
.
byte
197
249
115
216
4
/
/
vpsrldq
0x4
%
xmm0
%
xmm0
.
byte
196
193
57
115
219
6
/
/
vpsrldq
0x6
%
xmm11
%
xmm8
.
byte
197
169
115
219
6
/
/
vpsrldq
0x6
%
xmm3
%
xmm10
.
byte
197
241
115
218
6
/
/
vpsrldq
0x6
%
xmm2
%
xmm1
.
byte
197
177
115
216
6
/
/
vpsrldq
0x6
%
xmm0
%
xmm9
.
byte
196
65
113
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm1
%
xmm9
.
byte
197
233
97
192
/
/
vpunpcklwd
%
xmm0
%
xmm2
%
xmm0
.
byte
196
193
57
97
210
/
/
vpunpcklwd
%
xmm10
%
xmm8
%
xmm2
.
byte
197
161
97
219
/
/
vpunpcklwd
%
xmm3
%
xmm11
%
xmm3
.
byte
197
225
97
202
/
/
vpunpcklwd
%
xmm2
%
xmm3
%
xmm1
.
byte
197
225
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm3
%
xmm2
.
byte
196
193
121
97
217
/
/
vpunpcklwd
%
xmm9
%
xmm0
%
xmm3
.
byte
196
193
121
105
193
/
/
vpunpckhwd
%
xmm9
%
xmm0
%
xmm0
.
byte
197
105
108
216
/
/
vpunpcklqdq
%
xmm0
%
xmm2
%
xmm11
.
byte
197
241
108
211
/
/
vpunpcklqdq
%
xmm3
%
xmm1
%
xmm2
.
byte
197
241
109
203
/
/
vpunpckhqdq
%
xmm3
%
xmm1
%
xmm1
.
byte
197
121
111
53
24
156
2
0
/
/
vmovdqa
0x29c18
(
%
rip
)
%
xmm14
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
193
105
219
214
/
/
vpand
%
xmm14
%
xmm2
%
xmm2
.
byte
197
249
112
218
78
/
/
vpshufd
0x4e
%
xmm2
%
xmm3
.
byte
196
226
121
51
219
/
/
vpmovzxwd
%
xmm3
%
xmm3
.
byte
196
193
249
126
216
/
/
vmovq
%
xmm3
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
80
8
/
/
mov
0x8
(
%
rax
)
%
r10
.
byte
196
129
122
16
4
138
/
/
vmovss
(
%
r10
%
r9
4
)
%
xmm0
.
byte
196
195
249
22
217
1
/
/
vpextrq
0x1
%
xmm3
%
r9
.
byte
196
226
121
51
210
/
/
vpmovzxwd
%
xmm2
%
xmm2
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
196
3
121
33
4
2
16
/
/
vinsertps
0x10
(
%
r10
%
r8
1
)
%
xmm0
%
xmm8
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
196
1
122
16
20
130
/
/
vmovss
(
%
r10
%
r8
4
)
%
xmm10
.
byte
196
193
249
126
208
/
/
vmovq
%
xmm2
%
r8
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
1
122
16
36
10
/
/
vmovss
(
%
r10
%
r9
1
)
%
xmm12
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
196
129
122
16
4
138
/
/
vmovss
(
%
r10
%
r9
4
)
%
xmm0
.
byte
196
195
249
22
209
1
/
/
vpextrq
0x1
%
xmm2
%
r9
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
196
131
121
33
4
2
16
/
/
vinsertps
0x10
(
%
r10
%
r8
1
)
%
xmm0
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
131
121
33
4
130
32
/
/
vinsertps
0x20
(
%
r10
%
r8
4
)
%
xmm0
%
xmm0
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
196
3
121
33
12
10
48
/
/
vinsertps
0x30
(
%
r10
%
r9
1
)
%
xmm0
%
xmm9
.
byte
196
193
113
219
198
/
/
vpand
%
xmm14
%
xmm1
%
xmm0
.
byte
197
249
112
200
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm1
.
byte
196
226
121
51
201
/
/
vpmovzxwd
%
xmm1
%
xmm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
196
129
122
16
20
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
3
105
33
60
8
16
/
/
vinsertps
0x10
(
%
r8
%
r9
1
)
%
xmm2
%
xmm15
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
196
129
122
16
20
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
73
193
234
30
/
/
shr
0x1e
%
r10
.
byte
196
129
122
16
28
16
/
/
vmovss
(
%
r8
%
r10
1
)
%
xmm3
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
196
129
122
16
12
144
/
/
vmovss
(
%
r8
%
r10
4
)
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
131
113
33
4
8
16
/
/
vinsertps
0x10
(
%
r8
%
r9
1
)
%
xmm1
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
30
/
/
shr
0x1e
%
r10
.
byte
196
131
121
33
4
136
32
/
/
vinsertps
0x20
(
%
r8
%
r9
4
)
%
xmm0
%
xmm0
.
byte
196
3
121
33
44
16
48
/
/
vinsertps
0x30
(
%
r8
%
r10
1
)
%
xmm0
%
xmm13
.
byte
76
139
80
24
/
/
mov
0x18
(
%
rax
)
%
r10
.
byte
196
193
33
219
198
/
/
vpand
%
xmm14
%
xmm11
%
xmm0
.
byte
197
249
112
200
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm1
.
byte
196
226
121
51
201
/
/
vpmovzxwd
%
xmm1
%
xmm1
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
196
193
122
16
12
130
/
/
vmovss
(
%
r10
%
rax
4
)
%
xmm1
.
byte
196
195
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
r11
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
3
113
33
28
10
16
/
/
vinsertps
0x10
(
%
r10
%
r9
1
)
%
xmm1
%
xmm11
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
196
65
122
16
52
130
/
/
vmovss
(
%
r10
%
rax
4
)
%
xmm14
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
196
129
122
16
60
2
/
/
vmovss
(
%
r10
%
r8
1
)
%
xmm7
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
196
193
122
16
4
130
/
/
vmovss
(
%
r10
%
rax
4
)
%
xmm0
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
196
131
121
33
4
10
16
/
/
vinsertps
0x10
(
%
r10
%
r9
1
)
%
xmm0
%
xmm0
.
byte
68
137
216
/
/
mov
%
r11d
%
eax
.
byte
196
195
121
33
4
130
32
/
/
vinsertps
0x20
(
%
r10
%
rax
4
)
%
xmm0
%
xmm0
.
byte
73
193
235
30
/
/
shr
0x1e
%
r11
.
byte
196
131
121
33
52
26
48
/
/
vinsertps
0x30
(
%
r10
%
r11
1
)
%
xmm0
%
xmm6
.
byte
196
195
57
33
194
32
/
/
vinsertps
0x20
%
xmm10
%
xmm8
%
xmm0
.
byte
196
195
121
33
196
48
/
/
vinsertps
0x30
%
xmm12
%
xmm0
%
xmm0
.
byte
196
227
53
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm9
%
ymm0
.
byte
196
227
1
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm15
%
xmm1
.
byte
196
227
113
33
203
48
/
/
vinsertps
0x30
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
21
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm13
%
ymm1
.
byte
196
195
33
33
214
32
/
/
vinsertps
0x20
%
xmm14
%
xmm11
%
xmm2
.
byte
196
227
105
33
215
48
/
/
vinsertps
0x30
%
xmm7
%
xmm2
%
xmm2
.
byte
196
227
77
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm6
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
169
144
2
0
/
/
vbroadcastss
0x290a9
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
16
116
36
168
/
/
vmovups
-
0x58
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
110
4
72
/
/
vmovd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
92
72
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
r9
2
)
%
xmm0
%
xmm11
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
31
/
/
jne
13499
<
_sk_load_tables_rgb_u16_be_avx
+
0x28c
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
233
199
253
255
255
/
/
jmpq
13260
<
_sk_load_tables_rgb_u16_be_avx
+
0x53
>
.
byte
196
129
121
110
68
72
6
/
/
vmovd
0x6
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
68
72
10
2
/
/
vpinsrw
0x2
0xa
(
%
r8
%
r9
2
)
%
xmm0
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
48
/
/
jb
134e3
<
_sk_load_tables_rgb_u16_be_avx
+
0x2d6
>
.
byte
196
129
121
110
68
72
12
/
/
vmovd
0xc
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
92
72
16
2
/
/
vpinsrw
0x2
0x10
(
%
r8
%
r9
2
)
%
xmm0
%
xmm3
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
117
48
/
/
jne
134fd
<
_sk_load_tables_rgb_u16_be_avx
+
0x2f0
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
233
125
253
255
255
/
/
jmpq
13260
<
_sk_load_tables_rgb_u16_be_avx
+
0x53
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
233
99
253
255
255
/
/
jmpq
13260
<
_sk_load_tables_rgb_u16_be_avx
+
0x53
>
.
byte
196
129
121
110
68
72
18
/
/
vmovd
0x12
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
84
72
22
2
/
/
vpinsrw
0x2
0x16
(
%
r8
%
r9
2
)
%
xmm0
%
xmm10
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
39
/
/
jb
1353e
<
_sk_load_tables_rgb_u16_be_avx
+
0x331
>
.
byte
196
129
121
110
68
72
24
/
/
vmovd
0x18
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
84
72
28
2
/
/
vpinsrw
0x2
0x1c
(
%
r8
%
r9
2
)
%
xmm0
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
117
30
/
/
jne
1354f
<
_sk_load_tables_rgb_u16_be_avx
+
0x342
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
233
34
253
255
255
/
/
jmpq
13260
<
_sk_load_tables_rgb_u16_be_avx
+
0x53
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
233
17
253
255
255
/
/
jmpq
13260
<
_sk_load_tables_rgb_u16_be_avx
+
0x53
>
.
byte
196
129
121
110
68
72
30
/
/
vmovd
0x1e
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
76
72
34
2
/
/
vpinsrw
0x2
0x22
(
%
r8
%
r9
2
)
%
xmm0
%
xmm1
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
25
/
/
jb
13582
<
_sk_load_tables_rgb_u16_be_avx
+
0x375
>
.
byte
196
129
121
110
68
72
36
/
/
vmovd
0x24
(
%
r8
%
r9
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
68
72
40
2
/
/
vpinsrw
0x2
0x28
(
%
r8
%
r9
2
)
%
xmm0
%
xmm0
.
byte
233
222
252
255
255
/
/
jmpq
13260
<
_sk_load_tables_rgb_u16_be_avx
+
0x53
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
233
213
252
255
255
/
/
jmpq
13260
<
_sk_load_tables_rgb_u16_be_avx
+
0x53
>
HIDDEN
_sk_byte_tables_avx
.
globl
_sk_byte_tables_avx
FUNCTION
(
_sk_byte_tables_avx
)
_sk_byte_tables_avx
:
.
byte
197
252
17
124
36
200
/
/
vmovups
%
ymm7
-
0x38
(
%
rsp
)
.
byte
197
252
40
254
/
/
vmovaps
%
ymm6
%
ymm7
.
byte
197
252
40
245
/
/
vmovaps
%
ymm5
%
ymm6
.
byte
197
252
40
236
/
/
vmovaps
%
ymm4
%
ymm5
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
98
125
24
5
75
143
2
0
/
/
vbroadcastss
0x28f4b
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
105
143
2
0
/
/
vbroadcastss
0x28f69
(
%
rip
)
%
ymm10
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
125
91
216
/
/
vcvtps2dq
%
ymm0
%
ymm11
.
byte
196
65
249
126
216
/
/
vmovq
%
xmm11
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
193
121
110
193
/
/
vmovd
%
r9d
%
xmm0
.
byte
196
67
249
22
217
1
/
/
vpextrq
0x1
%
xmm11
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
121
32
224
1
/
/
vpinsrb
0x1
%
r8d
%
xmm0
%
xmm12
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
99
125
25
216
1
/
/
vextractf128
0x1
%
ymm11
%
xmm0
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
25
32
216
2
/
/
vpinsrb
0x2
%
r8d
%
xmm12
%
xmm11
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
33
32
217
3
/
/
vpinsrb
0x3
%
r9d
%
xmm11
%
xmm11
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
65
121
110
225
/
/
vmovd
%
r9d
%
xmm12
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
25
32
192
1
/
/
vpinsrb
0x1
%
r8d
%
xmm12
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
121
32
192
2
/
/
vpinsrb
0x2
%
r8d
%
xmm0
%
xmm0
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
121
32
233
3
/
/
vpinsrb
0x3
%
r9d
%
xmm0
%
xmm13
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
116
89
202
/
/
vmulps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
253
91
201
/
/
vcvtps2dq
%
ymm1
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
67
121
32
226
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm12
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
67
121
32
240
3
/
/
vpinsrb
0x3
%
r8d
%
xmm0
%
xmm14
.
byte
76
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8
.
byte
197
180
95
194
/
/
vmaxps
%
ymm2
%
ymm9
%
ymm0
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
253
91
208
/
/
vcvtps2dq
%
ymm0
%
ymm2
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
209
1
/
/
vextractf128
0x1
%
ymm2
%
xmm1
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
67
121
32
250
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm15
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
197
180
95
203
/
/
vmaxps
%
ymm3
%
ymm9
%
ymm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
195
121
32
224
3
/
/
vpinsrb
0x3
%
r8d
%
xmm0
%
xmm4
.
byte
76
139
72
24
/
/
mov
0x18
(
%
rax
)
%
r9
.
byte
196
193
116
93
192
/
/
vminps
%
ymm8
%
ymm1
%
ymm0
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
113
32
200
1
/
/
vpinsrb
0x1
%
eax
%
xmm1
%
xmm1
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
192
1
/
/
vextractf128
0x1
%
ymm0
%
xmm0
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
2
/
/
vpinsrb
0x2
%
eax
%
xmm1
%
xmm1
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
99
113
32
192
3
/
/
vpinsrb
0x3
%
eax
%
xmm1
%
xmm8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
113
32
192
1
/
/
vpinsrb
0x1
%
eax
%
xmm1
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
121
32
216
2
/
/
vpinsrb
0x2
%
eax
%
xmm0
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
194
121
49
195
/
/
vpmovzxbd
%
xmm11
%
xmm0
.
byte
196
194
121
49
205
/
/
vpmovzxbd
%
xmm13
%
xmm1
.
byte
196
227
125
24
193
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
26
141
2
0
/
/
vbroadcastss
0x28d1a
(
%
rip
)
%
ymm9
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
194
121
49
204
/
/
vpmovzxbd
%
xmm12
%
xmm1
.
byte
196
194
121
49
214
/
/
vpmovzxbd
%
xmm14
%
xmm2
.
byte
196
227
117
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
194
121
49
215
/
/
vpmovzxbd
%
xmm15
%
xmm2
.
byte
196
226
121
49
228
/
/
vpmovzxbd
%
xmm4
%
xmm4
.
byte
196
227
109
24
212
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
194
121
49
224
/
/
vpmovzxbd
%
xmm8
%
xmm4
.
byte
196
227
97
32
216
3
/
/
vpinsrb
0x3
%
eax
%
xmm3
%
xmm3
.
byte
196
226
121
49
219
/
/
vpmovzxbd
%
xmm3
%
xmm3
.
byte
196
227
93
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm4
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
217
/
/
vmulps
%
ymm9
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
229
/
/
vmovaps
%
ymm5
%
ymm4
.
byte
197
252
40
238
/
/
vmovaps
%
ymm6
%
ymm5
.
byte
197
252
40
247
/
/
vmovaps
%
ymm7
%
ymm6
.
byte
197
252
16
124
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_byte_tables_rgb_avx
.
globl
_sk_byte_tables_rgb_avx
FUNCTION
(
_sk_byte_tables_rgb_avx
)
_sk_byte_tables_rgb_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
196
65
121
110
192
/
/
vmovd
%
r8d
%
xmm8
.
byte
196
65
121
112
192
0
/
/
vpshufd
0x0
%
xmm8
%
xmm8
.
byte
196
67
61
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
98
125
24
21
5
140
2
0
/
/
vbroadcastss
0x28c05
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
194
/
/
vminps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
125
91
216
/
/
vcvtps2dq
%
ymm0
%
ymm11
.
byte
196
65
249
126
216
/
/
vmovq
%
xmm11
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
193
121
110
193
/
/
vmovd
%
r9d
%
xmm0
.
byte
196
67
249
22
217
1
/
/
vpextrq
0x1
%
xmm11
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
121
32
224
1
/
/
vpinsrb
0x1
%
r8d
%
xmm0
%
xmm12
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
196
99
125
25
216
1
/
/
vextractf128
0x1
%
ymm11
%
xmm0
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
67
25
32
216
2
/
/
vpinsrb
0x2
%
r8d
%
xmm12
%
xmm11
.
byte
196
193
249
126
192
/
/
vmovq
%
xmm0
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
33
32
217
3
/
/
vpinsrb
0x3
%
r9d
%
xmm11
%
xmm11
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
65
121
110
225
/
/
vmovd
%
r9d
%
xmm12
.
byte
196
195
249
22
193
1
/
/
vpextrq
0x1
%
xmm0
%
r9
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
25
32
192
1
/
/
vpinsrb
0x1
%
r8d
%
xmm12
%
xmm0
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
4
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
r8d
.
byte
196
195
121
32
192
2
/
/
vpinsrb
0x2
%
r8d
%
xmm0
%
xmm0
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
71
15
182
12
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
r9d
.
byte
196
67
121
32
233
3
/
/
vpinsrb
0x3
%
r9d
%
xmm0
%
xmm13
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
193
116
93
202
/
/
vminps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
253
91
201
/
/
vcvtps2dq
%
ymm1
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
67
121
32
226
3
/
/
vpinsrb
0x3
%
r10d
%
xmm0
%
xmm12
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
196
193
121
110
194
/
/
vmovd
%
r10d
%
xmm0
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
1
/
/
vpinsrb
0x1
%
r9d
%
xmm0
%
xmm0
.
byte
69
137
209
/
/
mov
%
r10d
%
r9d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
196
195
121
32
193
2
/
/
vpinsrb
0x2
%
r9d
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r8d
.
byte
196
67
121
32
240
3
/
/
vpinsrb
0x3
%
r8d
%
xmm0
%
xmm14
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
180
95
194
/
/
vmaxps
%
ymm2
%
ymm9
%
ymm0
.
byte
196
193
124
93
194
/
/
vminps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
253
91
208
/
/
vcvtps2dq
%
ymm0
%
ymm2
.
byte
196
193
249
126
208
/
/
vmovq
%
xmm2
%
r8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
121
32
192
1
/
/
vpinsrb
0x1
%
eax
%
xmm0
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
210
1
/
/
vextractf128
0x1
%
ymm2
%
xmm2
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
227
121
32
192
2
/
/
vpinsrb
0x2
%
eax
%
xmm0
%
xmm0
.
byte
196
193
249
126
208
/
/
vmovq
%
xmm2
%
r8
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
99
121
32
192
3
/
/
vpinsrb
0x3
%
eax
%
xmm0
%
xmm8
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
67
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
eax
.
byte
196
227
121
32
192
1
/
/
vpinsrb
0x1
%
eax
%
xmm0
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
1
/
/
movzbl
(
%
r9
%
rax
1
)
%
eax
.
byte
196
99
121
32
208
2
/
/
vpinsrb
0x2
%
eax
%
xmm0
%
xmm10
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
eax
.
byte
196
194
121
49
195
/
/
vpmovzxbd
%
xmm11
%
xmm0
.
byte
196
194
121
49
205
/
/
vpmovzxbd
%
xmm13
%
xmm1
.
byte
196
227
125
24
193
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
131
138
2
0
/
/
vbroadcastss
0x28a83
(
%
rip
)
%
ymm9
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
194
121
49
204
/
/
vpmovzxbd
%
xmm12
%
xmm1
.
byte
196
194
121
49
214
/
/
vpmovzxbd
%
xmm14
%
xmm2
.
byte
196
227
117
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
66
121
49
192
/
/
vpmovzxbd
%
xmm8
%
xmm8
.
byte
196
227
41
32
208
3
/
/
vpinsrb
0x3
%
eax
%
xmm10
%
xmm2
.
byte
196
226
121
49
210
/
/
vpmovzxbd
%
xmm2
%
xmm2
.
byte
196
227
61
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm8
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_r_avx
.
globl
_sk_table_r_avx
FUNCTION
(
_sk_table_r_avx
)
_sk_table_r_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
65
121
112
192
0
/
/
vpshufd
0x0
%
xmm8
%
xmm8
.
byte
196
67
61
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
98
125
24
13
146
137
2
0
/
/
vbroadcastss
0x28992
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
193
/
/
vminps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
227
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
192
1
/
/
vextractf128
0x1
%
ymm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
122
16
4
160
/
/
vmovss
(
%
r8
%
r12
4
)
%
xmm0
.
byte
196
3
121
33
4
184
16
/
/
vinsertps
0x10
(
%
r8
%
r15
4
)
%
xmm0
%
xmm8
.
byte
196
129
122
16
4
176
/
/
vmovss
(
%
r8
%
r14
4
)
%
xmm0
.
byte
196
99
57
33
192
32
/
/
vinsertps
0x20
%
xmm0
%
xmm8
%
xmm8
.
byte
196
193
122
16
4
152
/
/
vmovss
(
%
r8
%
rbx
4
)
%
xmm0
.
byte
196
99
57
33
192
48
/
/
vinsertps
0x30
%
xmm0
%
xmm8
%
xmm8
.
byte
196
129
122
16
4
152
/
/
vmovss
(
%
r8
%
r11
4
)
%
xmm0
.
byte
196
3
121
33
12
144
16
/
/
vinsertps
0x10
(
%
r8
%
r10
4
)
%
xmm0
%
xmm9
.
byte
196
129
122
16
4
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm0
.
byte
196
99
49
33
200
32
/
/
vinsertps
0x20
%
xmm0
%
xmm9
%
xmm9
.
byte
196
193
122
16
4
128
/
/
vmovss
(
%
r8
%
rax
4
)
%
xmm0
.
byte
196
227
49
33
192
48
/
/
vinsertps
0x30
%
xmm0
%
xmm9
%
xmm0
.
byte
196
195
125
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_g_avx
.
globl
_sk_table_g_avx
FUNCTION
(
_sk_table_g_avx
)
_sk_table_g_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
65
121
112
192
0
/
/
vpshufd
0x0
%
xmm8
%
xmm8
.
byte
196
67
61
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
196
98
125
24
13
185
136
2
0
/
/
vbroadcastss
0x288b9
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
116
93
201
/
/
vminps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
253
91
201
/
/
vcvtps2dq
%
ymm1
%
ymm1
.
byte
196
227
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
227
249
22
203
1
/
/
vpextrq
0x1
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
207
/
/
vmovq
%
xmm1
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
122
16
12
160
/
/
vmovss
(
%
r8
%
r12
4
)
%
xmm1
.
byte
196
3
113
33
4
184
16
/
/
vinsertps
0x10
(
%
r8
%
r15
4
)
%
xmm1
%
xmm8
.
byte
196
129
122
16
12
176
/
/
vmovss
(
%
r8
%
r14
4
)
%
xmm1
.
byte
196
99
57
33
193
32
/
/
vinsertps
0x20
%
xmm1
%
xmm8
%
xmm8
.
byte
196
193
122
16
12
152
/
/
vmovss
(
%
r8
%
rbx
4
)
%
xmm1
.
byte
196
99
57
33
193
48
/
/
vinsertps
0x30
%
xmm1
%
xmm8
%
xmm8
.
byte
196
129
122
16
12
152
/
/
vmovss
(
%
r8
%
r11
4
)
%
xmm1
.
byte
196
3
113
33
12
144
16
/
/
vinsertps
0x10
(
%
r8
%
r10
4
)
%
xmm1
%
xmm9
.
byte
196
129
122
16
12
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm1
.
byte
196
99
49
33
201
32
/
/
vinsertps
0x20
%
xmm1
%
xmm9
%
xmm9
.
byte
196
193
122
16
12
128
/
/
vmovss
(
%
r8
%
rax
4
)
%
xmm1
.
byte
196
227
49
33
201
48
/
/
vinsertps
0x30
%
xmm1
%
xmm9
%
xmm1
.
byte
196
195
117
24
200
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_b_avx
.
globl
_sk_table_b_avx
FUNCTION
(
_sk_table_b_avx
)
_sk_table_b_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
65
121
112
192
0
/
/
vpshufd
0x0
%
xmm8
%
xmm8
.
byte
196
67
61
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
210
/
/
vmaxps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
98
125
24
13
224
135
2
0
/
/
vbroadcastss
0x287e0
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
108
93
209
/
/
vminps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
253
91
210
/
/
vcvtps2dq
%
ymm2
%
ymm2
.
byte
196
227
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
210
/
/
vmovq
%
xmm2
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
210
1
/
/
vextractf128
0x1
%
ymm2
%
xmm2
.
byte
196
227
249
22
211
1
/
/
vpextrq
0x1
%
xmm2
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
215
/
/
vmovq
%
xmm2
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
122
16
20
160
/
/
vmovss
(
%
r8
%
r12
4
)
%
xmm2
.
byte
196
3
105
33
4
184
16
/
/
vinsertps
0x10
(
%
r8
%
r15
4
)
%
xmm2
%
xmm8
.
byte
196
129
122
16
20
176
/
/
vmovss
(
%
r8
%
r14
4
)
%
xmm2
.
byte
196
99
57
33
194
32
/
/
vinsertps
0x20
%
xmm2
%
xmm8
%
xmm8
.
byte
196
193
122
16
20
152
/
/
vmovss
(
%
r8
%
rbx
4
)
%
xmm2
.
byte
196
99
57
33
194
48
/
/
vinsertps
0x30
%
xmm2
%
xmm8
%
xmm8
.
byte
196
129
122
16
20
152
/
/
vmovss
(
%
r8
%
r11
4
)
%
xmm2
.
byte
196
3
105
33
12
144
16
/
/
vinsertps
0x10
(
%
r8
%
r10
4
)
%
xmm2
%
xmm9
.
byte
196
129
122
16
20
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
196
99
49
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm9
%
xmm9
.
byte
196
193
122
16
20
128
/
/
vmovss
(
%
r8
%
rax
4
)
%
xmm2
.
byte
196
227
49
33
210
48
/
/
vinsertps
0x30
%
xmm2
%
xmm9
%
xmm2
.
byte
196
195
109
24
208
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_a_avx
.
globl
_sk_table_a_avx
FUNCTION
(
_sk_table_a_avx
)
_sk_table_a_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
65
121
112
192
0
/
/
vpshufd
0x0
%
xmm8
%
xmm8
.
byte
196
67
61
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm8
%
ymm8
.
byte
196
65
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
219
/
/
vmaxps
%
ymm3
%
ymm9
%
ymm3
.
byte
196
98
125
24
13
7
135
2
0
/
/
vbroadcastss
0x28707
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
100
93
217
/
/
vminps
%
ymm9
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
253
91
219
/
/
vcvtps2dq
%
ymm3
%
ymm3
.
byte
196
227
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
218
/
/
vmovq
%
xmm3
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
219
1
/
/
vextractf128
0x1
%
ymm3
%
xmm3
.
byte
196
227
249
22
219
1
/
/
vpextrq
0x1
%
xmm3
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
223
/
/
vmovq
%
xmm3
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
122
16
28
160
/
/
vmovss
(
%
r8
%
r12
4
)
%
xmm3
.
byte
196
3
97
33
4
184
16
/
/
vinsertps
0x10
(
%
r8
%
r15
4
)
%
xmm3
%
xmm8
.
byte
196
129
122
16
28
176
/
/
vmovss
(
%
r8
%
r14
4
)
%
xmm3
.
byte
196
99
57
33
195
32
/
/
vinsertps
0x20
%
xmm3
%
xmm8
%
xmm8
.
byte
196
193
122
16
28
152
/
/
vmovss
(
%
r8
%
rbx
4
)
%
xmm3
.
byte
196
99
57
33
195
48
/
/
vinsertps
0x30
%
xmm3
%
xmm8
%
xmm8
.
byte
196
129
122
16
28
152
/
/
vmovss
(
%
r8
%
r11
4
)
%
xmm3
.
byte
196
3
97
33
12
144
16
/
/
vinsertps
0x10
(
%
r8
%
r10
4
)
%
xmm3
%
xmm9
.
byte
196
129
122
16
28
136
/
/
vmovss
(
%
r8
%
r9
4
)
%
xmm3
.
byte
196
99
49
33
203
32
/
/
vinsertps
0x20
%
xmm3
%
xmm9
%
xmm9
.
byte
196
193
122
16
28
128
/
/
vmovss
(
%
r8
%
rax
4
)
%
xmm3
.
byte
196
227
49
33
219
48
/
/
vinsertps
0x30
%
xmm3
%
xmm9
%
xmm3
.
byte
196
195
101
24
216
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_r_avx
.
globl
_sk_parametric_r_avx
FUNCTION
(
_sk_parametric_r_avx
)
_sk_parametric_r_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
124
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm0
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
197
52
89
200
/
/
vmulps
%
ymm0
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
197
172
89
192
/
/
vmulps
%
ymm0
%
ymm10
%
ymm0
.
byte
196
65
124
88
219
/
/
vaddps
%
ymm11
%
ymm0
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
36
194
208
0
/
/
vcmpeqps
%
ymm0
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
170
134
2
0
/
/
vbroadcastss
0x286aa
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
160
134
2
0
/
/
vbroadcastss
0x286a0
(
%
rip
)
%
ymm14
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
222
/
/
vandps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
242
133
2
0
/
/
vbroadcastss
0x285f2
(
%
rip
)
%
ymm14
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
222
/
/
vorps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
136
134
2
0
/
/
vbroadcastss
0x28688
(
%
rip
)
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
65
20
88
238
/
/
vaddps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
126
134
2
0
/
/
vbroadcastss
0x2867e
(
%
rip
)
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
65
36
89
246
/
/
vmulps
%
ymm14
%
ymm11
%
ymm14
.
byte
196
65
20
92
238
/
/
vsubps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
111
134
2
0
/
/
vbroadcastss
0x2866f
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
101
134
2
0
/
/
vbroadcastss
0x28665
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
70
134
2
0
/
/
vbroadcastss
0x28646
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
60
134
2
0
/
/
vbroadcastss
0x2863c
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
65
28
89
237
/
/
vmulps
%
ymm13
%
ymm12
%
ymm13
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
45
134
2
0
/
/
vbroadcastss
0x2862d
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
228
/
/
vsubps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
98
125
24
45
35
134
2
0
/
/
vbroadcastss
0x28623
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
228
/
/
vdivps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
20
134
2
0
/
/
vbroadcastss
0x28614
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
208
160
/
/
vblendvps
%
ymm10
%
ymm0
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
38
133
2
0
/
/
vbroadcastss
0x28526
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_g_avx
.
globl
_sk_parametric_g_avx
FUNCTION
(
_sk_parametric_g_avx
)
_sk_parametric_g_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
116
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
197
52
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
197
172
89
201
/
/
vmulps
%
ymm1
%
ymm10
%
ymm1
.
byte
196
65
116
88
219
/
/
vaddps
%
ymm11
%
ymm1
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
36
194
209
0
/
/
vcmpeqps
%
ymm1
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
97
133
2
0
/
/
vbroadcastss
0x28561
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
87
133
2
0
/
/
vbroadcastss
0x28557
(
%
rip
)
%
ymm14
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
222
/
/
vandps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
169
132
2
0
/
/
vbroadcastss
0x284a9
(
%
rip
)
%
ymm14
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
222
/
/
vorps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
63
133
2
0
/
/
vbroadcastss
0x2853f
(
%
rip
)
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
65
20
88
238
/
/
vaddps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
53
133
2
0
/
/
vbroadcastss
0x28535
(
%
rip
)
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
65
36
89
246
/
/
vmulps
%
ymm14
%
ymm11
%
ymm14
.
byte
196
65
20
92
238
/
/
vsubps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
38
133
2
0
/
/
vbroadcastss
0x28526
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
28
133
2
0
/
/
vbroadcastss
0x2851c
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
253
132
2
0
/
/
vbroadcastss
0x284fd
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
243
132
2
0
/
/
vbroadcastss
0x284f3
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
65
28
89
237
/
/
vmulps
%
ymm13
%
ymm12
%
ymm13
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
228
132
2
0
/
/
vbroadcastss
0x284e4
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
228
/
/
vsubps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
98
125
24
45
218
132
2
0
/
/
vbroadcastss
0x284da
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
228
/
/
vdivps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
203
132
2
0
/
/
vbroadcastss
0x284cb
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
209
160
/
/
vblendvps
%
ymm10
%
ymm1
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
201
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
98
125
24
5
221
131
2
0
/
/
vbroadcastss
0x283dd
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_b_avx
.
globl
_sk_parametric_b_avx
FUNCTION
(
_sk_parametric_b_avx
)
_sk_parametric_b_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
108
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm2
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
197
52
89
202
/
/
vmulps
%
ymm2
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
197
172
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm2
.
byte
196
65
108
88
219
/
/
vaddps
%
ymm11
%
ymm2
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
36
194
210
0
/
/
vcmpeqps
%
ymm2
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
24
132
2
0
/
/
vbroadcastss
0x28418
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
14
132
2
0
/
/
vbroadcastss
0x2840e
(
%
rip
)
%
ymm14
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
222
/
/
vandps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
96
131
2
0
/
/
vbroadcastss
0x28360
(
%
rip
)
%
ymm14
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
222
/
/
vorps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
246
131
2
0
/
/
vbroadcastss
0x283f6
(
%
rip
)
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
65
20
88
238
/
/
vaddps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
236
131
2
0
/
/
vbroadcastss
0x283ec
(
%
rip
)
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
65
36
89
246
/
/
vmulps
%
ymm14
%
ymm11
%
ymm14
.
byte
196
65
20
92
238
/
/
vsubps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
221
131
2
0
/
/
vbroadcastss
0x283dd
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
211
131
2
0
/
/
vbroadcastss
0x283d3
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
180
131
2
0
/
/
vbroadcastss
0x283b4
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
170
131
2
0
/
/
vbroadcastss
0x283aa
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
65
28
89
237
/
/
vmulps
%
ymm13
%
ymm12
%
ymm13
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
155
131
2
0
/
/
vbroadcastss
0x2839b
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
228
/
/
vsubps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
98
125
24
45
145
131
2
0
/
/
vbroadcastss
0x28391
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
228
/
/
vdivps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
130
131
2
0
/
/
vbroadcastss
0x28382
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
210
160
/
/
vblendvps
%
ymm10
%
ymm2
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
210
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm2
.
byte
196
98
125
24
5
148
130
2
0
/
/
vbroadcastss
0x28294
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_a_avx
.
globl
_sk_parametric_a_avx
FUNCTION
(
_sk_parametric_a_avx
)
_sk_parametric_a_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
100
194
192
2
/
/
vcmpleps
%
ymm8
%
ymm3
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
197
52
89
203
/
/
vmulps
%
ymm3
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm11
.
byte
197
172
89
219
/
/
vmulps
%
ymm3
%
ymm10
%
ymm3
.
byte
196
65
100
88
219
/
/
vaddps
%
ymm11
%
ymm3
%
ymm11
.
byte
196
98
125
24
32
/
/
vbroadcastss
(
%
rax
)
%
ymm12
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
36
194
211
0
/
/
vcmpeqps
%
ymm3
%
ymm11
%
ymm10
.
byte
196
65
124
91
235
/
/
vcvtdq2ps
%
ymm11
%
ymm13
.
byte
196
98
125
24
53
207
130
2
0
/
/
vbroadcastss
0x282cf
(
%
rip
)
%
ymm14
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
196
65
20
89
238
/
/
vmulps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
197
130
2
0
/
/
vbroadcastss
0x282c5
(
%
rip
)
%
ymm14
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
65
36
84
222
/
/
vandps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
23
130
2
0
/
/
vbroadcastss
0x28217
(
%
rip
)
%
ymm14
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
36
86
222
/
/
vorps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
173
130
2
0
/
/
vbroadcastss
0x282ad
(
%
rip
)
%
ymm14
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
65
20
88
238
/
/
vaddps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
163
130
2
0
/
/
vbroadcastss
0x282a3
(
%
rip
)
%
ymm14
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
65
36
89
246
/
/
vmulps
%
ymm14
%
ymm11
%
ymm14
.
byte
196
65
20
92
238
/
/
vsubps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
98
125
24
53
148
130
2
0
/
/
vbroadcastss
0x28294
(
%
rip
)
%
ymm14
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
65
36
88
222
/
/
vaddps
%
ymm14
%
ymm11
%
ymm11
.
byte
196
98
125
24
53
138
130
2
0
/
/
vbroadcastss
0x2828a
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
196
65
12
94
219
/
/
vdivps
%
ymm11
%
ymm14
%
ymm11
.
byte
196
65
20
92
219
/
/
vsubps
%
ymm11
%
ymm13
%
ymm11
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
67
125
8
227
1
/
/
vroundps
0x1
%
ymm11
%
ymm12
.
byte
196
65
36
92
228
/
/
vsubps
%
ymm12
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
107
130
2
0
/
/
vbroadcastss
0x2826b
(
%
rip
)
%
ymm13
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
65
36
88
221
/
/
vaddps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
97
130
2
0
/
/
vbroadcastss
0x28261
(
%
rip
)
%
ymm13
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
65
28
89
237
/
/
vmulps
%
ymm13
%
ymm12
%
ymm13
.
byte
196
65
36
92
221
/
/
vsubps
%
ymm13
%
ymm11
%
ymm11
.
byte
196
98
125
24
45
82
130
2
0
/
/
vbroadcastss
0x28252
(
%
rip
)
%
ymm13
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
196
65
20
92
228
/
/
vsubps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
98
125
24
45
72
130
2
0
/
/
vbroadcastss
0x28248
(
%
rip
)
%
ymm13
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
196
65
20
94
228
/
/
vdivps
%
ymm12
%
ymm13
%
ymm12
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
98
125
24
37
57
130
2
0
/
/
vbroadcastss
0x28239
(
%
rip
)
%
ymm12
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
99
37
74
211
160
/
/
vblendvps
%
ymm10
%
ymm3
%
ymm11
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
45
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm10
%
ymm8
.
byte
197
188
95
219
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm3
.
byte
196
98
125
24
5
75
129
2
0
/
/
vbroadcastss
0x2814b
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_avx
.
globl
_sk_gamma_avx
FUNCTION
(
_sk_gamma_avx
)
_sk_gamma_avx
:
.
byte
72
129
236
184
0
0
0
/
/
sub
0xb8
%
rsp
.
byte
197
252
17
188
36
128
0
0
0
/
/
vmovups
%
ymm7
0x80
(
%
rsp
)
.
byte
197
252
17
116
36
96
/
/
vmovups
%
ymm6
0x60
(
%
rsp
)
.
byte
197
252
17
108
36
64
/
/
vmovups
%
ymm5
0x40
(
%
rsp
)
.
byte
197
252
17
100
36
32
/
/
vmovups
%
ymm4
0x20
(
%
rsp
)
.
byte
197
252
17
28
36
/
/
vmovups
%
ymm3
(
%
rsp
)
.
byte
197
252
17
84
36
224
/
/
vmovups
%
ymm2
-
0x20
(
%
rsp
)
.
byte
197
252
17
76
36
128
/
/
vmovups
%
ymm1
-
0x80
(
%
rsp
)
.
byte
197
252
40
224
/
/
vmovaps
%
ymm0
%
ymm4
.
byte
197
252
91
204
/
/
vcvtdq2ps
%
ymm4
%
ymm1
.
byte
196
226
125
24
5
150
129
2
0
/
/
vbroadcastss
0x28196
(
%
rip
)
%
ymm0
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
197
244
89
200
/
/
vmulps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
124
40
192
/
/
vmovaps
%
ymm0
%
ymm8
.
byte
197
124
17
68
36
160
/
/
vmovups
%
ymm8
-
0x60
(
%
rsp
)
.
byte
196
98
125
24
13
131
129
2
0
/
/
vbroadcastss
0x28183
(
%
rip
)
%
ymm9
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
196
193
92
84
209
/
/
vandps
%
ymm9
%
ymm4
%
ymm2
.
byte
196
98
125
24
21
213
128
2
0
/
/
vbroadcastss
0x280d5
(
%
rip
)
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
108
86
210
/
/
vorps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
98
125
24
29
107
129
2
0
/
/
vbroadcastss
0x2816b
(
%
rip
)
%
ymm11
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
196
98
125
24
37
97
129
2
0
/
/
vbroadcastss
0x28161
(
%
rip
)
%
ymm12
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
193
108
89
220
/
/
vmulps
%
ymm12
%
ymm2
%
ymm3
.
byte
197
244
92
203
/
/
vsubps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
98
125
24
45
83
129
2
0
/
/
vbroadcastss
0x28153
(
%
rip
)
%
ymm13
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
193
108
88
213
/
/
vaddps
%
ymm13
%
ymm2
%
ymm2
.
byte
196
98
125
24
53
73
129
2
0
/
/
vbroadcastss
0x28149
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
197
140
94
210
/
/
vdivps
%
ymm2
%
ymm14
%
ymm2
.
byte
197
244
92
202
/
/
vsubps
%
ymm2
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
48
/
/
vbroadcastss
(
%
rax
)
%
ymm6
.
byte
197
244
89
206
/
/
vmulps
%
ymm6
%
ymm1
%
ymm1
.
byte
196
227
125
8
209
1
/
/
vroundps
0x1
%
ymm1
%
ymm2
.
byte
197
244
92
234
/
/
vsubps
%
ymm2
%
ymm1
%
ymm5
.
byte
196
98
125
24
61
39
129
2
0
/
/
vbroadcastss
0x28127
(
%
rip
)
%
ymm15
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
193
116
88
207
/
/
vaddps
%
ymm15
%
ymm1
%
ymm1
.
byte
196
226
125
24
61
29
129
2
0
/
/
vbroadcastss
0x2811d
(
%
rip
)
%
ymm7
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
197
212
89
215
/
/
vmulps
%
ymm7
%
ymm5
%
ymm2
.
byte
197
244
92
194
/
/
vsubps
%
ymm2
%
ymm1
%
ymm0
.
byte
196
226
125
24
29
16
129
2
0
/
/
vbroadcastss
0x28110
(
%
rip
)
%
ymm3
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
197
228
92
237
/
/
vsubps
%
ymm5
%
ymm3
%
ymm5
.
byte
196
226
125
24
21
7
129
2
0
/
/
vbroadcastss
0x28107
(
%
rip
)
%
ymm2
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
197
236
94
237
/
/
vdivps
%
ymm5
%
ymm2
%
ymm5
.
byte
197
252
88
197
/
/
vaddps
%
ymm5
%
ymm0
%
ymm0
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
220
194
205
0
/
/
vcmpeqps
%
ymm5
%
ymm4
%
ymm1
.
byte
196
226
125
24
37
241
128
2
0
/
/
vbroadcastss
0x280f1
(
%
rip
)
%
ymm4
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
74
197
16
/
/
vblendvps
%
ymm1
%
ymm5
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
192
/
/
vmovups
%
ymm0
-
0x40
(
%
rsp
)
.
byte
197
252
16
108
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm5
.
byte
197
252
91
197
/
/
vcvtdq2ps
%
ymm5
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
84
84
201
/
/
vandps
%
ymm9
%
ymm5
%
ymm1
.
byte
197
124
40
197
/
/
vmovaps
%
ymm5
%
ymm8
.
byte
196
193
116
86
202
/
/
vorps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
124
88
195
/
/
vaddps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
116
89
236
/
/
vmulps
%
ymm12
%
ymm1
%
ymm5
.
byte
197
252
92
197
/
/
vsubps
%
ymm5
%
ymm0
%
ymm0
.
byte
196
193
116
88
205
/
/
vaddps
%
ymm13
%
ymm1
%
ymm1
.
byte
197
140
94
201
/
/
vdivps
%
ymm1
%
ymm14
%
ymm1
.
byte
197
252
92
193
/
/
vsubps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
89
198
/
/
vmulps
%
ymm6
%
ymm0
%
ymm0
.
byte
196
227
125
8
200
1
/
/
vroundps
0x1
%
ymm0
%
ymm1
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
196
193
124
88
199
/
/
vaddps
%
ymm15
%
ymm0
%
ymm0
.
byte
197
244
89
239
/
/
vmulps
%
ymm7
%
ymm1
%
ymm5
.
byte
197
252
92
197
/
/
vsubps
%
ymm5
%
ymm0
%
ymm0
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
236
94
201
/
/
vdivps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
188
194
205
0
/
/
vcmpeqps
%
ymm5
%
ymm8
%
ymm1
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
74
197
16
/
/
vblendvps
%
ymm1
%
ymm5
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
128
/
/
vmovups
%
ymm0
-
0x80
(
%
rsp
)
.
byte
197
252
16
108
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
91
197
/
/
vcvtdq2ps
%
ymm5
%
ymm0
.
byte
197
252
89
68
36
160
/
/
vmulps
-
0x60
(
%
rsp
)
%
ymm0
%
ymm0
.
byte
196
193
84
84
201
/
/
vandps
%
ymm9
%
ymm5
%
ymm1
.
byte
197
124
40
197
/
/
vmovaps
%
ymm5
%
ymm8
.
byte
196
193
116
86
202
/
/
vorps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
124
88
195
/
/
vaddps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
116
89
236
/
/
vmulps
%
ymm12
%
ymm1
%
ymm5
.
byte
197
252
92
197
/
/
vsubps
%
ymm5
%
ymm0
%
ymm0
.
byte
196
193
116
88
205
/
/
vaddps
%
ymm13
%
ymm1
%
ymm1
.
byte
197
140
94
201
/
/
vdivps
%
ymm1
%
ymm14
%
ymm1
.
byte
197
252
92
193
/
/
vsubps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
89
198
/
/
vmulps
%
ymm6
%
ymm0
%
ymm0
.
byte
196
227
125
8
200
1
/
/
vroundps
0x1
%
ymm0
%
ymm1
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
196
193
124
88
199
/
/
vaddps
%
ymm15
%
ymm0
%
ymm0
.
byte
197
244
89
239
/
/
vmulps
%
ymm7
%
ymm1
%
ymm5
.
byte
197
252
92
197
/
/
vsubps
%
ymm5
%
ymm0
%
ymm0
.
byte
197
228
92
201
/
/
vsubps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
236
94
201
/
/
vdivps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
89
196
/
/
vmulps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
188
194
202
0
/
/
vcmpeqps
%
ymm2
%
ymm8
%
ymm1
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
74
210
16
/
/
vblendvps
%
ymm1
%
ymm2
%
ymm0
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
68
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
16
76
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm1
.
byte
197
252
16
28
36
/
/
vmovups
(
%
rsp
)
%
ymm3
.
byte
197
252
16
100
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm6
.
byte
197
252
16
188
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm7
.
byte
72
129
196
184
0
0
0
/
/
add
0xb8
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_dst_avx
.
globl
_sk_gamma_dst_avx
FUNCTION
(
_sk_gamma_dst_avx
)
_sk_gamma_dst_avx
:
.
byte
72
129
236
248
0
0
0
/
/
sub
0xf8
%
rsp
.
byte
197
252
17
188
36
192
0
0
0
/
/
vmovups
%
ymm7
0xc0
(
%
rsp
)
.
byte
197
252
17
116
36
160
/
/
vmovups
%
ymm6
-
0x60
(
%
rsp
)
.
byte
197
124
40
197
/
/
vmovaps
%
ymm5
%
ymm8
.
byte
197
252
17
156
36
160
0
0
0
/
/
vmovups
%
ymm3
0xa0
(
%
rsp
)
.
byte
197
252
17
148
36
128
0
0
0
/
/
vmovups
%
ymm2
0x80
(
%
rsp
)
.
byte
197
252
17
76
36
96
/
/
vmovups
%
ymm1
0x60
(
%
rsp
)
.
byte
197
252
17
68
36
64
/
/
vmovups
%
ymm0
0x40
(
%
rsp
)
.
byte
197
252
91
196
/
/
vcvtdq2ps
%
ymm4
%
ymm0
.
byte
196
226
125
24
29
70
127
2
0
/
/
vbroadcastss
0x27f46
(
%
rip
)
%
ymm3
#
3c594
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x348
>
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
61
127
2
0
/
/
vbroadcastss
0x27f3d
(
%
rip
)
%
ymm1
#
3c598
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x34c
>
.
byte
197
252
17
12
36
/
/
vmovups
%
ymm1
(
%
rsp
)
.
byte
197
220
84
201
/
/
vandps
%
ymm1
%
ymm4
%
ymm1
.
byte
196
226
125
24
21
139
126
2
0
/
/
vbroadcastss
0x27e8b
(
%
rip
)
%
ymm2
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
17
84
36
224
/
/
vmovups
%
ymm2
-
0x20
(
%
rsp
)
.
byte
197
244
86
202
/
/
vorps
%
ymm2
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
28
127
2
0
/
/
vbroadcastss
0x27f1c
(
%
rip
)
%
ymm2
#
3c59c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x350
>
.
byte
197
252
17
84
36
192
/
/
vmovups
%
ymm2
-
0x40
(
%
rsp
)
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
98
125
24
37
13
127
2
0
/
/
vbroadcastss
0x27f0d
(
%
rip
)
%
ymm12
#
3c5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x354
>
.
byte
196
193
116
89
212
/
/
vmulps
%
ymm12
%
ymm1
%
ymm2
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
98
125
24
45
255
126
2
0
/
/
vbroadcastss
0x27eff
(
%
rip
)
%
ymm13
#
3c5a4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x358
>
.
byte
196
193
116
88
205
/
/
vaddps
%
ymm13
%
ymm1
%
ymm1
.
byte
196
98
125
24
53
245
126
2
0
/
/
vbroadcastss
0x27ef5
(
%
rip
)
%
ymm14
#
3c5a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x35c
>
.
byte
197
140
94
201
/
/
vdivps
%
ymm1
%
ymm14
%
ymm1
.
byte
197
252
92
193
/
/
vsubps
%
ymm1
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
40
/
/
vbroadcastss
(
%
rax
)
%
ymm5
.
byte
197
252
89
197
/
/
vmulps
%
ymm5
%
ymm0
%
ymm0
.
byte
196
227
125
8
200
1
/
/
vroundps
0x1
%
ymm0
%
ymm1
.
byte
197
252
92
201
/
/
vsubps
%
ymm1
%
ymm0
%
ymm1
.
byte
196
98
125
24
61
211
126
2
0
/
/
vbroadcastss
0x27ed3
(
%
rip
)
%
ymm15
#
3c5ac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x360
>
.
byte
196
193
124
88
199
/
/
vaddps
%
ymm15
%
ymm0
%
ymm0
.
byte
196
98
125
24
29
201
126
2
0
/
/
vbroadcastss
0x27ec9
(
%
rip
)
%
ymm11
#
3c5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x364
>
.
byte
196
193
116
89
211
/
/
vmulps
%
ymm11
%
ymm1
%
ymm2
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
187
126
2
0
/
/
vbroadcastss
0x27ebb
(
%
rip
)
%
ymm10
#
3c5b4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x368
>
.
byte
197
172
92
241
/
/
vsubps
%
ymm1
%
ymm10
%
ymm6
.
byte
196
98
125
24
13
178
126
2
0
/
/
vbroadcastss
0x27eb2
(
%
rip
)
%
ymm9
#
3c5b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x36c
>
.
byte
197
180
94
246
/
/
vdivps
%
ymm6
%
ymm9
%
ymm6
.
byte
197
252
88
198
/
/
vaddps
%
ymm6
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
220
194
225
0
/
/
vcmpeqps
%
ymm1
%
ymm4
%
ymm4
.
byte
196
226
125
24
61
156
126
2
0
/
/
vbroadcastss
0x27e9c
(
%
rip
)
%
ymm7
#
3c5bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x370
>
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
74
193
64
/
/
vblendvps
%
ymm4
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
32
/
/
vmovups
%
ymm0
0x20
(
%
rsp
)
.
byte
197
124
17
68
36
128
/
/
vmovups
%
ymm8
-
0x80
(
%
rsp
)
.
byte
196
193
124
91
192
/
/
vcvtdq2ps
%
ymm8
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
252
16
20
36
/
/
vmovups
(
%
rsp
)
%
ymm2
.
byte
197
188
84
242
/
/
vandps
%
ymm2
%
ymm8
%
ymm6
.
byte
197
252
16
76
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm1
.
byte
197
204
86
241
/
/
vorps
%
ymm1
%
ymm6
%
ymm6
.
byte
197
252
16
100
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm4
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
65
76
89
196
/
/
vmulps
%
ymm12
%
ymm6
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
76
88
245
/
/
vaddps
%
ymm13
%
ymm6
%
ymm6
.
byte
197
140
94
246
/
/
vdivps
%
ymm6
%
ymm14
%
ymm6
.
byte
197
252
92
198
/
/
vsubps
%
ymm6
%
ymm0
%
ymm0
.
byte
197
252
89
197
/
/
vmulps
%
ymm5
%
ymm0
%
ymm0
.
byte
196
227
125
8
240
1
/
/
vroundps
0x1
%
ymm0
%
ymm6
.
byte
197
252
92
246
/
/
vsubps
%
ymm6
%
ymm0
%
ymm6
.
byte
196
193
124
88
199
/
/
vaddps
%
ymm15
%
ymm0
%
ymm0
.
byte
196
65
76
89
195
/
/
vmulps
%
ymm11
%
ymm6
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
172
92
246
/
/
vsubps
%
ymm6
%
ymm10
%
ymm6
.
byte
197
180
94
246
/
/
vdivps
%
ymm6
%
ymm9
%
ymm6
.
byte
197
252
88
198
/
/
vaddps
%
ymm6
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
194
116
36
128
0
/
/
vcmpeqps
-
0x80
(
%
rsp
)
%
ymm8
%
ymm6
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
195
125
74
192
96
/
/
vblendvps
%
ymm6
%
ymm8
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
128
/
/
vmovups
%
ymm0
-
0x80
(
%
rsp
)
.
byte
197
252
16
116
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm6
.
byte
197
252
91
198
/
/
vcvtdq2ps
%
ymm6
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
197
204
84
242
/
/
vandps
%
ymm2
%
ymm6
%
ymm6
.
byte
197
204
86
241
/
/
vorps
%
ymm1
%
ymm6
%
ymm6
.
byte
197
252
88
196
/
/
vaddps
%
ymm4
%
ymm0
%
ymm0
.
byte
196
65
76
89
196
/
/
vmulps
%
ymm12
%
ymm6
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
76
88
245
/
/
vaddps
%
ymm13
%
ymm6
%
ymm6
.
byte
197
140
94
246
/
/
vdivps
%
ymm6
%
ymm14
%
ymm6
.
byte
197
252
92
198
/
/
vsubps
%
ymm6
%
ymm0
%
ymm0
.
byte
197
252
89
197
/
/
vmulps
%
ymm5
%
ymm0
%
ymm0
.
byte
196
227
125
8
232
1
/
/
vroundps
0x1
%
ymm0
%
ymm5
.
byte
197
252
92
237
/
/
vsubps
%
ymm5
%
ymm0
%
ymm5
.
byte
196
193
124
88
199
/
/
vaddps
%
ymm15
%
ymm0
%
ymm0
.
byte
196
193
84
89
243
/
/
vmulps
%
ymm11
%
ymm5
%
ymm6
.
byte
197
252
92
198
/
/
vsubps
%
ymm6
%
ymm0
%
ymm0
.
byte
197
172
92
221
/
/
vsubps
%
ymm5
%
ymm10
%
ymm3
.
byte
197
180
94
211
/
/
vdivps
%
ymm3
%
ymm9
%
ymm2
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
252
89
199
/
/
vmulps
%
ymm7
%
ymm0
%
ymm0
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
236
194
76
36
160
0
/
/
vcmpeqps
-
0x60
(
%
rsp
)
%
ymm2
%
ymm1
.
byte
197
253
91
192
/
/
vcvtps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
74
242
16
/
/
vblendvps
%
ymm1
%
ymm2
%
ymm0
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
68
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
16
76
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm1
.
byte
197
252
16
148
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm2
.
byte
197
252
16
156
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm3
.
byte
197
252
16
100
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm5
.
byte
197
252
16
188
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm7
.
byte
72
129
196
248
0
0
0
/
/
add
0xf8
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lab_to_xyz_avx
.
globl
_sk_lab_to_xyz_avx
FUNCTION
(
_sk_lab_to_xyz_avx
)
_sk_lab_to_xyz_avx
:
.
byte
196
98
125
24
5
71
125
2
0
/
/
vbroadcastss
0x27d47
(
%
rip
)
%
ymm8
#
3c5c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x374
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
161
124
2
0
/
/
vbroadcastss
0x27ca1
(
%
rip
)
%
ymm8
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
98
125
24
13
47
125
2
0
/
/
vbroadcastss
0x27d2f
(
%
rip
)
%
ymm9
#
3c5c4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x378
>
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
108
88
209
/
/
vaddps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
27
125
2
0
/
/
vbroadcastss
0x27d1b
(
%
rip
)
%
ymm8
#
3c5c8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x37c
>
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
17
125
2
0
/
/
vbroadcastss
0x27d11
(
%
rip
)
%
ymm8
#
3c5cc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x380
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
7
125
2
0
/
/
vbroadcastss
0x27d07
(
%
rip
)
%
ymm8
#
3c5d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x384
>
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
252
88
201
/
/
vaddps
%
ymm1
%
ymm0
%
ymm1
.
byte
196
98
125
24
5
249
124
2
0
/
/
vbroadcastss
0x27cf9
(
%
rip
)
%
ymm8
#
3c5d4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x388
>
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
252
92
210
/
/
vsubps
%
ymm2
%
ymm0
%
ymm2
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
65
116
89
192
/
/
vmulps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
98
125
24
13
226
124
2
0
/
/
vbroadcastss
0x27ce2
(
%
rip
)
%
ymm9
#
3c5d8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x38c
>
.
byte
196
65
52
194
208
1
/
/
vcmpltps
%
ymm8
%
ymm9
%
ymm10
.
byte
196
98
125
24
29
215
124
2
0
/
/
vbroadcastss
0x27cd7
(
%
rip
)
%
ymm11
#
3c5dc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x390
>
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
196
98
125
24
37
205
124
2
0
/
/
vbroadcastss
0x27ccd
(
%
rip
)
%
ymm12
#
3c5e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x394
>
.
byte
196
193
116
89
204
/
/
vmulps
%
ymm12
%
ymm1
%
ymm1
.
byte
196
67
117
74
192
160
/
/
vblendvps
%
ymm10
%
ymm8
%
ymm1
%
ymm8
.
byte
197
252
89
200
/
/
vmulps
%
ymm0
%
ymm0
%
ymm1
.
byte
197
252
89
201
/
/
vmulps
%
ymm1
%
ymm0
%
ymm1
.
byte
197
52
194
209
1
/
/
vcmpltps
%
ymm1
%
ymm9
%
ymm10
.
byte
196
193
124
88
195
/
/
vaddps
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
124
89
196
/
/
vmulps
%
ymm12
%
ymm0
%
ymm0
.
byte
196
227
125
74
201
160
/
/
vblendvps
%
ymm10
%
ymm1
%
ymm0
%
ymm1
.
byte
197
236
89
194
/
/
vmulps
%
ymm2
%
ymm2
%
ymm0
.
byte
197
236
89
192
/
/
vmulps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
52
194
200
1
/
/
vcmpltps
%
ymm0
%
ymm9
%
ymm9
.
byte
196
193
108
88
211
/
/
vaddps
%
ymm11
%
ymm2
%
ymm2
.
byte
196
193
108
89
212
/
/
vmulps
%
ymm12
%
ymm2
%
ymm2
.
byte
196
227
109
74
208
144
/
/
vblendvps
%
ymm9
%
ymm0
%
ymm2
%
ymm2
.
byte
196
226
125
24
5
131
124
2
0
/
/
vbroadcastss
0x27c83
(
%
rip
)
%
ymm0
#
3c5e4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x398
>
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
122
124
2
0
/
/
vbroadcastss
0x27c7a
(
%
rip
)
%
ymm8
#
3c5e8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x39c
>
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_avx
.
globl
_sk_load_a8_avx
FUNCTION
(
_sk_load_a8_avx
)
_sk_load_a8_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
68
/
/
jne
149cd
<
_sk_load_a8_avx
+
0x56
>
.
byte
196
194
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
197
249
219
5
25
133
2
0
/
/
vpand
0x28519
(
%
rip
)
%
xmm0
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
187
123
2
0
/
/
vbroadcastss
0x27bbb
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
217
/
/
vmulps
%
ymm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
180
/
/
ja
1498f
<
_sk_load_a8_avx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
119
0
0
0
/
/
lea
0x77
(
%
rip
)
%
r9
#
14a5c
<
_sk_load_a8_avx
+
0xe5
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
235
150
/
/
jmp
1498f
<
_sk_load_a8_avx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
121
14
193
3
/
/
vpblendw
0x3
%
xmm1
%
xmm0
%
xmm0
.
byte
233
110
255
255
255
/
/
jmpq
1498f
<
_sk_load_a8_avx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
113
14
192
240
/
/
vpblendw
0xf0
%
xmm0
%
xmm1
%
xmm0
.
byte
233
51
255
255
255
/
/
jmpq
1498f
<
_sk_load_a8_avx
+
0x18
>
.
byte
146
/
/
xchg
%
eax
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
172
255
255
255
157
255
/
/
ljmp
*
-
0x620001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
234
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_a8_dst_avx
.
globl
_sk_load_a8_dst_avx
FUNCTION
(
_sk_load_a8_dst_avx
)
_sk_load_a8_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
68
/
/
jne
14ace
<
_sk_load_a8_dst_avx
+
0x56
>
.
byte
196
194
121
48
36
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
197
217
219
37
24
132
2
0
/
/
vpand
0x28418
(
%
rip
)
%
xmm4
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
121
51
236
/
/
vpmovzxwd
%
xmm4
%
xmm5
.
byte
197
249
112
228
78
/
/
vpshufd
0x4e
%
xmm4
%
xmm4
.
byte
196
226
121
51
228
/
/
vpmovzxwd
%
xmm4
%
xmm4
.
byte
196
227
85
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
186
122
2
0
/
/
vbroadcastss
0x27aba
(
%
rip
)
%
ymm5
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
220
89
253
/
/
vmulps
%
ymm5
%
ymm4
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
180
/
/
ja
14a90
<
_sk_load_a8_dst_avx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
122
0
0
0
/
/
lea
0x7a
(
%
rip
)
%
r9
#
14b60
<
_sk_load_a8_dst_avx
+
0xe8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
235
150
/
/
jmp
14a90
<
_sk_load_a8_dst_avx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
2
/
/
vpinsrw
0x2
%
eax
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
232
/
/
vmovd
%
eax
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
89
14
229
3
/
/
vpblendw
0x3
%
xmm5
%
xmm4
%
xmm4
.
byte
233
110
255
255
255
/
/
jmpq
14a90
<
_sk_load_a8_dst_avx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
6
/
/
vpinsrw
0x6
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
5
/
/
vpinsrw
0x5
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
4
/
/
vpinsrw
0x4
%
eax
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
81
14
228
240
/
/
vpblendw
0xf0
%
xmm4
%
xmm5
%
xmm4
.
byte
233
51
255
255
255
/
/
jmpq
14a90
<
_sk_load_a8_dst_avx
+
0x18
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
143
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
169
255
255
255
154
/
/
ljmp
*
-
0x65000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_a8_avx
.
globl
_sk_gather_a8_avx
FUNCTION
(
_sk_gather_a8_avx
)
_sk_gather_a8_avx
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
57
118
192
/
/
vpcmpeqd
%
xmm8
%
xmm8
%
xmm8
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
93
210
/
/
vminps
%
ymm2
%
ymm0
%
ymm2
.
byte
196
226
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm0
.
byte
196
227
125
25
195
1
/
/
vextractf128
0x1
%
ymm0
%
xmm3
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
121
254
192
/
/
vpaddd
%
xmm8
%
xmm0
%
xmm0
.
byte
196
227
125
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm0
%
ymm0
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
244
93
192
/
/
vminps
%
ymm0
%
ymm1
%
ymm0
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
200
/
/
vcvttps2dq
%
ymm0
%
ymm1
.
byte
197
249
110
64
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm0
.
byte
197
249
112
216
0
/
/
vpshufd
0x0
%
xmm0
%
xmm3
.
byte
196
226
97
64
193
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm0
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
97
64
201
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm1
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
197
249
254
194
/
/
vpaddd
%
xmm2
%
xmm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
60
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
r15d
.
byte
67
15
182
44
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
ebp
.
byte
197
249
110
197
/
/
vmovd
%
ebp
%
xmm0
.
byte
196
195
121
32
199
1
/
/
vpinsrb
0x1
%
r15d
%
xmm0
%
xmm0
.
byte
67
15
182
44
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
ebp
.
byte
196
227
121
32
197
2
/
/
vpinsrb
0x2
%
ebp
%
xmm0
%
xmm0
.
byte
65
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
ebx
.
byte
196
227
121
32
195
3
/
/
vpinsrb
0x3
%
ebx
%
xmm0
%
xmm0
.
byte
196
226
121
49
192
/
/
vpmovzxbd
%
xmm0
%
xmm0
.
byte
67
15
182
44
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
ebp
.
byte
67
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
ebx
.
byte
197
249
110
203
/
/
vmovd
%
ebx
%
xmm1
.
byte
196
227
113
32
205
1
/
/
vpinsrb
0x1
%
ebp
%
xmm1
%
xmm1
.
byte
67
15
182
44
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
ebp
.
byte
196
227
113
32
205
2
/
/
vpinsrb
0x2
%
ebp
%
xmm1
%
xmm1
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
3
/
/
vpinsrb
0x3
%
eax
%
xmm1
%
xmm1
.
byte
196
226
121
49
201
/
/
vpmovzxbd
%
xmm1
%
xmm1
.
byte
196
227
125
24
193
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
198
120
2
0
/
/
vbroadcastss
0x278c6
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
217
/
/
vmulps
%
ymm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_a8_avx
.
globl
_sk_store_a8_avx
FUNCTION
(
_sk_store_a8_avx
)
_sk_store_a8_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
19
120
2
0
/
/
vbroadcastss
0x27813
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
60
93
193
/
/
vminps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
49
120
2
0
/
/
vbroadcastss
0x27831
(
%
rip
)
%
ymm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
197
57
103
192
/
/
vpackuswb
%
xmm0
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
14d1f
<
_sk_store_a8_avx
+
0x55
>
.
byte
196
65
121
214
4
16
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
14d1b
<
_sk_store_a8_avx
+
0x51
>
.
byte
196
66
121
48
192
/
/
vpmovzxbw
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
88
0
0
0
/
/
lea
0x58
(
%
rip
)
%
r9
#
14d90
<
_sk_store_a8_avx
+
0xc6
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
20
4
16
0
/
/
vpextrb
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
209
/
/
jmp
14d1b
<
_sk_store_a8_avx
+
0x51
>
.
byte
196
67
121
20
68
16
2
4
/
/
vpextrb
0x4
%
xmm8
0x2
(
%
r8
%
rdx
1
)
.
byte
196
98
57
0
5
101
129
2
0
/
/
vpshufb
0x28165
(
%
rip
)
%
xmm8
%
xmm8
#
3cec0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc74
>
.
byte
196
67
121
21
4
16
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
183
/
/
jmp
14d1b
<
_sk_store_a8_avx
+
0x51
>
.
byte
196
67
121
20
68
16
6
12
/
/
vpextrb
0xc
%
xmm8
0x6
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
5
10
/
/
vpextrb
0xa
%
xmm8
0x5
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
4
8
/
/
vpextrb
0x8
%
xmm8
0x4
(
%
r8
%
rdx
1
)
.
byte
196
98
57
0
5
75
129
2
0
/
/
vpshufb
0x2814b
(
%
rip
)
%
xmm8
%
xmm8
#
3ced0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc84
>
.
byte
196
65
121
126
4
16
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
142
/
/
jmp
14d1b
<
_sk_store_a8_avx
+
0x51
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
177
255
/
/
mov
0xff
%
cl
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
186
255
255
255
236
/
/
mov
0xecffffff
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
228
/
/
jmpq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_avx
.
globl
_sk_load_g8_avx
FUNCTION
(
_sk_load_g8_avx
)
_sk_load_g8_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
73
/
/
jne
14e07
<
_sk_load_g8_avx
+
0x5b
>
.
byte
196
194
121
48
4
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
197
249
219
5
228
128
2
0
/
/
vpand
0x280e4
(
%
rip
)
%
xmm0
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
134
119
2
0
/
/
vbroadcastss
0x27786
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
255
118
2
0
/
/
vbroadcastss
0x276ff
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
175
/
/
ja
14dc4
<
_sk_load_g8_avx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
121
0
0
0
/
/
lea
0x79
(
%
rip
)
%
r9
#
14e98
<
_sk_load_g8_avx
+
0xec
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
235
145
/
/
jmp
14dc4
<
_sk_load_g8_avx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
2
/
/
vpinsrw
0x2
%
eax
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
121
14
193
3
/
/
vpblendw
0x3
%
xmm1
%
xmm0
%
xmm0
.
byte
233
105
255
255
255
/
/
jmpq
14dc4
<
_sk_load_g8_avx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm0
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
196
192
4
/
/
vpinsrw
0x4
%
eax
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
226
121
48
201
/
/
vpmovzxbw
%
xmm1
%
xmm1
.
byte
196
227
113
14
192
240
/
/
vpblendw
0xf0
%
xmm0
%
xmm1
%
xmm0
.
byte
233
46
255
255
255
/
/
jmpq
14dc4
<
_sk_load_g8_avx
+
0x18
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
144
/
/
nop
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
170
255
255
255
155
/
/
ljmp
*
-
0x64000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
221
/
/
callq
ffffffffde014ea8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffddfd8c5c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_dst_avx
.
globl
_sk_load_g8_dst_avx
FUNCTION
(
_sk_load_g8_dst_avx
)
_sk_load_g8_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
73
/
/
jne
14f0f
<
_sk_load_g8_dst_avx
+
0x5b
>
.
byte
196
194
121
48
36
16
/
/
vpmovzxbw
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
197
217
219
37
220
127
2
0
/
/
vpand
0x27fdc
(
%
rip
)
%
xmm4
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
196
226
121
51
236
/
/
vpmovzxwd
%
xmm4
%
xmm5
.
byte
197
249
112
228
78
/
/
vpshufd
0x4e
%
xmm4
%
xmm4
.
byte
196
226
121
51
228
/
/
vpmovzxwd
%
xmm4
%
xmm4
.
byte
196
227
85
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
126
118
2
0
/
/
vbroadcastss
0x2767e
(
%
rip
)
%
ymm5
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
220
89
229
/
/
vmulps
%
ymm5
%
ymm4
%
ymm4
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
247
117
2
0
/
/
vbroadcastss
0x275f7
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
236
/
/
vmovaps
%
ymm4
%
ymm5
.
byte
197
252
40
244
/
/
vmovaps
%
ymm4
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
175
/
/
ja
14ecc
<
_sk_load_g8_dst_avx
+
0x18
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
121
0
0
0
/
/
lea
0x79
(
%
rip
)
%
r9
#
14fa0
<
_sk_load_g8_dst_avx
+
0xec
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
235
145
/
/
jmp
14ecc
<
_sk_load_g8_dst_avx
+
0x18
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
2
/
/
vpinsrw
0x2
%
eax
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
232
/
/
vmovd
%
eax
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
89
14
229
3
/
/
vpblendw
0x3
%
xmm5
%
xmm4
%
xmm4
.
byte
233
105
255
255
255
/
/
jmpq
14ecc
<
_sk_load_g8_dst_avx
+
0x18
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
197
217
196
224
6
/
/
vpinsrw
0x6
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
5
/
/
vpinsrw
0x5
%
eax
%
xmm4
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
217
196
224
4
/
/
vpinsrw
0x4
%
eax
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
226
121
48
237
/
/
vpmovzxbw
%
xmm5
%
xmm5
.
byte
196
227
81
14
228
240
/
/
vpblendw
0xf0
%
xmm4
%
xmm5
%
xmm4
.
byte
233
46
255
255
255
/
/
jmpq
14ecc
<
_sk_load_g8_dst_avx
+
0x18
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
144
/
/
nop
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
170
255
255
255
155
/
/
ljmp
*
-
0x64000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
221
/
/
callq
ffffffffde014fb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffddfd8d64
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_g8_avx
.
globl
_sk_gather_g8_avx
FUNCTION
(
_sk_gather_g8_avx
)
_sk_gather_g8_avx
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
57
118
192
/
/
vpcmpeqd
%
xmm8
%
xmm8
%
xmm8
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
93
210
/
/
vminps
%
ymm2
%
ymm0
%
ymm2
.
byte
196
226
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm0
.
byte
196
227
125
25
195
1
/
/
vextractf128
0x1
%
ymm0
%
xmm3
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
121
254
192
/
/
vpaddd
%
xmm8
%
xmm0
%
xmm0
.
byte
196
227
125
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm0
%
ymm0
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
244
93
192
/
/
vminps
%
ymm0
%
ymm1
%
ymm0
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
200
/
/
vcvttps2dq
%
ymm0
%
ymm1
.
byte
197
249
110
64
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm0
.
byte
197
249
112
216
0
/
/
vpshufd
0x0
%
xmm0
%
xmm3
.
byte
196
226
97
64
193
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm0
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
97
64
201
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm1
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
197
249
254
194
/
/
vpaddd
%
xmm2
%
xmm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
60
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
r15d
.
byte
67
15
182
44
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
ebp
.
byte
197
249
110
197
/
/
vmovd
%
ebp
%
xmm0
.
byte
196
195
121
32
199
1
/
/
vpinsrb
0x1
%
r15d
%
xmm0
%
xmm0
.
byte
67
15
182
44
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
ebp
.
byte
196
227
121
32
197
2
/
/
vpinsrb
0x2
%
ebp
%
xmm0
%
xmm0
.
byte
65
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
ebx
.
byte
196
227
121
32
195
3
/
/
vpinsrb
0x3
%
ebx
%
xmm0
%
xmm0
.
byte
196
226
121
49
192
/
/
vpmovzxbd
%
xmm0
%
xmm0
.
byte
67
15
182
44
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
ebp
.
byte
67
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
ebx
.
byte
197
249
110
203
/
/
vmovd
%
ebx
%
xmm1
.
byte
196
227
113
32
205
1
/
/
vpinsrb
0x1
%
ebp
%
xmm1
%
xmm1
.
byte
67
15
182
44
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
ebp
.
byte
196
227
113
32
205
2
/
/
vpinsrb
0x2
%
ebp
%
xmm1
%
xmm1
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
3
/
/
vpinsrb
0x3
%
eax
%
xmm1
%
xmm1
.
byte
196
226
121
49
201
/
/
vpmovzxbd
%
xmm1
%
xmm1
.
byte
196
227
125
24
193
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
134
116
2
0
/
/
vbroadcastss
0x27486
(
%
rip
)
%
ymm1
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
255
115
2
0
/
/
vbroadcastss
0x273ff
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_avx
.
globl
_sk_load_565_avx
FUNCTION
(
_sk_load_565_avx
)
_sk_load_565_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
130
0
0
0
/
/
jne
151aa
<
_sk_load_565_avx
+
0x9b
>
.
byte
196
193
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
208
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm2
.
byte
196
226
125
24
5
44
116
2
0
/
/
vbroadcastss
0x2742c
(
%
rip
)
%
ymm0
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
236
84
192
/
/
vandps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
31
116
2
0
/
/
vbroadcastss
0x2741f
(
%
rip
)
%
ymm1
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
22
116
2
0
/
/
vbroadcastss
0x27416
(
%
rip
)
%
ymm1
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
236
84
201
/
/
vandps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
29
9
116
2
0
/
/
vbroadcastss
0x27409
(
%
rip
)
%
ymm3
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
226
125
24
29
0
116
2
0
/
/
vbroadcastss
0x27400
(
%
rip
)
%
ymm3
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
236
84
211
/
/
vandps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
226
125
24
29
243
115
2
0
/
/
vbroadcastss
0x273f3
(
%
rip
)
%
ymm3
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
84
115
2
0
/
/
vbroadcastss
0x27354
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
114
255
255
255
/
/
ja
1512e
<
_sk_load_565_avx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
98
0
0
0
/
/
lea
0x62
(
%
rip
)
%
r9
#
15228
<
_sk_load_565_avx
+
0x119
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
81
255
255
255
/
/
jmpq
1512e
<
_sk_load_565_avx
+
0x1f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
121
14
193
3
/
/
vpblendw
0x3
%
xmm1
%
xmm0
%
xmm0
.
byte
233
52
255
255
255
/
/
jmpq
1512e
<
_sk_load_565_avx
+
0x1f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
113
14
192
240
/
/
vpblendw
0xf0
%
xmm0
%
xmm1
%
xmm0
.
byte
233
7
255
255
255
/
/
jmpq
1512e
<
_sk_load_565_avx
+
0x1f
>
.
byte
144
/
/
nop
.
byte
167
/
/
cmpsl
%
es
:
(
%
rdi
)
%
ds
:
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
181
255
255
255
238
/
/
pushq
-
0x11000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
230
/
/
jmpq
*
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
255
/
/
fdivrp
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_565_dst_avx
.
globl
_sk_load_565_dst_avx
FUNCTION
(
_sk_load_565_dst_avx
)
_sk_load_565_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
130
0
0
0
/
/
jne
152df
<
_sk_load_565_dst_avx
+
0x9b
>
.
byte
196
193
122
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
196
226
121
51
236
/
/
vpmovzxwd
%
xmm4
%
xmm5
.
byte
197
249
112
228
78
/
/
vpshufd
0x4e
%
xmm4
%
xmm4
.
byte
196
226
121
51
228
/
/
vpmovzxwd
%
xmm4
%
xmm4
.
byte
196
227
85
24
244
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm6
.
byte
196
226
125
24
37
247
114
2
0
/
/
vbroadcastss
0x272f7
(
%
rip
)
%
ymm4
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
204
84
228
/
/
vandps
%
ymm4
%
ymm6
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
234
114
2
0
/
/
vbroadcastss
0x272ea
(
%
rip
)
%
ymm5
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
197
220
89
229
/
/
vmulps
%
ymm5
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
225
114
2
0
/
/
vbroadcastss
0x272e1
(
%
rip
)
%
ymm5
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
204
84
237
/
/
vandps
%
ymm5
%
ymm6
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
226
125
24
61
212
114
2
0
/
/
vbroadcastss
0x272d4
(
%
rip
)
%
ymm7
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
197
212
89
239
/
/
vmulps
%
ymm7
%
ymm5
%
ymm5
.
byte
196
226
125
24
61
203
114
2
0
/
/
vbroadcastss
0x272cb
(
%
rip
)
%
ymm7
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
204
84
247
/
/
vandps
%
ymm7
%
ymm6
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
226
125
24
61
190
114
2
0
/
/
vbroadcastss
0x272be
(
%
rip
)
%
ymm7
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
197
204
89
247
/
/
vmulps
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
61
31
114
2
0
/
/
vbroadcastss
0x2721f
(
%
rip
)
%
ymm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
114
255
255
255
/
/
ja
15263
<
_sk_load_565_dst_avx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
97
0
0
0
/
/
lea
0x61
(
%
rip
)
%
r9
#
1535c
<
_sk_load_565_dst_avx
+
0x118
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
81
255
255
255
/
/
jmpq
15263
<
_sk_load_565_dst_avx
+
0x1f
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
89
14
229
3
/
/
vpblendw
0x3
%
xmm5
%
xmm4
%
xmm4
.
byte
233
52
255
255
255
/
/
jmpq
15263
<
_sk_load_565_dst_avx
+
0x1f
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
81
14
228
240
/
/
vpblendw
0xf0
%
xmm4
%
xmm5
%
xmm4
.
byte
233
7
255
255
255
/
/
jmpq
15263
<
_sk_load_565_dst_avx
+
0x1f
>
.
byte
168
255
/
/
test
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
182
255
255
255
239
/
/
pushq
-
0x10000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_565_avx
.
globl
_sk_gather_565_avx
FUNCTION
(
_sk_gather_565_avx
)
_sk_gather_565_avx
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
57
118
192
/
/
vpcmpeqd
%
xmm8
%
xmm8
%
xmm8
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
93
210
/
/
vminps
%
ymm2
%
ymm0
%
ymm2
.
byte
196
226
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm0
.
byte
196
227
125
25
195
1
/
/
vextractf128
0x1
%
ymm0
%
xmm3
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
121
254
192
/
/
vpaddd
%
xmm8
%
xmm0
%
xmm0
.
byte
196
227
125
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm0
%
ymm0
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
244
93
192
/
/
vminps
%
ymm0
%
ymm1
%
ymm0
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
200
/
/
vcvttps2dq
%
ymm0
%
ymm1
.
byte
197
249
110
64
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm0
.
byte
197
249
112
216
0
/
/
vpshufd
0x0
%
xmm0
%
xmm3
.
byte
196
226
97
64
193
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm0
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
97
64
201
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm1
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
197
249
254
194
/
/
vpaddd
%
xmm2
%
xmm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
60
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
r15d
.
byte
67
15
183
44
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
ebp
.
byte
197
249
110
197
/
/
vmovd
%
ebp
%
xmm0
.
byte
196
193
121
196
199
1
/
/
vpinsrw
0x1
%
r15d
%
xmm0
%
xmm0
.
byte
67
15
183
44
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
ebp
.
byte
197
249
196
197
2
/
/
vpinsrw
0x2
%
ebp
%
xmm0
%
xmm0
.
byte
65
15
183
28
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
ebx
.
byte
197
249
196
195
3
/
/
vpinsrw
0x3
%
ebx
%
xmm0
%
xmm0
.
byte
67
15
183
44
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
ebp
.
byte
197
249
196
197
4
/
/
vpinsrw
0x4
%
ebp
%
xmm0
%
xmm0
.
byte
67
15
183
44
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
ebp
.
byte
197
249
196
197
5
/
/
vpinsrw
0x5
%
ebp
%
xmm0
%
xmm0
.
byte
67
15
183
44
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
ebp
.
byte
197
249
196
197
6
/
/
vpinsrw
0x6
%
ebp
%
xmm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
208
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm2
.
byte
196
226
125
24
5
209
112
2
0
/
/
vbroadcastss
0x270d1
(
%
rip
)
%
ymm0
#
3c578
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x32c
>
.
byte
197
236
84
192
/
/
vandps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
196
112
2
0
/
/
vbroadcastss
0x270c4
(
%
rip
)
%
ymm1
#
3c57c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x330
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
187
112
2
0
/
/
vbroadcastss
0x270bb
(
%
rip
)
%
ymm1
#
3c580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x334
>
.
byte
197
236
84
201
/
/
vandps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
29
174
112
2
0
/
/
vbroadcastss
0x270ae
(
%
rip
)
%
ymm3
#
3c584
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x338
>
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
226
125
24
29
165
112
2
0
/
/
vbroadcastss
0x270a5
(
%
rip
)
%
ymm3
#
3c588
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x33c
>
.
byte
197
236
84
211
/
/
vandps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
226
125
24
29
152
112
2
0
/
/
vbroadcastss
0x27098
(
%
rip
)
%
ymm3
#
3c58c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x340
>
.
byte
197
236
89
211
/
/
vmulps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
249
111
2
0
/
/
vbroadcastss
0x26ff9
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_565_avx
.
globl
_sk_store_565_avx
FUNCTION
(
_sk_store_565_avx
)
_sk_store_565_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
205
111
2
0
/
/
vbroadcastss
0x26fcd
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
175
112
2
0
/
/
vbroadcastss
0x270af
(
%
rip
)
%
ymm11
#
3c5ec
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a0
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
193
25
114
241
11
/
/
vpslld
0xb
%
xmm9
%
xmm12
.
byte
196
67
125
25
201
1
/
/
vextractf128
0x1
%
ymm9
%
xmm9
.
byte
196
193
49
114
241
11
/
/
vpslld
0xb
%
xmm9
%
xmm9
.
byte
196
67
29
24
201
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm12
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
98
125
24
45
127
112
2
0
/
/
vbroadcastss
0x2707f
(
%
rip
)
%
ymm13
#
3c5f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a4
>
.
byte
196
65
28
89
229
/
/
vmulps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
5
/
/
vpslld
0x5
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
5
/
/
vpslld
0x5
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
196
65
29
86
201
/
/
vorpd
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
194
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
65
53
86
192
/
/
vorpd
%
ymm8
%
ymm9
%
ymm8
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
155ca
<
_sk_store_565_avx
+
0xbd
>
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
155c6
<
_sk_store_565_avx
+
0xb9
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
66
0
0
0
/
/
lea
0x42
(
%
rip
)
%
r9
#
15620
<
_sk_store_565_avx
+
0x113
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
214
/
/
jmp
155c6
<
_sk_store_565_avx
+
0xb9
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
65
121
126
4
80
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
198
/
/
jmp
155c6
<
_sk_store_565_avx
+
0xb9
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
65
121
214
4
80
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
166
/
/
jmp
155c6
<
_sk_store_565_avx
+
0xb9
>
.
byte
199
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
248
/
/
clc
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
240
/
/
push
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
224
/
/
callq
ffffffffe1015638
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffe0fd93ec
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_avx
.
globl
_sk_load_4444_avx
FUNCTION
(
_sk_load_4444_avx
)
_sk_load_4444_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
154
0
0
0
/
/
jne
156ef
<
_sk_load_4444_avx
+
0xb3
>
.
byte
196
193
122
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
216
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm3
.
byte
196
226
125
24
5
123
111
2
0
/
/
vbroadcastss
0x26f7b
(
%
rip
)
%
ymm0
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
228
84
192
/
/
vandps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
110
111
2
0
/
/
vbroadcastss
0x26f6e
(
%
rip
)
%
ymm1
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
101
111
2
0
/
/
vbroadcastss
0x26f65
(
%
rip
)
%
ymm1
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
228
84
201
/
/
vandps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
88
111
2
0
/
/
vbroadcastss
0x26f58
(
%
rip
)
%
ymm2
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
197
244
89
202
/
/
vmulps
%
ymm2
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
79
111
2
0
/
/
vbroadcastss
0x26f4f
(
%
rip
)
%
ymm2
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
228
84
210
/
/
vandps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
66
111
2
0
/
/
vbroadcastss
0x26f42
(
%
rip
)
%
ymm8
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
56
111
2
0
/
/
vbroadcastss
0x26f38
(
%
rip
)
%
ymm8
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
196
193
100
84
216
/
/
vandps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
42
111
2
0
/
/
vbroadcastss
0x26f2a
(
%
rip
)
%
ymm8
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
90
255
255
255
/
/
ja
1565b
<
_sk_load_4444_avx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
97
0
0
0
/
/
lea
0x61
(
%
rip
)
%
r9
#
1576c
<
_sk_load_4444_avx
+
0x130
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
57
255
255
255
/
/
jmpq
1565b
<
_sk_load_4444_avx
+
0x1f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
121
14
193
3
/
/
vpblendw
0x3
%
xmm1
%
xmm0
%
xmm0
.
byte
233
28
255
255
255
/
/
jmpq
1565b
<
_sk_load_4444_avx
+
0x1f
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
121
196
68
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
113
14
192
240
/
/
vpblendw
0xf0
%
xmm0
%
xmm1
%
xmm0
.
byte
233
239
254
255
255
/
/
jmpq
1565b
<
_sk_load_4444_avx
+
0x1f
>
.
byte
168
255
/
/
test
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
182
255
255
255
239
/
/
pushq
-
0x10000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_dst_avx
.
globl
_sk_load_4444_dst_avx
FUNCTION
(
_sk_load_4444_dst_avx
)
_sk_load_4444_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
154
0
0
0
/
/
jne
1583b
<
_sk_load_4444_dst_avx
+
0xb3
>
.
byte
196
193
122
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
196
226
121
51
236
/
/
vpmovzxwd
%
xmm4
%
xmm5
.
byte
197
249
112
228
78
/
/
vpshufd
0x4e
%
xmm4
%
xmm4
.
byte
196
226
121
51
228
/
/
vpmovzxwd
%
xmm4
%
xmm4
.
byte
196
227
85
24
252
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm7
.
byte
196
226
125
24
37
47
110
2
0
/
/
vbroadcastss
0x26e2f
(
%
rip
)
%
ymm4
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
196
84
228
/
/
vandps
%
ymm4
%
ymm7
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
34
110
2
0
/
/
vbroadcastss
0x26e22
(
%
rip
)
%
ymm5
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
197
220
89
229
/
/
vmulps
%
ymm5
%
ymm4
%
ymm4
.
byte
196
226
125
24
45
25
110
2
0
/
/
vbroadcastss
0x26e19
(
%
rip
)
%
ymm5
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
196
84
237
/
/
vandps
%
ymm5
%
ymm7
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
226
125
24
53
12
110
2
0
/
/
vbroadcastss
0x26e0c
(
%
rip
)
%
ymm6
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
197
212
89
238
/
/
vmulps
%
ymm6
%
ymm5
%
ymm5
.
byte
196
226
125
24
53
3
110
2
0
/
/
vbroadcastss
0x26e03
(
%
rip
)
%
ymm6
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
196
84
246
/
/
vandps
%
ymm6
%
ymm7
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
98
125
24
5
246
109
2
0
/
/
vbroadcastss
0x26df6
(
%
rip
)
%
ymm8
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
196
98
125
24
5
236
109
2
0
/
/
vbroadcastss
0x26dec
(
%
rip
)
%
ymm8
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
196
193
68
84
248
/
/
vandps
%
ymm8
%
ymm7
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
5
222
109
2
0
/
/
vbroadcastss
0x26dde
(
%
rip
)
%
ymm8
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
90
255
255
255
/
/
ja
157a7
<
_sk_load_4444_dst_avx
+
0x1f
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
97
0
0
0
/
/
lea
0x61
(
%
rip
)
%
r9
#
158b8
<
_sk_load_4444_dst_avx
+
0x130
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
57
255
255
255
/
/
jmpq
157a7
<
_sk_load_4444_dst_avx
+
0x1f
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
89
14
229
3
/
/
vpblendw
0x3
%
xmm5
%
xmm4
%
xmm4
.
byte
233
28
255
255
255
/
/
jmpq
157a7
<
_sk_load_4444_dst_avx
+
0x1f
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
89
196
100
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
81
14
228
240
/
/
vpblendw
0xf0
%
xmm4
%
xmm5
%
xmm4
.
byte
233
239
254
255
255
/
/
jmpq
157a7
<
_sk_load_4444_dst_avx
+
0x1f
>
.
byte
168
255
/
/
test
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
182
255
255
255
239
/
/
pushq
-
0x10000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_4444_avx
.
globl
_sk_gather_4444_avx
FUNCTION
(
_sk_gather_4444_avx
)
_sk_gather_4444_avx
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
57
118
192
/
/
vpcmpeqd
%
xmm8
%
xmm8
%
xmm8
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
93
210
/
/
vminps
%
ymm2
%
ymm0
%
ymm2
.
byte
196
226
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm0
.
byte
196
227
125
25
195
1
/
/
vextractf128
0x1
%
ymm0
%
xmm3
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
121
254
192
/
/
vpaddd
%
xmm8
%
xmm0
%
xmm0
.
byte
196
227
125
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm0
%
ymm0
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
244
93
192
/
/
vminps
%
ymm0
%
ymm1
%
ymm0
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
200
/
/
vcvttps2dq
%
ymm0
%
ymm1
.
byte
197
249
110
64
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm0
.
byte
197
249
112
216
0
/
/
vpshufd
0x0
%
xmm0
%
xmm3
.
byte
196
226
97
64
193
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm0
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
97
64
201
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm1
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
197
249
254
194
/
/
vpaddd
%
xmm2
%
xmm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
60
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
r15d
.
byte
67
15
183
44
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
ebp
.
byte
197
249
110
197
/
/
vmovd
%
ebp
%
xmm0
.
byte
196
193
121
196
199
1
/
/
vpinsrw
0x1
%
r15d
%
xmm0
%
xmm0
.
byte
67
15
183
44
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
ebp
.
byte
197
249
196
197
2
/
/
vpinsrw
0x2
%
ebp
%
xmm0
%
xmm0
.
byte
65
15
183
28
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
ebx
.
byte
197
249
196
195
3
/
/
vpinsrw
0x3
%
ebx
%
xmm0
%
xmm0
.
byte
67
15
183
44
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
ebp
.
byte
197
249
196
197
4
/
/
vpinsrw
0x4
%
ebp
%
xmm0
%
xmm0
.
byte
67
15
183
44
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
ebp
.
byte
197
249
196
197
5
/
/
vpinsrw
0x5
%
ebp
%
xmm0
%
xmm0
.
byte
67
15
183
44
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
ebp
.
byte
197
249
196
197
6
/
/
vpinsrw
0x6
%
ebp
%
xmm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
216
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm3
.
byte
196
226
125
24
5
241
107
2
0
/
/
vbroadcastss
0x26bf1
(
%
rip
)
%
ymm0
#
3c5f4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3a8
>
.
byte
197
228
84
192
/
/
vandps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
228
107
2
0
/
/
vbroadcastss
0x26be4
(
%
rip
)
%
ymm1
#
3c5f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ac
>
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
219
107
2
0
/
/
vbroadcastss
0x26bdb
(
%
rip
)
%
ymm1
#
3c5fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b0
>
.
byte
197
228
84
201
/
/
vandps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
206
107
2
0
/
/
vbroadcastss
0x26bce
(
%
rip
)
%
ymm2
#
3c600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b4
>
.
byte
197
244
89
202
/
/
vmulps
%
ymm2
%
ymm1
%
ymm1
.
byte
196
226
125
24
21
197
107
2
0
/
/
vbroadcastss
0x26bc5
(
%
rip
)
%
ymm2
#
3c604
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3b8
>
.
byte
197
228
84
210
/
/
vandps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
184
107
2
0
/
/
vbroadcastss
0x26bb8
(
%
rip
)
%
ymm8
#
3c608
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3bc
>
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
98
125
24
5
174
107
2
0
/
/
vbroadcastss
0x26bae
(
%
rip
)
%
ymm8
#
3c60c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c0
>
.
byte
196
193
100
84
216
/
/
vandps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
160
107
2
0
/
/
vbroadcastss
0x26ba0
(
%
rip
)
%
ymm8
#
3c610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c4
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_4444_avx
.
globl
_sk_store_4444_avx
FUNCTION
(
_sk_store_4444_avx
)
_sk_store_4444_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
208
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm10
.
byte
196
98
125
24
13
89
106
2
0
/
/
vbroadcastss
0x26a59
(
%
rip
)
%
ymm9
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
44
93
209
/
/
vminps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
98
125
24
29
99
107
2
0
/
/
vbroadcastss
0x26b63
(
%
rip
)
%
ymm11
#
3c614
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3c8
>
.
byte
196
65
44
89
211
/
/
vmulps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
125
91
210
/
/
vcvtps2dq
%
ymm10
%
ymm10
.
byte
196
193
25
114
242
12
/
/
vpslld
0xc
%
xmm10
%
xmm12
.
byte
196
67
125
25
210
1
/
/
vextractf128
0x1
%
ymm10
%
xmm10
.
byte
196
193
41
114
242
12
/
/
vpslld
0xc
%
xmm10
%
xmm10
.
byte
196
67
29
24
210
1
/
/
vinsertf128
0x1
%
xmm10
%
ymm12
%
ymm10
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
225
/
/
vminps
%
ymm9
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
8
/
/
vpslld
0x8
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
8
/
/
vpslld
0x8
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
196
65
29
86
210
/
/
vorpd
%
ymm10
%
ymm12
%
ymm10
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
225
/
/
vminps
%
ymm9
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
4
/
/
vpslld
0x4
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
4
/
/
vpslld
0x4
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
193
/
/
vminps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
65
29
86
192
/
/
vorpd
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
45
86
192
/
/
vorpd
%
ymm8
%
ymm10
%
ymm8
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
66
57
43
193
/
/
vpackusdw
%
xmm9
%
xmm8
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
15b65
<
_sk_store_4444_avx
+
0xe4
>
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
15b61
<
_sk_store_4444_avx
+
0xe0
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
67
0
0
0
/
/
lea
0x43
(
%
rip
)
%
r9
#
15bbc
<
_sk_store_4444_avx
+
0x13b
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
214
/
/
jmp
15b61
<
_sk_store_4444_avx
+
0xe0
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
65
121
126
4
80
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
198
/
/
jmp
15b61
<
_sk_store_4444_avx
+
0xe0
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
65
121
214
4
80
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
166
/
/
jmp
15b61
<
_sk_store_4444_avx
+
0xe0
>
.
byte
144
/
/
nop
.
byte
198
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
215
/
/
callq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
207
/
/
dec
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
247
/
/
push
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
223
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_avx
.
globl
_sk_load_8888_avx
FUNCTION
(
_sk_load_8888_avx
)
_sk_load_8888_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
135
0
0
0
/
/
jne
15c79
<
_sk_load_8888_avx
+
0xa1
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
197
125
40
21
64
111
2
0
/
/
vmovapd
0x26f40
(
%
rip
)
%
ymm10
#
3cb40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x8f4
>
.
byte
196
193
53
84
194
/
/
vandpd
%
ymm10
%
ymm9
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
98
105
2
0
/
/
vbroadcastss
0x26962
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
113
114
209
8
/
/
vpsrld
0x8
%
xmm9
%
xmm1
.
byte
196
99
125
25
203
1
/
/
vextractf128
0x1
%
ymm9
%
xmm3
.
byte
197
233
114
211
8
/
/
vpsrld
0x8
%
xmm3
%
xmm2
.
byte
196
227
117
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
196
193
117
84
202
/
/
vandpd
%
ymm10
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
33
114
209
16
/
/
vpsrld
0x10
%
xmm9
%
xmm11
.
byte
197
233
114
211
16
/
/
vpsrld
0x10
%
xmm3
%
xmm2
.
byte
196
227
37
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm11
%
ymm2
.
byte
196
193
109
84
210
/
/
vandpd
%
ymm10
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
49
114
209
24
/
/
vpsrld
0x18
%
xmm9
%
xmm9
.
byte
197
225
114
211
24
/
/
vpsrld
0x18
%
xmm3
%
xmm3
.
byte
196
227
53
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm9
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
108
255
255
255
/
/
ja
15bf8
<
_sk_load_8888_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
146
0
0
0
/
/
lea
0x92
(
%
rip
)
%
r9
#
15d28
<
_sk_load_8888_avx
+
0x150
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
78
255
255
255
/
/
jmpq
15bf8
<
_sk_load_8888_avx
+
0x20
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
200
4
/
/
vblendps
0x4
%
ymm0
%
ymm1
%
ymm9
.
byte
196
193
123
16
4
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
99
53
13
200
1
/
/
vblendpd
0x1
%
ymm0
%
ymm9
%
ymm9
.
byte
233
39
255
255
255
/
/
jmpq
15bf8
<
_sk_load_8888_avx
+
0x20
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
196
227
125
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
200
64
/
/
vblendps
0x40
%
ymm0
%
ymm1
%
ymm9
.
byte
196
99
125
25
200
1
/
/
vextractf128
0x1
%
ymm9
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
53
24
200
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm9
%
ymm9
.
byte
196
99
125
25
200
1
/
/
vextractf128
0x1
%
ymm9
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
53
24
200
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm9
%
ymm9
.
byte
196
193
121
16
4
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
67
125
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm0
%
ymm9
.
byte
233
210
254
255
255
/
/
jmpq
15bf8
<
_sk_load_8888_avx
+
0x20
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
119
255
/
/
ja
15d29
<
_sk_load_8888_avx
+
0x151
>
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
130
/
/
lcall
*
-
0x7d000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
169
/
/
.
byte
0xa9
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_dst_avx
.
globl
_sk_load_8888_dst_avx
FUNCTION
(
_sk_load_8888_dst_avx
)
_sk_load_8888_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
135
0
0
0
/
/
jne
15de5
<
_sk_load_8888_dst_avx
+
0xa1
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
197
125
40
21
244
109
2
0
/
/
vmovapd
0x26df4
(
%
rip
)
%
ymm10
#
3cb60
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x914
>
.
byte
196
193
53
84
226
/
/
vandpd
%
ymm10
%
ymm9
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
246
103
2
0
/
/
vbroadcastss
0x267f6
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
196
193
81
114
209
8
/
/
vpsrld
0x8
%
xmm9
%
xmm5
.
byte
196
99
125
25
207
1
/
/
vextractf128
0x1
%
ymm9
%
xmm7
.
byte
197
201
114
215
8
/
/
vpsrld
0x8
%
xmm7
%
xmm6
.
byte
196
227
85
24
238
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm5
%
ymm5
.
byte
196
193
85
84
234
/
/
vandpd
%
ymm10
%
ymm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
84
89
232
/
/
vmulps
%
ymm8
%
ymm5
%
ymm5
.
byte
196
193
33
114
209
16
/
/
vpsrld
0x10
%
xmm9
%
xmm11
.
byte
197
201
114
215
16
/
/
vpsrld
0x10
%
xmm7
%
xmm6
.
byte
196
227
37
24
246
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm11
%
ymm6
.
byte
196
193
77
84
242
/
/
vandpd
%
ymm10
%
ymm6
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
196
193
49
114
209
24
/
/
vpsrld
0x18
%
xmm9
%
xmm9
.
byte
197
193
114
215
24
/
/
vpsrld
0x18
%
xmm7
%
xmm7
.
byte
196
227
53
24
255
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm9
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
108
255
255
255
/
/
ja
15d64
<
_sk_load_8888_dst_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
146
0
0
0
/
/
lea
0x92
(
%
rip
)
%
r9
#
15e94
<
_sk_load_8888_dst_avx
+
0x150
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
78
255
255
255
/
/
jmpq
15d64
<
_sk_load_8888_dst_avx
+
0x20
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
4
/
/
vblendps
0x4
%
ymm4
%
ymm5
%
ymm9
.
byte
196
193
123
16
36
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
99
53
13
204
1
/
/
vblendpd
0x1
%
ymm4
%
ymm9
%
ymm9
.
byte
233
39
255
255
255
/
/
jmpq
15d64
<
_sk_load_8888_dst_avx
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
196
227
125
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm0
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
64
/
/
vblendps
0x40
%
ymm4
%
ymm5
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
193
121
16
36
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
67
93
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm4
%
ymm9
.
byte
233
210
254
255
255
/
/
jmpq
15d64
<
_sk_load_8888_dst_avx
+
0x20
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
119
255
/
/
ja
15e95
<
_sk_load_8888_dst_avx
+
0x151
>
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
130
/
/
lcall
*
-
0x7d000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
169
/
/
.
byte
0xa9
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_8888_avx
.
globl
_sk_gather_8888_avx
FUNCTION
(
_sk_gather_8888_avx
)
_sk_gather_8888_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
57
118
192
/
/
vpcmpeqd
%
xmm8
%
xmm8
%
xmm8
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
249
110
80
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm2
.
byte
197
249
112
210
0
/
/
vpshufd
0x0
%
xmm2
%
xmm2
.
byte
196
226
105
64
217
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm3
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
105
64
201
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
25
194
1
/
/
vextractf128
0x1
%
ymm0
%
xmm2
.
byte
197
241
254
202
/
/
vpaddd
%
xmm2
%
xmm1
%
xmm1
.
byte
196
225
249
126
200
/
/
vmovq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
197
225
254
192
/
/
vpaddd
%
xmm0
%
xmm3
%
xmm0
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
249
22
203
1
/
/
vpextrq
0x1
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
195
249
22
199
1
/
/
vpextrq
0x1
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
121
110
4
152
/
/
vmovd
(
%
r8
%
r11
4
)
%
xmm0
.
byte
196
131
121
34
4
144
1
/
/
vpinsrd
0x1
(
%
r8
%
r10
4
)
%
xmm0
%
xmm0
.
byte
196
131
121
34
4
160
2
/
/
vpinsrd
0x2
(
%
r8
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
3
121
34
4
184
3
/
/
vpinsrd
0x3
(
%
r8
%
r15
4
)
%
xmm0
%
xmm8
.
byte
196
129
121
110
4
136
/
/
vmovd
(
%
r8
%
r9
4
)
%
xmm0
.
byte
196
195
121
34
4
128
1
/
/
vpinsrd
0x1
(
%
r8
%
rax
4
)
%
xmm0
%
xmm0
.
byte
196
131
121
34
4
176
2
/
/
vpinsrd
0x2
(
%
r8
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
195
121
34
28
152
3
/
/
vpinsrd
0x3
(
%
r8
%
rbx
4
)
%
xmm0
%
xmm3
.
byte
196
227
61
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm0
.
byte
197
124
40
21
204
107
2
0
/
/
vmovaps
0x26bcc
(
%
rip
)
%
ymm10
#
3cb80
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x934
>
.
byte
196
193
124
84
194
/
/
vandps
%
ymm10
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
174
101
2
0
/
/
vbroadcastss
0x265ae
(
%
rip
)
%
ymm9
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
113
114
208
8
/
/
vpsrld
0x8
%
xmm8
%
xmm1
.
byte
197
233
114
211
8
/
/
vpsrld
0x8
%
xmm3
%
xmm2
.
byte
196
227
117
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
196
193
116
84
202
/
/
vandps
%
ymm10
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
33
114
208
16
/
/
vpsrld
0x10
%
xmm8
%
xmm11
.
byte
197
233
114
211
16
/
/
vpsrld
0x10
%
xmm3
%
xmm2
.
byte
196
227
37
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm11
%
ymm2
.
byte
196
193
108
84
210
/
/
vandps
%
ymm10
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
57
114
208
24
/
/
vpsrld
0x18
%
xmm8
%
xmm8
.
byte
197
225
114
211
24
/
/
vpsrld
0x18
%
xmm3
%
xmm3
.
byte
196
227
61
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
217
/
/
vmulps
%
ymm9
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_8888_avx
.
globl
_sk_store_8888_avx
FUNCTION
(
_sk_store_8888_avx
)
_sk_store_8888_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
171
100
2
0
/
/
vbroadcastss
0x264ab
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
201
100
2
0
/
/
vbroadcastss
0x264c9
(
%
rip
)
%
ymm11
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
8
/
/
vpslld
0x8
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
8
/
/
vpslld
0x8
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
196
65
29
86
201
/
/
vorpd
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
16
/
/
vpslld
0x10
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
16
/
/
vpslld
0x10
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
41
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm10
.
byte
196
67
125
25
192
1
/
/
vextractf128
0x1
%
ymm8
%
xmm8
.
byte
196
193
57
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm8
.
byte
196
67
45
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm10
%
ymm8
.
byte
196
65
29
86
192
/
/
vorpd
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
86
192
/
/
vorpd
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
16108
<
_sk_store_8888_avx
+
0xda
>
.
byte
196
65
124
17
4
144
/
/
vmovups
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
16104
<
_sk_store_8888_avx
+
0xd6
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
84
0
0
0
/
/
lea
0x54
(
%
rip
)
%
r9
#
16170
<
_sk_store_8888_avx
+
0x142
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
16104
<
_sk_store_8888_avx
+
0xd6
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
199
/
/
jmp
16104
<
_sk_store_8888_avx
+
0xd6
>
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
65
122
17
76
144
16
/
/
vmovss
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
121
17
4
144
/
/
vmovupd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
150
/
/
jmp
16104
<
_sk_store_8888_avx
+
0xd6
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
181
255
/
/
mov
0xff
%
ch
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
189
255
255
255
246
/
/
mov
0xf6ffffff
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
233
255
255
255
219
/
/
jmpq
ffffffffdc016184
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffdbfd9f38
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_avx
.
globl
_sk_load_bgra_avx
FUNCTION
(
_sk_load_bgra_avx
)
_sk_load_bgra_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
135
0
0
0
/
/
jne
1622d
<
_sk_load_bgra_avx
+
0xa1
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
197
125
40
21
236
105
2
0
/
/
vmovapd
0x269ec
(
%
rip
)
%
ymm10
#
3cba0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x954
>
.
byte
196
193
53
84
202
/
/
vandpd
%
ymm10
%
ymm9
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
174
99
2
0
/
/
vbroadcastss
0x263ae
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
116
89
208
/
/
vmulps
%
ymm8
%
ymm1
%
ymm2
.
byte
196
193
113
114
209
8
/
/
vpsrld
0x8
%
xmm9
%
xmm1
.
byte
196
99
125
25
203
1
/
/
vextractf128
0x1
%
ymm9
%
xmm3
.
byte
197
249
114
211
8
/
/
vpsrld
0x8
%
xmm3
%
xmm0
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
193
125
84
194
/
/
vandpd
%
ymm10
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
200
/
/
vmulps
%
ymm8
%
ymm0
%
ymm1
.
byte
196
193
33
114
209
16
/
/
vpsrld
0x10
%
xmm9
%
xmm11
.
byte
197
249
114
211
16
/
/
vpsrld
0x10
%
xmm3
%
xmm0
.
byte
196
227
37
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm11
%
ymm0
.
byte
196
193
125
84
194
/
/
vandpd
%
ymm10
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
49
114
209
24
/
/
vpsrld
0x18
%
xmm9
%
xmm9
.
byte
197
225
114
211
24
/
/
vpsrld
0x18
%
xmm3
%
xmm3
.
byte
196
227
53
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm9
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
108
255
255
255
/
/
ja
161ac
<
_sk_load_bgra_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
146
0
0
0
/
/
lea
0x92
(
%
rip
)
%
r9
#
162dc
<
_sk_load_bgra_avx
+
0x150
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
78
255
255
255
/
/
jmpq
161ac
<
_sk_load_bgra_avx
+
0x20
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
200
4
/
/
vblendps
0x4
%
ymm0
%
ymm1
%
ymm9
.
byte
196
193
123
16
4
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
99
53
13
200
1
/
/
vblendpd
0x1
%
ymm0
%
ymm9
%
ymm9
.
byte
233
39
255
255
255
/
/
jmpq
161ac
<
_sk_load_bgra_avx
+
0x20
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
196
227
125
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
200
64
/
/
vblendps
0x40
%
ymm0
%
ymm1
%
ymm9
.
byte
196
99
125
25
200
1
/
/
vextractf128
0x1
%
ymm9
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
53
24
200
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm9
%
ymm9
.
byte
196
99
125
25
200
1
/
/
vextractf128
0x1
%
ymm9
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
53
24
200
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm9
%
ymm9
.
byte
196
193
121
16
4
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
67
125
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm0
%
ymm9
.
byte
233
210
254
255
255
/
/
jmpq
161ac
<
_sk_load_bgra_avx
+
0x20
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
119
255
/
/
ja
162dd
<
_sk_load_bgra_avx
+
0x151
>
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
130
/
/
lcall
*
-
0x7d000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
169
/
/
.
byte
0xa9
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_dst_avx
.
globl
_sk_load_bgra_dst_avx
FUNCTION
(
_sk_load_bgra_dst_avx
)
_sk_load_bgra_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
135
0
0
0
/
/
jne
16399
<
_sk_load_bgra_dst_avx
+
0xa1
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
197
125
40
21
160
104
2
0
/
/
vmovapd
0x268a0
(
%
rip
)
%
ymm10
#
3cbc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x974
>
.
byte
196
193
53
84
234
/
/
vandpd
%
ymm10
%
ymm9
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
98
125
24
5
66
98
2
0
/
/
vbroadcastss
0x26242
(
%
rip
)
%
ymm8
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
84
89
240
/
/
vmulps
%
ymm8
%
ymm5
%
ymm6
.
byte
196
193
81
114
209
8
/
/
vpsrld
0x8
%
xmm9
%
xmm5
.
byte
196
99
125
25
207
1
/
/
vextractf128
0x1
%
ymm9
%
xmm7
.
byte
197
217
114
215
8
/
/
vpsrld
0x8
%
xmm7
%
xmm4
.
byte
196
227
85
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
196
193
93
84
226
/
/
vandpd
%
ymm10
%
ymm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
92
89
232
/
/
vmulps
%
ymm8
%
ymm4
%
ymm5
.
byte
196
193
33
114
209
16
/
/
vpsrld
0x10
%
xmm9
%
xmm11
.
byte
197
217
114
215
16
/
/
vpsrld
0x10
%
xmm7
%
xmm4
.
byte
196
227
37
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm11
%
ymm4
.
byte
196
193
93
84
226
/
/
vandpd
%
ymm10
%
ymm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
196
193
49
114
209
24
/
/
vpsrld
0x18
%
xmm9
%
xmm9
.
byte
197
193
114
215
24
/
/
vpsrld
0x18
%
xmm7
%
xmm7
.
byte
196
227
53
24
255
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm9
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
108
255
255
255
/
/
ja
16318
<
_sk_load_bgra_dst_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
146
0
0
0
/
/
lea
0x92
(
%
rip
)
%
r9
#
16448
<
_sk_load_bgra_dst_avx
+
0x150
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
78
255
255
255
/
/
jmpq
16318
<
_sk_load_bgra_dst_avx
+
0x20
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
4
/
/
vblendps
0x4
%
ymm4
%
ymm5
%
ymm9
.
byte
196
193
123
16
36
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
99
53
13
204
1
/
/
vblendpd
0x1
%
ymm4
%
ymm9
%
ymm9
.
byte
233
39
255
255
255
/
/
jmpq
16318
<
_sk_load_bgra_dst_avx
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
196
227
125
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm0
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
64
/
/
vblendps
0x40
%
ymm4
%
ymm5
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
193
121
16
36
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
67
93
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm4
%
ymm9
.
byte
233
210
254
255
255
/
/
jmpq
16318
<
_sk_load_bgra_dst_avx
+
0x20
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
119
255
/
/
ja
16449
<
_sk_load_bgra_dst_avx
+
0x151
>
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
130
/
/
lcall
*
-
0x7d000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
169
/
/
.
byte
0xa9
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_bgra_avx
.
globl
_sk_gather_bgra_avx
FUNCTION
(
_sk_gather_bgra_avx
)
_sk_gather_bgra_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
57
118
192
/
/
vpcmpeqd
%
xmm8
%
xmm8
%
xmm8
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
249
110
80
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm2
.
byte
197
249
112
210
0
/
/
vpshufd
0x0
%
xmm2
%
xmm2
.
byte
196
226
105
64
217
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm3
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
105
64
201
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
25
194
1
/
/
vextractf128
0x1
%
ymm0
%
xmm2
.
byte
197
241
254
202
/
/
vpaddd
%
xmm2
%
xmm1
%
xmm1
.
byte
196
225
249
126
200
/
/
vmovq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
197
225
254
192
/
/
vpaddd
%
xmm0
%
xmm3
%
xmm0
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
249
22
203
1
/
/
vpextrq
0x1
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
195
249
22
199
1
/
/
vpextrq
0x1
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
121
110
4
152
/
/
vmovd
(
%
r8
%
r11
4
)
%
xmm0
.
byte
196
131
121
34
4
144
1
/
/
vpinsrd
0x1
(
%
r8
%
r10
4
)
%
xmm0
%
xmm0
.
byte
196
131
121
34
4
160
2
/
/
vpinsrd
0x2
(
%
r8
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
3
121
34
4
184
3
/
/
vpinsrd
0x3
(
%
r8
%
r15
4
)
%
xmm0
%
xmm8
.
byte
196
129
121
110
4
136
/
/
vmovd
(
%
r8
%
r9
4
)
%
xmm0
.
byte
196
195
121
34
4
128
1
/
/
vpinsrd
0x1
(
%
r8
%
rax
4
)
%
xmm0
%
xmm0
.
byte
196
131
121
34
4
176
2
/
/
vpinsrd
0x2
(
%
r8
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
195
121
34
28
152
3
/
/
vpinsrd
0x3
(
%
r8
%
rbx
4
)
%
xmm0
%
xmm3
.
byte
196
227
61
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm0
.
byte
197
124
40
13
120
102
2
0
/
/
vmovaps
0x26678
(
%
rip
)
%
ymm9
#
3cbe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x994
>
.
byte
196
193
124
84
193
/
/
vandps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
250
95
2
0
/
/
vbroadcastss
0x25ffa
(
%
rip
)
%
ymm10
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
196
193
124
89
210
/
/
vmulps
%
ymm10
%
ymm0
%
ymm2
.
byte
196
193
121
114
208
8
/
/
vpsrld
0x8
%
xmm8
%
xmm0
.
byte
197
241
114
211
8
/
/
vpsrld
0x8
%
xmm3
%
xmm1
.
byte
196
227
125
24
193
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
124
84
193
/
/
vandps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
202
/
/
vmulps
%
ymm10
%
ymm0
%
ymm1
.
byte
196
193
33
114
208
16
/
/
vpsrld
0x10
%
xmm8
%
xmm11
.
byte
197
249
114
211
16
/
/
vpsrld
0x10
%
xmm3
%
xmm0
.
byte
196
227
37
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm11
%
ymm0
.
byte
196
193
124
84
193
/
/
vandps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
194
/
/
vmulps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
57
114
208
24
/
/
vpsrld
0x18
%
xmm8
%
xmm8
.
byte
197
225
114
211
24
/
/
vpsrld
0x18
%
xmm3
%
xmm3
.
byte
196
227
61
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
218
/
/
vmulps
%
ymm10
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_bgra_avx
.
globl
_sk_store_bgra_avx
FUNCTION
(
_sk_store_bgra_avx
)
_sk_store_bgra_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
202
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
247
94
2
0
/
/
vbroadcastss
0x25ef7
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
21
95
2
0
/
/
vbroadcastss
0x25f15
(
%
rip
)
%
ymm11
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
8
/
/
vpslld
0x8
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
8
/
/
vpslld
0x8
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
196
65
29
86
201
/
/
vorpd
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
224
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
16
/
/
vpslld
0x10
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
16
/
/
vpslld
0x10
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
41
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm10
.
byte
196
67
125
25
192
1
/
/
vextractf128
0x1
%
ymm8
%
xmm8
.
byte
196
193
57
114
240
24
/
/
vpslld
0x18
%
xmm8
%
xmm8
.
byte
196
67
45
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm10
%
ymm8
.
byte
196
65
29
86
192
/
/
vorpd
%
ymm8
%
ymm12
%
ymm8
.
byte
196
65
53
86
192
/
/
vorpd
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
166bc
<
_sk_store_bgra_avx
+
0xda
>
.
byte
196
65
124
17
4
144
/
/
vmovups
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
166b8
<
_sk_store_bgra_avx
+
0xd6
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
84
0
0
0
/
/
lea
0x54
(
%
rip
)
%
r9
#
16724
<
_sk_store_bgra_avx
+
0x142
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
166b8
<
_sk_store_bgra_avx
+
0xd6
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
199
/
/
jmp
166b8
<
_sk_store_bgra_avx
+
0xd6
>
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
65
122
17
76
144
16
/
/
vmovss
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
121
17
4
144
/
/
vmovupd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
150
/
/
jmp
166b8
<
_sk_store_bgra_avx
+
0xd6
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
181
255
/
/
mov
0xff
%
ch
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
189
255
255
255
246
/
/
mov
0xf6ffffff
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
233
255
255
255
219
/
/
jmpq
ffffffffdc016738
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffdbfda4ec
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_1010102_avx
.
globl
_sk_load_1010102_avx
FUNCTION
(
_sk_load_1010102_avx
)
_sk_load_1010102_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
145
0
0
0
/
/
jne
167eb
<
_sk_load_1010102_avx
+
0xab
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
196
98
125
24
29
175
94
2
0
/
/
vbroadcastss
0x25eaf
(
%
rip
)
%
ymm11
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
196
193
53
84
195
/
/
vandpd
%
ymm11
%
ymm9
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
161
94
2
0
/
/
vbroadcastss
0x25ea1
(
%
rip
)
%
ymm8
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
41
114
209
10
/
/
vpsrld
0xa
%
xmm9
%
xmm10
.
byte
196
99
125
25
203
1
/
/
vextractf128
0x1
%
ymm9
%
xmm3
.
byte
197
241
114
211
10
/
/
vpsrld
0xa
%
xmm3
%
xmm1
.
byte
196
227
45
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm10
%
ymm1
.
byte
196
193
117
84
203
/
/
vandpd
%
ymm11
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
41
114
209
20
/
/
vpsrld
0x14
%
xmm9
%
xmm10
.
byte
197
233
114
211
20
/
/
vpsrld
0x14
%
xmm3
%
xmm2
.
byte
196
227
45
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm10
%
ymm2
.
byte
196
193
109
84
211
/
/
vandpd
%
ymm11
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
208
/
/
vmulps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
57
114
209
30
/
/
vpsrld
0x1e
%
xmm9
%
xmm8
.
byte
197
225
114
211
30
/
/
vpsrld
0x1e
%
xmm3
%
xmm3
.
byte
196
227
61
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
134
93
2
0
/
/
vbroadcastss
0x25d86
(
%
rip
)
%
ymm8
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
98
255
255
255
/
/
ja
16760
<
_sk_load_1010102_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
144
0
0
0
/
/
lea
0x90
(
%
rip
)
%
r9
#
16898
<
_sk_load_1010102_avx
+
0x158
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
68
255
255
255
/
/
jmpq
16760
<
_sk_load_1010102_avx
+
0x20
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
200
4
/
/
vblendps
0x4
%
ymm0
%
ymm1
%
ymm9
.
byte
196
193
123
16
4
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
99
53
13
200
1
/
/
vblendpd
0x1
%
ymm0
%
ymm9
%
ymm9
.
byte
233
29
255
255
255
/
/
jmpq
16760
<
_sk_load_1010102_avx
+
0x20
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
197
249
112
192
68
/
/
vpshufd
0x44
%
xmm0
%
xmm0
.
byte
196
227
125
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
117
12
200
64
/
/
vblendps
0x40
%
ymm0
%
ymm1
%
ymm9
.
byte
196
99
125
25
200
1
/
/
vextractf128
0x1
%
ymm9
%
xmm0
.
byte
196
195
121
34
68
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
53
24
200
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm9
%
ymm9
.
byte
196
99
125
25
200
1
/
/
vextractf128
0x1
%
ymm9
%
xmm0
.
byte
196
195
121
34
68
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm0
.
byte
196
99
53
24
200
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm9
%
ymm9
.
byte
196
193
121
16
4
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
67
125
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm0
%
ymm9
.
byte
233
200
254
255
255
/
/
jmpq
16760
<
_sk_load_1010102_avx
+
0x20
>
.
byte
121
255
/
/
jns
16899
<
_sk_load_1010102_avx
+
0x159
>
.
byte
255
/
/
(
bad
)
.
byte
255
154
255
255
255
132
/
/
lcall
*
-
0x7b000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_1010102_dst_avx
.
globl
_sk_load_1010102_dst_avx
FUNCTION
(
_sk_load_1010102_dst_avx
)
_sk_load_1010102_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
145
0
0
0
/
/
jne
1695f
<
_sk_load_1010102_dst_avx
+
0xab
>
.
byte
196
65
125
16
12
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
ymm9
.
byte
196
98
125
24
29
59
93
2
0
/
/
vbroadcastss
0x25d3b
(
%
rip
)
%
ymm11
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
196
193
53
84
227
/
/
vandpd
%
ymm11
%
ymm9
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
98
125
24
5
45
93
2
0
/
/
vbroadcastss
0x25d2d
(
%
rip
)
%
ymm8
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
196
193
92
89
224
/
/
vmulps
%
ymm8
%
ymm4
%
ymm4
.
byte
196
193
41
114
209
10
/
/
vpsrld
0xa
%
xmm9
%
xmm10
.
byte
196
99
125
25
207
1
/
/
vextractf128
0x1
%
ymm9
%
xmm7
.
byte
197
209
114
215
10
/
/
vpsrld
0xa
%
xmm7
%
xmm5
.
byte
196
227
45
24
237
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm10
%
ymm5
.
byte
196
193
85
84
235
/
/
vandpd
%
ymm11
%
ymm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
84
89
232
/
/
vmulps
%
ymm8
%
ymm5
%
ymm5
.
byte
196
193
41
114
209
20
/
/
vpsrld
0x14
%
xmm9
%
xmm10
.
byte
197
201
114
215
20
/
/
vpsrld
0x14
%
xmm7
%
xmm6
.
byte
196
227
45
24
246
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm10
%
ymm6
.
byte
196
193
77
84
243
/
/
vandpd
%
ymm11
%
ymm6
%
ymm6
.
byte
197
252
91
246
/
/
vcvtdq2ps
%
ymm6
%
ymm6
.
byte
196
193
76
89
240
/
/
vmulps
%
ymm8
%
ymm6
%
ymm6
.
byte
196
193
57
114
209
30
/
/
vpsrld
0x1e
%
xmm9
%
xmm8
.
byte
197
193
114
215
30
/
/
vpsrld
0x1e
%
xmm7
%
xmm7
.
byte
196
227
61
24
255
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm8
%
ymm7
.
byte
197
252
91
255
/
/
vcvtdq2ps
%
ymm7
%
ymm7
.
byte
196
98
125
24
5
18
92
2
0
/
/
vbroadcastss
0x25c12
(
%
rip
)
%
ymm8
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
193
68
89
248
/
/
vmulps
%
ymm8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
15
135
98
255
255
255
/
/
ja
168d4
<
_sk_load_1010102_dst_avx
+
0x20
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
144
0
0
0
/
/
lea
0x90
(
%
rip
)
%
r9
#
16a0c
<
_sk_load_1010102_dst_avx
+
0x158
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
122
16
12
144
/
/
vmovss
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
68
255
255
255
/
/
jmpq
168d4
<
_sk_load_1010102_dst_avx
+
0x20
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
4
/
/
vblendps
0x4
%
ymm4
%
ymm5
%
ymm9
.
byte
196
193
123
16
36
144
/
/
vmovsd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
99
53
13
204
1
/
/
vblendpd
0x1
%
ymm4
%
ymm9
%
ymm9
.
byte
233
29
255
255
255
/
/
jmpq
168d4
<
_sk_load_1010102_dst_avx
+
0x20
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
197
249
112
228
68
/
/
vpshufd
0x44
%
xmm4
%
xmm4
.
byte
196
227
125
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm0
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
196
99
85
12
204
64
/
/
vblendps
0x40
%
ymm4
%
ymm5
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
99
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm4
.
byte
196
195
89
34
100
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm4
.
byte
196
99
53
24
204
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm9
%
ymm9
.
byte
196
193
121
16
36
144
/
/
vmovupd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
67
93
13
201
12
/
/
vblendpd
0xc
%
ymm9
%
ymm4
%
ymm9
.
byte
233
200
254
255
255
/
/
jmpq
168d4
<
_sk_load_1010102_dst_avx
+
0x20
>
.
byte
121
255
/
/
jns
16a0d
<
_sk_load_1010102_dst_avx
+
0x159
>
.
byte
255
/
/
(
bad
)
.
byte
255
154
255
255
255
132
/
/
lcall
*
-
0x7b000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_1010102_avx
.
globl
_sk_gather_1010102_avx
FUNCTION
(
_sk_gather_1010102_avx
)
_sk_gather_1010102_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
57
118
192
/
/
vpcmpeqd
%
xmm8
%
xmm8
%
xmm8
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
180
95
192
/
/
vmaxps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
93
194
/
/
vminps
%
ymm2
%
ymm0
%
ymm0
.
byte
196
226
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
193
97
254
216
/
/
vpaddd
%
xmm8
%
xmm3
%
xmm3
.
byte
196
193
105
254
208
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
197
180
95
201
/
/
vmaxps
%
ymm1
%
ymm9
%
ymm1
.
byte
197
244
93
202
/
/
vminps
%
ymm2
%
ymm1
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
249
110
80
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm2
.
byte
197
249
112
210
0
/
/
vpshufd
0x0
%
xmm2
%
xmm2
.
byte
196
226
105
64
217
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm3
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
105
64
201
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm1
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
25
194
1
/
/
vextractf128
0x1
%
ymm0
%
xmm2
.
byte
197
241
254
202
/
/
vpaddd
%
xmm2
%
xmm1
%
xmm1
.
byte
196
225
249
126
200
/
/
vmovq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
197
225
254
192
/
/
vpaddd
%
xmm0
%
xmm3
%
xmm0
.
byte
196
193
249
126
194
/
/
vmovq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
249
22
203
1
/
/
vpextrq
0x1
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
195
249
22
199
1
/
/
vpextrq
0x1
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
121
110
4
152
/
/
vmovd
(
%
r8
%
r11
4
)
%
xmm0
.
byte
196
131
121
34
4
144
1
/
/
vpinsrd
0x1
(
%
r8
%
r10
4
)
%
xmm0
%
xmm0
.
byte
196
131
121
34
4
160
2
/
/
vpinsrd
0x2
(
%
r8
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
3
121
34
4
184
3
/
/
vpinsrd
0x3
(
%
r8
%
r15
4
)
%
xmm0
%
xmm8
.
byte
196
129
121
110
4
136
/
/
vmovd
(
%
r8
%
r9
4
)
%
xmm0
.
byte
196
195
121
34
4
128
1
/
/
vpinsrd
0x1
(
%
r8
%
rax
4
)
%
xmm0
%
xmm0
.
byte
196
131
121
34
4
176
2
/
/
vpinsrd
0x2
(
%
r8
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
195
121
34
28
152
3
/
/
vpinsrd
0x3
(
%
r8
%
rbx
4
)
%
xmm0
%
xmm3
.
byte
196
227
61
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm0
.
byte
196
98
125
24
29
235
90
2
0
/
/
vbroadcastss
0x25aeb
(
%
rip
)
%
ymm11
#
3c618
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3cc
>
.
byte
196
193
124
84
195
/
/
vandps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
221
90
2
0
/
/
vbroadcastss
0x25add
(
%
rip
)
%
ymm9
#
3c61c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d0
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
41
114
208
10
/
/
vpsrld
0xa
%
xmm8
%
xmm10
.
byte
197
241
114
211
10
/
/
vpsrld
0xa
%
xmm3
%
xmm1
.
byte
196
227
45
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm10
%
ymm1
.
byte
196
193
116
84
203
/
/
vandps
%
ymm11
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
41
114
208
20
/
/
vpsrld
0x14
%
xmm8
%
xmm10
.
byte
197
233
114
211
20
/
/
vpsrld
0x14
%
xmm3
%
xmm2
.
byte
196
227
45
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm10
%
ymm2
.
byte
196
193
108
84
211
/
/
vandps
%
ymm11
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
57
114
208
30
/
/
vpsrld
0x1e
%
xmm8
%
xmm8
.
byte
197
225
114
211
30
/
/
vpsrld
0x1e
%
xmm3
%
xmm3
.
byte
196
227
61
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
98
125
24
5
200
89
2
0
/
/
vbroadcastss
0x259c8
(
%
rip
)
%
ymm8
#
3c568
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x31c
>
.
byte
196
193
100
89
216
/
/
vmulps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_1010102_avx
.
globl
_sk_store_1010102_avx
FUNCTION
(
_sk_store_1010102_avx
)
_sk_store_1010102_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
41
89
2
0
/
/
vbroadcastss
0x25929
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
63
90
2
0
/
/
vbroadcastss
0x25a3f
(
%
rip
)
%
ymm11
#
3c620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d4
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
193
17
114
244
10
/
/
vpslld
0xa
%
xmm12
%
xmm13
.
byte
196
67
125
25
228
1
/
/
vextractf128
0x1
%
ymm12
%
xmm12
.
byte
196
193
25
114
244
10
/
/
vpslld
0xa
%
xmm12
%
xmm12
.
byte
196
67
21
24
228
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm12
.
byte
196
65
29
86
201
/
/
vorpd
%
ymm9
%
ymm12
%
ymm9
.
byte
197
60
95
226
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
219
/
/
vmulps
%
ymm11
%
ymm12
%
ymm11
.
byte
196
65
125
91
219
/
/
vcvtps2dq
%
ymm11
%
ymm11
.
byte
196
193
25
114
243
20
/
/
vpslld
0x14
%
xmm11
%
xmm12
.
byte
196
67
125
25
219
1
/
/
vextractf128
0x1
%
ymm11
%
xmm11
.
byte
196
193
33
114
243
20
/
/
vpslld
0x14
%
xmm11
%
xmm11
.
byte
196
67
29
24
219
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm12
%
ymm11
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
98
125
24
21
204
89
2
0
/
/
vbroadcastss
0x259cc
(
%
rip
)
%
ymm10
#
3c624
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d8
>
.
byte
196
65
60
89
194
/
/
vmulps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
193
41
114
240
30
/
/
vpslld
0x1e
%
xmm8
%
xmm10
.
byte
196
67
125
25
192
1
/
/
vextractf128
0x1
%
ymm8
%
xmm8
.
byte
196
193
57
114
240
30
/
/
vpslld
0x1e
%
xmm8
%
xmm8
.
byte
196
67
45
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm10
%
ymm8
.
byte
196
65
37
86
192
/
/
vorpd
%
ymm8
%
ymm11
%
ymm8
.
byte
196
65
53
86
192
/
/
vorpd
%
ymm8
%
ymm9
%
ymm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
16c93
<
_sk_store_1010102_avx
+
0xe3
>
.
byte
196
65
124
17
4
144
/
/
vmovups
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
242
/
/
ja
16c8f
<
_sk_store_1010102_avx
+
0xdf
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
85
0
0
0
/
/
lea
0x55
(
%
rip
)
%
r9
#
16cfc
<
_sk_store_1010102_avx
+
0x14c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
16c8f
<
_sk_store_1010102_avx
+
0xdf
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
199
/
/
jmp
16c8f
<
_sk_store_1010102_avx
+
0xdf
>
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
25
193
1
/
/
vextractf128
0x1
%
ymm8
%
xmm9
.
byte
196
65
122
17
76
144
16
/
/
vmovss
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
121
17
4
144
/
/
vmovupd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
150
/
/
jmp
16c8f
<
_sk_store_1010102_avx
+
0xdf
>
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
180
255
/
/
mov
0xff
%
ah
.
byte
255
/
/
(
bad
)
.
byte
255
196
/
/
inc
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
255
255
255
245
/
/
mov
0xf5ffffff
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
255
255
255
218
/
/
callq
ffffffffdb016d10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffdafdaac4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
204
/
/
dec
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_f16_avx
.
globl
_sk_load_f16_avx
FUNCTION
(
_sk_load_f16_avx
)
_sk_load_f16_avx
:
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
197
252
17
124
36
224
/
/
vmovups
%
ymm7
-
0x20
(
%
rsp
)
.
byte
197
252
17
116
36
192
/
/
vmovups
%
ymm6
-
0x40
(
%
rsp
)
.
byte
197
252
17
108
36
160
/
/
vmovups
%
ymm5
-
0x60
(
%
rsp
)
.
byte
197
252
17
100
36
128
/
/
vmovups
%
ymm4
-
0x80
(
%
rsp
)
.
byte
15
133
31
2
0
0
/
/
jne
16f6d
<
_sk_load_f16_avx
+
0x255
>
.
byte
196
65
121
16
4
208
/
/
vmovupd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
193
121
16
84
208
16
/
/
vmovupd
0x10
(
%
r8
%
rdx
8
)
%
xmm2
.
byte
196
193
121
16
92
208
32
/
/
vmovupd
0x20
(
%
r8
%
rdx
8
)
%
xmm3
.
byte
196
65
122
111
76
208
48
/
/
vmovdqu
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
242
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm14
.
byte
197
121
105
194
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
113
97
251
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm15
.
byte
197
113
105
203
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm9
.
byte
196
193
9
108
199
/
/
vpunpcklqdq
%
xmm15
%
xmm14
%
xmm0
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
197
123
18
21
193
105
2
0
/
/
vmovddup
0x269c1
(
%
rip
)
%
xmm10
#
3d768
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x151c
>
.
byte
196
193
121
219
218
/
/
vpand
%
xmm10
%
xmm0
%
xmm3
.
byte
196
98
121
24
29
219
88
2
0
/
/
vbroadcastss
0x258db
(
%
rip
)
%
xmm11
#
3c690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x444
>
.
byte
197
33
102
227
/
/
vpcmpgtd
%
xmm3
%
xmm11
%
xmm12
.
byte
196
193
113
219
210
/
/
vpand
%
xmm10
%
xmm1
%
xmm2
.
byte
197
33
102
234
/
/
vpcmpgtd
%
xmm2
%
xmm11
%
xmm13
.
byte
196
195
21
24
252
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm7
.
byte
197
123
18
37
160
105
2
0
/
/
vmovddup
0x269a0
(
%
rip
)
%
xmm12
#
3d770
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1524
>
.
byte
196
193
113
219
204
/
/
vpand
%
xmm12
%
xmm1
%
xmm1
.
byte
197
241
114
241
16
/
/
vpslld
0x10
%
xmm1
%
xmm1
.
byte
196
193
121
219
196
/
/
vpand
%
xmm12
%
xmm0
%
xmm0
.
byte
197
249
114
240
16
/
/
vpslld
0x10
%
xmm0
%
xmm0
.
byte
197
233
114
242
13
/
/
vpslld
0xd
%
xmm2
%
xmm2
.
byte
197
241
235
202
/
/
vpor
%
xmm2
%
xmm1
%
xmm1
.
byte
197
233
114
243
13
/
/
vpslld
0xd
%
xmm3
%
xmm2
.
byte
197
249
235
194
/
/
vpor
%
xmm2
%
xmm0
%
xmm0
.
byte
196
226
121
24
29
149
88
2
0
/
/
vbroadcastss
0x25895
(
%
rip
)
%
xmm3
#
3c694
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x448
>
.
byte
197
249
254
195
/
/
vpaddd
%
xmm3
%
xmm0
%
xmm0
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
227
125
74
196
112
/
/
vblendvps
%
ymm7
%
ymm4
%
ymm0
%
ymm0
.
byte
196
193
9
109
207
/
/
vpunpckhqdq
%
xmm15
%
xmm14
%
xmm1
.
byte
196
226
121
51
209
/
/
vpmovzxwd
%
xmm1
%
xmm2
.
byte
197
249
112
201
78
/
/
vpshufd
0x4e
%
xmm1
%
xmm1
.
byte
196
98
121
51
249
/
/
vpmovzxwd
%
xmm1
%
xmm15
.
byte
196
193
1
219
250
/
/
vpand
%
xmm10
%
xmm15
%
xmm7
.
byte
197
33
102
247
/
/
vpcmpgtd
%
xmm7
%
xmm11
%
xmm14
.
byte
196
193
105
219
202
/
/
vpand
%
xmm10
%
xmm2
%
xmm1
.
byte
197
33
102
233
/
/
vpcmpgtd
%
xmm1
%
xmm11
%
xmm13
.
byte
196
67
21
24
238
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm13
%
ymm13
.
byte
196
193
105
219
212
/
/
vpand
%
xmm12
%
xmm2
%
xmm2
.
byte
197
233
114
242
16
/
/
vpslld
0x10
%
xmm2
%
xmm2
.
byte
197
241
114
241
13
/
/
vpslld
0xd
%
xmm1
%
xmm1
.
byte
197
233
235
201
/
/
vpor
%
xmm1
%
xmm2
%
xmm1
.
byte
196
193
1
219
212
/
/
vpand
%
xmm12
%
xmm15
%
xmm2
.
byte
197
233
114
242
16
/
/
vpslld
0x10
%
xmm2
%
xmm2
.
byte
197
193
114
247
13
/
/
vpslld
0xd
%
xmm7
%
xmm7
.
byte
197
233
235
215
/
/
vpor
%
xmm7
%
xmm2
%
xmm2
.
byte
197
233
254
211
/
/
vpaddd
%
xmm3
%
xmm2
%
xmm2
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
117
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
196
227
117
74
204
208
/
/
vblendvps
%
ymm13
%
ymm4
%
ymm1
%
ymm1
.
byte
196
193
57
108
209
/
/
vpunpcklqdq
%
xmm9
%
xmm8
%
xmm2
.
byte
196
226
121
51
250
/
/
vpmovzxwd
%
xmm2
%
xmm7
.
byte
197
249
112
210
78
/
/
vpshufd
0x4e
%
xmm2
%
xmm2
.
byte
196
98
121
51
234
/
/
vpmovzxwd
%
xmm2
%
xmm13
.
byte
196
65
17
219
250
/
/
vpand
%
xmm10
%
xmm13
%
xmm15
.
byte
196
65
33
102
247
/
/
vpcmpgtd
%
xmm15
%
xmm11
%
xmm14
.
byte
196
193
65
219
210
/
/
vpand
%
xmm10
%
xmm7
%
xmm2
.
byte
197
161
102
242
/
/
vpcmpgtd
%
xmm2
%
xmm11
%
xmm6
.
byte
196
195
77
24
246
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm6
%
ymm6
.
byte
196
193
65
219
252
/
/
vpand
%
xmm12
%
xmm7
%
xmm7
.
byte
197
193
114
247
16
/
/
vpslld
0x10
%
xmm7
%
xmm7
.
byte
197
233
114
242
13
/
/
vpslld
0xd
%
xmm2
%
xmm2
.
byte
197
193
235
210
/
/
vpor
%
xmm2
%
xmm7
%
xmm2
.
byte
196
193
17
219
252
/
/
vpand
%
xmm12
%
xmm13
%
xmm7
.
byte
197
193
114
247
16
/
/
vpslld
0x10
%
xmm7
%
xmm7
.
byte
196
193
81
114
247
13
/
/
vpslld
0xd
%
xmm15
%
xmm5
.
byte
197
193
235
237
/
/
vpor
%
xmm5
%
xmm7
%
xmm5
.
byte
197
209
254
235
/
/
vpaddd
%
xmm3
%
xmm5
%
xmm5
.
byte
197
233
254
211
/
/
vpaddd
%
xmm3
%
xmm2
%
xmm2
.
byte
196
227
109
24
213
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm2
%
ymm2
.
byte
196
227
109
74
212
96
/
/
vblendvps
%
ymm6
%
ymm4
%
ymm2
%
ymm2
.
byte
196
193
57
109
233
/
/
vpunpckhqdq
%
xmm9
%
xmm8
%
xmm5
.
byte
196
226
121
51
245
/
/
vpmovzxwd
%
xmm5
%
xmm6
.
byte
197
249
112
237
78
/
/
vpshufd
0x4e
%
xmm5
%
xmm5
.
byte
196
226
121
51
237
/
/
vpmovzxwd
%
xmm5
%
xmm5
.
byte
196
65
81
219
202
/
/
vpand
%
xmm10
%
xmm5
%
xmm9
.
byte
196
65
33
102
193
/
/
vpcmpgtd
%
xmm9
%
xmm11
%
xmm8
.
byte
196
193
73
219
250
/
/
vpand
%
xmm10
%
xmm6
%
xmm7
.
byte
197
33
102
215
/
/
vpcmpgtd
%
xmm7
%
xmm11
%
xmm10
.
byte
196
67
45
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm10
%
ymm8
.
byte
196
193
73
219
244
/
/
vpand
%
xmm12
%
xmm6
%
xmm6
.
byte
196
193
81
219
236
/
/
vpand
%
xmm12
%
xmm5
%
xmm5
.
byte
197
201
114
246
16
/
/
vpslld
0x10
%
xmm6
%
xmm6
.
byte
197
193
114
247
13
/
/
vpslld
0xd
%
xmm7
%
xmm7
.
byte
197
201
235
247
/
/
vpor
%
xmm7
%
xmm6
%
xmm6
.
byte
197
209
114
245
16
/
/
vpslld
0x10
%
xmm5
%
xmm5
.
byte
196
193
65
114
241
13
/
/
vpslld
0xd
%
xmm9
%
xmm7
.
byte
197
209
235
239
/
/
vpor
%
xmm7
%
xmm5
%
xmm5
.
byte
197
209
254
235
/
/
vpaddd
%
xmm3
%
xmm5
%
xmm5
.
byte
197
201
254
219
/
/
vpaddd
%
xmm3
%
xmm6
%
xmm3
.
byte
196
227
101
24
221
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm3
%
ymm3
.
byte
196
227
101
74
220
128
/
/
vblendvps
%
ymm8
%
ymm4
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
100
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm7
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
123
16
4
208
/
/
vmovsd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
16fd3
<
_sk_load_f16_avx
+
0x2bb
>
.
byte
196
65
57
22
68
208
8
/
/
vmovhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
16fd3
<
_sk_load_f16_avx
+
0x2bb
>
.
byte
196
193
123
16
84
208
16
/
/
vmovsd
0x10
(
%
r8
%
rdx
8
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
16fe0
<
_sk_load_f16_avx
+
0x2c8
>
.
byte
196
193
105
22
84
208
24
/
/
vmovhpd
0x18
(
%
r8
%
rdx
8
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
16fe0
<
_sk_load_f16_avx
+
0x2c8
>
.
byte
196
193
123
16
92
208
32
/
/
vmovsd
0x20
(
%
r8
%
rdx
8
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
179
253
255
255
/
/
je
16d69
<
_sk_load_f16_avx
+
0x51
>
.
byte
196
193
97
22
92
208
40
/
/
vmovhpd
0x28
(
%
r8
%
rdx
8
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
162
253
255
255
/
/
jb
16d69
<
_sk_load_f16_avx
+
0x51
>
.
byte
196
65
122
126
76
208
48
/
/
vmovq
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
233
150
253
255
255
/
/
jmpq
16d69
<
_sk_load_f16_avx
+
0x51
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
137
253
255
255
/
/
jmpq
16d69
<
_sk_load_f16_avx
+
0x51
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
128
253
255
255
/
/
jmpq
16d69
<
_sk_load_f16_avx
+
0x51
>
HIDDEN
_sk_load_f16_dst_avx
.
globl
_sk_load_f16_dst_avx
FUNCTION
(
_sk_load_f16_dst_avx
)
_sk_load_f16_dst_avx
:
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
197
252
17
92
36
224
/
/
vmovups
%
ymm3
-
0x20
(
%
rsp
)
.
byte
197
252
17
84
36
192
/
/
vmovups
%
ymm2
-
0x40
(
%
rsp
)
.
byte
197
252
17
76
36
160
/
/
vmovups
%
ymm1
-
0x60
(
%
rsp
)
.
byte
197
252
17
68
36
128
/
/
vmovups
%
ymm0
-
0x80
(
%
rsp
)
.
byte
15
133
31
2
0
0
/
/
jne
1723e
<
_sk_load_f16_dst_avx
+
0x255
>
.
byte
196
65
121
16
4
208
/
/
vmovupd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
193
121
16
116
208
16
/
/
vmovupd
0x10
(
%
r8
%
rdx
8
)
%
xmm6
.
byte
196
193
121
16
124
208
32
/
/
vmovupd
0x20
(
%
r8
%
rdx
8
)
%
xmm7
.
byte
196
65
122
111
76
208
48
/
/
vmovdqu
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
197
185
97
230
/
/
vpunpcklwd
%
xmm6
%
xmm8
%
xmm4
.
byte
197
185
105
246
/
/
vpunpckhwd
%
xmm6
%
xmm8
%
xmm6
.
byte
196
193
65
97
233
/
/
vpunpcklwd
%
xmm9
%
xmm7
%
xmm5
.
byte
196
193
65
105
249
/
/
vpunpckhwd
%
xmm9
%
xmm7
%
xmm7
.
byte
197
89
97
246
/
/
vpunpcklwd
%
xmm6
%
xmm4
%
xmm14
.
byte
197
89
105
198
/
/
vpunpckhwd
%
xmm6
%
xmm4
%
xmm8
.
byte
197
81
97
255
/
/
vpunpcklwd
%
xmm7
%
xmm5
%
xmm15
.
byte
197
81
105
207
/
/
vpunpckhwd
%
xmm7
%
xmm5
%
xmm9
.
byte
196
193
9
108
231
/
/
vpunpcklqdq
%
xmm15
%
xmm14
%
xmm4
.
byte
196
226
121
51
236
/
/
vpmovzxwd
%
xmm4
%
xmm5
.
byte
197
249
112
228
78
/
/
vpshufd
0x4e
%
xmm4
%
xmm4
.
byte
196
226
121
51
228
/
/
vpmovzxwd
%
xmm4
%
xmm4
.
byte
197
123
18
21
240
102
2
0
/
/
vmovddup
0x266f0
(
%
rip
)
%
xmm10
#
3d768
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x151c
>
.
byte
196
193
89
219
250
/
/
vpand
%
xmm10
%
xmm4
%
xmm7
.
byte
196
98
121
24
29
10
86
2
0
/
/
vbroadcastss
0x2560a
(
%
rip
)
%
xmm11
#
3c690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x444
>
.
byte
197
33
102
231
/
/
vpcmpgtd
%
xmm7
%
xmm11
%
xmm12
.
byte
196
193
81
219
242
/
/
vpand
%
xmm10
%
xmm5
%
xmm6
.
byte
197
33
102
238
/
/
vpcmpgtd
%
xmm6
%
xmm11
%
xmm13
.
byte
196
195
21
24
220
1
/
/
vinsertf128
0x1
%
xmm12
%
ymm13
%
ymm3
.
byte
197
123
18
37
207
102
2
0
/
/
vmovddup
0x266cf
(
%
rip
)
%
xmm12
#
3d770
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1524
>
.
byte
196
193
81
219
236
/
/
vpand
%
xmm12
%
xmm5
%
xmm5
.
byte
197
209
114
245
16
/
/
vpslld
0x10
%
xmm5
%
xmm5
.
byte
196
193
89
219
228
/
/
vpand
%
xmm12
%
xmm4
%
xmm4
.
byte
197
217
114
244
16
/
/
vpslld
0x10
%
xmm4
%
xmm4
.
byte
197
201
114
246
13
/
/
vpslld
0xd
%
xmm6
%
xmm6
.
byte
197
209
235
238
/
/
vpor
%
xmm6
%
xmm5
%
xmm5
.
byte
197
201
114
247
13
/
/
vpslld
0xd
%
xmm7
%
xmm6
.
byte
197
217
235
230
/
/
vpor
%
xmm6
%
xmm4
%
xmm4
.
byte
196
226
121
24
61
196
85
2
0
/
/
vbroadcastss
0x255c4
(
%
rip
)
%
xmm7
#
3c694
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x448
>
.
byte
197
217
254
231
/
/
vpaddd
%
xmm7
%
xmm4
%
xmm4
.
byte
197
209
254
239
/
/
vpaddd
%
xmm7
%
xmm5
%
xmm5
.
byte
196
227
85
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
196
227
93
74
224
48
/
/
vblendvps
%
ymm3
%
ymm0
%
ymm4
%
ymm4
.
byte
196
193
9
109
223
/
/
vpunpckhqdq
%
xmm15
%
xmm14
%
xmm3
.
byte
196
226
121
51
235
/
/
vpmovzxwd
%
xmm3
%
xmm5
.
byte
197
249
112
219
78
/
/
vpshufd
0x4e
%
xmm3
%
xmm3
.
byte
196
98
121
51
251
/
/
vpmovzxwd
%
xmm3
%
xmm15
.
byte
196
193
1
219
242
/
/
vpand
%
xmm10
%
xmm15
%
xmm6
.
byte
197
33
102
246
/
/
vpcmpgtd
%
xmm6
%
xmm11
%
xmm14
.
byte
196
193
81
219
218
/
/
vpand
%
xmm10
%
xmm5
%
xmm3
.
byte
197
33
102
235
/
/
vpcmpgtd
%
xmm3
%
xmm11
%
xmm13
.
byte
196
67
21
24
238
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm13
%
ymm13
.
byte
196
193
81
219
236
/
/
vpand
%
xmm12
%
xmm5
%
xmm5
.
byte
197
209
114
245
16
/
/
vpslld
0x10
%
xmm5
%
xmm5
.
byte
197
225
114
243
13
/
/
vpslld
0xd
%
xmm3
%
xmm3
.
byte
197
209
235
219
/
/
vpor
%
xmm3
%
xmm5
%
xmm3
.
byte
196
193
1
219
236
/
/
vpand
%
xmm12
%
xmm15
%
xmm5
.
byte
197
209
114
245
16
/
/
vpslld
0x10
%
xmm5
%
xmm5
.
byte
197
201
114
246
13
/
/
vpslld
0xd
%
xmm6
%
xmm6
.
byte
197
209
235
238
/
/
vpor
%
xmm6
%
xmm5
%
xmm5
.
byte
197
209
254
239
/
/
vpaddd
%
xmm7
%
xmm5
%
xmm5
.
byte
197
225
254
223
/
/
vpaddd
%
xmm7
%
xmm3
%
xmm3
.
byte
196
227
101
24
221
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm3
%
ymm3
.
byte
196
227
101
74
232
208
/
/
vblendvps
%
ymm13
%
ymm0
%
ymm3
%
ymm5
.
byte
196
193
57
108
217
/
/
vpunpcklqdq
%
xmm9
%
xmm8
%
xmm3
.
byte
196
226
121
51
243
/
/
vpmovzxwd
%
xmm3
%
xmm6
.
byte
197
249
112
219
78
/
/
vpshufd
0x4e
%
xmm3
%
xmm3
.
byte
196
98
121
51
235
/
/
vpmovzxwd
%
xmm3
%
xmm13
.
byte
196
65
17
219
250
/
/
vpand
%
xmm10
%
xmm13
%
xmm15
.
byte
196
65
33
102
247
/
/
vpcmpgtd
%
xmm15
%
xmm11
%
xmm14
.
byte
196
193
73
219
218
/
/
vpand
%
xmm10
%
xmm6
%
xmm3
.
byte
197
161
102
211
/
/
vpcmpgtd
%
xmm3
%
xmm11
%
xmm2
.
byte
196
195
109
24
214
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm2
%
ymm2
.
byte
196
193
73
219
244
/
/
vpand
%
xmm12
%
xmm6
%
xmm6
.
byte
197
201
114
246
16
/
/
vpslld
0x10
%
xmm6
%
xmm6
.
byte
197
225
114
243
13
/
/
vpslld
0xd
%
xmm3
%
xmm3
.
byte
197
201
235
219
/
/
vpor
%
xmm3
%
xmm6
%
xmm3
.
byte
196
193
17
219
244
/
/
vpand
%
xmm12
%
xmm13
%
xmm6
.
byte
197
201
114
246
16
/
/
vpslld
0x10
%
xmm6
%
xmm6
.
byte
196
193
113
114
247
13
/
/
vpslld
0xd
%
xmm15
%
xmm1
.
byte
197
201
235
201
/
/
vpor
%
xmm1
%
xmm6
%
xmm1
.
byte
197
241
254
207
/
/
vpaddd
%
xmm7
%
xmm1
%
xmm1
.
byte
197
225
254
223
/
/
vpaddd
%
xmm7
%
xmm3
%
xmm3
.
byte
196
227
101
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm1
.
byte
196
227
117
74
240
32
/
/
vblendvps
%
ymm2
%
ymm0
%
ymm1
%
ymm6
.
byte
196
193
57
109
201
/
/
vpunpckhqdq
%
xmm9
%
xmm8
%
xmm1
.
byte
196
226
121
51
209
/
/
vpmovzxwd
%
xmm1
%
xmm2
.
byte
197
249
112
201
78
/
/
vpshufd
0x4e
%
xmm1
%
xmm1
.
byte
196
226
121
51
201
/
/
vpmovzxwd
%
xmm1
%
xmm1
.
byte
196
65
113
219
202
/
/
vpand
%
xmm10
%
xmm1
%
xmm9
.
byte
196
65
33
102
193
/
/
vpcmpgtd
%
xmm9
%
xmm11
%
xmm8
.
byte
196
193
105
219
218
/
/
vpand
%
xmm10
%
xmm2
%
xmm3
.
byte
197
33
102
211
/
/
vpcmpgtd
%
xmm3
%
xmm11
%
xmm10
.
byte
196
67
45
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm10
%
ymm8
.
byte
196
193
105
219
212
/
/
vpand
%
xmm12
%
xmm2
%
xmm2
.
byte
196
193
113
219
204
/
/
vpand
%
xmm12
%
xmm1
%
xmm1
.
byte
197
233
114
242
16
/
/
vpslld
0x10
%
xmm2
%
xmm2
.
byte
197
225
114
243
13
/
/
vpslld
0xd
%
xmm3
%
xmm3
.
byte
197
233
235
211
/
/
vpor
%
xmm3
%
xmm2
%
xmm2
.
byte
197
241
114
241
16
/
/
vpslld
0x10
%
xmm1
%
xmm1
.
byte
196
193
97
114
241
13
/
/
vpslld
0xd
%
xmm9
%
xmm3
.
byte
197
241
235
203
/
/
vpor
%
xmm3
%
xmm1
%
xmm1
.
byte
197
241
254
207
/
/
vpaddd
%
xmm7
%
xmm1
%
xmm1
.
byte
197
233
254
215
/
/
vpaddd
%
xmm7
%
xmm2
%
xmm2
.
byte
196
227
109
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm1
.
byte
196
227
117
74
248
128
/
/
vblendvps
%
ymm8
%
ymm0
%
ymm1
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
68
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm0
.
byte
197
252
16
76
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm1
.
byte
197
252
16
84
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm2
.
byte
197
252
16
92
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm3
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
123
16
4
208
/
/
vmovsd
(
%
r8
%
rdx
8
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
172a4
<
_sk_load_f16_dst_avx
+
0x2bb
>
.
byte
196
65
57
22
68
208
8
/
/
vmovhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
172a4
<
_sk_load_f16_dst_avx
+
0x2bb
>
.
byte
196
193
123
16
116
208
16
/
/
vmovsd
0x10
(
%
r8
%
rdx
8
)
%
xmm6
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
172b1
<
_sk_load_f16_dst_avx
+
0x2c8
>
.
byte
196
193
73
22
116
208
24
/
/
vmovhpd
0x18
(
%
r8
%
rdx
8
)
%
xmm6
%
xmm6
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
172b1
<
_sk_load_f16_dst_avx
+
0x2c8
>
.
byte
196
193
123
16
124
208
32
/
/
vmovsd
0x20
(
%
r8
%
rdx
8
)
%
xmm7
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
179
253
255
255
/
/
je
1703a
<
_sk_load_f16_dst_avx
+
0x51
>
.
byte
196
193
65
22
124
208
40
/
/
vmovhpd
0x28
(
%
r8
%
rdx
8
)
%
xmm7
%
xmm7
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
162
253
255
255
/
/
jb
1703a
<
_sk_load_f16_dst_avx
+
0x51
>
.
byte
196
65
122
126
76
208
48
/
/
vmovq
0x30
(
%
r8
%
rdx
8
)
%
xmm9
.
byte
233
150
253
255
255
/
/
jmpq
1703a
<
_sk_load_f16_dst_avx
+
0x51
>
.
byte
197
193
87
255
/
/
vxorpd
%
xmm7
%
xmm7
%
xmm7
.
byte
197
201
87
246
/
/
vxorpd
%
xmm6
%
xmm6
%
xmm6
.
byte
233
137
253
255
255
/
/
jmpq
1703a
<
_sk_load_f16_dst_avx
+
0x51
>
.
byte
197
193
87
255
/
/
vxorpd
%
xmm7
%
xmm7
%
xmm7
.
byte
233
128
253
255
255
/
/
jmpq
1703a
<
_sk_load_f16_dst_avx
+
0x51
>
HIDDEN
_sk_gather_f16_avx
.
globl
_sk_gather_f16_avx
FUNCTION
(
_sk_gather_f16_avx
)
_sk_gather_f16_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
197
252
17
124
36
224
/
/
vmovups
%
ymm7
-
0x20
(
%
rsp
)
.
byte
197
252
17
116
36
192
/
/
vmovups
%
ymm6
-
0x40
(
%
rsp
)
.
byte
197
252
17
108
36
160
/
/
vmovups
%
ymm5
-
0x60
(
%
rsp
)
.
byte
197
252
17
100
36
128
/
/
vmovups
%
ymm4
-
0x80
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
196
65
49
118
201
/
/
vpcmpeqd
%
xmm9
%
xmm9
%
xmm9
.
byte
196
193
97
254
217
/
/
vpaddd
%
xmm9
%
xmm3
%
xmm3
.
byte
196
193
105
254
209
/
/
vpaddd
%
xmm9
%
xmm2
%
xmm2
.
byte
196
227
109
24
211
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
220
95
192
/
/
vmaxps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
93
210
/
/
vminps
%
ymm2
%
ymm0
%
ymm2
.
byte
196
226
125
24
64
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm0
.
byte
196
227
125
25
195
1
/
/
vextractf128
0x1
%
ymm0
%
xmm3
.
byte
196
193
97
254
217
/
/
vpaddd
%
xmm9
%
xmm3
%
xmm3
.
byte
196
193
121
254
193
/
/
vpaddd
%
xmm9
%
xmm0
%
xmm0
.
byte
196
227
125
24
195
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm0
%
ymm0
.
byte
197
220
95
201
/
/
vmaxps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
244
93
192
/
/
vminps
%
ymm0
%
ymm1
%
ymm0
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
200
/
/
vcvttps2dq
%
ymm0
%
ymm1
.
byte
197
249
110
64
8
/
/
vmovd
0x8
(
%
rax
)
%
xmm0
.
byte
197
249
112
216
0
/
/
vpshufd
0x0
%
xmm0
%
xmm3
.
byte
196
226
97
64
193
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm0
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
226
97
64
201
/
/
vpmulld
%
xmm1
%
xmm3
%
xmm1
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
227
125
25
211
1
/
/
vextractf128
0x1
%
ymm2
%
xmm3
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
197
249
254
194
/
/
vpaddd
%
xmm2
%
xmm0
%
xmm0
.
byte
196
227
249
22
195
1
/
/
vpextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
199
/
/
vmovq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
122
126
4
248
/
/
vmovq
(
%
r8
%
r15
8
)
%
xmm0
.
byte
196
129
122
126
12
224
/
/
vmovq
(
%
r8
%
r12
8
)
%
xmm1
.
byte
197
113
108
208
/
/
vpunpcklqdq
%
xmm0
%
xmm1
%
xmm10
.
byte
196
193
122
126
12
216
/
/
vmovq
(
%
r8
%
rbx
8
)
%
xmm1
.
byte
196
129
122
126
20
240
/
/
vmovq
(
%
r8
%
r14
8
)
%
xmm2
.
byte
197
233
108
201
/
/
vpunpcklqdq
%
xmm1
%
xmm2
%
xmm1
.
byte
196
129
122
126
20
208
/
/
vmovq
(
%
r8
%
r10
8
)
%
xmm2
.
byte
196
129
122
126
28
216
/
/
vmovq
(
%
r8
%
r11
8
)
%
xmm3
.
byte
197
225
108
210
/
/
vpunpcklqdq
%
xmm2
%
xmm3
%
xmm2
.
byte
196
65
122
126
12
192
/
/
vmovq
(
%
r8
%
rax
8
)
%
xmm9
.
byte
196
129
122
126
28
200
/
/
vmovq
(
%
r8
%
r9
8
)
%
xmm3
.
byte
196
193
97
108
217
/
/
vpunpcklqdq
%
xmm9
%
xmm3
%
xmm3
.
byte
197
169
97
193
/
/
vpunpcklwd
%
xmm1
%
xmm10
%
xmm0
.
byte
197
169
105
201
/
/
vpunpckhwd
%
xmm1
%
xmm10
%
xmm1
.
byte
197
105
97
211
/
/
vpunpcklwd
%
xmm3
%
xmm2
%
xmm10
.
byte
197
233
105
211
/
/
vpunpckhwd
%
xmm3
%
xmm2
%
xmm2
.
byte
197
121
97
241
/
/
vpunpcklwd
%
xmm1
%
xmm0
%
xmm14
.
byte
197
121
105
201
/
/
vpunpckhwd
%
xmm1
%
xmm0
%
xmm9
.
byte
197
41
97
250
/
/
vpunpcklwd
%
xmm2
%
xmm10
%
xmm15
.
byte
197
41
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm10
%
xmm10
.
byte
196
193
9
108
199
/
/
vpunpcklqdq
%
xmm15
%
xmm14
%
xmm0
.
byte
196
226
121
51
208
/
/
vpmovzxwd
%
xmm0
%
xmm2
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
197
123
18
29
86
99
2
0
/
/
vmovddup
0x26356
(
%
rip
)
%
xmm11
#
3d768
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x151c
>
.
byte
196
193
121
219
219
/
/
vpand
%
xmm11
%
xmm0
%
xmm3
.
byte
196
98
121
24
37
112
82
2
0
/
/
vbroadcastss
0x25270
(
%
rip
)
%
xmm12
#
3c690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x444
>
.
byte
197
25
102
235
/
/
vpcmpgtd
%
xmm3
%
xmm12
%
xmm13
.
byte
196
193
105
219
203
/
/
vpand
%
xmm11
%
xmm2
%
xmm1
.
byte
197
25
102
193
/
/
vpcmpgtd
%
xmm1
%
xmm12
%
xmm8
.
byte
196
67
61
24
197
1
/
/
vinsertf128
0x1
%
xmm13
%
ymm8
%
ymm8
.
byte
197
123
18
45
53
99
2
0
/
/
vmovddup
0x26335
(
%
rip
)
%
xmm13
#
3d770
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1524
>
.
byte
196
193
105
219
213
/
/
vpand
%
xmm13
%
xmm2
%
xmm2
.
byte
197
233
114
242
16
/
/
vpslld
0x10
%
xmm2
%
xmm2
.
byte
197
241
114
241
13
/
/
vpslld
0xd
%
xmm1
%
xmm1
.
byte
197
233
235
201
/
/
vpor
%
xmm1
%
xmm2
%
xmm1
.
byte
196
193
121
219
197
/
/
vpand
%
xmm13
%
xmm0
%
xmm0
.
byte
197
249
114
240
16
/
/
vpslld
0x10
%
xmm0
%
xmm0
.
byte
197
233
114
243
13
/
/
vpslld
0xd
%
xmm3
%
xmm2
.
byte
197
249
235
194
/
/
vpor
%
xmm2
%
xmm0
%
xmm0
.
byte
196
226
121
24
29
42
82
2
0
/
/
vbroadcastss
0x2522a
(
%
rip
)
%
xmm3
#
3c694
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x448
>
.
byte
197
249
254
195
/
/
vpaddd
%
xmm3
%
xmm0
%
xmm0
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
227
125
74
196
128
/
/
vblendvps
%
ymm8
%
ymm4
%
ymm0
%
ymm0
.
byte
196
193
9
109
207
/
/
vpunpckhqdq
%
xmm15
%
xmm14
%
xmm1
.
byte
196
226
121
51
209
/
/
vpmovzxwd
%
xmm1
%
xmm2
.
byte
197
249
112
201
78
/
/
vpshufd
0x4e
%
xmm1
%
xmm1
.
byte
196
98
121
51
193
/
/
vpmovzxwd
%
xmm1
%
xmm8
.
byte
196
65
57
219
251
/
/
vpand
%
xmm11
%
xmm8
%
xmm15
.
byte
196
65
25
102
247
/
/
vpcmpgtd
%
xmm15
%
xmm12
%
xmm14
.
byte
196
193
105
219
203
/
/
vpand
%
xmm11
%
xmm2
%
xmm1
.
byte
197
153
102
249
/
/
vpcmpgtd
%
xmm1
%
xmm12
%
xmm7
.
byte
196
195
69
24
254
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm7
%
ymm7
.
byte
196
193
105
219
213
/
/
vpand
%
xmm13
%
xmm2
%
xmm2
.
byte
197
233
114
242
16
/
/
vpslld
0x10
%
xmm2
%
xmm2
.
byte
197
241
114
241
13
/
/
vpslld
0xd
%
xmm1
%
xmm1
.
byte
197
233
235
201
/
/
vpor
%
xmm1
%
xmm2
%
xmm1
.
byte
196
193
57
219
213
/
/
vpand
%
xmm13
%
xmm8
%
xmm2
.
byte
197
233
114
242
16
/
/
vpslld
0x10
%
xmm2
%
xmm2
.
byte
196
193
73
114
247
13
/
/
vpslld
0xd
%
xmm15
%
xmm6
.
byte
197
233
235
214
/
/
vpor
%
xmm6
%
xmm2
%
xmm2
.
byte
197
233
254
211
/
/
vpaddd
%
xmm3
%
xmm2
%
xmm2
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
117
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
196
227
117
74
204
112
/
/
vblendvps
%
ymm7
%
ymm4
%
ymm1
%
ymm1
.
byte
196
193
49
108
210
/
/
vpunpcklqdq
%
xmm10
%
xmm9
%
xmm2
.
byte
196
226
121
51
242
/
/
vpmovzxwd
%
xmm2
%
xmm6
.
byte
197
249
112
210
78
/
/
vpshufd
0x4e
%
xmm2
%
xmm2
.
byte
196
98
121
51
242
/
/
vpmovzxwd
%
xmm2
%
xmm14
.
byte
196
193
9
219
251
/
/
vpand
%
xmm11
%
xmm14
%
xmm7
.
byte
197
25
102
199
/
/
vpcmpgtd
%
xmm7
%
xmm12
%
xmm8
.
byte
196
193
73
219
211
/
/
vpand
%
xmm11
%
xmm6
%
xmm2
.
byte
197
25
102
250
/
/
vpcmpgtd
%
xmm2
%
xmm12
%
xmm15
.
byte
196
67
5
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm15
%
ymm8
.
byte
196
193
73
219
245
/
/
vpand
%
xmm13
%
xmm6
%
xmm6
.
byte
197
201
114
246
16
/
/
vpslld
0x10
%
xmm6
%
xmm6
.
byte
197
233
114
242
13
/
/
vpslld
0xd
%
xmm2
%
xmm2
.
byte
197
201
235
210
/
/
vpor
%
xmm2
%
xmm6
%
xmm2
.
byte
196
193
9
219
245
/
/
vpand
%
xmm13
%
xmm14
%
xmm6
.
byte
197
201
114
246
16
/
/
vpslld
0x10
%
xmm6
%
xmm6
.
byte
197
193
114
247
13
/
/
vpslld
0xd
%
xmm7
%
xmm7
.
byte
197
201
235
247
/
/
vpor
%
xmm7
%
xmm6
%
xmm6
.
byte
197
201
254
243
/
/
vpaddd
%
xmm3
%
xmm6
%
xmm6
.
byte
197
233
254
211
/
/
vpaddd
%
xmm3
%
xmm2
%
xmm2
.
byte
196
227
109
24
214
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm2
%
ymm2
.
byte
196
227
109
74
212
128
/
/
vblendvps
%
ymm8
%
ymm4
%
ymm2
%
ymm2
.
byte
196
193
49
109
242
/
/
vpunpckhqdq
%
xmm10
%
xmm9
%
xmm6
.
byte
196
226
121
51
254
/
/
vpmovzxwd
%
xmm6
%
xmm7
.
byte
197
249
112
246
78
/
/
vpshufd
0x4e
%
xmm6
%
xmm6
.
byte
196
98
121
51
214
/
/
vpmovzxwd
%
xmm6
%
xmm10
.
byte
196
65
41
219
195
/
/
vpand
%
xmm11
%
xmm10
%
xmm8
.
byte
196
65
25
102
200
/
/
vpcmpgtd
%
xmm8
%
xmm12
%
xmm9
.
byte
196
193
65
219
243
/
/
vpand
%
xmm11
%
xmm7
%
xmm6
.
byte
197
25
102
222
/
/
vpcmpgtd
%
xmm6
%
xmm12
%
xmm11
.
byte
196
67
37
24
201
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm11
%
ymm9
.
byte
196
193
65
219
253
/
/
vpand
%
xmm13
%
xmm7
%
xmm7
.
byte
196
65
41
219
213
/
/
vpand
%
xmm13
%
xmm10
%
xmm10
.
byte
197
193
114
247
16
/
/
vpslld
0x10
%
xmm7
%
xmm7
.
byte
197
201
114
246
13
/
/
vpslld
0xd
%
xmm6
%
xmm6
.
byte
197
193
235
246
/
/
vpor
%
xmm6
%
xmm7
%
xmm6
.
byte
196
193
65
114
242
16
/
/
vpslld
0x10
%
xmm10
%
xmm7
.
byte
196
193
81
114
240
13
/
/
vpslld
0xd
%
xmm8
%
xmm5
.
byte
197
193
235
237
/
/
vpor
%
xmm5
%
xmm7
%
xmm5
.
byte
197
209
254
235
/
/
vpaddd
%
xmm3
%
xmm5
%
xmm5
.
byte
197
201
254
219
/
/
vpaddd
%
xmm3
%
xmm6
%
xmm3
.
byte
196
227
101
24
221
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm3
%
ymm3
.
byte
196
227
101
74
220
144
/
/
vblendvps
%
ymm9
%
ymm4
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
100
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm7
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_f16_avx
.
globl
_sk_store_f16_avx
FUNCTION
(
_sk_store_f16_avx
)
_sk_store_f16_avx
:
.
byte
72
131
236
56
/
/
sub
0x38
%
rsp
.
byte
197
252
17
60
36
/
/
vmovups
%
ymm7
(
%
rsp
)
.
byte
197
252
17
116
36
224
/
/
vmovups
%
ymm6
-
0x20
(
%
rsp
)
.
byte
197
252
17
108
36
192
/
/
vmovups
%
ymm5
-
0x40
(
%
rsp
)
.
byte
197
252
17
100
36
160
/
/
vmovups
%
ymm4
-
0x60
(
%
rsp
)
.
byte
197
252
40
225
/
/
vmovaps
%
ymm1
%
ymm4
.
byte
196
98
125
24
5
72
80
2
0
/
/
vbroadcastss
0x25048
(
%
rip
)
%
ymm8
#
3c64c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x400
>
.
byte
196
65
124
84
216
/
/
vandps
%
ymm8
%
ymm0
%
ymm11
.
byte
196
98
125
24
21
134
80
2
0
/
/
vbroadcastss
0x25086
(
%
rip
)
%
ymm10
#
3c698
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x44c
>
.
byte
196
65
124
84
226
/
/
vandps
%
ymm10
%
ymm0
%
ymm12
.
byte
196
67
125
25
229
1
/
/
vextractf128
0x1
%
ymm12
%
xmm13
.
byte
196
98
121
24
13
118
80
2
0
/
/
vbroadcastss
0x25076
(
%
rip
)
%
xmm9
#
3c69c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x450
>
.
byte
196
65
49
102
245
/
/
vpcmpgtd
%
xmm13
%
xmm9
%
xmm14
.
byte
196
65
49
102
252
/
/
vpcmpgtd
%
xmm12
%
xmm9
%
xmm15
.
byte
196
67
5
24
246
1
/
/
vinsertf128
0x1
%
xmm14
%
ymm15
%
ymm14
.
byte
196
67
125
25
223
1
/
/
vextractf128
0x1
%
ymm11
%
xmm15
.
byte
196
193
1
114
215
16
/
/
vpsrld
0x10
%
xmm15
%
xmm15
.
byte
196
193
33
114
211
16
/
/
vpsrld
0x10
%
xmm11
%
xmm11
.
byte
196
193
17
114
213
13
/
/
vpsrld
0xd
%
xmm13
%
xmm13
.
byte
196
65
17
254
239
/
/
vpaddd
%
xmm15
%
xmm13
%
xmm13
.
byte
196
193
25
114
212
13
/
/
vpsrld
0xd
%
xmm12
%
xmm12
.
byte
196
65
25
254
227
/
/
vpaddd
%
xmm11
%
xmm12
%
xmm12
.
byte
196
98
121
24
29
57
80
2
0
/
/
vbroadcastss
0x25039
(
%
rip
)
%
xmm11
#
3c6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x454
>
.
byte
196
65
17
254
235
/
/
vpaddd
%
xmm11
%
xmm13
%
xmm13
.
byte
196
65
25
254
227
/
/
vpaddd
%
xmm11
%
xmm12
%
xmm12
.
byte
196
67
29
24
237
1
/
/
vinsertf128
0x1
%
xmm13
%
ymm12
%
ymm13
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
99
21
74
233
224
/
/
vblendvps
%
ymm14
%
ymm1
%
ymm13
%
ymm13
.
byte
196
65
92
84
242
/
/
vandps
%
ymm10
%
ymm4
%
ymm14
.
byte
196
67
125
25
247
1
/
/
vextractf128
0x1
%
ymm14
%
xmm15
.
byte
196
193
49
102
255
/
/
vpcmpgtd
%
xmm15
%
xmm9
%
xmm7
.
byte
196
65
49
102
230
/
/
vpcmpgtd
%
xmm14
%
xmm9
%
xmm12
.
byte
196
227
29
24
255
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm12
%
ymm7
.
byte
197
252
17
100
36
128
/
/
vmovups
%
ymm4
-
0x80
(
%
rsp
)
.
byte
196
65
92
84
224
/
/
vandps
%
ymm8
%
ymm4
%
ymm12
.
byte
196
99
125
25
230
1
/
/
vextractf128
0x1
%
ymm12
%
xmm6
.
byte
197
201
114
214
16
/
/
vpsrld
0x10
%
xmm6
%
xmm6
.
byte
196
193
81
114
215
13
/
/
vpsrld
0xd
%
xmm15
%
xmm5
.
byte
197
124
40
250
/
/
vmovaps
%
ymm2
%
ymm15
.
byte
197
209
254
238
/
/
vpaddd
%
xmm6
%
xmm5
%
xmm5
.
byte
196
193
73
114
212
16
/
/
vpsrld
0x10
%
xmm12
%
xmm6
.
byte
196
193
89
114
214
13
/
/
vpsrld
0xd
%
xmm14
%
xmm4
.
byte
197
217
254
230
/
/
vpaddd
%
xmm6
%
xmm4
%
xmm4
.
byte
196
193
81
254
235
/
/
vpaddd
%
xmm11
%
xmm5
%
xmm5
.
byte
196
193
89
254
227
/
/
vpaddd
%
xmm11
%
xmm4
%
xmm4
.
byte
196
227
93
24
229
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
99
93
74
241
112
/
/
vblendvps
%
ymm7
%
ymm1
%
ymm4
%
ymm14
.
byte
196
193
4
84
226
/
/
vandps
%
ymm10
%
ymm15
%
ymm4
.
byte
196
227
125
25
229
1
/
/
vextractf128
0x1
%
ymm4
%
xmm5
.
byte
197
177
102
245
/
/
vpcmpgtd
%
xmm5
%
xmm9
%
xmm6
.
byte
197
177
102
252
/
/
vpcmpgtd
%
xmm4
%
xmm9
%
xmm7
.
byte
196
227
69
24
246
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm7
%
ymm6
.
byte
196
193
4
84
248
/
/
vandps
%
ymm8
%
ymm15
%
ymm7
.
byte
196
227
125
25
250
1
/
/
vextractf128
0x1
%
ymm7
%
xmm2
.
byte
197
233
114
210
16
/
/
vpsrld
0x10
%
xmm2
%
xmm2
.
byte
197
209
114
213
13
/
/
vpsrld
0xd
%
xmm5
%
xmm5
.
byte
197
209
254
210
/
/
vpaddd
%
xmm2
%
xmm5
%
xmm2
.
byte
197
209
114
215
16
/
/
vpsrld
0x10
%
xmm7
%
xmm5
.
byte
197
217
114
212
13
/
/
vpsrld
0xd
%
xmm4
%
xmm4
.
byte
197
217
254
229
/
/
vpaddd
%
xmm5
%
xmm4
%
xmm4
.
byte
196
193
105
254
211
/
/
vpaddd
%
xmm11
%
xmm2
%
xmm2
.
byte
196
193
89
254
227
/
/
vpaddd
%
xmm11
%
xmm4
%
xmm4
.
byte
196
227
93
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm4
%
ymm2
.
byte
196
99
109
74
225
96
/
/
vblendvps
%
ymm6
%
ymm1
%
ymm2
%
ymm12
.
byte
196
193
100
84
226
/
/
vandps
%
ymm10
%
ymm3
%
ymm4
.
byte
196
227
125
25
229
1
/
/
vextractf128
0x1
%
ymm4
%
xmm5
.
byte
197
177
102
245
/
/
vpcmpgtd
%
xmm5
%
xmm9
%
xmm6
.
byte
197
177
102
252
/
/
vpcmpgtd
%
xmm4
%
xmm9
%
xmm7
.
byte
196
227
69
24
246
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm7
%
ymm6
.
byte
196
193
100
84
248
/
/
vandps
%
ymm8
%
ymm3
%
ymm7
.
byte
196
227
125
25
250
1
/
/
vextractf128
0x1
%
ymm7
%
xmm2
.
byte
197
233
114
210
16
/
/
vpsrld
0x10
%
xmm2
%
xmm2
.
byte
197
209
114
213
13
/
/
vpsrld
0xd
%
xmm5
%
xmm5
.
byte
197
209
254
210
/
/
vpaddd
%
xmm2
%
xmm5
%
xmm2
.
byte
197
209
114
215
16
/
/
vpsrld
0x10
%
xmm7
%
xmm5
.
byte
197
217
114
212
13
/
/
vpsrld
0xd
%
xmm4
%
xmm4
.
byte
197
217
254
229
/
/
vpaddd
%
xmm5
%
xmm4
%
xmm4
.
byte
196
193
105
254
211
/
/
vpaddd
%
xmm11
%
xmm2
%
xmm2
.
byte
196
193
89
254
227
/
/
vpaddd
%
xmm11
%
xmm4
%
xmm4
.
byte
196
227
93
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm4
%
ymm2
.
byte
196
227
109
74
201
96
/
/
vblendvps
%
ymm6
%
ymm1
%
ymm2
%
ymm1
.
byte
196
99
125
25
234
1
/
/
vextractf128
0x1
%
ymm13
%
xmm2
.
byte
196
226
17
43
210
/
/
vpackusdw
%
xmm2
%
xmm13
%
xmm2
.
byte
196
99
125
25
244
1
/
/
vextractf128
0x1
%
ymm14
%
xmm4
.
byte
196
226
9
43
228
/
/
vpackusdw
%
xmm4
%
xmm14
%
xmm4
.
byte
196
99
125
25
229
1
/
/
vextractf128
0x1
%
ymm12
%
xmm5
.
byte
196
226
25
43
245
/
/
vpackusdw
%
xmm5
%
xmm12
%
xmm6
.
byte
196
227
125
25
205
1
/
/
vextractf128
0x1
%
ymm1
%
xmm5
.
byte
196
226
113
43
205
/
/
vpackusdw
%
xmm5
%
xmm1
%
xmm1
.
byte
197
233
97
236
/
/
vpunpcklwd
%
xmm4
%
xmm2
%
xmm5
.
byte
197
233
105
212
/
/
vpunpckhwd
%
xmm4
%
xmm2
%
xmm2
.
byte
197
201
97
225
/
/
vpunpcklwd
%
xmm1
%
xmm6
%
xmm4
.
byte
197
201
105
201
/
/
vpunpckhwd
%
xmm1
%
xmm6
%
xmm1
.
byte
197
81
98
220
/
/
vpunpckldq
%
xmm4
%
xmm5
%
xmm11
.
byte
197
81
106
212
/
/
vpunpckhdq
%
xmm4
%
xmm5
%
xmm10
.
byte
197
105
98
201
/
/
vpunpckldq
%
xmm1
%
xmm2
%
xmm9
.
byte
197
105
106
193
/
/
vpunpckhdq
%
xmm1
%
xmm2
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
68
/
/
jne
17838
<
_sk_store_f16_avx
+
0x25c
>
.
byte
196
65
122
127
28
208
/
/
vmovdqu
%
xmm11
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
84
208
16
/
/
vmovdqu
%
xmm10
0x10
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
76
208
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r8
%
rdx
8
)
.
byte
196
65
122
127
68
208
48
/
/
vmovdqu
%
xmm8
0x30
(
%
r8
%
rdx
8
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
76
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm1
.
byte
197
124
41
250
/
/
vmovaps
%
ymm15
%
ymm2
.
byte
197
252
16
100
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm6
.
byte
197
252
16
60
36
/
/
vmovups
(
%
rsp
)
%
ymm7
.
byte
72
131
196
56
/
/
add
0x38
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
214
28
208
/
/
vmovq
%
xmm11
(
%
r8
%
rdx
8
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
203
/
/
je
1780f
<
_sk_store_f16_avx
+
0x233
>
.
byte
196
65
121
23
92
208
8
/
/
vmovhpd
%
xmm11
0x8
(
%
r8
%
rdx
8
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
190
/
/
jb
1780f
<
_sk_store_f16_avx
+
0x233
>
.
byte
196
65
121
214
84
208
16
/
/
vmovq
%
xmm10
0x10
(
%
r8
%
rdx
8
)
.
byte
116
181
/
/
je
1780f
<
_sk_store_f16_avx
+
0x233
>
.
byte
196
65
121
23
84
208
24
/
/
vmovhpd
%
xmm10
0x18
(
%
r8
%
rdx
8
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
168
/
/
jb
1780f
<
_sk_store_f16_avx
+
0x233
>
.
byte
196
65
121
214
76
208
32
/
/
vmovq
%
xmm9
0x20
(
%
r8
%
rdx
8
)
.
byte
116
159
/
/
je
1780f
<
_sk_store_f16_avx
+
0x233
>
.
byte
196
65
121
23
76
208
40
/
/
vmovhpd
%
xmm9
0x28
(
%
r8
%
rdx
8
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
146
/
/
jb
1780f
<
_sk_store_f16_avx
+
0x233
>
.
byte
196
65
121
214
68
208
48
/
/
vmovq
%
xmm8
0x30
(
%
r8
%
rdx
8
)
.
byte
235
137
/
/
jmp
1780f
<
_sk_store_f16_avx
+
0x233
>
HIDDEN
_sk_load_u16_be_avx
.
globl
_sk_load_u16_be_avx
FUNCTION
(
_sk_load_u16_be_avx
)
_sk_load_u16_be_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
12
1
0
0
/
/
jne
179b3
<
_sk_load_u16_be_avx
+
0x12d
>
.
byte
196
1
121
16
4
65
/
/
vmovupd
(
%
r9
%
r8
2
)
%
xmm8
.
byte
196
129
121
16
84
65
16
/
/
vmovupd
0x10
(
%
r9
%
r8
2
)
%
xmm2
.
byte
196
129
121
16
92
65
32
/
/
vmovupd
0x20
(
%
r9
%
r8
2
)
%
xmm3
.
byte
196
1
122
111
76
65
48
/
/
vmovdqu
0x30
(
%
r9
%
r8
2
)
%
xmm9
.
byte
197
185
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm8
%
xmm0
.
byte
197
185
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm8
%
xmm2
.
byte
196
193
97
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm3
%
xmm1
.
byte
196
193
97
105
217
/
/
vpunpckhwd
%
xmm9
%
xmm3
%
xmm3
.
byte
197
121
97
210
/
/
vpunpcklwd
%
xmm2
%
xmm0
%
xmm10
.
byte
197
121
105
194
/
/
vpunpckhwd
%
xmm2
%
xmm0
%
xmm8
.
byte
197
241
97
211
/
/
vpunpcklwd
%
xmm3
%
xmm1
%
xmm2
.
byte
197
113
105
219
/
/
vpunpckhwd
%
xmm3
%
xmm1
%
xmm11
.
byte
197
169
108
194
/
/
vpunpcklqdq
%
xmm2
%
xmm10
%
xmm0
.
byte
197
241
113
240
8
/
/
vpsllw
0x8
%
xmm0
%
xmm1
.
byte
197
249
113
208
8
/
/
vpsrlw
0x8
%
xmm0
%
xmm0
.
byte
197
241
235
192
/
/
vpor
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
120
76
2
0
/
/
vbroadcastss
0x24c78
(
%
rip
)
%
ymm9
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
169
109
202
/
/
vpunpckhqdq
%
xmm2
%
xmm10
%
xmm1
.
byte
197
233
113
241
8
/
/
vpsllw
0x8
%
xmm1
%
xmm2
.
byte
197
241
113
209
8
/
/
vpsrlw
0x8
%
xmm1
%
xmm1
.
byte
197
233
235
201
/
/
vpor
%
xmm1
%
xmm2
%
xmm1
.
byte
196
226
121
51
209
/
/
vpmovzxwd
%
xmm1
%
xmm2
.
byte
197
249
112
201
78
/
/
vpshufd
0x4e
%
xmm1
%
xmm1
.
byte
196
226
121
51
201
/
/
vpmovzxwd
%
xmm1
%
xmm1
.
byte
196
227
109
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
57
108
211
/
/
vpunpcklqdq
%
xmm11
%
xmm8
%
xmm2
.
byte
197
225
113
242
8
/
/
vpsllw
0x8
%
xmm2
%
xmm3
.
byte
197
233
113
210
8
/
/
vpsrlw
0x8
%
xmm2
%
xmm2
.
byte
197
225
235
210
/
/
vpor
%
xmm2
%
xmm3
%
xmm2
.
byte
196
226
121
51
218
/
/
vpmovzxwd
%
xmm2
%
xmm3
.
byte
197
249
112
210
78
/
/
vpshufd
0x4e
%
xmm2
%
xmm2
.
byte
196
226
121
51
210
/
/
vpmovzxwd
%
xmm2
%
xmm2
.
byte
196
227
101
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
196
193
57
109
219
/
/
vpunpckhqdq
%
xmm11
%
xmm8
%
xmm3
.
byte
197
185
113
243
8
/
/
vpsllw
0x8
%
xmm3
%
xmm8
.
byte
197
225
113
211
8
/
/
vpsrlw
0x8
%
xmm3
%
xmm3
.
byte
197
185
235
219
/
/
vpor
%
xmm3
%
xmm8
%
xmm3
.
byte
196
98
121
51
195
/
/
vpmovzxwd
%
xmm3
%
xmm8
.
byte
197
249
112
219
78
/
/
vpshufd
0x4e
%
xmm3
%
xmm3
.
byte
196
226
121
51
219
/
/
vpmovzxwd
%
xmm3
%
xmm3
.
byte
196
227
61
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm8
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
196
193
100
89
217
/
/
vmulps
%
ymm9
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
123
16
4
65
/
/
vmovsd
(
%
r9
%
r8
2
)
%
xmm8
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
85
/
/
je
17a19
<
_sk_load_u16_be_avx
+
0x193
>
.
byte
196
1
57
22
68
65
8
/
/
vmovhpd
0x8
(
%
r9
%
r8
2
)
%
xmm8
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
72
/
/
jb
17a19
<
_sk_load_u16_be_avx
+
0x193
>
.
byte
196
129
123
16
84
65
16
/
/
vmovsd
0x10
(
%
r9
%
r8
2
)
%
xmm2
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
116
72
/
/
je
17a26
<
_sk_load_u16_be_avx
+
0x1a0
>
.
byte
196
129
105
22
84
65
24
/
/
vmovhpd
0x18
(
%
r9
%
r8
2
)
%
xmm2
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
59
/
/
jb
17a26
<
_sk_load_u16_be_avx
+
0x1a0
>
.
byte
196
129
123
16
92
65
32
/
/
vmovsd
0x20
(
%
r9
%
r8
2
)
%
xmm3
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
15
132
198
254
255
255
/
/
je
178c2
<
_sk_load_u16_be_avx
+
0x3c
>
.
byte
196
129
97
22
92
65
40
/
/
vmovhpd
0x28
(
%
r9
%
r8
2
)
%
xmm3
%
xmm3
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
130
181
254
255
255
/
/
jb
178c2
<
_sk_load_u16_be_avx
+
0x3c
>
.
byte
196
1
122
126
76
65
48
/
/
vmovq
0x30
(
%
r9
%
r8
2
)
%
xmm9
.
byte
233
169
254
255
255
/
/
jmpq
178c2
<
_sk_load_u16_be_avx
+
0x3c
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
87
210
/
/
vxorpd
%
xmm2
%
xmm2
%
xmm2
.
byte
233
156
254
255
255
/
/
jmpq
178c2
<
_sk_load_u16_be_avx
+
0x3c
>
.
byte
197
225
87
219
/
/
vxorpd
%
xmm3
%
xmm3
%
xmm3
.
byte
233
147
254
255
255
/
/
jmpq
178c2
<
_sk_load_u16_be_avx
+
0x3c
>
HIDDEN
_sk_load_rgb_u16_be_avx
.
globl
_sk_load_rgb_u16_be_avx
FUNCTION
(
_sk_load_rgb_u16_be_avx
)
_sk_load_rgb_u16_be_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
253
0
0
0
/
/
jne
17b49
<
_sk_load_rgb_u16_be_avx
+
0x11a
>
.
byte
196
1
122
111
28
65
/
/
vmovdqu
(
%
r9
%
r8
2
)
%
xmm11
.
byte
196
129
122
111
92
65
12
/
/
vmovdqu
0xc
(
%
r9
%
r8
2
)
%
xmm3
.
byte
196
129
122
111
84
65
24
/
/
vmovdqu
0x18
(
%
r9
%
r8
2
)
%
xmm2
.
byte
196
129
122
111
68
65
32
/
/
vmovdqu
0x20
(
%
r9
%
r8
2
)
%
xmm0
.
byte
197
249
115
216
4
/
/
vpsrldq
0x4
%
xmm0
%
xmm0
.
byte
196
193
57
115
219
6
/
/
vpsrldq
0x6
%
xmm11
%
xmm8
.
byte
197
169
115
219
6
/
/
vpsrldq
0x6
%
xmm3
%
xmm10
.
byte
197
241
115
218
6
/
/
vpsrldq
0x6
%
xmm2
%
xmm1
.
byte
197
177
115
216
6
/
/
vpsrldq
0x6
%
xmm0
%
xmm9
.
byte
196
193
113
97
201
/
/
vpunpcklwd
%
xmm9
%
xmm1
%
xmm1
.
byte
197
233
97
192
/
/
vpunpcklwd
%
xmm0
%
xmm2
%
xmm0
.
byte
196
193
57
97
210
/
/
vpunpcklwd
%
xmm10
%
xmm8
%
xmm2
.
byte
197
161
97
219
/
/
vpunpcklwd
%
xmm3
%
xmm11
%
xmm3
.
byte
197
97
97
194
/
/
vpunpcklwd
%
xmm2
%
xmm3
%
xmm8
.
byte
197
225
105
210
/
/
vpunpckhwd
%
xmm2
%
xmm3
%
xmm2
.
byte
197
249
97
217
/
/
vpunpcklwd
%
xmm1
%
xmm0
%
xmm3
.
byte
197
249
105
193
/
/
vpunpckhwd
%
xmm1
%
xmm0
%
xmm0
.
byte
197
233
108
208
/
/
vpunpcklqdq
%
xmm0
%
xmm2
%
xmm2
.
byte
197
185
108
195
/
/
vpunpcklqdq
%
xmm3
%
xmm8
%
xmm0
.
byte
197
241
113
240
8
/
/
vpsllw
0x8
%
xmm0
%
xmm1
.
byte
197
249
113
208
8
/
/
vpsrlw
0x8
%
xmm0
%
xmm0
.
byte
197
241
235
192
/
/
vpor
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
121
51
200
/
/
vpmovzxwd
%
xmm0
%
xmm1
.
byte
197
249
112
192
78
/
/
vpshufd
0x4e
%
xmm0
%
xmm0
.
byte
196
226
121
51
192
/
/
vpmovzxwd
%
xmm0
%
xmm0
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
181
74
2
0
/
/
vbroadcastss
0x24ab5
(
%
rip
)
%
ymm9
#
3c590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x344
>
.
byte
196
193
124
89
193
/
/
vmulps
%
ymm9
%
ymm0
%
ymm0
.
byte
197
185
109
203
/
/
vpunpckhqdq
%
xmm3
%
xmm8
%
xmm1
.
byte
197
225
113
241
8
/
/
vpsllw
0x8
%
xmm1
%
xmm3
.
byte
197
241
113
209
8
/
/
vpsrlw
0x8
%
xmm1
%
xmm1
.
byte
197
225
235
201
/
/
vpor
%
xmm1
%
xmm3
%
xmm1
.
byte
196
226
121
51
217
/
/
vpmovzxwd
%
xmm1
%
xmm3
.
byte
197
249
112
201
78
/
/
vpshufd
0x4e
%
xmm1
%
xmm1
.
byte
196
226
121
51
201
/
/
vpmovzxwd
%
xmm1
%
xmm1
.
byte
196
227
101
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
196
193
116
89
201
/
/
vmulps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
225
113
242
8
/
/
vpsllw
0x8
%
xmm2
%
xmm3
.
byte
197
233
113
210
8
/
/
vpsrlw
0x8
%
xmm2
%
xmm2
.
byte
197
225
235
210
/
/
vpor
%
xmm2
%
xmm3
%
xmm2
.
byte
196
226
121
51
218
/
/
vpmovzxwd
%
xmm2
%
xmm3
.
byte
197
249
112
210
78
/
/
vpshufd
0x4e
%
xmm2
%
xmm2
.
byte
196
226
121
51
210
/
/
vpmovzxwd
%
xmm2
%
xmm2
.
byte
196
227
101
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
193
108
89
209
/
/
vmulps
%
ymm9
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
181
73
2
0
/
/
vbroadcastss
0x249b5
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
110
4
65
/
/
vmovd
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
92
65
4
2
/
/
vpinsrw
0x2
0x4
(
%
r9
%
r8
2
)
%
xmm0
%
xmm11
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
31
/
/
jne
17b81
<
_sk_load_rgb_u16_be_avx
+
0x152
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
233
0
255
255
255
/
/
jmpq
17a81
<
_sk_load_rgb_u16_be_avx
+
0x52
>
.
byte
196
129
121
110
68
65
6
/
/
vmovd
0x6
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
68
65
10
2
/
/
vpinsrw
0x2
0xa
(
%
r9
%
r8
2
)
%
xmm0
%
xmm8
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
48
/
/
jb
17bcb
<
_sk_load_rgb_u16_be_avx
+
0x19c
>
.
byte
196
129
121
110
68
65
12
/
/
vmovd
0xc
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
92
65
16
2
/
/
vpinsrw
0x2
0x10
(
%
r9
%
r8
2
)
%
xmm0
%
xmm3
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
117
48
/
/
jne
17be5
<
_sk_load_rgb_u16_be_avx
+
0x1b6
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
233
182
254
255
255
/
/
jmpq
17a81
<
_sk_load_rgb_u16_be_avx
+
0x52
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
196
65
41
239
210
/
/
vpxor
%
xmm10
%
xmm10
%
xmm10
.
byte
197
225
239
219
/
/
vpxor
%
xmm3
%
xmm3
%
xmm3
.
byte
233
156
254
255
255
/
/
jmpq
17a81
<
_sk_load_rgb_u16_be_avx
+
0x52
>
.
byte
196
129
121
110
68
65
18
/
/
vmovd
0x12
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
1
121
196
84
65
22
2
/
/
vpinsrw
0x2
0x16
(
%
r9
%
r8
2
)
%
xmm0
%
xmm10
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
39
/
/
jb
17c26
<
_sk_load_rgb_u16_be_avx
+
0x1f7
>
.
byte
196
129
121
110
68
65
24
/
/
vmovd
0x18
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
84
65
28
2
/
/
vpinsrw
0x2
0x1c
(
%
r9
%
r8
2
)
%
xmm0
%
xmm2
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
117
30
/
/
jne
17c37
<
_sk_load_rgb_u16_be_avx
+
0x208
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
233
91
254
255
255
/
/
jmpq
17a81
<
_sk_load_rgb_u16_be_avx
+
0x52
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
197
241
239
201
/
/
vpxor
%
xmm1
%
xmm1
%
xmm1
.
byte
197
233
239
210
/
/
vpxor
%
xmm2
%
xmm2
%
xmm2
.
byte
233
74
254
255
255
/
/
jmpq
17a81
<
_sk_load_rgb_u16_be_avx
+
0x52
>
.
byte
196
129
121
110
68
65
30
/
/
vmovd
0x1e
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
76
65
34
2
/
/
vpinsrw
0x2
0x22
(
%
r9
%
r8
2
)
%
xmm0
%
xmm1
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
25
/
/
jb
17c6a
<
_sk_load_rgb_u16_be_avx
+
0x23b
>
.
byte
196
129
121
110
68
65
36
/
/
vmovd
0x24
(
%
r9
%
r8
2
)
%
xmm0
.
byte
196
65
49
239
201
/
/
vpxor
%
xmm9
%
xmm9
%
xmm9
.
byte
196
129
121
196
68
65
40
2
/
/
vpinsrw
0x2
0x28
(
%
r9
%
r8
2
)
%
xmm0
%
xmm0
.
byte
233
23
254
255
255
/
/
jmpq
17a81
<
_sk_load_rgb_u16_be_avx
+
0x52
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
233
14
254
255
255
/
/
jmpq
17a81
<
_sk_load_rgb_u16_be_avx
+
0x52
>
HIDDEN
_sk_store_u16_be_avx
.
globl
_sk_store_u16_be_avx
FUNCTION
(
_sk_store_u16_be_avx
)
_sk_store_u16_be_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
95
200
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
95
72
2
0
/
/
vbroadcastss
0x2485f
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
65
52
93
202
/
/
vminps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
29
125
73
2
0
/
/
vbroadcastss
0x2497d
(
%
rip
)
%
ymm11
#
3c628
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3dc
>
.
byte
196
65
52
89
203
/
/
vmulps
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
125
91
201
/
/
vcvtps2dq
%
ymm9
%
ymm9
.
byte
196
67
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm12
.
byte
196
66
49
43
204
/
/
vpackusdw
%
xmm12
%
xmm9
%
xmm9
.
byte
196
193
25
113
241
8
/
/
vpsllw
0x8
%
xmm9
%
xmm12
.
byte
196
193
49
113
209
8
/
/
vpsrlw
0x8
%
xmm9
%
xmm9
.
byte
196
65
25
235
201
/
/
vpor
%
xmm9
%
xmm12
%
xmm9
.
byte
197
60
95
225
/
/
vmaxps
%
ymm1
%
ymm8
%
ymm12
.
byte
196
65
28
93
226
/
/
vminps
%
ymm10
%
ymm12
%
ymm12
.
byte
196
65
28
89
227
/
/
vmulps
%
ymm11
%
ymm12
%
ymm12
.
byte
196
65
125
91
228
/
/
vcvtps2dq
%
ymm12
%
ymm12
.
byte
196
67
125
25
229
1
/
/
vextractf128
0x1
%
ymm12
%
xmm13
.
byte
196
66
25
43
229
/
/
vpackusdw
%
xmm13
%
xmm12
%
xmm12
.
byte
196
193
17
113
244
8
/
/
vpsllw
0x8
%
xmm12
%
xmm13
.
byte
196
193
25
113
212
8
/
/
vpsrlw
0x8
%
xmm12
%
xmm12
.
byte
196
65
17
235
228
/
/
vpor
%
xmm12
%
xmm13
%
xmm12
.
byte
197
60
95
234
/
/
vmaxps
%
ymm2
%
ymm8
%
ymm13
.
byte
196
65
20
93
234
/
/
vminps
%
ymm10
%
ymm13
%
ymm13
.
byte
196
65
20
89
235
/
/
vmulps
%
ymm11
%
ymm13
%
ymm13
.
byte
196
65
125
91
237
/
/
vcvtps2dq
%
ymm13
%
ymm13
.
byte
196
67
125
25
238
1
/
/
vextractf128
0x1
%
ymm13
%
xmm14
.
byte
196
66
17
43
238
/
/
vpackusdw
%
xmm14
%
xmm13
%
xmm13
.
byte
196
193
9
113
245
8
/
/
vpsllw
0x8
%
xmm13
%
xmm14
.
byte
196
193
17
113
213
8
/
/
vpsrlw
0x8
%
xmm13
%
xmm13
.
byte
196
65
9
235
237
/
/
vpor
%
xmm13
%
xmm14
%
xmm13
.
byte
197
60
95
195
/
/
vmaxps
%
ymm3
%
ymm8
%
ymm8
.
byte
196
65
60
93
194
/
/
vminps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
60
89
195
/
/
vmulps
%
ymm11
%
ymm8
%
ymm8
.
byte
196
65
125
91
192
/
/
vcvtps2dq
%
ymm8
%
ymm8
.
byte
196
67
125
25
194
1
/
/
vextractf128
0x1
%
ymm8
%
xmm10
.
byte
196
66
57
43
194
/
/
vpackusdw
%
xmm10
%
xmm8
%
xmm8
.
byte
196
193
41
113
240
8
/
/
vpsllw
0x8
%
xmm8
%
xmm10
.
byte
196
193
57
113
208
8
/
/
vpsrlw
0x8
%
xmm8
%
xmm8
.
byte
196
65
41
235
192
/
/
vpor
%
xmm8
%
xmm10
%
xmm8
.
byte
196
65
49
97
212
/
/
vpunpcklwd
%
xmm12
%
xmm9
%
xmm10
.
byte
196
65
49
105
228
/
/
vpunpckhwd
%
xmm12
%
xmm9
%
xmm12
.
byte
196
65
17
97
200
/
/
vpunpcklwd
%
xmm8
%
xmm13
%
xmm9
.
byte
196
65
17
105
192
/
/
vpunpckhwd
%
xmm8
%
xmm13
%
xmm8
.
byte
196
65
41
98
217
/
/
vpunpckldq
%
xmm9
%
xmm10
%
xmm11
.
byte
196
65
41
106
209
/
/
vpunpckhdq
%
xmm9
%
xmm10
%
xmm10
.
byte
196
65
25
98
200
/
/
vpunpckldq
%
xmm8
%
xmm12
%
xmm9
.
byte
196
65
25
106
192
/
/
vpunpckhdq
%
xmm8
%
xmm12
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
31
/
/
jne
17daa
<
_sk_store_u16_be_avx
+
0x137
>
.
byte
196
1
122
127
28
65
/
/
vmovdqu
%
xmm11
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
84
65
16
/
/
vmovdqu
%
xmm10
0x10
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
76
65
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r9
%
r8
2
)
.
byte
196
1
122
127
68
65
48
/
/
vmovdqu
%
xmm8
0x30
(
%
r9
%
r8
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
121
214
28
65
/
/
vmovq
%
xmm11
(
%
r9
%
r8
2
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
17da6
<
_sk_store_u16_be_avx
+
0x133
>
.
byte
196
1
121
23
92
65
8
/
/
vmovhpd
%
xmm11
0x8
(
%
r9
%
r8
2
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
17da6
<
_sk_store_u16_be_avx
+
0x133
>
.
byte
196
1
121
214
84
65
16
/
/
vmovq
%
xmm10
0x10
(
%
r9
%
r8
2
)
.
byte
116
218
/
/
je
17da6
<
_sk_store_u16_be_avx
+
0x133
>
.
byte
196
1
121
23
84
65
24
/
/
vmovhpd
%
xmm10
0x18
(
%
r9
%
r8
2
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
17da6
<
_sk_store_u16_be_avx
+
0x133
>
.
byte
196
1
121
214
76
65
32
/
/
vmovq
%
xmm9
0x20
(
%
r9
%
r8
2
)
.
byte
116
196
/
/
je
17da6
<
_sk_store_u16_be_avx
+
0x133
>
.
byte
196
1
121
23
76
65
40
/
/
vmovhpd
%
xmm9
0x28
(
%
r9
%
r8
2
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
183
/
/
jb
17da6
<
_sk_store_u16_be_avx
+
0x133
>
.
byte
196
1
121
214
68
65
48
/
/
vmovq
%
xmm8
0x30
(
%
r9
%
r8
2
)
.
byte
235
174
/
/
jmp
17da6
<
_sk_store_u16_be_avx
+
0x133
>
HIDDEN
_sk_load_f32_avx
.
globl
_sk_load_f32_avx
FUNCTION
(
_sk_load_f32_avx
)
_sk_load_f32_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
253
87
192
/
/
vxorpd
%
ymm0
%
ymm0
%
ymm0
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
135
91
1
0
0
/
/
ja
17f63
<
_sk_load_f32_avx
+
0x16b
>
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
197
253
17
100
36
128
/
/
vmovupd
%
ymm4
-
0x80
(
%
rsp
)
.
byte
197
253
17
108
36
160
/
/
vmovupd
%
ymm5
-
0x60
(
%
rsp
)
.
byte
197
253
17
116
36
192
/
/
vmovupd
%
ymm6
-
0x40
(
%
rsp
)
.
byte
197
253
17
124
36
224
/
/
vmovupd
%
ymm7
-
0x20
(
%
rsp
)
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
76
141
21
50
1
0
0
/
/
lea
0x132
(
%
rip
)
%
r10
#
17f74
<
_sk_load_f32_avx
+
0x17c
>
.
byte
73
99
4
186
/
/
movslq
(
%
r10
%
rdi
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
197
221
87
228
/
/
vxorpd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
237
87
210
/
/
vxorpd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
37
87
219
/
/
vxorpd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
29
87
228
/
/
vxorpd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
13
87
246
/
/
vxorpd
%
ymm14
%
ymm14
%
ymm14
.
byte
197
205
87
246
/
/
vxorpd
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
5
87
255
/
/
vxorpd
%
ymm15
%
ymm15
%
ymm15
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
196
65
21
87
237
/
/
vxorpd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
16
68
129
112
/
/
vmovupd
0x70
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
6
192
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm0
.
byte
196
129
121
16
76
129
96
/
/
vmovupd
0x60
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
225
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm4
.
byte
196
129
121
16
76
129
80
/
/
vmovupd
0x50
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
209
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm2
.
byte
197
125
40
204
/
/
vmovapd
%
ymm4
%
ymm9
.
byte
196
129
121
16
76
129
64
/
/
vmovupd
0x40
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
99
125
6
193
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm8
.
byte
196
65
125
40
217
/
/
vmovapd
%
ymm9
%
ymm11
.
byte
197
125
40
226
/
/
vmovapd
%
ymm2
%
ymm12
.
byte
196
129
121
16
76
129
48
/
/
vmovupd
0x30
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
117
13
192
12
/
/
vblendpd
0xc
%
ymm0
%
ymm1
%
ymm0
.
byte
196
65
125
40
243
/
/
vmovapd
%
ymm11
%
ymm14
.
byte
197
125
41
230
/
/
vmovapd
%
ymm12
%
ymm6
.
byte
196
65
125
40
248
/
/
vmovapd
%
ymm8
%
ymm15
.
byte
196
129
121
16
76
129
32
/
/
vmovupd
0x20
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
195
117
13
206
12
/
/
vblendpd
0xc
%
ymm14
%
ymm1
%
ymm1
.
byte
197
125
40
238
/
/
vmovapd
%
ymm6
%
ymm13
.
byte
197
125
41
253
/
/
vmovapd
%
ymm15
%
ymm5
.
byte
196
129
121
16
84
129
16
/
/
vmovupd
0x10
(
%
r9
%
r8
4
)
%
xmm2
.
byte
196
67
109
13
213
12
/
/
vblendpd
0xc
%
ymm13
%
ymm2
%
ymm10
.
byte
197
253
40
217
/
/
vmovapd
%
ymm1
%
ymm3
.
byte
197
253
40
253
/
/
vmovapd
%
ymm5
%
ymm7
.
byte
196
129
121
16
12
129
/
/
vmovupd
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
117
13
207
12
/
/
vblendpd
0xc
%
ymm7
%
ymm1
%
ymm1
.
byte
197
252
16
124
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm7
.
byte
197
252
16
116
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
16
108
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm5
.
byte
197
252
16
100
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm4
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
196
193
116
20
210
/
/
vunpcklps
%
ymm10
%
ymm1
%
ymm2
.
byte
196
65
116
21
194
/
/
vunpckhps
%
ymm10
%
ymm1
%
ymm8
.
byte
197
228
20
200
/
/
vunpcklps
%
ymm0
%
ymm3
%
ymm1
.
byte
197
228
21
216
/
/
vunpckhps
%
ymm0
%
ymm3
%
ymm3
.
byte
197
237
20
193
/
/
vunpcklpd
%
ymm1
%
ymm2
%
ymm0
.
byte
197
237
21
201
/
/
vunpckhpd
%
ymm1
%
ymm2
%
ymm1
.
byte
197
189
20
211
/
/
vunpcklpd
%
ymm3
%
ymm8
%
ymm2
.
byte
197
189
21
219
/
/
vunpckhpd
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
235
203
/
/
jmp
17f3d
<
_sk_load_f32_avx
+
0x145
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
27
255
/
/
sbb
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
161
255
255
255
140
/
/
jmpq
*
-
0x73000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
119
255
/
/
pushq
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
70
255
/
/
incl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
53
255
255
255
40
/
/
pushq
0x28ffffff
(
%
rip
)
#
29017f90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x28fdbd44
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_f32_dst_avx
.
globl
_sk_load_f32_dst_avx
FUNCTION
(
_sk_load_f32_dst_avx
)
_sk_load_f32_dst_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
221
87
228
/
/
vxorpd
%
ymm4
%
ymm4
%
ymm4
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
15
135
91
1
0
0
/
/
ja
180ff
<
_sk_load_f32_dst_avx
+
0x16b
>
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
197
253
17
68
36
128
/
/
vmovupd
%
ymm0
-
0x80
(
%
rsp
)
.
byte
197
253
17
76
36
160
/
/
vmovupd
%
ymm1
-
0x60
(
%
rsp
)
.
byte
197
253
17
84
36
192
/
/
vmovupd
%
ymm2
-
0x40
(
%
rsp
)
.
byte
197
253
17
92
36
224
/
/
vmovupd
%
ymm3
-
0x20
(
%
rsp
)
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
76
141
21
50
1
0
0
/
/
lea
0x132
(
%
rip
)
%
r10
#
18110
<
_sk_load_f32_dst_avx
+
0x17c
>
.
byte
73
99
4
186
/
/
movslq
(
%
r10
%
rdi
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
197
253
87
192
/
/
vxorpd
%
ymm0
%
ymm0
%
ymm0
.
byte
196
65
53
87
201
/
/
vxorpd
%
ymm9
%
ymm9
%
ymm9
.
byte
197
205
87
246
/
/
vxorpd
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
37
87
219
/
/
vxorpd
%
ymm11
%
ymm11
%
ymm11
.
byte
196
65
29
87
228
/
/
vxorpd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
65
61
87
192
/
/
vxorpd
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
13
87
246
/
/
vxorpd
%
ymm14
%
ymm14
%
ymm14
.
byte
197
237
87
210
/
/
vxorpd
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
5
87
255
/
/
vxorpd
%
ymm15
%
ymm15
%
ymm15
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
65
21
87
237
/
/
vxorpd
%
ymm13
%
ymm13
%
ymm13
.
byte
197
245
87
201
/
/
vxorpd
%
ymm1
%
ymm1
%
ymm1
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
229
87
219
/
/
vxorpd
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
129
121
16
68
129
112
/
/
vmovupd
0x70
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
6
224
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm4
.
byte
196
129
121
16
68
129
96
/
/
vmovupd
0x60
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
6
192
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm0
.
byte
196
129
121
16
76
129
80
/
/
vmovupd
0x50
(
%
r9
%
r8
4
)
%
xmm1
.
byte
196
227
125
6
241
40
/
/
vperm2f128
0x28
%
ymm1
%
ymm0
%
ymm6
.
byte
197
125
40
200
/
/
vmovapd
%
ymm0
%
ymm9
.
byte
196
129
121
16
68
129
64
/
/
vmovupd
0x40
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
99
125
6
192
40
/
/
vperm2f128
0x28
%
ymm0
%
ymm0
%
ymm8
.
byte
196
65
125
40
217
/
/
vmovapd
%
ymm9
%
ymm11
.
byte
197
125
40
230
/
/
vmovapd
%
ymm6
%
ymm12
.
byte
196
129
121
16
68
129
48
/
/
vmovupd
0x30
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
13
228
12
/
/
vblendpd
0xc
%
ymm4
%
ymm0
%
ymm4
.
byte
196
65
125
40
243
/
/
vmovapd
%
ymm11
%
ymm14
.
byte
197
125
41
226
/
/
vmovapd
%
ymm12
%
ymm2
.
byte
196
65
125
40
248
/
/
vmovapd
%
ymm8
%
ymm15
.
byte
196
129
121
16
68
129
32
/
/
vmovupd
0x20
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
195
125
13
238
12
/
/
vblendpd
0xc
%
ymm14
%
ymm0
%
ymm5
.
byte
197
125
40
234
/
/
vmovapd
%
ymm2
%
ymm13
.
byte
197
125
41
249
/
/
vmovapd
%
ymm15
%
ymm1
.
byte
196
129
121
16
68
129
16
/
/
vmovupd
0x10
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
67
125
13
213
12
/
/
vblendpd
0xc
%
ymm13
%
ymm0
%
ymm10
.
byte
197
253
40
253
/
/
vmovapd
%
ymm5
%
ymm7
.
byte
197
253
40
217
/
/
vmovapd
%
ymm1
%
ymm3
.
byte
196
129
121
16
4
129
/
/
vmovupd
(
%
r9
%
r8
4
)
%
xmm0
.
byte
196
227
125
13
235
12
/
/
vblendpd
0xc
%
ymm3
%
ymm0
%
ymm5
.
byte
197
252
16
92
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm3
.
byte
197
252
16
84
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm2
.
byte
197
252
16
76
36
160
/
/
vmovups
-
0x60
(
%
rsp
)
%
ymm1
.
byte
197
252
16
68
36
128
/
/
vmovups
-
0x80
(
%
rsp
)
%
ymm0
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
196
193
84
20
242
/
/
vunpcklps
%
ymm10
%
ymm5
%
ymm6
.
byte
196
65
84
21
194
/
/
vunpckhps
%
ymm10
%
ymm5
%
ymm8
.
byte
197
196
20
236
/
/
vunpcklps
%
ymm4
%
ymm7
%
ymm5
.
byte
197
196
21
252
/
/
vunpckhps
%
ymm4
%
ymm7
%
ymm7
.
byte
197
205
20
229
/
/
vunpcklpd
%
ymm5
%
ymm6
%
ymm4
.
byte
197
205
21
237
/
/
vunpckhpd
%
ymm5
%
ymm6
%
ymm5
.
byte
197
189
20
247
/
/
vunpcklpd
%
ymm7
%
ymm8
%
ymm6
.
byte
197
189
21
255
/
/
vunpckhpd
%
ymm7
%
ymm8
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
197
197
87
255
/
/
vxorpd
%
ymm7
%
ymm7
%
ymm7
.
byte
196
65
45
87
210
/
/
vxorpd
%
ymm10
%
ymm10
%
ymm10
.
byte
197
213
87
237
/
/
vxorpd
%
ymm5
%
ymm5
%
ymm5
.
byte
235
203
/
/
jmp
180d9
<
_sk_load_f32_dst_avx
+
0x145
>
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
27
255
/
/
sbb
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
161
255
255
255
140
/
/
jmpq
*
-
0x73000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
119
255
/
/
pushq
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
70
255
/
/
incl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
53
255
255
255
40
/
/
pushq
0x28ffffff
(
%
rip
)
#
2901812c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x28fdbee0
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_f32_avx
.
globl
_sk_store_f32_avx
FUNCTION
(
_sk_store_f32_avx
)
_sk_store_f32_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
197
124
20
193
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm8
.
byte
197
124
21
217
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm11
.
byte
197
108
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm9
.
byte
197
108
21
227
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm12
.
byte
196
65
61
20
209
/
/
vunpcklpd
%
ymm9
%
ymm8
%
ymm10
.
byte
196
65
61
21
201
/
/
vunpckhpd
%
ymm9
%
ymm8
%
ymm9
.
byte
196
65
37
20
196
/
/
vunpcklpd
%
ymm12
%
ymm11
%
ymm8
.
byte
196
65
37
21
220
/
/
vunpckhpd
%
ymm12
%
ymm11
%
ymm11
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
55
/
/
jne
181a9
<
_sk_store_f32_avx
+
0x79
>
.
byte
196
67
45
24
225
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm12
.
byte
196
67
61
24
235
1
/
/
vinsertf128
0x1
%
xmm11
%
ymm8
%
ymm13
.
byte
196
67
45
6
201
49
/
/
vperm2f128
0x31
%
ymm9
%
ymm10
%
ymm9
.
byte
196
67
61
6
195
49
/
/
vperm2f128
0x31
%
ymm11
%
ymm8
%
ymm8
.
byte
196
1
124
17
36
129
/
/
vmovups
%
ymm12
(
%
r9
%
r8
4
)
.
byte
196
1
124
17
108
129
32
/
/
vmovups
%
ymm13
0x20
(
%
r9
%
r8
4
)
.
byte
196
1
125
17
76
129
64
/
/
vmovupd
%
ymm9
0x40
(
%
r9
%
r8
4
)
.
byte
196
1
125
17
68
129
96
/
/
vmovupd
%
ymm8
0x60
(
%
r9
%
r8
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
1
121
17
20
129
/
/
vmovupd
%
xmm10
(
%
r9
%
r8
4
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
181a5
<
_sk_store_f32_avx
+
0x75
>
.
byte
196
1
121
17
76
129
16
/
/
vmovupd
%
xmm9
0x10
(
%
r9
%
r8
4
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
181a5
<
_sk_store_f32_avx
+
0x75
>
.
byte
196
1
121
17
68
129
32
/
/
vmovupd
%
xmm8
0x20
(
%
r9
%
r8
4
)
.
byte
116
218
/
/
je
181a5
<
_sk_store_f32_avx
+
0x75
>
.
byte
196
1
121
17
92
129
48
/
/
vmovupd
%
xmm11
0x30
(
%
r9
%
r8
4
)
.
byte
72
131
255
5
/
/
cmp
0x5
%
rdi
.
byte
114
205
/
/
jb
181a5
<
_sk_store_f32_avx
+
0x75
>
.
byte
196
3
125
25
84
129
64
1
/
/
vextractf128
0x1
%
ymm10
0x40
(
%
r9
%
r8
4
)
.
byte
116
195
/
/
je
181a5
<
_sk_store_f32_avx
+
0x75
>
.
byte
196
3
125
25
76
129
80
1
/
/
vextractf128
0x1
%
ymm9
0x50
(
%
r9
%
r8
4
)
.
byte
72
131
255
7
/
/
cmp
0x7
%
rdi
.
byte
114
181
/
/
jb
181a5
<
_sk_store_f32_avx
+
0x75
>
.
byte
196
3
125
25
68
129
96
1
/
/
vextractf128
0x1
%
ymm8
0x60
(
%
r9
%
r8
4
)
.
byte
235
171
/
/
jmp
181a5
<
_sk_store_f32_avx
+
0x75
>
HIDDEN
_sk_repeat_x_avx
.
globl
_sk_repeat_x_avx
FUNCTION
(
_sk_repeat_x_avx
)
_sk_repeat_x_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
197
60
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
67
125
8
192
1
/
/
vroundps
0x1
%
ymm8
%
ymm8
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_y_avx
.
globl
_sk_repeat_y_avx
FUNCTION
(
_sk_repeat_y_avx
)
_sk_repeat_y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
197
60
89
193
/
/
vmulps
%
ymm1
%
ymm8
%
ymm8
.
byte
196
67
125
8
192
1
/
/
vroundps
0x1
%
ymm8
%
ymm8
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
116
92
200
/
/
vsubps
%
ymm8
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_avx
.
globl
_sk_mirror_x_avx
FUNCTION
(
_sk_mirror_x_avx
)
_sk_mirror_x_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
72
4
/
/
vmovss
0x4
(
%
rax
)
%
xmm9
.
byte
196
67
121
4
208
0
/
/
vpermilps
0x0
%
xmm8
%
xmm10
.
byte
196
67
45
24
210
1
/
/
vinsertf128
0x1
%
xmm10
%
ymm10
%
ymm10
.
byte
196
65
124
92
218
/
/
vsubps
%
ymm10
%
ymm0
%
ymm11
.
byte
196
193
58
88
192
/
/
vaddss
%
xmm8
%
xmm8
%
xmm0
.
byte
196
227
121
4
192
0
/
/
vpermilps
0x0
%
xmm0
%
xmm0
.
byte
196
99
125
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm0
%
ymm8
.
byte
197
178
89
5
127
66
2
0
/
/
vmulss
0x2427f
(
%
rip
)
%
xmm9
%
xmm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
227
121
4
192
0
/
/
vpermilps
0x0
%
xmm0
%
xmm0
.
byte
196
227
125
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm0
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
196
227
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm0
.
byte
196
193
124
89
192
/
/
vmulps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
164
92
192
/
/
vsubps
%
ymm0
%
ymm11
%
ymm0
.
byte
196
193
124
92
194
/
/
vsubps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_y_avx
.
globl
_sk_mirror_y_avx
FUNCTION
(
_sk_mirror_y_avx
)
_sk_mirror_y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
72
4
/
/
vmovss
0x4
(
%
rax
)
%
xmm9
.
byte
196
67
121
4
208
0
/
/
vpermilps
0x0
%
xmm8
%
xmm10
.
byte
196
67
45
24
210
1
/
/
vinsertf128
0x1
%
xmm10
%
ymm10
%
ymm10
.
byte
196
65
116
92
218
/
/
vsubps
%
ymm10
%
ymm1
%
ymm11
.
byte
196
193
58
88
200
/
/
vaddss
%
xmm8
%
xmm8
%
xmm1
.
byte
196
227
121
4
201
0
/
/
vpermilps
0x0
%
xmm1
%
xmm1
.
byte
196
99
117
24
193
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm8
.
byte
197
178
89
13
21
66
2
0
/
/
vmulss
0x24215
(
%
rip
)
%
xmm9
%
xmm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
227
121
4
201
0
/
/
vpermilps
0x0
%
xmm1
%
xmm1
.
byte
196
227
117
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm1
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
196
227
125
8
201
1
/
/
vroundps
0x1
%
ymm1
%
ymm1
.
byte
196
193
116
89
200
/
/
vmulps
%
ymm8
%
ymm1
%
ymm1
.
byte
197
164
92
201
/
/
vsubps
%
ymm1
%
ymm11
%
ymm1
.
byte
196
193
116
92
202
/
/
vsubps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
193
/
/
vsubps
%
ymm1
%
ymm8
%
ymm8
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_x_1_avx
.
globl
_sk_clamp_x_1_avx
FUNCTION
(
_sk_clamp_x_1_avx
)
_sk_clamp_x_1_avx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
210
65
2
0
/
/
vbroadcastss
0x241d2
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_avx
.
globl
_sk_repeat_x_1_avx
FUNCTION
(
_sk_repeat_x_1_avx
)
_sk_repeat_x_1_avx
:
.
byte
196
99
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm8
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
172
65
2
0
/
/
vbroadcastss
0x241ac
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_avx
.
globl
_sk_mirror_x_1_avx
FUNCTION
(
_sk_mirror_x_1_avx
)
_sk_mirror_x_1_avx
:
.
byte
196
98
125
24
5
178
65
2
0
/
/
vbroadcastss
0x241b2
(
%
rip
)
%
ymm8
#
3c514
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c8
>
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
13
136
65
2
0
/
/
vbroadcastss
0x24188
(
%
rip
)
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
124
89
201
/
/
vmulps
%
ymm9
%
ymm0
%
ymm9
.
byte
196
67
125
8
201
1
/
/
vroundps
0x1
%
ymm9
%
ymm9
.
byte
196
65
52
88
201
/
/
vaddps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
193
124
92
193
/
/
vsubps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
200
/
/
vsubps
%
ymm0
%
ymm8
%
ymm9
.
byte
197
180
84
192
/
/
vandps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
188
95
192
/
/
vmaxps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
5
88
65
2
0
/
/
vbroadcastss
0x24158
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_avx
.
globl
_sk_decal_x_avx
FUNCTION
(
_sk_decal_x_avx
)
_sk_decal_x_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
200
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
98
125
24
80
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm10
.
byte
196
65
124
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm0
%
ymm10
.
byte
196
65
44
84
201
/
/
vandps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
65
60
194
208
15
/
/
vcmptrueps
%
ymm8
%
ymm8
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_avx
.
globl
_sk_decal_y_avx
FUNCTION
(
_sk_decal_y_avx
)
_sk_decal_y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
201
2
/
/
vcmpleps
%
ymm1
%
ymm8
%
ymm9
.
byte
196
98
125
24
80
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm10
.
byte
196
65
116
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm1
%
ymm10
.
byte
196
65
44
84
201
/
/
vandps
%
ymm9
%
ymm10
%
ymm9
.
byte
196
65
60
194
208
15
/
/
vcmptrueps
%
ymm8
%
ymm8
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_avx
.
globl
_sk_decal_x_and_y_avx
FUNCTION
(
_sk_decal_x_and_y_avx
)
_sk_decal_x_and_y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
200
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm9
.
byte
196
67
125
25
202
1
/
/
vextractf128
0x1
%
ymm9
%
xmm10
.
byte
196
65
49
99
202
/
/
vpacksswb
%
xmm10
%
xmm9
%
xmm9
.
byte
196
98
125
24
80
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm10
.
byte
196
65
124
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm0
%
ymm10
.
byte
196
67
125
25
211
1
/
/
vextractf128
0x1
%
ymm10
%
xmm11
.
byte
196
65
41
99
211
/
/
vpacksswb
%
xmm11
%
xmm10
%
xmm10
.
byte
197
60
194
217
2
/
/
vcmpleps
%
ymm1
%
ymm8
%
ymm11
.
byte
196
67
125
25
220
1
/
/
vextractf128
0x1
%
ymm11
%
xmm12
.
byte
196
65
33
99
220
/
/
vpacksswb
%
xmm12
%
xmm11
%
xmm11
.
byte
196
65
33
219
201
/
/
vpand
%
xmm9
%
xmm11
%
xmm9
.
byte
196
65
49
219
202
/
/
vpand
%
xmm10
%
xmm9
%
xmm9
.
byte
196
98
125
24
80
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm10
.
byte
196
65
116
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm1
%
ymm10
.
byte
196
67
125
25
211
1
/
/
vextractf128
0x1
%
ymm10
%
xmm11
.
byte
196
65
41
99
211
/
/
vpacksswb
%
xmm11
%
xmm10
%
xmm10
.
byte
196
65
49
219
202
/
/
vpand
%
xmm10
%
xmm9
%
xmm9
.
byte
196
66
121
51
209
/
/
vpmovzxwd
%
xmm9
%
xmm10
.
byte
196
193
41
114
242
31
/
/
vpslld
0x1f
%
xmm10
%
xmm10
.
byte
196
193
41
114
226
31
/
/
vpsrad
0x1f
%
xmm10
%
xmm10
.
byte
197
49
105
200
/
/
vpunpckhwd
%
xmm0
%
xmm9
%
xmm9
.
byte
196
193
49
114
241
31
/
/
vpslld
0x1f
%
xmm9
%
xmm9
.
byte
196
193
49
114
225
31
/
/
vpsrad
0x1f
%
xmm9
%
xmm9
.
byte
196
67
45
24
201
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm9
.
byte
196
65
60
194
208
15
/
/
vcmptrueps
%
ymm8
%
ymm8
%
ymm10
.
byte
196
67
61
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm8
%
ymm8
.
byte
197
124
17
0
/
/
vmovups
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_avx
.
globl
_sk_check_decal_mask_avx
FUNCTION
(
_sk_check_decal_mask_avx
)
_sk_check_decal_mask_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
0
/
/
vmovups
(
%
rax
)
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
84
210
/
/
vandps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
84
219
/
/
vandps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminance_to_alpha_avx
.
globl
_sk_luminance_to_alpha_avx
FUNCTION
(
_sk_luminance_to_alpha_avx
)
_sk_luminance_to_alpha_avx
:
.
byte
196
226
125
24
29
95
65
2
0
/
/
vbroadcastss
0x2415f
(
%
rip
)
%
ymm3
#
3c630
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e4
>
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
125
24
29
78
65
2
0
/
/
vbroadcastss
0x2414e
(
%
rip
)
%
ymm3
#
3c62c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e0
>
.
byte
197
244
89
203
/
/
vmulps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
69
65
2
0
/
/
vbroadcastss
0x24145
(
%
rip
)
%
ymm1
#
3c634
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3e8
>
.
byte
197
236
89
201
/
/
vmulps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
88
217
/
/
vaddps
%
ymm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_avx
.
globl
_sk_matrix_translate_avx
FUNCTION
(
_sk_matrix_translate_avx
)
_sk_matrix_translate_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
197
188
88
201
/
/
vaddps
%
ymm1
%
ymm8
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_avx
.
globl
_sk_matrix_scale_translate_avx
FUNCTION
(
_sk_matrix_scale_translate_avx
)
_sk_matrix_scale_translate_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm9
.
byte
197
188
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm0
.
byte
196
193
124
88
193
/
/
vaddps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
197
188
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm1
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_avx
.
globl
_sk_matrix_2x3_avx
FUNCTION
(
_sk_matrix_2x3_avx
)
_sk_matrix_2x3_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm10
.
byte
197
52
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
197
60
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
197
172
89
201
/
/
vmulps
%
ymm1
%
ymm10
%
ymm1
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
252
88
201
/
/
vaddps
%
ymm1
%
ymm0
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_3x4_avx
.
globl
_sk_matrix_3x4_avx
FUNCTION
(
_sk_matrix_3x4_avx
)
_sk_matrix_3x4_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
36
/
/
vbroadcastss
0x24
(
%
rax
)
%
ymm11
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
197
52
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
197
60
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
40
/
/
vbroadcastss
0x28
(
%
rax
)
%
ymm12
.
byte
197
36
89
218
/
/
vmulps
%
ymm2
%
ymm11
%
ymm11
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
44
89
209
/
/
vmulps
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
197
52
89
200
/
/
vmulps
%
ymm0
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
44
/
/
vbroadcastss
0x2c
(
%
rax
)
%
ymm13
.
byte
197
156
89
210
/
/
vmulps
%
ymm2
%
ymm12
%
ymm2
.
byte
196
193
108
88
213
/
/
vaddps
%
ymm13
%
ymm2
%
ymm2
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
244
88
202
/
/
vaddps
%
ymm2
%
ymm1
%
ymm1
.
byte
197
172
89
192
/
/
vmulps
%
ymm0
%
ymm10
%
ymm0
.
byte
197
252
88
209
/
/
vaddps
%
ymm1
%
ymm0
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x5_avx
.
globl
_sk_matrix_4x5_avx
FUNCTION
(
_sk_matrix_4x5_avx
)
_sk_matrix_4x5_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
48
/
/
vbroadcastss
0x30
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm12
.
byte
197
36
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm11
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
44
89
210
/
/
vmulps
%
ymm2
%
ymm10
%
ymm10
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
197
52
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
197
60
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
36
/
/
vbroadcastss
0x24
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
52
/
/
vbroadcastss
0x34
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm13
.
byte
197
28
89
227
/
/
vmulps
%
ymm3
%
ymm12
%
ymm12
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
197
36
89
218
/
/
vmulps
%
ymm2
%
ymm11
%
ymm11
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
44
89
209
/
/
vmulps
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
197
52
89
200
/
/
vmulps
%
ymm0
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
40
/
/
vbroadcastss
0x28
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
56
/
/
vbroadcastss
0x38
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
112
72
/
/
vbroadcastss
0x48
(
%
rax
)
%
ymm14
.
byte
197
20
89
235
/
/
vmulps
%
ymm3
%
ymm13
%
ymm13
.
byte
196
65
20
88
238
/
/
vaddps
%
ymm14
%
ymm13
%
ymm13
.
byte
197
28
89
226
/
/
vmulps
%
ymm2
%
ymm12
%
ymm12
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
197
36
89
217
/
/
vmulps
%
ymm1
%
ymm11
%
ymm11
.
byte
196
65
36
88
220
/
/
vaddps
%
ymm12
%
ymm11
%
ymm11
.
byte
197
44
89
208
/
/
vmulps
%
ymm0
%
ymm10
%
ymm10
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
44
/
/
vbroadcastss
0x2c
(
%
rax
)
%
ymm13
.
byte
196
98
125
24
112
60
/
/
vbroadcastss
0x3c
(
%
rax
)
%
ymm14
.
byte
196
98
125
24
120
76
/
/
vbroadcastss
0x4c
(
%
rax
)
%
ymm15
.
byte
197
140
89
219
/
/
vmulps
%
ymm3
%
ymm14
%
ymm3
.
byte
196
193
100
88
223
/
/
vaddps
%
ymm15
%
ymm3
%
ymm3
.
byte
197
148
89
210
/
/
vmulps
%
ymm2
%
ymm13
%
ymm2
.
byte
197
236
88
211
/
/
vaddps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
156
89
201
/
/
vmulps
%
ymm1
%
ymm12
%
ymm1
.
byte
197
244
88
202
/
/
vaddps
%
ymm2
%
ymm1
%
ymm1
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
252
88
217
/
/
vaddps
%
ymm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
197
124
41
210
/
/
vmovaps
%
ymm10
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x3_avx
.
globl
_sk_matrix_4x3_avx
FUNCTION
(
_sk_matrix_4x3_avx
)
_sk_matrix_4x3_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
64
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm8
.
byte
197
228
89
217
/
/
vmulps
%
ymm1
%
ymm3
%
ymm3
.
byte
196
193
100
88
216
/
/
vaddps
%
ymm8
%
ymm3
%
ymm3
.
byte
197
236
89
208
/
/
vmulps
%
ymm0
%
ymm2
%
ymm2
.
byte
197
108
88
195
/
/
vaddps
%
ymm3
%
ymm2
%
ymm8
.
byte
196
226
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
72
36
/
/
vbroadcastss
0x24
(
%
rax
)
%
ymm9
.
byte
197
228
89
217
/
/
vmulps
%
ymm1
%
ymm3
%
ymm3
.
byte
196
193
100
88
217
/
/
vaddps
%
ymm9
%
ymm3
%
ymm3
.
byte
197
236
89
208
/
/
vmulps
%
ymm0
%
ymm2
%
ymm2
.
byte
197
108
88
203
/
/
vaddps
%
ymm3
%
ymm2
%
ymm9
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
80
40
/
/
vbroadcastss
0x28
(
%
rax
)
%
ymm10
.
byte
197
228
89
217
/
/
vmulps
%
ymm1
%
ymm3
%
ymm3
.
byte
196
193
100
88
218
/
/
vaddps
%
ymm10
%
ymm3
%
ymm3
.
byte
197
236
89
208
/
/
vmulps
%
ymm0
%
ymm2
%
ymm2
.
byte
197
236
88
211
/
/
vaddps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
226
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
80
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
44
/
/
vbroadcastss
0x2c
(
%
rax
)
%
ymm11
.
byte
197
172
89
201
/
/
vmulps
%
ymm1
%
ymm10
%
ymm1
.
byte
196
193
116
88
203
/
/
vaddps
%
ymm11
%
ymm1
%
ymm1
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
252
88
217
/
/
vaddps
%
ymm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_avx
.
globl
_sk_matrix_perspective_avx
FUNCTION
(
_sk_matrix_perspective_avx
)
_sk_matrix_perspective_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
197
52
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
197
60
89
192
/
/
vmulps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm11
.
byte
197
44
89
209
/
/
vmulps
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
197
52
89
200
/
/
vmulps
%
ymm0
%
ymm9
%
ymm9
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
80
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm12
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
196
193
116
88
204
/
/
vaddps
%
ymm12
%
ymm1
%
ymm1
.
byte
197
172
89
192
/
/
vmulps
%
ymm0
%
ymm10
%
ymm0
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
83
200
/
/
vrcpps
%
ymm0
%
ymm1
.
byte
197
188
89
193
/
/
vmulps
%
ymm1
%
ymm8
%
ymm0
.
byte
197
180
89
201
/
/
vmulps
%
ymm1
%
ymm9
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_avx
.
globl
_sk_evenly_spaced_gradient_avx
FUNCTION
(
_sk_evenly_spaced_gradient_avx
)
_sk_evenly_spaced_gradient_avx
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
197
252
17
124
36
216
/
/
vmovups
%
ymm7
-
0x28
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
72
139
104
8
/
/
mov
0x8
(
%
rax
)
%
rbp
.
byte
72
255
203
/
/
dec
%
rbx
.
byte
120
7
/
/
js
18893
<
_sk_evenly_spaced_gradient_avx
+
0x25
>
.
byte
196
225
242
42
203
/
/
vcvtsi2ss
%
rbx
%
xmm1
%
xmm1
.
byte
235
21
/
/
jmp
188a8
<
_sk_evenly_spaced_gradient_avx
+
0x3a
>
.
byte
73
137
216
/
/
mov
%
rbx
%
r8
.
byte
73
209
232
/
/
shr
%
r8
.
byte
131
227
1
/
/
and
0x1
%
ebx
.
byte
76
9
195
/
/
or
%
r8
%
rbx
.
byte
196
225
242
42
203
/
/
vcvtsi2ss
%
rbx
%
xmm1
%
xmm1
.
byte
197
242
88
201
/
/
vaddss
%
xmm1
%
xmm1
%
xmm1
.
byte
196
227
121
4
201
0
/
/
vpermilps
0x0
%
xmm1
%
xmm1
.
byte
196
227
117
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm1
.
byte
197
244
89
200
/
/
vmulps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
195
249
22
207
1
/
/
vpextrq
0x1
%
xmm1
%
r15
.
byte
69
137
254
/
/
mov
%
r15d
%
r14d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
193
249
126
205
/
/
vmovq
%
xmm1
%
r13
.
byte
69
137
236
/
/
mov
%
r13d
%
r12d
.
byte
73
193
237
32
/
/
shr
0x20
%
r13
.
byte
196
161
122
16
76
165
0
/
/
vmovss
0x0
(
%
rbp
%
r12
4
)
%
xmm1
.
byte
196
163
113
33
76
173
0
16
/
/
vinsertps
0x10
0x0
(
%
rbp
%
r13
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
84
181
0
/
/
vmovss
0x0
(
%
rbp
%
r14
4
)
%
xmm2
.
byte
196
33
122
16
68
189
0
/
/
vmovss
0x0
(
%
rbp
%
r15
4
)
%
xmm8
.
byte
196
161
122
16
92
157
0
/
/
vmovss
0x0
(
%
rbp
%
r11
4
)
%
xmm3
.
byte
196
35
97
33
76
149
0
16
/
/
vinsertps
0x10
0x0
(
%
rbp
%
r10
4
)
%
xmm3
%
xmm9
.
byte
196
161
122
16
124
141
0
/
/
vmovss
0x0
(
%
rbp
%
r9
4
)
%
xmm7
.
byte
196
33
122
16
92
133
0
/
/
vmovss
0x0
(
%
rbp
%
r8
4
)
%
xmm11
.
byte
196
99
113
33
226
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm12
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
196
161
122
16
20
163
/
/
vmovss
(
%
rbx
%
r12
4
)
%
xmm2
.
byte
196
35
105
33
44
171
16
/
/
vinsertps
0x10
(
%
rbx
%
r13
4
)
%
xmm2
%
xmm13
.
byte
196
161
122
16
28
179
/
/
vmovss
(
%
rbx
%
r14
4
)
%
xmm3
.
byte
196
161
122
16
12
187
/
/
vmovss
(
%
rbx
%
r15
4
)
%
xmm1
.
byte
196
161
122
16
20
155
/
/
vmovss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
196
35
105
33
52
147
16
/
/
vinsertps
0x10
(
%
rbx
%
r10
4
)
%
xmm2
%
xmm14
.
byte
196
33
122
16
60
139
/
/
vmovss
(
%
rbx
%
r9
4
)
%
xmm15
.
byte
196
33
122
16
20
131
/
/
vmovss
(
%
rbx
%
r8
4
)
%
xmm10
.
byte
196
67
25
33
192
48
/
/
vinsertps
0x30
%
xmm8
%
xmm12
%
xmm8
.
byte
196
227
49
33
215
32
/
/
vinsertps
0x20
%
xmm7
%
xmm9
%
xmm2
.
byte
196
195
105
33
211
48
/
/
vinsertps
0x30
%
xmm11
%
xmm2
%
xmm2
.
byte
196
67
109
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm2
%
ymm8
.
byte
196
227
17
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm13
%
xmm2
.
byte
196
99
105
33
201
48
/
/
vinsertps
0x30
%
xmm1
%
xmm2
%
xmm9
.
byte
72
139
88
16
/
/
mov
0x10
(
%
rax
)
%
rbx
.
byte
196
161
122
16
20
163
/
/
vmovss
(
%
rbx
%
r12
4
)
%
xmm2
.
byte
196
35
105
33
28
171
16
/
/
vinsertps
0x10
(
%
rbx
%
r13
4
)
%
xmm2
%
xmm11
.
byte
196
33
122
16
36
179
/
/
vmovss
(
%
rbx
%
r14
4
)
%
xmm12
.
byte
196
161
122
16
12
187
/
/
vmovss
(
%
rbx
%
r15
4
)
%
xmm1
.
byte
196
161
122
16
60
155
/
/
vmovss
(
%
rbx
%
r11
4
)
%
xmm7
.
byte
196
163
65
33
60
147
16
/
/
vinsertps
0x10
(
%
rbx
%
r10
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
28
139
/
/
vmovss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
196
33
122
16
44
131
/
/
vmovss
(
%
rbx
%
r8
4
)
%
xmm13
.
byte
196
195
9
33
215
32
/
/
vinsertps
0x20
%
xmm15
%
xmm14
%
xmm2
.
byte
196
195
105
33
210
48
/
/
vinsertps
0x30
%
xmm10
%
xmm2
%
xmm2
.
byte
196
67
109
24
241
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm2
%
ymm14
.
byte
196
195
33
33
212
32
/
/
vinsertps
0x20
%
xmm12
%
xmm11
%
xmm2
.
byte
196
99
105
33
201
48
/
/
vinsertps
0x30
%
xmm1
%
xmm2
%
xmm9
.
byte
196
99
65
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm7
%
xmm10
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
196
161
122
16
28
163
/
/
vmovss
(
%
rbx
%
r12
4
)
%
xmm3
.
byte
196
35
97
33
28
171
16
/
/
vinsertps
0x10
(
%
rbx
%
r13
4
)
%
xmm3
%
xmm11
.
byte
196
33
122
16
60
179
/
/
vmovss
(
%
rbx
%
r14
4
)
%
xmm15
.
byte
196
33
122
16
36
187
/
/
vmovss
(
%
rbx
%
r15
4
)
%
xmm12
.
byte
196
161
122
16
20
155
/
/
vmovss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
196
163
105
33
20
147
16
/
/
vinsertps
0x10
(
%
rbx
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
60
139
/
/
vmovss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
196
161
122
16
28
131
/
/
vmovss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
196
67
41
33
213
48
/
/
vinsertps
0x30
%
xmm13
%
xmm10
%
xmm10
.
byte
196
67
45
24
233
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm13
.
byte
196
195
33
33
207
32
/
/
vinsertps
0x20
%
xmm15
%
xmm11
%
xmm1
.
byte
196
67
113
33
204
48
/
/
vinsertps
0x30
%
xmm12
%
xmm1
%
xmm9
.
byte
196
227
105
33
215
32
/
/
vinsertps
0x20
%
xmm7
%
xmm2
%
xmm2
.
byte
196
99
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm10
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
196
161
122
16
28
163
/
/
vmovss
(
%
rbx
%
r12
4
)
%
xmm3
.
byte
196
35
97
33
28
171
16
/
/
vinsertps
0x10
(
%
rbx
%
r13
4
)
%
xmm3
%
xmm11
.
byte
196
33
122
16
36
179
/
/
vmovss
(
%
rbx
%
r14
4
)
%
xmm12
.
byte
196
33
122
16
60
187
/
/
vmovss
(
%
rbx
%
r15
4
)
%
xmm15
.
byte
196
161
122
16
20
155
/
/
vmovss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
196
163
105
33
20
147
16
/
/
vinsertps
0x10
(
%
rbx
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
139
/
/
vmovss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
196
161
122
16
60
131
/
/
vmovss
(
%
rbx
%
r8
4
)
%
xmm7
.
byte
196
67
45
24
201
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm9
.
byte
196
195
33
33
204
32
/
/
vinsertps
0x20
%
xmm12
%
xmm11
%
xmm1
.
byte
196
195
113
33
207
48
/
/
vinsertps
0x30
%
xmm15
%
xmm1
%
xmm1
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
196
227
105
33
215
48
/
/
vinsertps
0x30
%
xmm7
%
xmm2
%
xmm2
.
byte
196
99
109
24
209
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm10
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
196
161
122
16
12
163
/
/
vmovss
(
%
rbx
%
r12
4
)
%
xmm1
.
byte
196
35
113
33
28
171
16
/
/
vinsertps
0x10
(
%
rbx
%
r13
4
)
%
xmm1
%
xmm11
.
byte
196
33
122
16
36
179
/
/
vmovss
(
%
rbx
%
r14
4
)
%
xmm12
.
byte
196
33
122
16
60
187
/
/
vmovss
(
%
rbx
%
r15
4
)
%
xmm15
.
byte
196
161
122
16
60
155
/
/
vmovss
(
%
rbx
%
r11
4
)
%
xmm7
.
byte
196
163
65
33
60
147
16
/
/
vinsertps
0x10
(
%
rbx
%
r10
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
12
139
/
/
vmovss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
196
161
122
16
20
131
/
/
vmovss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
196
195
33
33
220
32
/
/
vinsertps
0x20
%
xmm12
%
xmm11
%
xmm3
.
byte
196
195
97
33
223
48
/
/
vinsertps
0x30
%
xmm15
%
xmm3
%
xmm3
.
byte
196
227
65
33
201
32
/
/
vinsertps
0x20
%
xmm1
%
xmm7
%
xmm1
.
byte
196
227
113
33
202
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm1
.
byte
196
99
117
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm1
%
ymm11
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
196
161
122
16
12
163
/
/
vmovss
(
%
rbx
%
r12
4
)
%
xmm1
.
byte
196
163
113
33
12
171
16
/
/
vinsertps
0x10
(
%
rbx
%
r13
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
20
179
/
/
vmovss
(
%
rbx
%
r14
4
)
%
xmm2
.
byte
196
227
113
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm1
.
byte
196
161
122
16
20
187
/
/
vmovss
(
%
rbx
%
r15
4
)
%
xmm2
.
byte
196
161
122
16
28
155
/
/
vmovss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
196
35
97
33
36
147
16
/
/
vinsertps
0x10
(
%
rbx
%
r10
4
)
%
xmm3
%
xmm12
.
byte
196
161
122
16
60
139
/
/
vmovss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
196
161
122
16
28
131
/
/
vmovss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
196
99
113
33
250
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm15
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
161
122
16
20
160
/
/
vmovss
(
%
rax
%
r12
4
)
%
xmm2
.
byte
196
163
105
33
20
168
16
/
/
vinsertps
0x10
(
%
rax
%
r13
4
)
%
xmm2
%
xmm2
.
byte
196
227
25
33
255
32
/
/
vinsertps
0x20
%
xmm7
%
xmm12
%
xmm7
.
byte
196
161
122
16
12
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
196
227
65
33
219
48
/
/
vinsertps
0x30
%
xmm3
%
xmm7
%
xmm3
.
byte
196
161
122
16
60
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm7
.
byte
196
67
101
24
231
1
/
/
vinsertf128
0x1
%
xmm15
%
ymm3
%
ymm12
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
163
97
33
28
144
16
/
/
vinsertps
0x10
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
227
105
33
201
32
/
/
vinsertps
0x20
%
xmm1
%
xmm2
%
xmm1
.
byte
196
161
122
16
20
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm2
.
byte
196
227
113
33
207
48
/
/
vinsertps
0x30
%
xmm7
%
xmm1
%
xmm1
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
97
33
210
32
/
/
vinsertps
0x20
%
xmm2
%
xmm3
%
xmm2
.
byte
196
227
105
33
215
48
/
/
vinsertps
0x30
%
xmm7
%
xmm2
%
xmm2
.
byte
196
227
109
24
217
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm3
.
byte
197
188
89
200
/
/
vmulps
%
ymm0
%
ymm8
%
ymm1
.
byte
196
65
116
88
198
/
/
vaddps
%
ymm14
%
ymm1
%
ymm8
.
byte
197
148
89
200
/
/
vmulps
%
ymm0
%
ymm13
%
ymm1
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
172
89
208
/
/
vmulps
%
ymm0
%
ymm10
%
ymm2
.
byte
196
193
108
88
211
/
/
vaddps
%
ymm11
%
ymm2
%
ymm2
.
byte
197
156
89
192
/
/
vmulps
%
ymm0
%
ymm12
%
ymm0
.
byte
197
252
88
219
/
/
vaddps
%
ymm3
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
252
16
124
36
216
/
/
vmovups
-
0x28
(
%
rsp
)
%
ymm7
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_avx
.
globl
_sk_gradient_avx
FUNCTION
(
_sk_gradient_avx
)
_sk_gradient_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
197
252
17
124
36
208
/
/
vmovups
%
ymm7
-
0x30
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
73
131
248
2
/
/
cmp
0x2
%
r8
.
byte
114
81
/
/
jb
18c42
<
_sk_gradient_avx
+
0x6f
>
.
byte
72
139
88
72
/
/
mov
0x48
(
%
rax
)
%
rbx
.
byte
73
255
200
/
/
dec
%
r8
.
byte
72
131
195
4
/
/
add
0x4
%
rbx
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
246
56
2
0
/
/
vbroadcastss
0x238f6
(
%
rip
)
%
ymm10
#
3c500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b4
>
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
196
98
125
24
3
/
/
vbroadcastss
(
%
rbx
)
%
ymm8
.
byte
197
60
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
67
53
74
194
128
/
/
vblendvps
%
ymm8
%
ymm10
%
ymm9
%
ymm8
.
byte
196
99
125
25
194
1
/
/
vextractf128
0x1
%
ymm8
%
xmm2
.
byte
196
227
125
25
203
1
/
/
vextractf128
0x1
%
ymm1
%
xmm3
.
byte
197
225
254
210
/
/
vpaddd
%
xmm2
%
xmm3
%
xmm2
.
byte
196
193
113
254
200
/
/
vpaddd
%
xmm8
%
xmm1
%
xmm1
.
byte
196
227
117
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
72
131
195
4
/
/
add
0x4
%
rbx
.
byte
73
255
200
/
/
dec
%
r8
.
byte
117
204
/
/
jne
18c0e
<
_sk_gradient_avx
+
0x3b
>
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
193
249
126
202
/
/
vmovq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
25
201
1
/
/
vextractf128
0x1
%
ymm1
%
xmm1
.
byte
196
195
249
22
207
1
/
/
vpextrq
0x1
%
xmm1
%
r15
.
byte
69
137
254
/
/
mov
%
r15d
%
r14d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
220
/
/
mov
%
ebx
%
r12d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
76
139
104
8
/
/
mov
0x8
(
%
rax
)
%
r13
.
byte
196
129
122
16
76
165
0
/
/
vmovss
0x0
(
%
r13
%
r12
4
)
%
xmm1
.
byte
196
195
113
33
76
157
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
129
122
16
84
181
0
/
/
vmovss
0x0
(
%
r13
%
r14
4
)
%
xmm2
.
byte
196
1
122
16
68
189
0
/
/
vmovss
0x0
(
%
r13
%
r15
4
)
%
xmm8
.
byte
196
129
122
16
92
157
0
/
/
vmovss
0x0
(
%
r13
%
r11
4
)
%
xmm3
.
byte
196
3
97
33
76
149
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
r10
4
)
%
xmm3
%
xmm9
.
byte
196
129
122
16
124
141
0
/
/
vmovss
0x0
(
%
r13
%
r9
4
)
%
xmm7
.
byte
196
1
122
16
92
133
0
/
/
vmovss
0x0
(
%
r13
%
r8
4
)
%
xmm11
.
byte
196
99
113
33
226
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm12
.
byte
76
139
104
40
/
/
mov
0x28
(
%
rax
)
%
r13
.
byte
196
129
122
16
84
165
0
/
/
vmovss
0x0
(
%
r13
%
r12
4
)
%
xmm2
.
byte
196
67
105
33
108
157
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
rbx
4
)
%
xmm2
%
xmm13
.
byte
196
129
122
16
92
181
0
/
/
vmovss
0x0
(
%
r13
%
r14
4
)
%
xmm3
.
byte
196
129
122
16
76
189
0
/
/
vmovss
0x0
(
%
r13
%
r15
4
)
%
xmm1
.
byte
196
129
122
16
84
157
0
/
/
vmovss
0x0
(
%
r13
%
r11
4
)
%
xmm2
.
byte
196
3
105
33
116
149
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
r10
4
)
%
xmm2
%
xmm14
.
byte
196
1
122
16
124
141
0
/
/
vmovss
0x0
(
%
r13
%
r9
4
)
%
xmm15
.
byte
196
1
122
16
84
133
0
/
/
vmovss
0x0
(
%
r13
%
r8
4
)
%
xmm10
.
byte
196
67
25
33
192
48
/
/
vinsertps
0x30
%
xmm8
%
xmm12
%
xmm8
.
byte
196
227
49
33
215
32
/
/
vinsertps
0x20
%
xmm7
%
xmm9
%
xmm2
.
byte
196
195
105
33
211
48
/
/
vinsertps
0x30
%
xmm11
%
xmm2
%
xmm2
.
byte
196
67
109
24
192
1
/
/
vinsertf128
0x1
%
xmm8
%
ymm2
%
ymm8
.
byte
196
227
17
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm13
%
xmm2
.
byte
196
99
105
33
201
48
/
/
vinsertps
0x30
%
xmm1
%
xmm2
%
xmm9
.
byte
76
139
104
16
/
/
mov
0x10
(
%
rax
)
%
r13
.
byte
196
129
122
16
84
165
0
/
/
vmovss
0x0
(
%
r13
%
r12
4
)
%
xmm2
.
byte
196
67
105
33
92
157
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
rbx
4
)
%
xmm2
%
xmm11
.
byte
196
1
122
16
100
181
0
/
/
vmovss
0x0
(
%
r13
%
r14
4
)
%
xmm12
.
byte
196
129
122
16
76
189
0
/
/
vmovss
0x0
(
%
r13
%
r15
4
)
%
xmm1
.
byte
196
129
122
16
124
157
0
/
/
vmovss
0x0
(
%
r13
%
r11
4
)
%
xmm7
.
byte
196
131
65
33
124
149
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
r10
4
)
%
xmm7
%
xmm7
.
byte
196
129
122
16
92
141
0
/
/
vmovss
0x0
(
%
r13
%
r9
4
)
%
xmm3
.
byte
196
1
122
16
108
133
0
/
/
vmovss
0x0
(
%
r13
%
r8
4
)
%
xmm13
.
byte
196
195
9
33
215
32
/
/
vinsertps
0x20
%
xmm15
%
xmm14
%
xmm2
.
byte
196
195
105
33
210
48
/
/
vinsertps
0x30
%
xmm10
%
xmm2
%
xmm2
.
byte
196
67
109
24
241
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm2
%
ymm14
.
byte
196
195
33
33
212
32
/
/
vinsertps
0x20
%
xmm12
%
xmm11
%
xmm2
.
byte
196
99
105
33
201
48
/
/
vinsertps
0x30
%
xmm1
%
xmm2
%
xmm9
.
byte
196
99
65
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm7
%
xmm10
.
byte
76
139
104
48
/
/
mov
0x30
(
%
rax
)
%
r13
.
byte
196
129
122
16
92
165
0
/
/
vmovss
0x0
(
%
r13
%
r12
4
)
%
xmm3
.
byte
196
67
97
33
92
157
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
rbx
4
)
%
xmm3
%
xmm11
.
byte
196
1
122
16
124
181
0
/
/
vmovss
0x0
(
%
r13
%
r14
4
)
%
xmm15
.
byte
196
1
122
16
100
189
0
/
/
vmovss
0x0
(
%
r13
%
r15
4
)
%
xmm12
.
byte
196
129
122
16
84
157
0
/
/
vmovss
0x0
(
%
r13
%
r11
4
)
%
xmm2
.
byte
196
131
105
33
84
149
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
129
122
16
124
141
0
/
/
vmovss
0x0
(
%
r13
%
r9
4
)
%
xmm7
.
byte
196
129
122
16
92
133
0
/
/
vmovss
0x0
(
%
r13
%
r8
4
)
%
xmm3
.
byte
196
67
41
33
213
48
/
/
vinsertps
0x30
%
xmm13
%
xmm10
%
xmm10
.
byte
196
67
45
24
233
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm13
.
byte
196
195
33
33
207
32
/
/
vinsertps
0x20
%
xmm15
%
xmm11
%
xmm1
.
byte
196
67
113
33
204
48
/
/
vinsertps
0x30
%
xmm12
%
xmm1
%
xmm9
.
byte
196
227
105
33
215
32
/
/
vinsertps
0x20
%
xmm7
%
xmm2
%
xmm2
.
byte
196
99
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm10
.
byte
76
139
104
24
/
/
mov
0x18
(
%
rax
)
%
r13
.
byte
196
129
122
16
92
165
0
/
/
vmovss
0x0
(
%
r13
%
r12
4
)
%
xmm3
.
byte
196
67
97
33
92
157
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
rbx
4
)
%
xmm3
%
xmm11
.
byte
196
1
122
16
100
181
0
/
/
vmovss
0x0
(
%
r13
%
r14
4
)
%
xmm12
.
byte
196
1
122
16
124
189
0
/
/
vmovss
0x0
(
%
r13
%
r15
4
)
%
xmm15
.
byte
196
129
122
16
84
157
0
/
/
vmovss
0x0
(
%
r13
%
r11
4
)
%
xmm2
.
byte
196
131
105
33
84
149
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
129
122
16
92
141
0
/
/
vmovss
0x0
(
%
r13
%
r9
4
)
%
xmm3
.
byte
196
129
122
16
124
133
0
/
/
vmovss
0x0
(
%
r13
%
r8
4
)
%
xmm7
.
byte
196
67
45
24
201
1
/
/
vinsertf128
0x1
%
xmm9
%
ymm10
%
ymm9
.
byte
196
195
33
33
204
32
/
/
vinsertps
0x20
%
xmm12
%
xmm11
%
xmm1
.
byte
196
195
113
33
207
48
/
/
vinsertps
0x30
%
xmm15
%
xmm1
%
xmm1
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
196
227
105
33
215
48
/
/
vinsertps
0x30
%
xmm7
%
xmm2
%
xmm2
.
byte
196
99
109
24
209
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm10
.
byte
76
139
104
56
/
/
mov
0x38
(
%
rax
)
%
r13
.
byte
196
129
122
16
76
165
0
/
/
vmovss
0x0
(
%
r13
%
r12
4
)
%
xmm1
.
byte
196
67
113
33
92
157
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
rbx
4
)
%
xmm1
%
xmm11
.
byte
196
1
122
16
100
181
0
/
/
vmovss
0x0
(
%
r13
%
r14
4
)
%
xmm12
.
byte
196
1
122
16
124
189
0
/
/
vmovss
0x0
(
%
r13
%
r15
4
)
%
xmm15
.
byte
196
129
122
16
124
157
0
/
/
vmovss
0x0
(
%
r13
%
r11
4
)
%
xmm7
.
byte
196
131
65
33
124
149
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
r10
4
)
%
xmm7
%
xmm7
.
byte
196
129
122
16
76
141
0
/
/
vmovss
0x0
(
%
r13
%
r9
4
)
%
xmm1
.
byte
196
129
122
16
84
133
0
/
/
vmovss
0x0
(
%
r13
%
r8
4
)
%
xmm2
.
byte
196
195
33
33
220
32
/
/
vinsertps
0x20
%
xmm12
%
xmm11
%
xmm3
.
byte
196
195
97
33
223
48
/
/
vinsertps
0x30
%
xmm15
%
xmm3
%
xmm3
.
byte
196
227
65
33
201
32
/
/
vinsertps
0x20
%
xmm1
%
xmm7
%
xmm1
.
byte
196
227
113
33
202
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm1
.
byte
196
99
117
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm1
%
ymm11
.
byte
76
139
104
32
/
/
mov
0x20
(
%
rax
)
%
r13
.
byte
196
129
122
16
76
165
0
/
/
vmovss
0x0
(
%
r13
%
r12
4
)
%
xmm1
.
byte
196
195
113
33
76
157
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
129
122
16
84
181
0
/
/
vmovss
0x0
(
%
r13
%
r14
4
)
%
xmm2
.
byte
196
227
113
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm1
.
byte
196
129
122
16
84
189
0
/
/
vmovss
0x0
(
%
r13
%
r15
4
)
%
xmm2
.
byte
196
129
122
16
92
157
0
/
/
vmovss
0x0
(
%
r13
%
r11
4
)
%
xmm3
.
byte
196
3
97
33
100
149
0
16
/
/
vinsertps
0x10
0x0
(
%
r13
%
r10
4
)
%
xmm3
%
xmm12
.
byte
196
129
122
16
124
141
0
/
/
vmovss
0x0
(
%
r13
%
r9
4
)
%
xmm7
.
byte
196
129
122
16
92
133
0
/
/
vmovss
0x0
(
%
r13
%
r8
4
)
%
xmm3
.
byte
196
99
113
33
250
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm15
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
161
122
16
20
160
/
/
vmovss
(
%
rax
%
r12
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
227
25
33
255
32
/
/
vinsertps
0x20
%
xmm7
%
xmm12
%
xmm7
.
byte
196
161
122
16
12
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
196
227
65
33
219
48
/
/
vinsertps
0x30
%
xmm3
%
xmm7
%
xmm3
.
byte
196
161
122
16
60
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm7
.
byte
196
67
101
24
231
1
/
/
vinsertf128
0x1
%
xmm15
%
ymm3
%
ymm12
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
163
97
33
28
144
16
/
/
vinsertps
0x10
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
227
105
33
201
32
/
/
vinsertps
0x20
%
xmm1
%
xmm2
%
xmm1
.
byte
196
161
122
16
20
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm2
.
byte
196
227
113
33
207
48
/
/
vinsertps
0x30
%
xmm7
%
xmm1
%
xmm1
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
97
33
210
32
/
/
vinsertps
0x20
%
xmm2
%
xmm3
%
xmm2
.
byte
196
227
105
33
215
48
/
/
vinsertps
0x30
%
xmm7
%
xmm2
%
xmm2
.
byte
196
227
109
24
217
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm3
.
byte
197
188
89
200
/
/
vmulps
%
ymm0
%
ymm8
%
ymm1
.
byte
196
65
116
88
198
/
/
vaddps
%
ymm14
%
ymm1
%
ymm8
.
byte
197
148
89
200
/
/
vmulps
%
ymm0
%
ymm13
%
ymm1
.
byte
196
193
116
88
201
/
/
vaddps
%
ymm9
%
ymm1
%
ymm1
.
byte
197
172
89
208
/
/
vmulps
%
ymm0
%
ymm10
%
ymm2
.
byte
196
193
108
88
211
/
/
vaddps
%
ymm11
%
ymm2
%
ymm2
.
byte
197
156
89
192
/
/
vmulps
%
ymm0
%
ymm12
%
ymm0
.
byte
197
252
88
219
/
/
vaddps
%
ymm3
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
197
252
16
124
36
208
/
/
vmovups
-
0x30
(
%
rsp
)
%
ymm7
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_avx
.
globl
_sk_evenly_spaced_2_stop_gradient_avx
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_avx
)
_sk_evenly_spaced_2_stop_gradient_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm2
.
byte
197
244
89
200
/
/
vmulps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
116
88
194
/
/
vaddps
%
ymm2
%
ymm1
%
ymm8
.
byte
196
226
125
24
72
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm1
.
byte
196
226
125
24
80
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm2
.
byte
197
244
89
200
/
/
vmulps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
244
88
202
/
/
vaddps
%
ymm2
%
ymm1
%
ymm1
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm3
.
byte
197
236
89
208
/
/
vmulps
%
ymm0
%
ymm2
%
ymm2
.
byte
197
236
88
211
/
/
vaddps
%
ymm3
%
ymm2
%
ymm2
.
byte
196
226
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm3
.
byte
196
98
125
24
72
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm9
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
124
88
217
/
/
vaddps
%
ymm9
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_avx
.
globl
_sk_xy_to_unit_angle_avx
FUNCTION
(
_sk_xy_to_unit_angle_avx
)
_sk_xy_to_unit_angle_avx
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
92
200
/
/
vsubps
%
ymm0
%
ymm8
%
ymm9
.
byte
197
52
84
200
/
/
vandps
%
ymm0
%
ymm9
%
ymm9
.
byte
197
60
92
209
/
/
vsubps
%
ymm1
%
ymm8
%
ymm10
.
byte
197
44
84
209
/
/
vandps
%
ymm1
%
ymm10
%
ymm10
.
byte
196
65
52
93
218
/
/
vminps
%
ymm10
%
ymm9
%
ymm11
.
byte
196
65
52
95
226
/
/
vmaxps
%
ymm10
%
ymm9
%
ymm12
.
byte
196
65
36
94
220
/
/
vdivps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
36
89
227
/
/
vmulps
%
ymm11
%
ymm11
%
ymm12
.
byte
196
98
125
24
45
32
54
2
0
/
/
vbroadcastss
0x23620
(
%
rip
)
%
ymm13
#
3c638
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ec
>
.
byte
196
65
28
89
237
/
/
vmulps
%
ymm13
%
ymm12
%
ymm13
.
byte
196
98
125
24
53
22
54
2
0
/
/
vbroadcastss
0x23616
(
%
rip
)
%
ymm14
#
3c63c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f0
>
.
byte
196
65
20
88
238
/
/
vaddps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
28
89
237
/
/
vmulps
%
ymm13
%
ymm12
%
ymm13
.
byte
196
98
125
24
53
7
54
2
0
/
/
vbroadcastss
0x23607
(
%
rip
)
%
ymm14
#
3c640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f4
>
.
byte
196
65
20
88
238
/
/
vaddps
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
28
89
229
/
/
vmulps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
98
125
24
45
248
53
2
0
/
/
vbroadcastss
0x235f8
(
%
rip
)
%
ymm13
#
3c644
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f8
>
.
byte
196
65
28
88
229
/
/
vaddps
%
ymm13
%
ymm12
%
ymm12
.
byte
196
65
36
89
220
/
/
vmulps
%
ymm12
%
ymm11
%
ymm11
.
byte
196
65
52
194
202
1
/
/
vcmpltps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
98
125
24
21
227
53
2
0
/
/
vbroadcastss
0x235e3
(
%
rip
)
%
ymm10
#
3c648
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3fc
>
.
byte
196
65
44
92
211
/
/
vsubps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
67
37
74
202
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm11
%
ymm9
.
byte
196
193
124
194
192
1
/
/
vcmpltps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
21
121
52
2
0
/
/
vbroadcastss
0x23479
(
%
rip
)
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
44
92
209
/
/
vsubps
%
ymm9
%
ymm10
%
ymm10
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
196
65
116
194
200
1
/
/
vcmpltps
%
ymm8
%
ymm1
%
ymm9
.
byte
196
98
125
24
21
99
52
2
0
/
/
vbroadcastss
0x23463
(
%
rip
)
%
ymm10
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
44
92
208
/
/
vsubps
%
ymm0
%
ymm10
%
ymm10
.
byte
196
195
125
74
194
144
/
/
vblendvps
%
ymm9
%
ymm10
%
ymm0
%
ymm0
.
byte
196
65
124
194
200
3
/
/
vcmpunordps
%
ymm8
%
ymm0
%
ymm9
.
byte
196
195
125
74
192
144
/
/
vblendvps
%
ymm9
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_avx
.
globl
_sk_xy_to_radius_avx
FUNCTION
(
_sk_xy_to_radius_avx
)
_sk_xy_to_radius_avx
:
.
byte
197
252
89
192
/
/
vmulps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
252
81
192
/
/
vsqrtps
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_negate_x_avx
.
globl
_sk_negate_x_avx
FUNCTION
(
_sk_negate_x_avx
)
_sk_negate_x_avx
:
.
byte
196
98
125
24
5
123
53
2
0
/
/
vbroadcastss
0x2357b
(
%
rip
)
%
ymm8
#
3c64c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x400
>
.
byte
196
193
124
87
192
/
/
vxorps
%
ymm8
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_strip_avx
.
globl
_sk_xy_to_2pt_conical_strip_avx
FUNCTION
(
_sk_xy_to_2pt_conical_strip_avx
)
_sk_xy_to_2pt_conical_strip_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm8
.
byte
197
116
89
201
/
/
vmulps
%
ymm1
%
ymm1
%
ymm9
.
byte
196
65
60
92
193
/
/
vsubps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_focal_on_circle_avx
.
globl
_sk_xy_to_2pt_conical_focal_on_circle_avx
FUNCTION
(
_sk_xy_to_2pt_conical_focal_on_circle_avx
)
_sk_xy_to_2pt_conical_focal_on_circle_avx
:
.
byte
197
116
89
193
/
/
vmulps
%
ymm1
%
ymm1
%
ymm8
.
byte
197
60
94
192
/
/
vdivps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_well_behaved_avx
.
globl
_sk_xy_to_2pt_conical_well_behaved_avx
FUNCTION
(
_sk_xy_to_2pt_conical_well_behaved_avx
)
_sk_xy_to_2pt_conical_well_behaved_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
89
192
/
/
vmulps
%
ymm0
%
ymm0
%
ymm8
.
byte
197
116
89
201
/
/
vmulps
%
ymm1
%
ymm1
%
ymm9
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_greater_avx
.
globl
_sk_xy_to_2pt_conical_greater_avx
FUNCTION
(
_sk_xy_to_2pt_conical_greater_avx
)
_sk_xy_to_2pt_conical_greater_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
89
192
/
/
vmulps
%
ymm0
%
ymm0
%
ymm8
.
byte
197
116
89
201
/
/
vmulps
%
ymm1
%
ymm1
%
ymm9
.
byte
196
65
60
92
193
/
/
vsubps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_smaller_avx
.
globl
_sk_xy_to_2pt_conical_smaller_avx
FUNCTION
(
_sk_xy_to_2pt_conical_smaller_avx
)
_sk_xy_to_2pt_conical_smaller_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
89
192
/
/
vmulps
%
ymm0
%
ymm0
%
ymm8
.
byte
197
116
89
201
/
/
vmulps
%
ymm1
%
ymm1
%
ymm9
.
byte
196
65
60
92
193
/
/
vsubps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
219
52
2
0
/
/
vbroadcastss
0x234db
(
%
rip
)
%
ymm9
#
3c64c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x400
>
.
byte
196
65
60
87
193
/
/
vxorps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
197
180
89
192
/
/
vmulps
%
ymm0
%
ymm9
%
ymm0
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_compensate_focal_avx
.
globl
_sk_alter_2pt_conical_compensate_focal_avx
FUNCTION
(
_sk_alter_2pt_conical_compensate_focal_avx
)
_sk_alter_2pt_conical_compensate_focal_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
64
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm8
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_unswap_avx
.
globl
_sk_alter_2pt_conical_unswap_avx
FUNCTION
(
_sk_alter_2pt_conical_unswap_avx
)
_sk_alter_2pt_conical_unswap_avx
:
.
byte
196
98
125
24
5
91
51
2
0
/
/
vbroadcastss
0x2335b
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
188
92
192
/
/
vsubps
%
ymm0
%
ymm8
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_nan_avx
.
globl
_sk_mask_2pt_conical_nan_avx
FUNCTION
(
_sk_mask_2pt_conical_nan_avx
)
_sk_mask_2pt_conical_nan_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
124
194
193
3
/
/
vcmpunordps
%
ymm9
%
ymm0
%
ymm8
.
byte
196
67
125
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm0
%
ymm8
.
byte
196
193
124
194
193
7
/
/
vcmpordps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
65
52
194
209
15
/
/
vcmptrueps
%
ymm9
%
ymm9
%
ymm10
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_degenerates_avx
.
globl
_sk_mask_2pt_conical_degenerates_avx
FUNCTION
(
_sk_mask_2pt_conical_degenerates_avx
)
_sk_mask_2pt_conical_degenerates_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
52
194
192
5
/
/
vcmpnltps
%
ymm0
%
ymm9
%
ymm8
.
byte
196
67
125
74
193
128
/
/
vblendvps
%
ymm8
%
ymm9
%
ymm0
%
ymm8
.
byte
197
180
194
192
1
/
/
vcmpltps
%
ymm0
%
ymm9
%
ymm0
.
byte
196
65
52
194
209
15
/
/
vcmptrueps
%
ymm9
%
ymm9
%
ymm10
.
byte
196
195
53
74
194
0
/
/
vblendvps
%
ymm0
%
ymm10
%
ymm9
%
ymm0
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
192
/
/
vmovaps
%
ymm8
%
ymm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_apply_vector_mask_avx
.
globl
_sk_apply_vector_mask_avx
FUNCTION
(
_sk_apply_vector_mask_avx
)
_sk_apply_vector_mask_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
0
/
/
vmovups
(
%
rax
)
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
84
210
/
/
vandps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
84
219
/
/
vandps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_save_xy_avx
.
globl
_sk_save_xy_avx
FUNCTION
(
_sk_save_xy_avx
)
_sk_save_xy_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
202
50
2
0
/
/
vbroadcastss
0x232ca
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
124
88
200
/
/
vaddps
%
ymm8
%
ymm0
%
ymm9
.
byte
196
67
125
8
209
1
/
/
vroundps
0x1
%
ymm9
%
ymm10
.
byte
196
65
52
92
202
/
/
vsubps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
116
88
192
/
/
vaddps
%
ymm8
%
ymm1
%
ymm8
.
byte
196
67
125
8
208
1
/
/
vroundps
0x1
%
ymm8
%
ymm10
.
byte
196
65
60
92
194
/
/
vsubps
%
ymm10
%
ymm8
%
ymm8
.
byte
197
252
17
0
/
/
vmovups
%
ymm0
(
%
rax
)
.
byte
197
252
17
72
64
/
/
vmovups
%
ymm1
0x40
(
%
rax
)
.
byte
197
124
17
136
128
0
0
0
/
/
vmovups
%
ymm9
0x80
(
%
rax
)
.
byte
197
124
17
128
192
0
0
0
/
/
vmovups
%
ymm8
0xc0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_accumulate_avx
.
globl
_sk_accumulate_avx
FUNCTION
(
_sk_accumulate_avx
)
_sk_accumulate_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
128
0
1
0
0
/
/
vmovups
0x100
(
%
rax
)
%
ymm8
.
byte
197
60
89
128
64
1
0
0
/
/
vmulps
0x140
(
%
rax
)
%
ymm8
%
ymm8
.
byte
197
60
89
200
/
/
vmulps
%
ymm0
%
ymm8
%
ymm9
.
byte
197
180
88
228
/
/
vaddps
%
ymm4
%
ymm9
%
ymm4
.
byte
197
60
89
201
/
/
vmulps
%
ymm1
%
ymm8
%
ymm9
.
byte
197
180
88
237
/
/
vaddps
%
ymm5
%
ymm9
%
ymm5
.
byte
197
60
89
202
/
/
vmulps
%
ymm2
%
ymm8
%
ymm9
.
byte
197
180
88
246
/
/
vaddps
%
ymm6
%
ymm9
%
ymm6
.
byte
197
60
89
195
/
/
vmulps
%
ymm3
%
ymm8
%
ymm8
.
byte
197
188
88
255
/
/
vaddps
%
ymm7
%
ymm8
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_nx_avx
.
globl
_sk_bilinear_nx_avx
FUNCTION
(
_sk_bilinear_nx_avx
)
_sk_bilinear_nx_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
164
51
2
0
/
/
vbroadcastss
0x233a4
(
%
rip
)
%
ymm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
67
50
2
0
/
/
vbroadcastss
0x23243
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_px_avx
.
globl
_sk_bilinear_px_avx
FUNCTION
(
_sk_bilinear_px_avx
)
_sk_bilinear_px_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
32
50
2
0
/
/
vbroadcastss
0x23220
(
%
rip
)
%
ymm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
124
16
128
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_ny_avx
.
globl
_sk_bilinear_ny_avx
FUNCTION
(
_sk_bilinear_ny_avx
)
_sk_bilinear_ny_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
85
51
2
0
/
/
vbroadcastss
0x23355
(
%
rip
)
%
ymm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
243
49
2
0
/
/
vbroadcastss
0x231f3
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_py_avx
.
globl
_sk_bilinear_py_avx
FUNCTION
(
_sk_bilinear_py_avx
)
_sk_bilinear_py_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
208
49
2
0
/
/
vbroadcastss
0x231d0
(
%
rip
)
%
ymm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
197
124
16
128
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3x_avx
.
globl
_sk_bicubic_n3x_avx
FUNCTION
(
_sk_bicubic_n3x_avx
)
_sk_bicubic_n3x_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
8
51
2
0
/
/
vbroadcastss
0x23308
(
%
rip
)
%
ymm0
#
3c654
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x408
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
163
49
2
0
/
/
vbroadcastss
0x231a3
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
233
50
2
0
/
/
vbroadcastss
0x232e9
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
65
60
89
194
/
/
vmulps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
98
125
24
21
243
49
2
0
/
/
vbroadcastss
0x231f3
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
60
88
194
/
/
vaddps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
52
89
192
/
/
vmulps
%
ymm8
%
ymm9
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1x_avx
.
globl
_sk_bicubic_n1x_avx
FUNCTION
(
_sk_bicubic_n1x_avx
)
_sk_bicubic_n1x_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
178
50
2
0
/
/
vbroadcastss
0x232b2
(
%
rip
)
%
ymm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
81
49
2
0
/
/
vbroadcastss
0x23151
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
128
0
0
0
/
/
vsubps
0x80
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
160
50
2
0
/
/
vbroadcastss
0x232a0
(
%
rip
)
%
ymm9
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
65
60
89
201
/
/
vmulps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
150
50
2
0
/
/
vbroadcastss
0x23296
(
%
rip
)
%
ymm10
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
60
89
201
/
/
vmulps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
27
49
2
0
/
/
vbroadcastss
0x2311b
(
%
rip
)
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
116
50
2
0
/
/
vbroadcastss
0x23274
(
%
rip
)
%
ymm9
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1x_avx
.
globl
_sk_bicubic_p1x_avx
FUNCTION
(
_sk_bicubic_p1x_avx
)
_sk_bicubic_p1x_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
236
48
2
0
/
/
vbroadcastss
0x230ec
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
188
88
0
/
/
vaddps
(
%
rax
)
%
ymm8
%
ymm0
.
byte
197
124
16
136
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
21
59
50
2
0
/
/
vbroadcastss
0x2323b
(
%
rip
)
%
ymm10
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
65
52
89
210
/
/
vmulps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
98
125
24
29
49
50
2
0
/
/
vbroadcastss
0x23231
(
%
rip
)
%
ymm11
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
52
89
210
/
/
vmulps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
65
44
88
192
/
/
vaddps
%
ymm8
%
ymm10
%
ymm8
.
byte
196
65
52
89
192
/
/
vmulps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
98
125
24
13
24
50
2
0
/
/
vbroadcastss
0x23218
(
%
rip
)
%
ymm9
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3x_avx
.
globl
_sk_bicubic_p3x_avx
FUNCTION
(
_sk_bicubic_p3x_avx
)
_sk_bicubic_p3x_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
5
248
49
2
0
/
/
vbroadcastss
0x231f8
(
%
rip
)
%
ymm0
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
124
16
128
128
0
0
0
/
/
vmovups
0x80
(
%
rax
)
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
214
49
2
0
/
/
vbroadcastss
0x231d6
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
65
60
89
194
/
/
vmulps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
98
125
24
21
224
48
2
0
/
/
vbroadcastss
0x230e0
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
60
88
194
/
/
vaddps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
52
89
192
/
/
vmulps
%
ymm8
%
ymm9
%
ymm8
.
byte
197
124
17
128
0
1
0
0
/
/
vmovups
%
ymm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3y_avx
.
globl
_sk_bicubic_n3y_avx
FUNCTION
(
_sk_bicubic_n3y_avx
)
_sk_bicubic_n3y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
163
49
2
0
/
/
vbroadcastss
0x231a3
(
%
rip
)
%
ymm1
#
3c654
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x408
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
61
48
2
0
/
/
vbroadcastss
0x2303d
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
131
49
2
0
/
/
vbroadcastss
0x23183
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
65
60
89
194
/
/
vmulps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
98
125
24
21
141
48
2
0
/
/
vbroadcastss
0x2308d
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
60
88
194
/
/
vaddps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
52
89
192
/
/
vmulps
%
ymm8
%
ymm9
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1y_avx
.
globl
_sk_bicubic_n1y_avx
FUNCTION
(
_sk_bicubic_n1y_avx
)
_sk_bicubic_n1y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
76
49
2
0
/
/
vbroadcastss
0x2314c
(
%
rip
)
%
ymm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
98
125
24
5
234
47
2
0
/
/
vbroadcastss
0x22fea
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
128
192
0
0
0
/
/
vsubps
0xc0
(
%
rax
)
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
57
49
2
0
/
/
vbroadcastss
0x23139
(
%
rip
)
%
ymm9
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
65
60
89
201
/
/
vmulps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
47
49
2
0
/
/
vbroadcastss
0x2312f
(
%
rip
)
%
ymm10
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
60
89
201
/
/
vmulps
%
ymm9
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
180
47
2
0
/
/
vbroadcastss
0x22fb4
(
%
rip
)
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
52
88
202
/
/
vaddps
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
60
89
193
/
/
vmulps
%
ymm9
%
ymm8
%
ymm8
.
byte
196
98
125
24
13
13
49
2
0
/
/
vbroadcastss
0x2310d
(
%
rip
)
%
ymm9
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1y_avx
.
globl
_sk_bicubic_p1y_avx
FUNCTION
(
_sk_bicubic_p1y_avx
)
_sk_bicubic_p1y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
5
133
47
2
0
/
/
vbroadcastss
0x22f85
(
%
rip
)
%
ymm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
188
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm8
%
ymm1
.
byte
197
124
16
136
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
21
211
48
2
0
/
/
vbroadcastss
0x230d3
(
%
rip
)
%
ymm10
#
3c65c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x410
>
.
byte
196
65
52
89
210
/
/
vmulps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
98
125
24
29
201
48
2
0
/
/
vbroadcastss
0x230c9
(
%
rip
)
%
ymm11
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
196
65
44
88
211
/
/
vaddps
%
ymm11
%
ymm10
%
ymm10
.
byte
196
65
52
89
210
/
/
vmulps
%
ymm10
%
ymm9
%
ymm10
.
byte
196
65
44
88
192
/
/
vaddps
%
ymm8
%
ymm10
%
ymm8
.
byte
196
65
52
89
192
/
/
vmulps
%
ymm8
%
ymm9
%
ymm8
.
byte
196
98
125
24
13
176
48
2
0
/
/
vbroadcastss
0x230b0
(
%
rip
)
%
ymm9
#
3c664
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x418
>
.
byte
196
65
60
88
193
/
/
vaddps
%
ymm9
%
ymm8
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3y_avx
.
globl
_sk_bicubic_p3y_avx
FUNCTION
(
_sk_bicubic_p3y_avx
)
_sk_bicubic_p3y_avx
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
13
144
48
2
0
/
/
vbroadcastss
0x23090
(
%
rip
)
%
ymm1
#
3c660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x414
>
.
byte
197
244
88
72
64
/
/
vaddps
0x40
(
%
rax
)
%
ymm1
%
ymm1
.
byte
197
124
16
128
192
0
0
0
/
/
vmovups
0xc0
(
%
rax
)
%
ymm8
.
byte
196
65
60
89
200
/
/
vmulps
%
ymm8
%
ymm8
%
ymm9
.
byte
196
98
125
24
21
109
48
2
0
/
/
vbroadcastss
0x2306d
(
%
rip
)
%
ymm10
#
3c658
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40c
>
.
byte
196
65
60
89
194
/
/
vmulps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
98
125
24
21
119
47
2
0
/
/
vbroadcastss
0x22f77
(
%
rip
)
%
ymm10
#
3c570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x324
>
.
byte
196
65
60
88
194
/
/
vaddps
%
ymm10
%
ymm8
%
ymm8
.
byte
196
65
52
89
192
/
/
vmulps
%
ymm8
%
ymm9
%
ymm8
.
byte
197
124
17
128
64
1
0
0
/
/
vmovups
%
ymm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_callback_avx
.
globl
_sk_callback_avx
FUNCTION
(
_sk_callback_avx
)
_sk_callback_avx
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
228
224
/
/
and
0xffffffffffffffe0
%
rsp
.
byte
72
129
236
160
0
0
0
/
/
sub
0xa0
%
rsp
.
byte
197
252
41
124
36
96
/
/
vmovaps
%
ymm7
0x60
(
%
rsp
)
.
byte
197
252
41
116
36
64
/
/
vmovaps
%
ymm6
0x40
(
%
rsp
)
.
byte
197
252
41
108
36
32
/
/
vmovaps
%
ymm5
0x20
(
%
rsp
)
.
byte
197
252
41
36
36
/
/
vmovaps
%
ymm4
(
%
rsp
)
.
byte
73
137
206
/
/
mov
%
rcx
%
r14
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
73
137
253
/
/
mov
%
rdi
%
r13
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
137
195
/
/
mov
%
rax
%
rbx
.
byte
73
137
244
/
/
mov
%
rsi
%
r12
.
byte
197
252
20
225
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm4
.
byte
197
252
21
193
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
236
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm1
.
byte
197
236
21
211
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
221
20
217
/
/
vunpcklpd
%
ymm1
%
ymm4
%
ymm3
.
byte
197
221
21
201
/
/
vunpckhpd
%
ymm1
%
ymm4
%
ymm1
.
byte
197
253
20
226
/
/
vunpcklpd
%
ymm2
%
ymm0
%
ymm4
.
byte
197
253
21
194
/
/
vunpckhpd
%
ymm2
%
ymm0
%
ymm0
.
byte
196
227
101
24
209
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm2
.
byte
196
227
93
24
232
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm4
%
ymm5
.
byte
196
227
101
6
201
49
/
/
vperm2f128
0x31
%
ymm1
%
ymm3
%
ymm1
.
byte
196
227
93
6
192
49
/
/
vperm2f128
0x31
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
17
83
8
/
/
vmovups
%
ymm2
0x8
(
%
rbx
)
.
byte
197
252
17
107
40
/
/
vmovups
%
ymm5
0x28
(
%
rbx
)
.
byte
197
253
17
75
72
/
/
vmovupd
%
ymm1
0x48
(
%
rbx
)
.
byte
197
253
17
67
104
/
/
vmovupd
%
ymm0
0x68
(
%
rbx
)
.
byte
77
133
237
/
/
test
%
r13
%
r13
.
byte
190
8
0
0
0
/
/
mov
0x8
%
esi
.
byte
65
15
69
245
/
/
cmovne
%
r13d
%
esi
.
byte
72
137
223
/
/
mov
%
rbx
%
rdi
.
byte
197
248
119
/
/
vzeroupper
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
72
139
131
8
1
0
0
/
/
mov
0x108
(
%
rbx
)
%
rax
.
byte
197
248
16
0
/
/
vmovups
(
%
rax
)
%
xmm0
.
byte
197
248
16
72
16
/
/
vmovups
0x10
(
%
rax
)
%
xmm1
.
byte
197
248
16
80
32
/
/
vmovups
0x20
(
%
rax
)
%
xmm2
.
byte
197
248
16
88
48
/
/
vmovups
0x30
(
%
rax
)
%
xmm3
.
byte
196
227
101
24
88
112
1
/
/
vinsertf128
0x1
0x70
(
%
rax
)
%
ymm3
%
ymm3
.
byte
196
227
109
24
80
96
1
/
/
vinsertf128
0x1
0x60
(
%
rax
)
%
ymm2
%
ymm2
.
byte
196
227
117
24
72
80
1
/
/
vinsertf128
0x1
0x50
(
%
rax
)
%
ymm1
%
ymm1
.
byte
196
227
125
24
64
64
1
/
/
vinsertf128
0x1
0x40
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
252
20
225
/
/
vunpcklps
%
ymm1
%
ymm0
%
ymm4
.
byte
197
252
21
233
/
/
vunpckhps
%
ymm1
%
ymm0
%
ymm5
.
byte
197
236
20
203
/
/
vunpcklps
%
ymm3
%
ymm2
%
ymm1
.
byte
197
236
21
219
/
/
vunpckhps
%
ymm3
%
ymm2
%
ymm3
.
byte
197
221
20
193
/
/
vunpcklpd
%
ymm1
%
ymm4
%
ymm0
.
byte
197
221
21
201
/
/
vunpckhpd
%
ymm1
%
ymm4
%
ymm1
.
byte
197
213
20
211
/
/
vunpcklpd
%
ymm3
%
ymm5
%
ymm2
.
byte
197
213
21
219
/
/
vunpckhpd
%
ymm3
%
ymm5
%
ymm3
.
byte
76
137
230
/
/
mov
%
r12
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
137
239
/
/
mov
%
r13
%
rdi
.
byte
76
137
250
/
/
mov
%
r15
%
rdx
.
byte
76
137
241
/
/
mov
%
r14
%
rcx
.
byte
197
252
40
36
36
/
/
vmovaps
(
%
rsp
)
%
ymm4
.
byte
197
252
40
108
36
32
/
/
vmovaps
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
40
116
36
64
/
/
vmovaps
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
40
124
36
96
/
/
vmovaps
0x60
(
%
rsp
)
%
ymm7
.
byte
72
141
101
216
/
/
lea
-
0x28
(
%
rbp
)
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_3D_avx
.
globl
_sk_clut_3D_avx
FUNCTION
(
_sk_clut_3D_avx
)
_sk_clut_3D_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
129
236
216
1
0
0
/
/
sub
0x1d8
%
rsp
.
byte
197
252
17
188
36
160
1
0
0
/
/
vmovups
%
ymm7
0x1a0
(
%
rsp
)
.
byte
197
252
17
180
36
128
1
0
0
/
/
vmovups
%
ymm6
0x180
(
%
rsp
)
.
byte
197
252
17
172
36
96
1
0
0
/
/
vmovups
%
ymm5
0x160
(
%
rsp
)
.
byte
197
252
17
164
36
64
1
0
0
/
/
vmovups
%
ymm4
0x140
(
%
rsp
)
.
byte
197
252
17
156
36
32
1
0
0
/
/
vmovups
%
ymm3
0x120
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
217
/
/
vmovd
%
r9d
%
xmm3
.
byte
197
249
112
219
0
/
/
vpshufd
0x0
%
xmm3
%
xmm3
.
byte
196
227
101
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
252
17
148
36
192
0
0
0
/
/
vmovups
%
ymm2
0xc0
(
%
rsp
)
.
byte
197
254
91
218
/
/
vcvttps2dq
%
ymm2
%
ymm3
.
byte
68
139
72
12
/
/
mov
0xc
(
%
rax
)
%
r9d
.
byte
69
141
81
255
/
/
lea
-
0x1
(
%
r9
)
%
r10d
.
byte
196
193
121
110
210
/
/
vmovd
%
r10d
%
xmm2
.
byte
197
249
112
210
0
/
/
vpshufd
0x0
%
xmm2
%
xmm2
.
byte
196
227
109
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
197
236
89
201
/
/
vmulps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
252
17
76
36
192
/
/
vmovups
%
ymm1
-
0x40
(
%
rsp
)
.
byte
197
254
91
225
/
/
vcvttps2dq
%
ymm1
%
ymm4
.
byte
196
193
121
110
200
/
/
vmovd
%
r8d
%
xmm1
.
byte
197
121
112
201
0
/
/
vpshufd
0x0
%
xmm1
%
xmm9
.
byte
196
227
125
25
226
1
/
/
vextractf128
0x1
%
ymm4
%
xmm2
.
byte
197
253
111
244
/
/
vmovdqa
%
ymm4
%
ymm6
.
byte
197
254
127
116
36
64
/
/
vmovdqu
%
ymm6
0x40
(
%
rsp
)
.
byte
196
226
49
64
226
/
/
vpmulld
%
xmm2
%
xmm9
%
xmm4
.
byte
197
249
127
164
36
144
0
0
0
/
/
vmovdqa
%
xmm4
0x90
(
%
rsp
)
.
byte
196
227
125
25
223
1
/
/
vextractf128
0x1
%
ymm3
%
xmm7
.
byte
197
249
127
124
36
224
/
/
vmovdqa
%
xmm7
-
0x20
(
%
rsp
)
.
byte
197
253
111
235
/
/
vmovdqa
%
ymm3
%
ymm5
.
byte
197
254
127
172
36
160
0
0
0
/
/
vmovdqu
%
ymm5
0xa0
(
%
rsp
)
.
byte
196
193
121
110
209
/
/
vmovd
%
r9d
%
xmm2
.
byte
196
226
105
64
201
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm1
.
byte
197
249
112
217
0
/
/
vpshufd
0x0
%
xmm1
%
xmm3
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
196
193
121
110
200
/
/
vmovd
%
r8d
%
xmm1
.
byte
197
249
112
201
0
/
/
vpshufd
0x0
%
xmm1
%
xmm1
.
byte
196
227
117
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
116
89
232
/
/
vmulps
%
ymm0
%
ymm1
%
ymm13
.
byte
196
65
126
91
221
/
/
vcvttps2dq
%
ymm13
%
ymm11
.
byte
196
99
125
25
216
1
/
/
vextractf128
0x1
%
ymm11
%
xmm0
.
byte
196
226
97
64
192
/
/
vpmulld
%
xmm0
%
xmm3
%
xmm0
.
byte
197
249
127
68
36
176
/
/
vmovdqa
%
xmm0
-
0x50
(
%
rsp
)
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
197
217
254
207
/
/
vpaddd
%
xmm7
%
xmm4
%
xmm1
.
byte
197
249
254
193
/
/
vpaddd
%
xmm1
%
xmm0
%
xmm0
.
byte
196
226
121
24
37
5
46
2
0
/
/
vbroadcastss
0x22e05
(
%
rip
)
%
xmm4
#
3c66c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x420
>
.
byte
196
98
121
64
228
/
/
vpmulld
%
xmm4
%
xmm0
%
xmm12
.
byte
196
226
49
64
198
/
/
vpmulld
%
xmm6
%
xmm9
%
xmm0
.
byte
197
249
127
132
36
128
0
0
0
/
/
vmovdqa
%
xmm0
0x80
(
%
rsp
)
.
byte
197
249
254
197
/
/
vpaddd
%
xmm5
%
xmm0
%
xmm0
.
byte
196
194
97
64
211
/
/
vpmulld
%
xmm11
%
xmm3
%
xmm2
.
byte
197
249
127
84
36
160
/
/
vmovdqa
%
xmm2
-
0x60
(
%
rsp
)
.
byte
197
233
254
208
/
/
vpaddd
%
xmm0
%
xmm2
%
xmm2
.
byte
196
226
105
64
212
/
/
vpmulld
%
xmm4
%
xmm2
%
xmm2
.
byte
196
193
121
126
208
/
/
vmovd
%
xmm2
%
r8d
.
byte
196
195
121
22
209
1
/
/
vpextrd
0x1
%
xmm2
%
r9d
.
byte
196
195
121
22
210
2
/
/
vpextrd
0x2
%
xmm2
%
r10d
.
byte
196
195
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
r11d
.
byte
196
65
121
126
231
/
/
vmovd
%
xmm12
%
r15d
.
byte
196
67
121
22
230
1
/
/
vpextrd
0x1
%
xmm12
%
r14d
.
byte
196
67
121
22
228
2
/
/
vpextrd
0x2
%
xmm12
%
r12d
.
byte
196
99
121
22
227
3
/
/
vpextrd
0x3
%
xmm12
%
ebx
.
byte
196
161
122
16
44
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm5
.
byte
196
163
81
33
44
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm5
%
xmm5
.
byte
196
163
81
33
44
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm5
%
xmm5
.
byte
196
227
81
33
44
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
163
73
33
52
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm6
%
xmm6
.
byte
196
163
73
33
52
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm6
%
xmm6
.
byte
196
99
77
24
245
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm6
%
ymm14
.
byte
197
201
118
246
/
/
vpcmpeqd
%
xmm6
%
xmm6
%
xmm6
.
byte
197
233
250
238
/
/
vpsubd
%
xmm6
%
xmm2
%
xmm5
.
byte
196
195
249
22
232
1
/
/
vpextrq
0x1
%
xmm5
%
r8
.
byte
196
193
249
126
233
/
/
vmovq
%
xmm5
%
r9
.
byte
197
153
250
238
/
/
vpsubd
%
xmm6
%
xmm12
%
xmm5
.
byte
196
195
249
22
234
1
/
/
vpextrq
0x1
%
xmm5
%
r10
.
byte
196
225
249
126
235
/
/
vmovq
%
xmm5
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
44
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm5
.
byte
196
227
81
33
44
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
81
33
238
32
/
/
vinsertps
0x20
%
xmm6
%
xmm5
%
xmm5
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
81
33
238
48
/
/
vinsertps
0x30
%
xmm6
%
xmm5
%
xmm5
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
237
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm6
%
ymm5
.
byte
196
98
121
24
21
118
43
2
0
/
/
vbroadcastss
0x22b76
(
%
rip
)
%
xmm10
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
196
193
25
254
210
/
/
vpaddd
%
xmm10
%
xmm12
%
xmm2
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
105
33
214
32
/
/
vinsertps
0x20
%
xmm6
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
105
33
214
48
/
/
vinsertps
0x30
%
xmm6
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm6
%
ymm2
.
byte
196
226
125
24
61
66
44
2
0
/
/
vbroadcastss
0x22c42
(
%
rip
)
%
ymm7
#
3c668
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x41c
>
.
byte
197
148
88
247
/
/
vaddps
%
ymm7
%
ymm13
%
ymm6
.
byte
197
124
40
231
/
/
vmovaps
%
ymm7
%
ymm12
.
byte
197
124
17
36
36
/
/
vmovups
%
ymm12
(
%
rsp
)
.
byte
197
254
91
246
/
/
vcvttps2dq
%
ymm6
%
ymm6
.
byte
196
227
125
25
247
1
/
/
vextractf128
0x1
%
ymm6
%
xmm7
.
byte
196
226
97
64
255
/
/
vpmulld
%
xmm7
%
xmm3
%
xmm7
.
byte
197
249
127
124
36
144
/
/
vmovdqa
%
xmm7
-
0x70
(
%
rsp
)
.
byte
196
226
97
64
222
/
/
vpmulld
%
xmm6
%
xmm3
%
xmm3
.
byte
197
249
127
92
36
128
/
/
vmovdqa
%
xmm3
-
0x80
(
%
rsp
)
.
byte
197
225
254
192
/
/
vpaddd
%
xmm0
%
xmm3
%
xmm0
.
byte
197
193
254
201
/
/
vpaddd
%
xmm1
%
xmm7
%
xmm1
.
byte
196
226
113
64
204
/
/
vpmulld
%
xmm4
%
xmm1
%
xmm1
.
byte
196
226
121
64
220
/
/
vpmulld
%
xmm4
%
xmm0
%
xmm3
.
byte
196
193
121
126
217
/
/
vmovd
%
xmm3
%
r9d
.
byte
196
195
121
22
216
1
/
/
vpextrd
0x1
%
xmm3
%
r8d
.
byte
196
195
121
22
218
2
/
/
vpextrd
0x2
%
xmm3
%
r10d
.
byte
196
195
121
22
219
3
/
/
vpextrd
0x3
%
xmm3
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
52
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm6
.
byte
196
163
73
33
52
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm6
%
xmm6
.
byte
196
163
73
33
52
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm6
%
xmm6
.
byte
196
163
73
33
52
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm6
%
xmm6
.
byte
196
99
77
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm6
%
ymm8
.
byte
197
201
118
246
/
/
vpcmpeqd
%
xmm6
%
xmm6
%
xmm6
.
byte
197
225
250
198
/
/
vpsubd
%
xmm6
%
xmm3
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
197
241
250
198
/
/
vpsubd
%
xmm6
%
xmm1
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
60
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm7
.
byte
196
227
121
33
199
32
/
/
vinsertps
0x20
%
xmm7
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
121
33
199
48
/
/
vinsertps
0x30
%
xmm7
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
60
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
196
163
65
33
60
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
%
xmm7
.
byte
196
33
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm15
.
byte
196
195
65
33
255
32
/
/
vinsertps
0x20
%
xmm15
%
xmm7
%
xmm7
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
65
33
246
48
/
/
vinsertps
0x30
%
xmm6
%
xmm7
%
xmm6
.
byte
196
227
77
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm6
%
ymm0
.
byte
196
193
97
254
218
/
/
vpaddd
%
xmm10
%
xmm3
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
196
193
113
254
202
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm1
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
113
33
203
32
/
/
vinsertps
0x20
%
xmm3
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
113
33
203
48
/
/
vinsertps
0x30
%
xmm3
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
196
227
101
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm1
.
byte
196
193
124
91
219
/
/
vcvtdq2ps
%
ymm11
%
ymm3
.
byte
197
20
92
219
/
/
vsubps
%
ymm3
%
ymm13
%
ymm11
.
byte
196
193
60
92
222
/
/
vsubps
%
ymm14
%
ymm8
%
ymm3
.
byte
197
164
89
219
/
/
vmulps
%
ymm3
%
ymm11
%
ymm3
.
byte
197
140
88
219
/
/
vaddps
%
ymm3
%
ymm14
%
ymm3
.
byte
197
252
17
156
36
224
0
0
0
/
/
vmovups
%
ymm3
0xe0
(
%
rsp
)
.
byte
197
252
92
197
/
/
vsubps
%
ymm5
%
ymm0
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
212
88
192
/
/
vaddps
%
ymm0
%
ymm5
%
ymm0
.
byte
197
252
17
68
36
32
/
/
vmovups
%
ymm0
0x20
(
%
rsp
)
.
byte
197
244
92
194
/
/
vsubps
%
ymm2
%
ymm1
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
236
88
192
/
/
vaddps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
252
17
132
36
0
1
0
0
/
/
vmovups
%
ymm0
0x100
(
%
rsp
)
.
byte
197
156
88
68
36
192
/
/
vaddps
-
0x40
(
%
rsp
)
%
ymm12
%
ymm0
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
226
49
64
200
/
/
vpmulld
%
xmm0
%
xmm9
%
xmm1
.
byte
197
249
127
76
36
96
/
/
vmovdqa
%
xmm1
0x60
(
%
rsp
)
.
byte
196
227
125
25
192
1
/
/
vextractf128
0x1
%
ymm0
%
xmm0
.
byte
196
226
49
64
192
/
/
vpmulld
%
xmm0
%
xmm9
%
xmm0
.
byte
197
249
127
68
36
112
/
/
vmovdqa
%
xmm0
0x70
(
%
rsp
)
.
byte
197
249
254
116
36
224
/
/
vpaddd
-
0x20
(
%
rsp
)
%
xmm0
%
xmm6
.
byte
197
121
111
100
36
176
/
/
vmovdqa
-
0x50
(
%
rsp
)
%
xmm12
.
byte
196
193
73
254
196
/
/
vpaddd
%
xmm12
%
xmm6
%
xmm0
.
byte
196
98
121
64
196
/
/
vpmulld
%
xmm4
%
xmm0
%
xmm8
.
byte
197
241
254
140
36
160
0
0
0
/
/
vpaddd
0xa0
(
%
rsp
)
%
xmm1
%
xmm1
.
byte
197
249
111
108
36
160
/
/
vmovdqa
-
0x60
(
%
rsp
)
%
xmm5
.
byte
197
241
254
197
/
/
vpaddd
%
xmm5
%
xmm1
%
xmm0
.
byte
196
226
121
64
196
/
/
vpmulld
%
xmm4
%
xmm0
%
xmm0
.
byte
196
193
121
126
193
/
/
vmovd
%
xmm0
%
r9d
.
byte
196
195
121
22
192
1
/
/
vpextrd
0x1
%
xmm0
%
r8d
.
byte
196
195
121
22
194
2
/
/
vpextrd
0x2
%
xmm0
%
r10d
.
byte
196
195
121
22
195
3
/
/
vpextrd
0x3
%
xmm0
%
r11d
.
byte
196
65
121
126
199
/
/
vmovd
%
xmm8
%
r15d
.
byte
196
67
121
22
198
1
/
/
vpextrd
0x1
%
xmm8
%
r14d
.
byte
196
67
121
22
196
2
/
/
vpextrd
0x2
%
xmm8
%
r12d
.
byte
196
99
121
22
195
3
/
/
vpextrd
0x3
%
xmm8
%
ebx
.
byte
196
161
122
16
60
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm7
.
byte
196
163
65
33
60
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm7
%
xmm7
.
byte
196
163
65
33
60
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm7
%
xmm7
.
byte
196
227
65
33
60
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
20
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm2
.
byte
196
163
105
33
20
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm2
%
xmm2
.
byte
196
227
109
24
215
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm2
%
ymm2
.
byte
197
252
17
84
36
224
/
/
vmovups
%
ymm2
-
0x20
(
%
rsp
)
.
byte
197
225
118
219
/
/
vpcmpeqd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
249
250
211
/
/
vpsubd
%
xmm3
%
xmm0
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
197
185
250
211
/
/
vpsubd
%
xmm3
%
xmm8
%
xmm2
.
byte
196
65
17
118
237
/
/
vpcmpeqd
%
xmm13
%
xmm13
%
xmm13
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
60
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm7
.
byte
196
227
105
33
215
32
/
/
vinsertps
0x20
%
xmm7
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
105
33
215
48
/
/
vinsertps
0x30
%
xmm7
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
60
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
196
163
65
33
60
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
65
33
219
32
/
/
vinsertps
0x20
%
xmm3
%
xmm7
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
97
33
223
48
/
/
vinsertps
0x30
%
xmm7
%
xmm3
%
xmm3
.
byte
196
99
101
24
202
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm9
.
byte
196
193
121
254
194
/
/
vpaddd
%
xmm10
%
xmm0
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
57
254
194
/
/
vpaddd
%
xmm10
%
xmm8
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
20
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
196
227
121
33
194
32
/
/
vinsertps
0x20
%
xmm2
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
121
33
194
48
/
/
vinsertps
0x30
%
xmm2
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
20
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
196
163
105
33
20
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
28
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
196
99
109
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm8
.
byte
197
121
111
124
36
128
/
/
vmovdqa
-
0x80
(
%
rsp
)
%
xmm15
.
byte
197
129
254
193
/
/
vpaddd
%
xmm1
%
xmm15
%
xmm0
.
byte
197
121
111
116
36
144
/
/
vmovdqa
-
0x70
(
%
rsp
)
%
xmm14
.
byte
197
137
254
206
/
/
vpaddd
%
xmm6
%
xmm14
%
xmm1
.
byte
196
226
113
64
244
/
/
vpmulld
%
xmm4
%
xmm1
%
xmm6
.
byte
196
226
121
64
204
/
/
vpmulld
%
xmm4
%
xmm0
%
xmm1
.
byte
196
193
121
126
201
/
/
vmovd
%
xmm1
%
r9d
.
byte
196
195
121
22
200
1
/
/
vpextrd
0x1
%
xmm1
%
r8d
.
byte
196
195
121
22
202
2
/
/
vpextrd
0x2
%
xmm1
%
r10d
.
byte
196
195
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
r11d
.
byte
196
193
121
126
247
/
/
vmovd
%
xmm6
%
r15d
.
byte
196
195
121
22
246
1
/
/
vpextrd
0x1
%
xmm6
%
r14d
.
byte
196
195
121
22
244
2
/
/
vpextrd
0x2
%
xmm6
%
r12d
.
byte
196
227
121
22
243
3
/
/
vpextrd
0x3
%
xmm6
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
20
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm2
.
byte
196
163
105
33
20
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm2
%
xmm2
.
byte
196
227
109
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm0
.
byte
196
193
113
250
213
/
/
vpsubd
%
xmm13
%
xmm1
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
196
193
73
250
213
/
/
vpsubd
%
xmm13
%
xmm6
%
xmm2
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
97
33
223
32
/
/
vinsertps
0x20
%
xmm7
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
97
33
223
48
/
/
vinsertps
0x30
%
xmm7
%
xmm3
%
xmm3
.
byte
196
99
101
24
234
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm13
.
byte
196
193
113
254
202
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm1
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
196
193
73
254
202
/
/
vpaddd
%
xmm10
%
xmm6
%
xmm1
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
20
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
196
227
113
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
113
33
202
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
20
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
196
163
105
33
20
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
28
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
196
227
109
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm1
.
byte
197
252
16
84
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm2
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
236
88
208
/
/
vaddps
%
ymm0
%
ymm2
%
ymm2
.
byte
196
193
20
92
193
/
/
vsubps
%
ymm9
%
ymm13
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
180
88
216
/
/
vaddps
%
ymm0
%
ymm9
%
ymm3
.
byte
196
193
116
92
192
/
/
vsubps
%
ymm8
%
ymm1
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
188
88
200
/
/
vaddps
%
ymm0
%
ymm8
%
ymm1
.
byte
197
252
91
68
36
64
/
/
vcvtdq2ps
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
16
116
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm6
.
byte
197
76
92
232
/
/
vsubps
%
ymm0
%
ymm6
%
ymm13
.
byte
197
252
16
132
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm0
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
197
148
89
210
/
/
vmulps
%
ymm2
%
ymm13
%
ymm2
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
192
/
/
vmovups
%
ymm0
-
0x40
(
%
rsp
)
.
byte
197
252
16
68
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm0
.
byte
197
228
92
208
/
/
vsubps
%
ymm0
%
ymm3
%
ymm2
.
byte
197
148
89
210
/
/
vmulps
%
ymm2
%
ymm13
%
ymm2
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
64
/
/
vmovups
%
ymm0
0x40
(
%
rsp
)
.
byte
197
252
16
132
36
0
1
0
0
/
/
vmovups
0x100
(
%
rsp
)
%
ymm0
.
byte
197
244
92
200
/
/
vsubps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
148
89
201
/
/
vmulps
%
ymm1
%
ymm13
%
ymm1
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
17
68
36
32
/
/
vmovups
%
ymm0
0x20
(
%
rsp
)
.
byte
197
252
16
4
36
/
/
vmovups
(
%
rsp
)
%
ymm0
.
byte
197
252
88
140
36
192
0
0
0
/
/
vaddps
0xc0
(
%
rsp
)
%
ymm0
%
ymm1
.
byte
197
126
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm9
.
byte
196
99
125
25
200
1
/
/
vextractf128
0x1
%
ymm9
%
xmm0
.
byte
197
249
127
4
36
/
/
vmovdqa
%
xmm0
(
%
rsp
)
.
byte
197
249
254
180
36
144
0
0
0
/
/
vpaddd
0x90
(
%
rsp
)
%
xmm0
%
xmm6
.
byte
197
177
254
156
36
128
0
0
0
/
/
vpaddd
0x80
(
%
rsp
)
%
xmm9
%
xmm3
.
byte
197
153
254
206
/
/
vpaddd
%
xmm6
%
xmm12
%
xmm1
.
byte
196
226
113
64
204
/
/
vpmulld
%
xmm4
%
xmm1
%
xmm1
.
byte
197
209
254
211
/
/
vpaddd
%
xmm3
%
xmm5
%
xmm2
.
byte
196
226
105
64
212
/
/
vpmulld
%
xmm4
%
xmm2
%
xmm2
.
byte
196
193
121
126
209
/
/
vmovd
%
xmm2
%
r9d
.
byte
196
195
121
22
208
1
/
/
vpextrd
0x1
%
xmm2
%
r8d
.
byte
196
195
121
22
210
2
/
/
vpextrd
0x2
%
xmm2
%
r10d
.
byte
196
195
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
60
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm7
.
byte
196
163
65
33
60
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm7
%
xmm7
.
byte
196
163
65
33
60
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm7
%
xmm7
.
byte
196
227
65
33
60
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
4
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm0
.
byte
196
163
121
33
4
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm0
%
xmm0
.
byte
196
99
125
24
231
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm0
%
ymm12
.
byte
197
209
118
237
/
/
vpcmpeqd
%
xmm5
%
xmm5
%
xmm5
.
byte
197
233
250
197
/
/
vpsubd
%
xmm5
%
xmm2
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
197
241
250
197
/
/
vpsubd
%
xmm5
%
xmm1
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
60
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm7
.
byte
196
227
121
33
199
32
/
/
vinsertps
0x20
%
xmm7
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
121
33
199
48
/
/
vinsertps
0x30
%
xmm7
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
60
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
196
163
65
33
60
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
65
33
237
32
/
/
vinsertps
0x20
%
xmm5
%
xmm7
%
xmm5
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
81
33
239
48
/
/
vinsertps
0x30
%
xmm7
%
xmm5
%
xmm5
.
byte
196
227
85
24
248
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm5
%
ymm7
.
byte
196
193
105
254
194
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
113
254
194
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
12
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
196
227
121
33
193
32
/
/
vinsertps
0x20
%
xmm1
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
12
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm1
.
byte
196
227
121
33
193
48
/
/
vinsertps
0x30
%
xmm1
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
12
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
196
163
113
33
12
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
113
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm1
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
20
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm2
.
byte
196
227
113
33
202
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm1
.
byte
196
99
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm8
.
byte
197
129
254
195
/
/
vpaddd
%
xmm3
%
xmm15
%
xmm0
.
byte
197
137
254
206
/
/
vpaddd
%
xmm6
%
xmm14
%
xmm1
.
byte
196
226
113
64
204
/
/
vpmulld
%
xmm4
%
xmm1
%
xmm1
.
byte
196
226
121
64
212
/
/
vpmulld
%
xmm4
%
xmm0
%
xmm2
.
byte
196
193
121
126
209
/
/
vmovd
%
xmm2
%
r9d
.
byte
196
195
121
22
208
1
/
/
vpextrd
0x1
%
xmm2
%
r8d
.
byte
196
195
121
22
210
2
/
/
vpextrd
0x2
%
xmm2
%
r10d
.
byte
196
195
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
216
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm3
%
ymm3
.
byte
197
209
118
237
/
/
vpcmpeqd
%
xmm5
%
xmm5
%
xmm5
.
byte
197
233
250
197
/
/
vpsubd
%
xmm5
%
xmm2
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
197
241
250
197
/
/
vpsubd
%
xmm5
%
xmm1
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
44
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm5
.
byte
196
227
121
33
197
32
/
/
vinsertps
0x20
%
xmm5
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
121
33
197
48
/
/
vinsertps
0x30
%
xmm5
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
44
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
196
163
81
33
44
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
81
33
238
32
/
/
vinsertps
0x20
%
xmm6
%
xmm5
%
xmm5
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
81
33
238
48
/
/
vinsertps
0x30
%
xmm6
%
xmm5
%
xmm5
.
byte
196
227
85
24
240
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm5
%
ymm6
.
byte
196
193
105
254
194
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
113
254
194
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
12
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
196
227
121
33
193
32
/
/
vinsertps
0x20
%
xmm1
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
12
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm1
.
byte
196
227
121
33
193
48
/
/
vinsertps
0x30
%
xmm1
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
12
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
196
163
113
33
12
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
113
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm1
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
20
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm2
.
byte
196
227
113
33
202
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm1
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
193
100
92
204
/
/
vsubps
%
ymm12
%
ymm3
%
ymm1
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
28
88
225
/
/
vaddps
%
ymm1
%
ymm12
%
ymm12
.
byte
197
204
92
207
/
/
vsubps
%
ymm7
%
ymm6
%
ymm1
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
68
88
241
/
/
vaddps
%
ymm1
%
ymm7
%
ymm14
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
164
89
192
/
/
vmulps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
60
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
249
111
4
36
/
/
vmovdqa
(
%
rsp
)
%
xmm0
.
byte
197
249
254
116
36
112
/
/
vpaddd
0x70
(
%
rsp
)
%
xmm0
%
xmm6
.
byte
197
177
254
76
36
96
/
/
vpaddd
0x60
(
%
rsp
)
%
xmm9
%
xmm1
.
byte
197
241
254
68
36
160
/
/
vpaddd
-
0x60
(
%
rsp
)
%
xmm1
%
xmm0
.
byte
197
201
254
84
36
176
/
/
vpaddd
-
0x50
(
%
rsp
)
%
xmm6
%
xmm2
.
byte
196
98
105
64
204
/
/
vpmulld
%
xmm4
%
xmm2
%
xmm9
.
byte
196
226
121
64
220
/
/
vpmulld
%
xmm4
%
xmm0
%
xmm3
.
byte
196
193
121
126
217
/
/
vmovd
%
xmm3
%
r9d
.
byte
196
195
121
22
216
1
/
/
vpextrd
0x1
%
xmm3
%
r8d
.
byte
196
195
121
22
218
2
/
/
vpextrd
0x2
%
xmm3
%
r10d
.
byte
196
195
121
22
219
3
/
/
vpextrd
0x3
%
xmm3
%
r11d
.
byte
196
65
121
126
207
/
/
vmovd
%
xmm9
%
r15d
.
byte
196
67
121
22
206
1
/
/
vpextrd
0x1
%
xmm9
%
r14d
.
byte
196
67
121
22
204
2
/
/
vpextrd
0x2
%
xmm9
%
r12d
.
byte
196
99
121
22
203
3
/
/
vpextrd
0x3
%
xmm9
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
20
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm2
.
byte
196
163
105
33
20
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm2
%
xmm2
.
byte
196
227
109
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm0
.
byte
197
209
118
237
/
/
vpcmpeqd
%
xmm5
%
xmm5
%
xmm5
.
byte
197
225
250
213
/
/
vpsubd
%
xmm5
%
xmm3
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
197
177
250
213
/
/
vpsubd
%
xmm5
%
xmm9
%
xmm2
.
byte
196
65
1
118
255
/
/
vpcmpeqd
%
xmm15
%
xmm15
%
xmm15
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
44
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm5
.
byte
196
227
105
33
213
32
/
/
vinsertps
0x20
%
xmm5
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
105
33
213
48
/
/
vinsertps
0x30
%
xmm5
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
44
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
196
163
81
33
44
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
81
33
239
32
/
/
vinsertps
0x20
%
xmm7
%
xmm5
%
xmm5
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
81
33
239
48
/
/
vinsertps
0x30
%
xmm7
%
xmm5
%
xmm5
.
byte
196
227
85
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm5
%
ymm2
.
byte
196
193
97
254
218
/
/
vpaddd
%
xmm10
%
xmm3
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
196
193
49
254
218
/
/
vpaddd
%
xmm10
%
xmm9
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
44
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm5
.
byte
196
227
97
33
221
32
/
/
vinsertps
0x20
%
xmm5
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
97
33
221
48
/
/
vinsertps
0x30
%
xmm5
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
44
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
196
163
81
33
44
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
81
33
239
32
/
/
vinsertps
0x20
%
xmm7
%
xmm5
%
xmm5
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
81
33
239
48
/
/
vinsertps
0x30
%
xmm7
%
xmm5
%
xmm5
.
byte
196
99
85
24
203
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm5
%
ymm9
.
byte
197
241
254
76
36
128
/
/
vpaddd
-
0x80
(
%
rsp
)
%
xmm1
%
xmm1
.
byte
197
201
254
92
36
144
/
/
vpaddd
-
0x70
(
%
rsp
)
%
xmm6
%
xmm3
.
byte
196
226
97
64
220
/
/
vpmulld
%
xmm4
%
xmm3
%
xmm3
.
byte
196
226
113
64
228
/
/
vpmulld
%
xmm4
%
xmm1
%
xmm4
.
byte
196
193
121
126
225
/
/
vmovd
%
xmm4
%
r9d
.
byte
196
195
121
22
224
1
/
/
vpextrd
0x1
%
xmm4
%
r8d
.
byte
196
195
121
22
226
2
/
/
vpextrd
0x2
%
xmm4
%
r10d
.
byte
196
195
121
22
227
3
/
/
vpextrd
0x3
%
xmm4
%
r11d
.
byte
196
193
121
126
223
/
/
vmovd
%
xmm3
%
r15d
.
byte
196
195
121
22
222
1
/
/
vpextrd
0x1
%
xmm3
%
r14d
.
byte
196
195
121
22
220
2
/
/
vpextrd
0x2
%
xmm3
%
r12d
.
byte
196
227
121
22
219
3
/
/
vpextrd
0x3
%
xmm3
%
ebx
.
byte
196
161
122
16
12
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm1
.
byte
196
163
113
33
12
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm1
%
xmm1
.
byte
196
163
113
33
12
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm1
%
xmm1
.
byte
196
227
113
33
12
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
44
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm5
.
byte
196
163
81
33
44
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm5
%
xmm5
.
byte
196
163
81
33
44
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm5
%
xmm5
.
byte
196
163
81
33
44
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm5
%
xmm5
.
byte
196
227
85
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm5
%
ymm1
.
byte
196
193
89
250
239
/
/
vpsubd
%
xmm15
%
xmm4
%
xmm5
.
byte
196
195
249
22
232
1
/
/
vpextrq
0x1
%
xmm5
%
r8
.
byte
196
193
249
126
233
/
/
vmovq
%
xmm5
%
r9
.
byte
196
193
97
250
239
/
/
vpsubd
%
xmm15
%
xmm3
%
xmm5
.
byte
196
195
249
22
234
1
/
/
vpextrq
0x1
%
xmm5
%
r10
.
byte
196
225
249
126
235
/
/
vmovq
%
xmm5
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
44
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm5
.
byte
196
227
81
33
44
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
81
33
238
32
/
/
vinsertps
0x20
%
xmm6
%
xmm5
%
xmm5
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
81
33
238
48
/
/
vinsertps
0x30
%
xmm6
%
xmm5
%
xmm5
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
245
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm6
%
ymm6
.
byte
196
193
89
254
226
/
/
vpaddd
%
xmm10
%
xmm4
%
xmm4
.
byte
196
195
249
22
224
1
/
/
vpextrq
0x1
%
xmm4
%
r8
.
byte
196
193
249
126
225
/
/
vmovq
%
xmm4
%
r9
.
byte
196
193
97
254
218
/
/
vpaddd
%
xmm10
%
xmm3
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
36
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
196
163
89
33
36
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm4
%
xmm4
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
89
33
229
32
/
/
vinsertps
0x20
%
xmm5
%
xmm4
%
xmm4
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
44
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
196
227
89
33
229
48
/
/
vinsertps
0x30
%
xmm5
%
xmm4
%
xmm4
.
byte
196
227
93
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm4
%
ymm3
.
byte
197
244
92
200
/
/
vsubps
%
ymm0
%
ymm1
%
ymm1
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
204
92
202
/
/
vsubps
%
ymm2
%
ymm6
%
ymm1
.
byte
197
164
89
201
/
/
vmulps
%
ymm1
%
ymm11
%
ymm1
.
byte
197
236
88
201
/
/
vaddps
%
ymm1
%
ymm2
%
ymm1
.
byte
196
193
100
92
209
/
/
vsubps
%
ymm9
%
ymm3
%
ymm2
.
byte
197
164
89
210
/
/
vmulps
%
ymm2
%
ymm11
%
ymm2
.
byte
197
180
88
210
/
/
vaddps
%
ymm2
%
ymm9
%
ymm2
.
byte
196
193
124
92
196
/
/
vsubps
%
ymm12
%
ymm0
%
ymm0
.
byte
197
148
89
192
/
/
vmulps
%
ymm0
%
ymm13
%
ymm0
.
byte
197
156
88
192
/
/
vaddps
%
ymm0
%
ymm12
%
ymm0
.
byte
196
193
116
92
206
/
/
vsubps
%
ymm14
%
ymm1
%
ymm1
.
byte
197
148
89
201
/
/
vmulps
%
ymm1
%
ymm13
%
ymm1
.
byte
197
140
88
201
/
/
vaddps
%
ymm1
%
ymm14
%
ymm1
.
byte
196
193
108
92
208
/
/
vsubps
%
ymm8
%
ymm2
%
ymm2
.
byte
197
148
89
210
/
/
vmulps
%
ymm2
%
ymm13
%
ymm2
.
byte
197
188
88
210
/
/
vaddps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
252
91
156
36
160
0
0
0
/
/
vcvtdq2ps
0xa0
(
%
rsp
)
%
ymm3
.
byte
197
252
16
164
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm4
.
byte
197
220
92
219
/
/
vsubps
%
ymm3
%
ymm4
%
ymm3
.
byte
197
252
16
100
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
16
100
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm4
.
byte
197
244
92
204
/
/
vsubps
%
ymm4
%
ymm1
%
ymm1
.
byte
197
228
89
201
/
/
vmulps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
220
88
201
/
/
vaddps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
252
16
100
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm4
.
byte
197
236
92
212
/
/
vsubps
%
ymm4
%
ymm2
%
ymm2
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
220
88
210
/
/
vaddps
%
ymm2
%
ymm4
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
156
36
32
1
0
0
/
/
vmovups
0x120
(
%
rsp
)
%
ymm3
.
byte
197
252
16
164
36
64
1
0
0
/
/
vmovups
0x140
(
%
rsp
)
%
ymm4
.
byte
197
252
16
172
36
96
1
0
0
/
/
vmovups
0x160
(
%
rsp
)
%
ymm5
.
byte
197
252
16
180
36
128
1
0
0
/
/
vmovups
0x180
(
%
rsp
)
%
ymm6
.
byte
197
252
16
188
36
160
1
0
0
/
/
vmovups
0x1a0
(
%
rsp
)
%
ymm7
.
byte
72
129
196
216
1
0
0
/
/
add
0x1d8
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_4D_avx
.
globl
_sk_clut_4D_avx
FUNCTION
(
_sk_clut_4D_avx
)
_sk_clut_4D_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
129
236
216
2
0
0
/
/
sub
0x2d8
%
rsp
.
byte
197
252
17
188
36
160
2
0
0
/
/
vmovups
%
ymm7
0x2a0
(
%
rsp
)
.
byte
197
252
17
180
36
128
2
0
0
/
/
vmovups
%
ymm6
0x280
(
%
rsp
)
.
byte
197
252
17
172
36
96
2
0
0
/
/
vmovups
%
ymm5
0x260
(
%
rsp
)
.
byte
197
252
17
164
36
64
2
0
0
/
/
vmovups
%
ymm4
0x240
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
20
/
/
mov
0x14
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
225
/
/
vmovd
%
r9d
%
xmm4
.
byte
197
249
112
228
0
/
/
vpshufd
0x0
%
xmm4
%
xmm4
.
byte
196
227
93
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
197
92
89
203
/
/
vmulps
%
ymm3
%
ymm4
%
ymm9
.
byte
197
124
17
140
36
224
1
0
0
/
/
vmovups
%
ymm9
0x1e0
(
%
rsp
)
.
byte
68
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9d
.
byte
69
141
81
255
/
/
lea
-
0x1
(
%
r9
)
%
r10d
.
byte
196
193
121
110
218
/
/
vmovd
%
r10d
%
xmm3
.
byte
197
249
112
219
0
/
/
vpshufd
0x0
%
xmm3
%
xmm3
.
byte
196
227
101
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
252
17
148
36
0
1
0
0
/
/
vmovups
%
ymm2
0x100
(
%
rsp
)
.
byte
197
254
91
218
/
/
vcvttps2dq
%
ymm2
%
ymm3
.
byte
196
227
125
25
218
1
/
/
vextractf128
0x1
%
ymm3
%
xmm2
.
byte
197
253
111
251
/
/
vmovdqa
%
ymm3
%
ymm7
.
byte
197
254
127
188
36
64
1
0
0
/
/
vmovdqu
%
ymm7
0x140
(
%
rsp
)
.
byte
196
193
121
110
216
/
/
vmovd
%
r8d
%
xmm3
.
byte
197
249
112
227
0
/
/
vpshufd
0x0
%
xmm3
%
xmm4
.
byte
196
226
89
64
242
/
/
vpmulld
%
xmm2
%
xmm4
%
xmm6
.
byte
197
249
111
236
/
/
vmovdqa
%
xmm4
%
xmm5
.
byte
197
249
127
108
36
192
/
/
vmovdqa
%
xmm5
-
0x40
(
%
rsp
)
.
byte
197
249
127
180
36
160
0
0
0
/
/
vmovdqa
%
xmm6
0xa0
(
%
rsp
)
.
byte
196
193
121
110
209
/
/
vmovd
%
r9d
%
xmm2
.
byte
196
226
105
64
211
/
/
vpmulld
%
xmm3
%
xmm2
%
xmm2
.
byte
197
121
112
194
0
/
/
vpshufd
0x0
%
xmm2
%
xmm8
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
196
193
121
110
217
/
/
vmovd
%
r9d
%
xmm3
.
byte
197
249
112
219
0
/
/
vpshufd
0x0
%
xmm3
%
xmm3
.
byte
196
227
101
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm3
%
ymm3
.
byte
197
252
91
219
/
/
vcvtdq2ps
%
ymm3
%
ymm3
.
byte
197
228
89
201
/
/
vmulps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
252
17
140
36
128
0
0
0
/
/
vmovups
%
ymm1
0x80
(
%
rsp
)
.
byte
197
254
91
217
/
/
vcvttps2dq
%
ymm1
%
ymm3
.
byte
196
227
125
25
217
1
/
/
vextractf128
0x1
%
ymm3
%
xmm1
.
byte
197
125
111
211
/
/
vmovdqa
%
ymm3
%
ymm10
.
byte
197
126
127
148
36
192
0
0
0
/
/
vmovdqu
%
ymm10
0xc0
(
%
rsp
)
.
byte
196
226
57
64
217
/
/
vpmulld
%
xmm1
%
xmm8
%
xmm3
.
byte
197
249
127
156
36
224
0
0
0
/
/
vmovdqa
%
xmm3
0xe0
(
%
rsp
)
.
byte
196
193
121
110
200
/
/
vmovd
%
r8d
%
xmm1
.
byte
196
226
113
64
202
/
/
vpmulld
%
xmm2
%
xmm1
%
xmm1
.
byte
197
249
112
225
0
/
/
vpshufd
0x0
%
xmm1
%
xmm4
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
196
193
121
110
200
/
/
vmovd
%
r8d
%
xmm1
.
byte
197
249
112
201
0
/
/
vpshufd
0x0
%
xmm1
%
xmm1
.
byte
196
227
117
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm1
.
byte
197
252
91
201
/
/
vcvtdq2ps
%
ymm1
%
ymm1
.
byte
197
116
89
248
/
/
vmulps
%
ymm0
%
ymm1
%
ymm15
.
byte
196
65
126
91
223
/
/
vcvttps2dq
%
ymm15
%
ymm11
.
byte
196
99
125
25
216
1
/
/
vextractf128
0x1
%
ymm11
%
xmm0
.
byte
196
226
89
64
208
/
/
vpmulld
%
xmm0
%
xmm4
%
xmm2
.
byte
197
249
127
84
36
128
/
/
vmovdqa
%
xmm2
-
0x80
(
%
rsp
)
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
196
193
126
91
201
/
/
vcvttps2dq
%
ymm9
%
ymm1
.
byte
197
254
127
140
36
192
1
0
0
/
/
vmovdqu
%
ymm1
0x1c0
(
%
rsp
)
.
byte
196
227
125
25
200
1
/
/
vextractf128
0x1
%
ymm1
%
xmm0
.
byte
197
249
127
68
36
96
/
/
vmovdqa
%
xmm0
0x60
(
%
rsp
)
.
byte
197
201
254
192
/
/
vpaddd
%
xmm0
%
xmm6
%
xmm0
.
byte
197
249
127
68
36
32
/
/
vmovdqa
%
xmm0
0x20
(
%
rsp
)
.
byte
197
225
254
216
/
/
vpaddd
%
xmm0
%
xmm3
%
xmm3
.
byte
197
233
254
195
/
/
vpaddd
%
xmm3
%
xmm2
%
xmm0
.
byte
196
98
121
24
13
6
29
2
0
/
/
vbroadcastss
0x21d06
(
%
rip
)
%
xmm9
#
3c66c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x420
>
.
byte
196
194
121
64
209
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm2
.
byte
196
226
81
64
199
/
/
vpmulld
%
xmm7
%
xmm5
%
xmm0
.
byte
197
249
127
132
36
96
1
0
0
/
/
vmovdqa
%
xmm0
0x160
(
%
rsp
)
.
byte
197
121
254
241
/
/
vpaddd
%
xmm1
%
xmm0
%
xmm14
.
byte
196
194
57
64
194
/
/
vpmulld
%
xmm10
%
xmm8
%
xmm0
.
byte
197
249
127
68
36
240
/
/
vmovdqa
%
xmm0
-
0x10
(
%
rsp
)
.
byte
196
193
121
254
206
/
/
vpaddd
%
xmm14
%
xmm0
%
xmm1
.
byte
196
194
89
64
195
/
/
vpmulld
%
xmm11
%
xmm4
%
xmm0
.
byte
197
249
127
68
36
144
/
/
vmovdqa
%
xmm0
-
0x70
(
%
rsp
)
.
byte
197
249
254
193
/
/
vpaddd
%
xmm1
%
xmm0
%
xmm0
.
byte
196
194
121
64
193
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm0
.
byte
196
193
121
126
192
/
/
vmovd
%
xmm0
%
r8d
.
byte
196
195
121
22
193
1
/
/
vpextrd
0x1
%
xmm0
%
r9d
.
byte
196
195
121
22
194
2
/
/
vpextrd
0x2
%
xmm0
%
r10d
.
byte
196
195
121
22
195
3
/
/
vpextrd
0x3
%
xmm0
%
r11d
.
byte
196
193
121
126
215
/
/
vmovd
%
xmm2
%
r15d
.
byte
196
195
121
22
214
1
/
/
vpextrd
0x1
%
xmm2
%
r14d
.
byte
196
195
121
22
212
2
/
/
vpextrd
0x2
%
xmm2
%
r12d
.
byte
196
227
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
ebx
.
byte
196
161
122
16
44
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm5
.
byte
196
163
81
33
44
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm5
%
xmm5
.
byte
196
163
81
33
44
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm5
%
xmm5
.
byte
196
227
81
33
44
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
163
65
33
60
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
%
xmm7
.
byte
196
163
65
33
60
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm7
%
xmm7
.
byte
196
163
65
33
60
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm7
%
xmm7
.
byte
196
99
69
24
237
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm7
%
ymm13
.
byte
197
201
118
246
/
/
vpcmpeqd
%
xmm6
%
xmm6
%
xmm6
.
byte
197
249
250
238
/
/
vpsubd
%
xmm6
%
xmm0
%
xmm5
.
byte
196
195
249
22
232
1
/
/
vpextrq
0x1
%
xmm5
%
r8
.
byte
196
193
249
126
233
/
/
vmovq
%
xmm5
%
r9
.
byte
197
233
250
238
/
/
vpsubd
%
xmm6
%
xmm2
%
xmm5
.
byte
196
195
249
22
234
1
/
/
vpextrq
0x1
%
xmm5
%
r10
.
byte
196
225
249
126
235
/
/
vmovq
%
xmm5
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
44
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm5
.
byte
196
227
81
33
44
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
60
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm7
.
byte
196
227
81
33
239
32
/
/
vinsertps
0x20
%
xmm7
%
xmm5
%
xmm5
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
81
33
239
48
/
/
vinsertps
0x30
%
xmm7
%
xmm5
%
xmm5
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
60
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
196
163
65
33
60
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
65
33
246
32
/
/
vinsertps
0x20
%
xmm6
%
xmm7
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
237
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm6
%
ymm5
.
byte
196
98
121
24
21
103
26
2
0
/
/
vbroadcastss
0x21a67
(
%
rip
)
%
xmm10
#
3c508
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2bc
>
.
byte
196
193
121
254
194
/
/
vpaddd
%
xmm10
%
xmm0
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
105
254
194
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
20
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
196
227
121
33
194
32
/
/
vinsertps
0x20
%
xmm2
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
121
33
194
48
/
/
vinsertps
0x30
%
xmm2
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
20
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
196
163
105
33
20
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
105
33
214
32
/
/
vinsertps
0x20
%
xmm6
%
xmm2
%
xmm2
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
105
33
214
48
/
/
vinsertps
0x30
%
xmm6
%
xmm2
%
xmm2
.
byte
196
227
109
24
208
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm2
.
byte
196
98
125
24
37
51
27
2
0
/
/
vbroadcastss
0x21b33
(
%
rip
)
%
ymm12
#
3c668
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x41c
>
.
byte
196
193
4
88
196
/
/
vaddps
%
ymm12
%
ymm15
%
ymm0
.
byte
197
124
17
36
36
/
/
vmovups
%
ymm12
(
%
rsp
)
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
25
198
1
/
/
vextractf128
0x1
%
ymm0
%
xmm6
.
byte
196
226
89
64
246
/
/
vpmulld
%
xmm6
%
xmm4
%
xmm6
.
byte
197
249
127
116
36
176
/
/
vmovdqa
%
xmm6
-
0x50
(
%
rsp
)
.
byte
196
226
89
64
192
/
/
vpmulld
%
xmm0
%
xmm4
%
xmm0
.
byte
197
249
127
68
36
160
/
/
vmovdqa
%
xmm0
-
0x60
(
%
rsp
)
.
byte
197
249
254
193
/
/
vpaddd
%
xmm1
%
xmm0
%
xmm0
.
byte
197
201
254
203
/
/
vpaddd
%
xmm3
%
xmm6
%
xmm1
.
byte
196
194
113
64
201
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm1
.
byte
196
194
121
64
225
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm4
.
byte
196
193
121
126
225
/
/
vmovd
%
xmm4
%
r9d
.
byte
196
195
121
22
224
1
/
/
vpextrd
0x1
%
xmm4
%
r8d
.
byte
196
195
121
22
226
2
/
/
vpextrd
0x2
%
xmm4
%
r10d
.
byte
196
195
121
22
227
3
/
/
vpextrd
0x3
%
xmm4
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm3
%
ymm0
.
byte
197
201
118
246
/
/
vpcmpeqd
%
xmm6
%
xmm6
%
xmm6
.
byte
197
217
250
222
/
/
vpsubd
%
xmm6
%
xmm4
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
197
241
250
222
/
/
vpsubd
%
xmm6
%
xmm1
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm6
%
ymm3
.
byte
196
193
89
254
226
/
/
vpaddd
%
xmm10
%
xmm4
%
xmm4
.
byte
196
195
249
22
224
1
/
/
vpextrq
0x1
%
xmm4
%
r8
.
byte
196
193
249
126
225
/
/
vmovq
%
xmm4
%
r9
.
byte
196
193
113
254
202
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm1
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
36
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm4
.
byte
196
227
113
33
204
32
/
/
vinsertps
0x20
%
xmm4
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
113
33
204
48
/
/
vinsertps
0x30
%
xmm4
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
36
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
196
163
89
33
36
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm4
%
xmm4
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
89
33
230
32
/
/
vinsertps
0x20
%
xmm6
%
xmm4
%
xmm4
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
89
33
230
48
/
/
vinsertps
0x30
%
xmm6
%
xmm4
%
xmm4
.
byte
196
227
93
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm4
%
ymm1
.
byte
196
193
124
91
227
/
/
vcvtdq2ps
%
ymm11
%
ymm4
.
byte
197
4
92
252
/
/
vsubps
%
ymm4
%
ymm15
%
ymm15
.
byte
196
193
124
92
197
/
/
vsubps
%
ymm13
%
ymm0
%
ymm0
.
byte
197
132
89
192
/
/
vmulps
%
ymm0
%
ymm15
%
ymm0
.
byte
197
148
88
192
/
/
vaddps
%
ymm0
%
ymm13
%
ymm0
.
byte
197
252
17
132
36
32
1
0
0
/
/
vmovups
%
ymm0
0x120
(
%
rsp
)
.
byte
197
228
92
197
/
/
vsubps
%
ymm5
%
ymm3
%
ymm0
.
byte
197
132
89
192
/
/
vmulps
%
ymm0
%
ymm15
%
ymm0
.
byte
197
212
88
192
/
/
vaddps
%
ymm0
%
ymm5
%
ymm0
.
byte
197
252
17
68
36
64
/
/
vmovups
%
ymm0
0x40
(
%
rsp
)
.
byte
197
244
92
194
/
/
vsubps
%
ymm2
%
ymm1
%
ymm0
.
byte
197
132
89
192
/
/
vmulps
%
ymm0
%
ymm15
%
ymm0
.
byte
197
108
88
232
/
/
vaddps
%
ymm0
%
ymm2
%
ymm13
.
byte
197
124
16
156
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm11
.
byte
196
193
36
88
196
/
/
vaddps
%
ymm12
%
ymm11
%
ymm0
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
226
57
64
200
/
/
vpmulld
%
xmm0
%
xmm8
%
xmm1
.
byte
197
249
127
76
36
224
/
/
vmovdqa
%
xmm1
-
0x20
(
%
rsp
)
.
byte
196
227
125
25
192
1
/
/
vextractf128
0x1
%
ymm0
%
xmm0
.
byte
196
226
57
64
192
/
/
vpmulld
%
xmm0
%
xmm8
%
xmm0
.
byte
197
249
127
68
36
208
/
/
vmovdqa
%
xmm0
-
0x30
(
%
rsp
)
.
byte
197
249
254
108
36
32
/
/
vpaddd
0x20
(
%
rsp
)
%
xmm0
%
xmm5
.
byte
196
193
113
254
198
/
/
vpaddd
%
xmm14
%
xmm1
%
xmm0
.
byte
197
121
111
100
36
128
/
/
vmovdqa
-
0x80
(
%
rsp
)
%
xmm12
.
byte
196
193
81
254
204
/
/
vpaddd
%
xmm12
%
xmm5
%
xmm1
.
byte
196
194
113
64
209
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm2
.
byte
197
121
111
68
36
144
/
/
vmovdqa
-
0x70
(
%
rsp
)
%
xmm8
.
byte
196
193
121
254
200
/
/
vpaddd
%
xmm8
%
xmm0
%
xmm1
.
byte
196
194
113
64
241
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm6
.
byte
196
193
121
126
241
/
/
vmovd
%
xmm6
%
r9d
.
byte
196
195
121
22
240
1
/
/
vpextrd
0x1
%
xmm6
%
r8d
.
byte
196
195
121
22
242
2
/
/
vpextrd
0x2
%
xmm6
%
r10d
.
byte
196
195
121
22
243
3
/
/
vpextrd
0x3
%
xmm6
%
r11d
.
byte
196
193
121
126
215
/
/
vmovd
%
xmm2
%
r15d
.
byte
196
195
121
22
214
1
/
/
vpextrd
0x1
%
xmm2
%
r14d
.
byte
196
195
121
22
212
2
/
/
vpextrd
0x2
%
xmm2
%
r12d
.
byte
196
227
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
ebx
.
byte
196
161
122
16
12
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm1
.
byte
196
163
113
33
12
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm1
%
xmm1
.
byte
196
163
113
33
12
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm1
%
xmm1
.
byte
196
227
113
33
12
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
99
101
24
241
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm14
.
byte
197
225
118
219
/
/
vpcmpeqd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
201
250
203
/
/
vpsubd
%
xmm3
%
xmm6
%
xmm1
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
197
233
250
203
/
/
vpsubd
%
xmm3
%
xmm2
%
xmm1
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
113
33
203
32
/
/
vinsertps
0x20
%
xmm3
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
113
33
203
48
/
/
vinsertps
0x30
%
xmm3
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
36
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
196
227
101
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm1
.
byte
196
193
73
254
218
/
/
vpaddd
%
xmm10
%
xmm6
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
36
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
196
227
101
24
242
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm6
.
byte
197
249
254
68
36
160
/
/
vpaddd
-
0x60
(
%
rsp
)
%
xmm0
%
xmm0
.
byte
197
209
254
84
36
176
/
/
vpaddd
-
0x50
(
%
rsp
)
%
xmm5
%
xmm2
.
byte
196
194
105
64
209
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm2
.
byte
196
194
121
64
233
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm5
.
byte
196
193
121
126
233
/
/
vmovd
%
xmm5
%
r9d
.
byte
196
195
121
22
232
1
/
/
vpextrd
0x1
%
xmm5
%
r8d
.
byte
196
195
121
22
234
2
/
/
vpextrd
0x2
%
xmm5
%
r10d
.
byte
196
195
121
22
235
3
/
/
vpextrd
0x3
%
xmm5
%
r11d
.
byte
196
193
121
126
215
/
/
vmovd
%
xmm2
%
r15d
.
byte
196
195
121
22
214
1
/
/
vpextrd
0x1
%
xmm2
%
r14d
.
byte
196
195
121
22
212
2
/
/
vpextrd
0x2
%
xmm2
%
r12d
.
byte
196
227
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm3
%
ymm0
.
byte
197
217
118
228
/
/
vpcmpeqd
%
xmm4
%
xmm4
%
xmm4
.
byte
197
209
250
220
/
/
vpsubd
%
xmm4
%
xmm5
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
197
233
250
220
/
/
vpsubd
%
xmm4
%
xmm2
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
36
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
196
163
89
33
36
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm4
%
xmm4
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
89
33
231
32
/
/
vinsertps
0x20
%
xmm7
%
xmm4
%
xmm4
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
89
33
231
48
/
/
vinsertps
0x30
%
xmm7
%
xmm4
%
xmm4
.
byte
196
227
93
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm4
%
ymm3
.
byte
196
193
81
254
226
/
/
vpaddd
%
xmm10
%
xmm5
%
xmm4
.
byte
196
195
249
22
224
1
/
/
vpextrq
0x1
%
xmm4
%
r8
.
byte
196
193
249
126
225
/
/
vmovq
%
xmm4
%
r9
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
36
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm4
.
byte
196
227
105
33
212
32
/
/
vinsertps
0x20
%
xmm4
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
105
33
212
48
/
/
vinsertps
0x30
%
xmm4
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
36
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
196
163
89
33
36
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm4
%
xmm4
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
89
33
229
32
/
/
vinsertps
0x20
%
xmm5
%
xmm4
%
xmm4
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
44
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
196
227
89
33
229
48
/
/
vinsertps
0x30
%
xmm5
%
xmm4
%
xmm4
.
byte
196
227
93
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm4
%
ymm2
.
byte
196
193
124
92
198
/
/
vsubps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
132
89
192
/
/
vmulps
%
ymm0
%
ymm15
%
ymm0
.
byte
197
140
88
192
/
/
vaddps
%
ymm0
%
ymm14
%
ymm0
.
byte
197
228
92
217
/
/
vsubps
%
ymm1
%
ymm3
%
ymm3
.
byte
197
132
89
219
/
/
vmulps
%
ymm3
%
ymm15
%
ymm3
.
byte
197
244
88
203
/
/
vaddps
%
ymm3
%
ymm1
%
ymm1
.
byte
197
236
92
214
/
/
vsubps
%
ymm6
%
ymm2
%
ymm2
.
byte
197
132
89
210
/
/
vmulps
%
ymm2
%
ymm15
%
ymm2
.
byte
197
204
88
210
/
/
vaddps
%
ymm2
%
ymm6
%
ymm2
.
byte
197
252
91
156
36
192
0
0
0
/
/
vcvtdq2ps
0xc0
(
%
rsp
)
%
ymm3
.
byte
197
164
92
219
/
/
vsubps
%
ymm3
%
ymm11
%
ymm3
.
byte
197
252
17
156
36
192
0
0
0
/
/
vmovups
%
ymm3
0xc0
(
%
rsp
)
.
byte
197
252
16
164
36
32
1
0
0
/
/
vmovups
0x120
(
%
rsp
)
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
17
132
36
128
0
0
0
/
/
vmovups
%
ymm0
0x80
(
%
rsp
)
.
byte
197
252
16
100
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm4
.
byte
197
244
92
196
/
/
vsubps
%
ymm4
%
ymm1
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
17
68
36
64
/
/
vmovups
%
ymm0
0x40
(
%
rsp
)
.
byte
196
193
108
92
197
/
/
vsubps
%
ymm13
%
ymm2
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
148
88
192
/
/
vaddps
%
ymm0
%
ymm13
%
ymm0
.
byte
197
252
17
68
36
32
/
/
vmovups
%
ymm0
0x20
(
%
rsp
)
.
byte
197
252
16
4
36
/
/
vmovups
(
%
rsp
)
%
ymm0
.
byte
197
252
88
132
36
0
1
0
0
/
/
vaddps
0x100
(
%
rsp
)
%
ymm0
%
ymm0
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
227
125
25
193
1
/
/
vextractf128
0x1
%
ymm0
%
xmm1
.
byte
197
249
111
84
36
192
/
/
vmovdqa
-
0x40
(
%
rsp
)
%
xmm2
.
byte
196
226
105
64
201
/
/
vpmulld
%
xmm1
%
xmm2
%
xmm1
.
byte
197
249
127
140
36
32
1
0
0
/
/
vmovdqa
%
xmm1
0x120
(
%
rsp
)
.
byte
196
226
105
64
192
/
/
vpmulld
%
xmm0
%
xmm2
%
xmm0
.
byte
197
249
127
68
36
192
/
/
vmovdqa
%
xmm0
-
0x40
(
%
rsp
)
.
byte
197
241
254
76
36
96
/
/
vpaddd
0x60
(
%
rsp
)
%
xmm1
%
xmm1
.
byte
197
249
127
140
36
176
1
0
0
/
/
vmovdqa
%
xmm1
0x1b0
(
%
rsp
)
.
byte
197
241
254
172
36
224
0
0
0
/
/
vpaddd
0xe0
(
%
rsp
)
%
xmm1
%
xmm5
.
byte
196
193
81
254
204
/
/
vpaddd
%
xmm12
%
xmm5
%
xmm1
.
byte
196
194
113
64
225
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm4
.
byte
197
121
254
180
36
192
1
0
0
/
/
vpaddd
0x1c0
(
%
rsp
)
%
xmm0
%
xmm14
.
byte
197
137
254
84
36
240
/
/
vpaddd
-
0x10
(
%
rsp
)
%
xmm14
%
xmm2
.
byte
196
193
105
254
248
/
/
vpaddd
%
xmm8
%
xmm2
%
xmm7
.
byte
196
194
65
64
249
/
/
vpmulld
%
xmm9
%
xmm7
%
xmm7
.
byte
196
193
121
126
249
/
/
vmovd
%
xmm7
%
r9d
.
byte
196
195
121
22
248
1
/
/
vpextrd
0x1
%
xmm7
%
r8d
.
byte
196
195
121
22
250
2
/
/
vpextrd
0x2
%
xmm7
%
r10d
.
byte
196
195
121
22
251
3
/
/
vpextrd
0x3
%
xmm7
%
r11d
.
byte
196
193
121
126
231
/
/
vmovd
%
xmm4
%
r15d
.
byte
196
195
121
22
230
1
/
/
vpextrd
0x1
%
xmm4
%
r14d
.
byte
196
195
121
22
228
2
/
/
vpextrd
0x2
%
xmm4
%
r12d
.
byte
196
227
121
22
227
3
/
/
vpextrd
0x3
%
xmm4
%
ebx
.
byte
196
161
122
16
28
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm3
.
byte
196
163
97
33
28
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm3
%
xmm3
.
byte
196
227
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm6
.
byte
196
163
73
33
52
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm6
%
xmm6
.
byte
196
163
73
33
52
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm6
%
xmm6
.
byte
196
163
73
33
52
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm6
%
xmm6
.
byte
196
227
77
24
203
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm6
%
ymm1
.
byte
197
249
118
192
/
/
vpcmpeqd
%
xmm0
%
xmm0
%
xmm0
.
byte
197
193
250
216
/
/
vpsubd
%
xmm0
%
xmm7
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
197
217
250
216
/
/
vpsubd
%
xmm0
%
xmm4
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
4
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm0
.
byte
196
227
73
33
192
32
/
/
vinsertps
0x20
%
xmm0
%
xmm6
%
xmm0
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
121
33
198
48
/
/
vinsertps
0x30
%
xmm6
%
xmm0
%
xmm0
.
byte
196
99
125
24
235
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm0
%
ymm13
.
byte
196
193
65
254
194
/
/
vpaddd
%
xmm10
%
xmm7
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
89
254
194
/
/
vpaddd
%
xmm10
%
xmm4
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
121
33
195
32
/
/
vinsertps
0x20
%
xmm3
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
121
33
195
48
/
/
vinsertps
0x30
%
xmm3
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
36
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
196
99
101
24
216
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm3
%
ymm11
.
byte
197
121
111
100
36
160
/
/
vmovdqa
-
0x60
(
%
rsp
)
%
xmm12
.
byte
197
153
254
194
/
/
vpaddd
%
xmm2
%
xmm12
%
xmm0
.
byte
197
121
111
68
36
176
/
/
vmovdqa
-
0x50
(
%
rsp
)
%
xmm8
.
byte
197
185
254
213
/
/
vpaddd
%
xmm5
%
xmm8
%
xmm2
.
byte
196
194
105
64
225
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm4
.
byte
196
194
121
64
233
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm5
.
byte
196
193
121
126
233
/
/
vmovd
%
xmm5
%
r9d
.
byte
196
195
121
22
232
1
/
/
vpextrd
0x1
%
xmm5
%
r8d
.
byte
196
195
121
22
234
2
/
/
vpextrd
0x2
%
xmm5
%
r10d
.
byte
196
195
121
22
235
3
/
/
vpextrd
0x3
%
xmm5
%
r11d
.
byte
196
193
121
126
231
/
/
vmovd
%
xmm4
%
r15d
.
byte
196
195
121
22
230
1
/
/
vpextrd
0x1
%
xmm4
%
r14d
.
byte
196
195
121
22
228
2
/
/
vpextrd
0x2
%
xmm4
%
r12d
.
byte
196
227
121
22
227
3
/
/
vpextrd
0x3
%
xmm4
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
20
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm2
.
byte
196
163
105
33
20
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm2
%
xmm2
.
byte
196
227
109
24
208
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm2
.
byte
197
225
118
219
/
/
vpcmpeqd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
209
250
195
/
/
vpsubd
%
xmm3
%
xmm5
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
197
217
250
195
/
/
vpsubd
%
xmm3
%
xmm4
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
121
33
195
32
/
/
vinsertps
0x20
%
xmm3
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
121
33
195
48
/
/
vinsertps
0x30
%
xmm3
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
196
227
101
24
248
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm3
%
ymm7
.
byte
196
193
81
254
194
/
/
vpaddd
%
xmm10
%
xmm5
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
89
254
194
/
/
vpaddd
%
xmm10
%
xmm4
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
121
33
195
32
/
/
vinsertps
0x20
%
xmm3
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
121
33
195
48
/
/
vinsertps
0x30
%
xmm3
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
36
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
196
227
101
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm3
%
ymm0
.
byte
197
236
92
209
/
/
vsubps
%
ymm1
%
ymm2
%
ymm2
.
byte
197
132
89
210
/
/
vmulps
%
ymm2
%
ymm15
%
ymm2
.
byte
197
244
88
202
/
/
vaddps
%
ymm2
%
ymm1
%
ymm1
.
byte
197
252
17
76
36
96
/
/
vmovups
%
ymm1
0x60
(
%
rsp
)
.
byte
196
193
68
92
213
/
/
vsubps
%
ymm13
%
ymm7
%
ymm2
.
byte
197
132
89
210
/
/
vmulps
%
ymm2
%
ymm15
%
ymm2
.
byte
197
148
88
202
/
/
vaddps
%
ymm2
%
ymm13
%
ymm1
.
byte
197
252
17
140
36
32
2
0
0
/
/
vmovups
%
ymm1
0x220
(
%
rsp
)
.
byte
196
193
124
92
195
/
/
vsubps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
132
89
192
/
/
vmulps
%
ymm0
%
ymm15
%
ymm0
.
byte
197
164
88
192
/
/
vaddps
%
ymm0
%
ymm11
%
ymm0
.
byte
197
252
17
132
36
0
2
0
0
/
/
vmovups
%
ymm0
0x200
(
%
rsp
)
.
byte
197
249
111
132
36
176
1
0
0
/
/
vmovdqa
0x1b0
(
%
rsp
)
%
xmm0
.
byte
197
249
254
108
36
208
/
/
vpaddd
-
0x30
(
%
rsp
)
%
xmm0
%
xmm5
.
byte
197
137
254
100
36
224
/
/
vpaddd
-
0x20
(
%
rsp
)
%
xmm14
%
xmm4
.
byte
197
209
254
68
36
128
/
/
vpaddd
-
0x80
(
%
rsp
)
%
xmm5
%
xmm0
.
byte
196
66
121
64
217
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm11
.
byte
197
121
111
108
36
144
/
/
vmovdqa
-
0x70
(
%
rsp
)
%
xmm13
.
byte
196
193
89
254
197
/
/
vpaddd
%
xmm13
%
xmm4
%
xmm0
.
byte
196
194
121
64
209
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm2
.
byte
196
193
121
126
209
/
/
vmovd
%
xmm2
%
r9d
.
byte
196
195
121
22
208
1
/
/
vpextrd
0x1
%
xmm2
%
r8d
.
byte
196
195
121
22
210
2
/
/
vpextrd
0x2
%
xmm2
%
r10d
.
byte
196
195
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
r11d
.
byte
196
65
121
126
223
/
/
vmovd
%
xmm11
%
r15d
.
byte
196
67
121
22
222
1
/
/
vpextrd
0x1
%
xmm11
%
r14d
.
byte
196
67
121
22
220
2
/
/
vpextrd
0x2
%
xmm11
%
r12d
.
byte
196
99
121
22
219
3
/
/
vpextrd
0x3
%
xmm11
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
12
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm1
.
byte
196
163
113
33
12
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm1
%
xmm1
.
byte
196
163
113
33
12
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm1
%
xmm1
.
byte
196
163
113
33
12
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm1
%
xmm1
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
197
225
118
219
/
/
vpcmpeqd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
250
203
/
/
vpsubd
%
xmm3
%
xmm2
%
xmm1
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
197
161
250
203
/
/
vpsubd
%
xmm3
%
xmm11
%
xmm1
.
byte
197
193
118
255
/
/
vpcmpeqd
%
xmm7
%
xmm7
%
xmm7
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
113
33
203
32
/
/
vinsertps
0x20
%
xmm3
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
113
33
203
48
/
/
vinsertps
0x30
%
xmm3
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
196
227
101
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm1
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
196
193
33
254
210
/
/
vpaddd
%
xmm10
%
xmm11
%
xmm2
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
196
99
101
24
218
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm11
.
byte
197
153
254
212
/
/
vpaddd
%
xmm4
%
xmm12
%
xmm2
.
byte
197
185
254
221
/
/
vpaddd
%
xmm5
%
xmm8
%
xmm3
.
byte
196
194
97
64
225
/
/
vpmulld
%
xmm9
%
xmm3
%
xmm4
.
byte
196
194
105
64
233
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm5
.
byte
196
193
121
126
233
/
/
vmovd
%
xmm5
%
r9d
.
byte
196
195
121
22
232
1
/
/
vpextrd
0x1
%
xmm5
%
r8d
.
byte
196
195
121
22
234
2
/
/
vpextrd
0x2
%
xmm5
%
r10d
.
byte
196
195
121
22
235
3
/
/
vpextrd
0x3
%
xmm5
%
r11d
.
byte
196
193
121
126
231
/
/
vmovd
%
xmm4
%
r15d
.
byte
196
195
121
22
230
1
/
/
vpextrd
0x1
%
xmm4
%
r14d
.
byte
196
195
121
22
228
2
/
/
vpextrd
0x2
%
xmm4
%
r12d
.
byte
196
227
121
22
227
3
/
/
vpextrd
0x3
%
xmm4
%
ebx
.
byte
196
161
122
16
20
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm2
.
byte
196
163
105
33
20
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm2
%
xmm2
.
byte
196
227
105
33
20
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm2
.
byte
197
209
250
223
/
/
vpsubd
%
xmm7
%
xmm5
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
197
217
250
223
/
/
vpsubd
%
xmm7
%
xmm4
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
99
77
24
243
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm6
%
ymm14
.
byte
196
193
81
254
218
/
/
vpaddd
%
xmm10
%
xmm5
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
196
193
89
254
218
/
/
vpaddd
%
xmm10
%
xmm4
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
36
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
196
163
89
33
36
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm4
%
xmm4
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
89
33
229
32
/
/
vinsertps
0x20
%
xmm5
%
xmm4
%
xmm4
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
44
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
196
227
89
33
229
48
/
/
vinsertps
0x30
%
xmm5
%
xmm4
%
xmm4
.
byte
196
227
93
24
219
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm4
%
ymm3
.
byte
197
236
92
208
/
/
vsubps
%
ymm0
%
ymm2
%
ymm2
.
byte
197
124
17
188
36
128
1
0
0
/
/
vmovups
%
ymm15
0x180
(
%
rsp
)
.
byte
197
132
89
210
/
/
vmulps
%
ymm2
%
ymm15
%
ymm2
.
byte
197
252
88
194
/
/
vaddps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
140
92
209
/
/
vsubps
%
ymm1
%
ymm14
%
ymm2
.
byte
197
132
89
210
/
/
vmulps
%
ymm2
%
ymm15
%
ymm2
.
byte
197
244
88
202
/
/
vaddps
%
ymm2
%
ymm1
%
ymm1
.
byte
196
193
100
92
211
/
/
vsubps
%
ymm11
%
ymm3
%
ymm2
.
byte
197
132
89
210
/
/
vmulps
%
ymm2
%
ymm15
%
ymm2
.
byte
197
164
88
210
/
/
vaddps
%
ymm2
%
ymm11
%
ymm2
.
byte
197
252
16
100
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
252
16
156
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm3
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
16
164
36
32
2
0
0
/
/
vmovups
0x220
(
%
rsp
)
%
ymm4
.
byte
197
244
92
204
/
/
vsubps
%
ymm4
%
ymm1
%
ymm1
.
byte
197
228
89
201
/
/
vmulps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
220
88
201
/
/
vaddps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
252
16
164
36
0
2
0
0
/
/
vmovups
0x200
(
%
rsp
)
%
ymm4
.
byte
197
236
92
212
/
/
vsubps
%
ymm4
%
ymm2
%
ymm2
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
220
88
210
/
/
vaddps
%
ymm2
%
ymm4
%
ymm2
.
byte
197
252
91
156
36
64
1
0
0
/
/
vcvtdq2ps
0x140
(
%
rsp
)
%
ymm3
.
byte
197
252
16
164
36
0
1
0
0
/
/
vmovups
0x100
(
%
rsp
)
%
ymm4
.
byte
197
220
92
219
/
/
vsubps
%
ymm3
%
ymm4
%
ymm3
.
byte
197
252
17
92
36
96
/
/
vmovups
%
ymm3
0x60
(
%
rsp
)
.
byte
197
252
16
164
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
17
132
36
0
1
0
0
/
/
vmovups
%
ymm0
0x100
(
%
rsp
)
.
byte
197
252
16
100
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm4
.
byte
197
244
92
196
/
/
vsubps
%
ymm4
%
ymm1
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
17
132
36
64
1
0
0
/
/
vmovups
%
ymm0
0x140
(
%
rsp
)
.
byte
197
252
16
76
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm1
.
byte
197
236
92
193
/
/
vsubps
%
ymm1
%
ymm2
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
244
88
192
/
/
vaddps
%
ymm0
%
ymm1
%
ymm0
.
byte
197
252
17
132
36
128
0
0
0
/
/
vmovups
%
ymm0
0x80
(
%
rsp
)
.
byte
197
252
16
4
36
/
/
vmovups
(
%
rsp
)
%
ymm0
.
byte
197
252
88
132
36
224
1
0
0
/
/
vaddps
0x1e0
(
%
rsp
)
%
ymm0
%
ymm0
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
254
127
68
36
64
/
/
vmovdqu
%
ymm0
0x40
(
%
rsp
)
.
byte
197
249
254
148
36
96
1
0
0
/
/
vpaddd
0x160
(
%
rsp
)
%
xmm0
%
xmm2
.
byte
197
249
127
20
36
/
/
vmovdqa
%
xmm2
(
%
rsp
)
.
byte
196
227
125
25
192
1
/
/
vextractf128
0x1
%
ymm0
%
xmm0
.
byte
197
249
127
68
36
32
/
/
vmovdqa
%
xmm0
0x20
(
%
rsp
)
.
byte
197
121
254
188
36
160
0
0
0
/
/
vpaddd
0xa0
(
%
rsp
)
%
xmm0
%
xmm15
.
byte
197
129
254
172
36
224
0
0
0
/
/
vpaddd
0xe0
(
%
rsp
)
%
xmm15
%
xmm5
.
byte
197
249
111
100
36
128
/
/
vmovdqa
-
0x80
(
%
rsp
)
%
xmm4
.
byte
197
217
254
197
/
/
vpaddd
%
xmm5
%
xmm4
%
xmm0
.
byte
196
194
121
64
201
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm1
.
byte
197
233
254
84
36
240
/
/
vpaddd
-
0x10
(
%
rsp
)
%
xmm2
%
xmm2
.
byte
197
145
254
194
/
/
vpaddd
%
xmm2
%
xmm13
%
xmm0
.
byte
196
194
121
64
193
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm0
.
byte
196
193
121
126
193
/
/
vmovd
%
xmm0
%
r9d
.
byte
196
195
121
22
192
1
/
/
vpextrd
0x1
%
xmm0
%
r8d
.
byte
196
195
121
22
194
2
/
/
vpextrd
0x2
%
xmm0
%
r10d
.
byte
196
195
121
22
195
3
/
/
vpextrd
0x3
%
xmm0
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
52
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm6
.
byte
196
163
73
33
52
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm6
%
xmm6
.
byte
196
163
73
33
52
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm6
%
xmm6
.
byte
196
227
73
33
52
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm7
.
byte
196
163
65
33
60
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm7
%
xmm7
.
byte
196
163
65
33
60
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm7
%
xmm7
.
byte
196
163
65
33
60
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm7
%
xmm7
.
byte
196
99
69
24
246
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm7
%
ymm14
.
byte
197
225
118
219
/
/
vpcmpeqd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
249
250
243
/
/
vpsubd
%
xmm3
%
xmm0
%
xmm6
.
byte
196
195
249
22
240
1
/
/
vpextrq
0x1
%
xmm6
%
r8
.
byte
196
193
249
126
241
/
/
vmovq
%
xmm6
%
r9
.
byte
197
241
250
243
/
/
vpsubd
%
xmm3
%
xmm1
%
xmm6
.
byte
196
65
17
118
237
/
/
vpcmpeqd
%
xmm13
%
xmm13
%
xmm13
.
byte
196
195
249
22
242
1
/
/
vpextrq
0x1
%
xmm6
%
r10
.
byte
196
225
249
126
243
/
/
vmovq
%
xmm6
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
52
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm6
.
byte
196
227
73
33
52
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
60
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
196
163
65
33
60
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
65
33
219
32
/
/
vinsertps
0x20
%
xmm3
%
xmm7
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
97
33
223
48
/
/
vinsertps
0x30
%
xmm7
%
xmm3
%
xmm3
.
byte
196
227
101
24
254
1
/
/
vinsertf128
0x1
%
xmm6
%
ymm3
%
ymm7
.
byte
196
193
121
254
194
/
/
vpaddd
%
xmm10
%
xmm0
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
113
254
194
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
12
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
196
227
121
33
193
32
/
/
vinsertps
0x20
%
xmm1
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
12
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm1
.
byte
196
227
121
33
193
48
/
/
vinsertps
0x30
%
xmm1
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
12
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
196
163
113
33
12
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
113
33
203
32
/
/
vinsertps
0x20
%
xmm3
%
xmm1
%
xmm1
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
28
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm3
.
byte
196
227
113
33
203
48
/
/
vinsertps
0x30
%
xmm3
%
xmm1
%
xmm1
.
byte
196
99
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm8
.
byte
197
121
111
100
36
160
/
/
vmovdqa
-
0x60
(
%
rsp
)
%
xmm12
.
byte
197
153
254
194
/
/
vpaddd
%
xmm2
%
xmm12
%
xmm0
.
byte
197
121
111
92
36
176
/
/
vmovdqa
-
0x50
(
%
rsp
)
%
xmm11
.
byte
197
161
254
205
/
/
vpaddd
%
xmm5
%
xmm11
%
xmm1
.
byte
196
194
113
64
201
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm1
.
byte
196
194
121
64
193
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm0
.
byte
196
193
121
126
193
/
/
vmovd
%
xmm0
%
r9d
.
byte
196
195
121
22
192
1
/
/
vpextrd
0x1
%
xmm0
%
r8d
.
byte
196
195
121
22
194
2
/
/
vpextrd
0x2
%
xmm0
%
r10d
.
byte
196
195
121
22
195
3
/
/
vpextrd
0x3
%
xmm0
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
20
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm2
.
byte
196
163
105
33
20
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm2
%
xmm2
.
byte
196
227
105
33
20
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm2
.
byte
196
193
121
250
221
/
/
vpsubd
%
xmm13
%
xmm0
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
196
193
113
250
221
/
/
vpsubd
%
xmm13
%
xmm1
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
44
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm5
.
byte
196
227
97
33
221
32
/
/
vinsertps
0x20
%
xmm5
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
97
33
221
48
/
/
vinsertps
0x30
%
xmm5
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
44
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
196
163
81
33
44
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
81
33
238
32
/
/
vinsertps
0x20
%
xmm6
%
xmm5
%
xmm5
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
81
33
238
48
/
/
vinsertps
0x30
%
xmm6
%
xmm5
%
xmm5
.
byte
196
227
85
24
235
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm5
%
ymm5
.
byte
196
193
121
254
194
/
/
vpaddd
%
xmm10
%
xmm0
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
113
254
194
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
12
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
196
227
121
33
193
32
/
/
vinsertps
0x20
%
xmm1
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
12
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm1
.
byte
196
227
121
33
193
48
/
/
vinsertps
0x30
%
xmm1
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
12
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
196
163
113
33
12
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
113
33
203
32
/
/
vinsertps
0x20
%
xmm3
%
xmm1
%
xmm1
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
28
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm3
.
byte
196
227
113
33
203
48
/
/
vinsertps
0x30
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
193
108
92
206
/
/
vsubps
%
ymm14
%
ymm2
%
ymm1
.
byte
197
252
16
148
36
128
1
0
0
/
/
vmovups
0x180
(
%
rsp
)
%
ymm2
.
byte
197
236
89
201
/
/
vmulps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
12
88
241
/
/
vaddps
%
ymm1
%
ymm14
%
ymm14
.
byte
197
212
92
207
/
/
vsubps
%
ymm7
%
ymm5
%
ymm1
.
byte
197
236
89
201
/
/
vmulps
%
ymm1
%
ymm2
%
ymm1
.
byte
197
196
88
201
/
/
vaddps
%
ymm1
%
ymm7
%
ymm1
.
byte
197
252
17
140
36
160
0
0
0
/
/
vmovups
%
ymm1
0xa0
(
%
rsp
)
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
197
236
89
192
/
/
vmulps
%
ymm0
%
ymm2
%
ymm0
.
byte
197
60
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm8
.
byte
197
129
254
68
36
208
/
/
vpaddd
-
0x30
(
%
rsp
)
%
xmm15
%
xmm0
.
byte
197
249
111
12
36
/
/
vmovdqa
(
%
rsp
)
%
xmm1
.
byte
197
241
254
124
36
224
/
/
vpaddd
-
0x20
(
%
rsp
)
%
xmm1
%
xmm7
.
byte
197
249
254
204
/
/
vpaddd
%
xmm4
%
xmm0
%
xmm1
.
byte
196
194
113
64
201
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm1
.
byte
197
121
111
124
36
144
/
/
vmovdqa
-
0x70
(
%
rsp
)
%
xmm15
.
byte
196
193
65
254
215
/
/
vpaddd
%
xmm15
%
xmm7
%
xmm2
.
byte
196
194
105
64
209
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm2
.
byte
196
193
121
126
209
/
/
vmovd
%
xmm2
%
r9d
.
byte
196
195
121
22
208
1
/
/
vpextrd
0x1
%
xmm2
%
r8d
.
byte
196
195
121
22
210
2
/
/
vpextrd
0x2
%
xmm2
%
r10d
.
byte
196
195
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
28
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm3
.
byte
196
163
97
33
28
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm3
%
xmm3
.
byte
196
227
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm4
.
byte
196
163
89
33
36
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm4
%
xmm4
.
byte
196
163
89
33
36
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm4
%
xmm4
.
byte
196
163
89
33
36
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm4
%
xmm4
.
byte
196
227
93
24
227
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm4
%
ymm4
.
byte
197
209
118
237
/
/
vpcmpeqd
%
xmm5
%
xmm5
%
xmm5
.
byte
197
233
250
221
/
/
vpsubd
%
xmm5
%
xmm2
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
197
241
250
221
/
/
vpsubd
%
xmm5
%
xmm1
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
44
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm5
.
byte
196
227
97
33
221
32
/
/
vinsertps
0x20
%
xmm5
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
97
33
221
48
/
/
vinsertps
0x30
%
xmm5
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
44
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
196
163
81
33
44
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
81
33
238
32
/
/
vinsertps
0x20
%
xmm6
%
xmm5
%
xmm5
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
81
33
238
48
/
/
vinsertps
0x30
%
xmm6
%
xmm5
%
xmm5
.
byte
196
227
85
24
235
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm5
%
ymm5
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
196
193
113
254
202
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm1
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
20
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
196
227
113
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
113
33
202
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
20
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
196
163
105
33
20
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
28
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
196
99
109
24
233
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm13
.
byte
197
153
254
215
/
/
vpaddd
%
xmm7
%
xmm12
%
xmm2
.
byte
197
161
254
192
/
/
vpaddd
%
xmm0
%
xmm11
%
xmm0
.
byte
196
65
121
111
227
/
/
vmovdqa
%
xmm11
%
xmm12
.
byte
196
194
121
64
201
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm1
.
byte
196
194
105
64
209
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm2
.
byte
196
193
121
126
209
/
/
vmovd
%
xmm2
%
r9d
.
byte
196
195
121
22
208
1
/
/
vpextrd
0x1
%
xmm2
%
r8d
.
byte
196
195
121
22
210
2
/
/
vpextrd
0x2
%
xmm2
%
r10d
.
byte
196
195
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
r11d
.
byte
196
193
121
126
207
/
/
vmovd
%
xmm1
%
r15d
.
byte
196
195
121
22
206
1
/
/
vpextrd
0x1
%
xmm1
%
r14d
.
byte
196
195
121
22
204
2
/
/
vpextrd
0x2
%
xmm1
%
r12d
.
byte
196
227
121
22
203
3
/
/
vpextrd
0x3
%
xmm1
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm3
%
ymm0
.
byte
197
201
118
246
/
/
vpcmpeqd
%
xmm6
%
xmm6
%
xmm6
.
byte
197
233
250
222
/
/
vpsubd
%
xmm6
%
xmm2
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
197
241
250
222
/
/
vpsubd
%
xmm6
%
xmm1
%
xmm3
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
97
33
222
32
/
/
vinsertps
0x20
%
xmm6
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
97
33
222
48
/
/
vinsertps
0x30
%
xmm6
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
243
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm6
%
ymm6
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
196
193
113
254
202
/
/
vpaddd
%
xmm10
%
xmm1
%
xmm1
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
20
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
196
227
113
33
202
32
/
/
vinsertps
0x20
%
xmm2
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
113
33
202
48
/
/
vinsertps
0x30
%
xmm2
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
20
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
196
163
105
33
20
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
28
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
196
227
109
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm2
%
ymm1
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
252
16
156
36
128
1
0
0
/
/
vmovups
0x180
(
%
rsp
)
%
ymm3
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
204
92
213
/
/
vsubps
%
ymm5
%
ymm6
%
ymm2
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
212
88
210
/
/
vaddps
%
ymm2
%
ymm5
%
ymm2
.
byte
196
193
116
92
205
/
/
vsubps
%
ymm13
%
ymm1
%
ymm1
.
byte
197
228
89
201
/
/
vmulps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
148
88
201
/
/
vaddps
%
ymm1
%
ymm13
%
ymm1
.
byte
196
193
124
92
198
/
/
vsubps
%
ymm14
%
ymm0
%
ymm0
.
byte
197
252
16
156
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm3
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
140
88
192
/
/
vaddps
%
ymm0
%
ymm14
%
ymm0
.
byte
197
252
17
4
36
/
/
vmovups
%
ymm0
(
%
rsp
)
.
byte
197
252
16
164
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm4
.
byte
197
236
92
196
/
/
vsubps
%
ymm4
%
ymm2
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
17
132
36
160
0
0
0
/
/
vmovups
%
ymm0
0xa0
(
%
rsp
)
.
byte
196
193
116
92
192
/
/
vsubps
%
ymm8
%
ymm1
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
188
88
192
/
/
vaddps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
252
17
132
36
96
1
0
0
/
/
vmovups
%
ymm0
0x160
(
%
rsp
)
.
byte
197
254
111
68
36
64
/
/
vmovdqu
0x40
(
%
rsp
)
%
ymm0
.
byte
197
121
254
116
36
192
/
/
vpaddd
-
0x40
(
%
rsp
)
%
xmm0
%
xmm14
.
byte
197
249
111
68
36
32
/
/
vmovdqa
0x20
(
%
rsp
)
%
xmm0
.
byte
197
121
254
172
36
32
1
0
0
/
/
vpaddd
0x120
(
%
rsp
)
%
xmm0
%
xmm13
.
byte
197
145
254
188
36
224
0
0
0
/
/
vpaddd
0xe0
(
%
rsp
)
%
xmm13
%
xmm7
.
byte
197
137
254
108
36
240
/
/
vpaddd
-
0x10
(
%
rsp
)
%
xmm14
%
xmm5
.
byte
197
193
254
76
36
128
/
/
vpaddd
-
0x80
(
%
rsp
)
%
xmm7
%
xmm1
.
byte
196
194
113
64
209
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm2
.
byte
196
193
81
254
207
/
/
vpaddd
%
xmm15
%
xmm5
%
xmm1
.
byte
196
194
113
64
225
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm4
.
byte
196
193
121
126
225
/
/
vmovd
%
xmm4
%
r9d
.
byte
196
195
121
22
224
1
/
/
vpextrd
0x1
%
xmm4
%
r8d
.
byte
196
195
121
22
226
2
/
/
vpextrd
0x2
%
xmm4
%
r10d
.
byte
196
195
121
22
227
3
/
/
vpextrd
0x3
%
xmm4
%
r11d
.
byte
196
193
121
126
215
/
/
vmovd
%
xmm2
%
r15d
.
byte
196
195
121
22
214
1
/
/
vpextrd
0x1
%
xmm2
%
r14d
.
byte
196
195
121
22
212
2
/
/
vpextrd
0x2
%
xmm2
%
r12d
.
byte
196
227
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
ebx
.
byte
196
161
122
16
12
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm1
.
byte
196
163
113
33
12
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm1
%
xmm1
.
byte
196
163
113
33
12
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm1
%
xmm1
.
byte
196
227
113
33
12
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
217
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm3
.
byte
197
249
118
192
/
/
vpcmpeqd
%
xmm0
%
xmm0
%
xmm0
.
byte
197
217
250
200
/
/
vpsubd
%
xmm0
%
xmm4
%
xmm1
.
byte
196
195
249
22
200
1
/
/
vpextrq
0x1
%
xmm1
%
r8
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
197
233
250
200
/
/
vpsubd
%
xmm0
%
xmm2
%
xmm1
.
byte
196
65
1
118
255
/
/
vpcmpeqd
%
xmm15
%
xmm15
%
xmm15
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
196
225
249
126
203
/
/
vmovq
%
xmm1
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
12
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm1
.
byte
196
227
113
33
12
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
113
33
206
32
/
/
vinsertps
0x20
%
xmm6
%
xmm1
%
xmm1
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
113
33
206
48
/
/
vinsertps
0x30
%
xmm6
%
xmm1
%
xmm1
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
4
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm0
.
byte
196
227
73
33
192
32
/
/
vinsertps
0x20
%
xmm0
%
xmm6
%
xmm0
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
52
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
196
227
121
33
198
48
/
/
vinsertps
0x30
%
xmm6
%
xmm0
%
xmm0
.
byte
196
227
125
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm0
%
ymm1
.
byte
196
193
89
254
194
/
/
vpaddd
%
xmm10
%
xmm4
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
105
254
194
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
20
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
196
227
121
33
194
32
/
/
vinsertps
0x20
%
xmm2
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
20
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm2
.
byte
196
227
121
33
194
48
/
/
vinsertps
0x30
%
xmm2
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
20
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
196
163
105
33
20
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
105
33
212
32
/
/
vinsertps
0x20
%
xmm4
%
xmm2
%
xmm2
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
36
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
196
227
105
33
212
48
/
/
vinsertps
0x30
%
xmm4
%
xmm2
%
xmm2
.
byte
196
99
109
24
216
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm11
.
byte
197
121
111
68
36
160
/
/
vmovdqa
-
0x60
(
%
rsp
)
%
xmm8
.
byte
197
185
254
197
/
/
vpaddd
%
xmm5
%
xmm8
%
xmm0
.
byte
197
153
254
215
/
/
vpaddd
%
xmm7
%
xmm12
%
xmm2
.
byte
196
194
105
64
233
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm5
.
byte
196
194
121
64
225
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm4
.
byte
196
193
121
126
225
/
/
vmovd
%
xmm4
%
r9d
.
byte
196
195
121
22
224
1
/
/
vpextrd
0x1
%
xmm4
%
r8d
.
byte
196
195
121
22
226
2
/
/
vpextrd
0x2
%
xmm4
%
r10d
.
byte
196
195
121
22
227
3
/
/
vpextrd
0x3
%
xmm4
%
r11d
.
byte
196
193
121
126
239
/
/
vmovd
%
xmm5
%
r15d
.
byte
196
195
121
22
238
1
/
/
vpextrd
0x1
%
xmm5
%
r14d
.
byte
196
195
121
22
236
2
/
/
vpextrd
0x2
%
xmm5
%
r12d
.
byte
196
227
121
22
235
3
/
/
vpextrd
0x3
%
xmm5
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
20
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm2
.
byte
196
163
105
33
20
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm2
%
xmm2
.
byte
196
163
105
33
20
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm2
%
xmm2
.
byte
196
227
109
24
208
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm2
%
ymm2
.
byte
196
193
89
250
199
/
/
vpsubd
%
xmm15
%
xmm4
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
81
250
199
/
/
vpsubd
%
xmm15
%
xmm5
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
52
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
196
227
121
33
198
32
/
/
vinsertps
0x20
%
xmm6
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
121
33
198
48
/
/
vinsertps
0x30
%
xmm6
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
52
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
196
163
73
33
52
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm6
%
xmm6
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
73
33
247
32
/
/
vinsertps
0x20
%
xmm7
%
xmm6
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
240
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm6
%
ymm6
.
byte
196
193
89
254
194
/
/
vpaddd
%
xmm10
%
xmm4
%
xmm0
.
byte
196
195
249
22
192
1
/
/
vpextrq
0x1
%
xmm0
%
r8
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
196
193
81
254
194
/
/
vpaddd
%
xmm10
%
xmm5
%
xmm0
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
196
225
249
126
195
/
/
vmovq
%
xmm0
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
4
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm0
.
byte
196
227
121
33
4
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
36
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm4
.
byte
196
227
121
33
196
32
/
/
vinsertps
0x20
%
xmm4
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
121
33
196
48
/
/
vinsertps
0x30
%
xmm4
%
xmm0
%
xmm0
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
36
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
196
163
89
33
36
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm4
%
xmm4
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
89
33
229
32
/
/
vinsertps
0x20
%
xmm5
%
xmm4
%
xmm4
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
44
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
196
227
89
33
229
48
/
/
vinsertps
0x30
%
xmm5
%
xmm4
%
xmm4
.
byte
196
227
93
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm4
%
ymm0
.
byte
197
236
92
211
/
/
vsubps
%
ymm3
%
ymm2
%
ymm2
.
byte
197
252
16
164
36
128
1
0
0
/
/
vmovups
0x180
(
%
rsp
)
%
ymm4
.
byte
197
220
89
210
/
/
vmulps
%
ymm2
%
ymm4
%
ymm2
.
byte
197
228
88
210
/
/
vaddps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
252
17
148
36
224
0
0
0
/
/
vmovups
%
ymm2
0xe0
(
%
rsp
)
.
byte
197
204
92
209
/
/
vsubps
%
ymm1
%
ymm6
%
ymm2
.
byte
197
220
89
210
/
/
vmulps
%
ymm2
%
ymm4
%
ymm2
.
byte
197
116
88
250
/
/
vaddps
%
ymm2
%
ymm1
%
ymm15
.
byte
196
193
124
92
195
/
/
vsubps
%
ymm11
%
ymm0
%
ymm0
.
byte
197
220
89
192
/
/
vmulps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
124
40
228
/
/
vmovaps
%
ymm4
%
ymm12
.
byte
197
36
88
216
/
/
vaddps
%
ymm0
%
ymm11
%
ymm11
.
byte
197
145
254
124
36
208
/
/
vpaddd
-
0x30
(
%
rsp
)
%
xmm13
%
xmm7
.
byte
197
137
254
68
36
224
/
/
vpaddd
-
0x20
(
%
rsp
)
%
xmm14
%
xmm0
.
byte
197
249
254
76
36
144
/
/
vpaddd
-
0x70
(
%
rsp
)
%
xmm0
%
xmm1
.
byte
197
193
254
84
36
128
/
/
vpaddd
-
0x80
(
%
rsp
)
%
xmm7
%
xmm2
.
byte
196
66
105
64
241
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm14
.
byte
196
194
113
64
209
/
/
vpmulld
%
xmm9
%
xmm1
%
xmm2
.
byte
196
193
121
126
209
/
/
vmovd
%
xmm2
%
r9d
.
byte
196
195
121
22
208
1
/
/
vpextrd
0x1
%
xmm2
%
r8d
.
byte
196
195
121
22
210
2
/
/
vpextrd
0x2
%
xmm2
%
r10d
.
byte
196
195
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
r11d
.
byte
196
65
121
126
247
/
/
vmovd
%
xmm14
%
r15d
.
byte
196
67
121
22
246
1
/
/
vpextrd
0x1
%
xmm14
%
r14d
.
byte
196
67
121
22
244
2
/
/
vpextrd
0x2
%
xmm14
%
r12d
.
byte
196
99
121
22
243
3
/
/
vpextrd
0x3
%
xmm14
%
ebx
.
byte
196
161
122
16
12
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm1
.
byte
196
163
113
33
12
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm1
%
xmm1
.
byte
196
163
113
33
12
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm1
%
xmm1
.
byte
196
227
113
33
12
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
161
122
16
28
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
196
163
97
33
28
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
%
xmm3
.
byte
196
163
97
33
28
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm3
%
xmm3
.
byte
196
227
101
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm3
%
ymm1
.
byte
197
217
118
228
/
/
vpcmpeqd
%
xmm4
%
xmm4
%
xmm4
.
byte
197
233
250
220
/
/
vpsubd
%
xmm4
%
xmm2
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
197
137
250
220
/
/
vpsubd
%
xmm4
%
xmm14
%
xmm3
.
byte
197
201
118
246
/
/
vpcmpeqd
%
xmm6
%
xmm6
%
xmm6
.
byte
196
195
249
22
218
1
/
/
vpextrq
0x1
%
xmm3
%
r10
.
byte
196
225
249
126
219
/
/
vmovq
%
xmm3
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
28
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm3
.
byte
196
227
97
33
28
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
44
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm5
.
byte
196
227
97
33
221
32
/
/
vinsertps
0x20
%
xmm5
%
xmm3
%
xmm3
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
44
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm5
.
byte
196
227
97
33
221
48
/
/
vinsertps
0x30
%
xmm5
%
xmm3
%
xmm3
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
44
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
196
163
81
33
44
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
%
xmm5
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
81
33
228
32
/
/
vinsertps
0x20
%
xmm4
%
xmm5
%
xmm4
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
44
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
196
227
89
33
229
48
/
/
vinsertps
0x30
%
xmm5
%
xmm4
%
xmm4
.
byte
196
227
93
24
235
1
/
/
vinsertf128
0x1
%
xmm3
%
ymm4
%
ymm5
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
208
1
/
/
vpextrq
0x1
%
xmm2
%
r8
.
byte
196
193
249
126
209
/
/
vmovq
%
xmm2
%
r9
.
byte
196
193
9
254
210
/
/
vpaddd
%
xmm10
%
xmm14
%
xmm2
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
36
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
196
99
101
24
242
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm14
.
byte
197
185
254
192
/
/
vpaddd
%
xmm0
%
xmm8
%
xmm0
.
byte
197
193
254
84
36
176
/
/
vpaddd
-
0x50
(
%
rsp
)
%
xmm7
%
xmm2
.
byte
196
194
105
64
209
/
/
vpmulld
%
xmm9
%
xmm2
%
xmm2
.
byte
196
194
121
64
217
/
/
vpmulld
%
xmm9
%
xmm0
%
xmm3
.
byte
196
193
121
126
217
/
/
vmovd
%
xmm3
%
r9d
.
byte
196
195
121
22
216
1
/
/
vpextrd
0x1
%
xmm3
%
r8d
.
byte
196
195
121
22
218
2
/
/
vpextrd
0x2
%
xmm3
%
r10d
.
byte
196
195
121
22
219
3
/
/
vpextrd
0x3
%
xmm3
%
r11d
.
byte
196
193
121
126
215
/
/
vmovd
%
xmm2
%
r15d
.
byte
196
195
121
22
214
1
/
/
vpextrd
0x1
%
xmm2
%
r14d
.
byte
196
195
121
22
212
2
/
/
vpextrd
0x2
%
xmm2
%
r12d
.
byte
196
227
121
22
211
3
/
/
vpextrd
0x3
%
xmm2
%
ebx
.
byte
196
161
122
16
4
184
/
/
vmovss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
196
163
121
33
4
176
16
/
/
vinsertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
%
xmm0
.
byte
196
163
121
33
4
160
32
/
/
vinsertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
%
xmm0
.
byte
196
227
121
33
4
152
48
/
/
vinsertps
0x30
(
%
rax
%
rbx
4
)
%
xmm0
%
xmm0
.
byte
196
161
122
16
36
136
/
/
vmovss
(
%
rax
%
r9
4
)
%
xmm4
.
byte
196
163
89
33
36
128
16
/
/
vinsertps
0x10
(
%
rax
%
r8
4
)
%
xmm4
%
xmm4
.
byte
196
163
89
33
36
144
32
/
/
vinsertps
0x20
(
%
rax
%
r10
4
)
%
xmm4
%
xmm4
.
byte
196
163
89
33
36
152
48
/
/
vinsertps
0x30
(
%
rax
%
r11
4
)
%
xmm4
%
xmm4
.
byte
196
227
93
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm4
%
ymm0
.
byte
197
225
250
230
/
/
vpsubd
%
xmm6
%
xmm3
%
xmm4
.
byte
196
195
249
22
224
1
/
/
vpextrq
0x1
%
xmm4
%
r8
.
byte
196
193
249
126
225
/
/
vmovq
%
xmm4
%
r9
.
byte
197
233
250
230
/
/
vpsubd
%
xmm6
%
xmm2
%
xmm4
.
byte
196
195
249
22
226
1
/
/
vpextrq
0x1
%
xmm4
%
r10
.
byte
196
225
249
126
227
/
/
vmovq
%
xmm4
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
36
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm4
.
byte
196
227
89
33
36
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm4
%
xmm4
.
byte
196
161
122
16
60
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm7
.
byte
196
227
89
33
231
32
/
/
vinsertps
0x20
%
xmm7
%
xmm4
%
xmm4
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
60
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
196
227
89
33
231
48
/
/
vinsertps
0x30
%
xmm7
%
xmm4
%
xmm4
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
60
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
196
163
65
33
60
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
%
xmm7
.
byte
196
161
122
16
52
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
196
227
65
33
246
32
/
/
vinsertps
0x20
%
xmm6
%
xmm7
%
xmm6
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
60
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
196
227
73
33
247
48
/
/
vinsertps
0x30
%
xmm7
%
xmm6
%
xmm6
.
byte
196
227
77
24
252
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm6
%
ymm7
.
byte
196
193
97
254
218
/
/
vpaddd
%
xmm10
%
xmm3
%
xmm3
.
byte
196
195
249
22
216
1
/
/
vpextrq
0x1
%
xmm3
%
r8
.
byte
196
193
249
126
217
/
/
vmovq
%
xmm3
%
r9
.
byte
196
193
105
254
210
/
/
vpaddd
%
xmm10
%
xmm2
%
xmm2
.
byte
196
195
249
22
210
1
/
/
vpextrq
0x1
%
xmm2
%
r10
.
byte
196
225
249
126
211
/
/
vmovq
%
xmm2
%
rbx
.
byte
65
137
219
/
/
mov
%
ebx
%
r11d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
196
161
122
16
20
152
/
/
vmovss
(
%
rax
%
r11
4
)
%
xmm2
.
byte
196
227
105
33
20
152
16
/
/
vinsertps
0x10
(
%
rax
%
rbx
4
)
%
xmm2
%
xmm2
.
byte
196
161
122
16
28
176
/
/
vmovss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
196
227
105
33
211
32
/
/
vinsertps
0x20
%
xmm3
%
xmm2
%
xmm2
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
161
122
16
28
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm3
.
byte
196
227
105
33
211
48
/
/
vinsertps
0x30
%
xmm3
%
xmm2
%
xmm2
.
byte
68
137
203
/
/
mov
%
r9d
%
ebx
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
194
/
/
mov
%
r8d
%
r10d
.
byte
197
250
16
28
152
/
/
vmovss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
196
163
97
33
28
136
16
/
/
vinsertps
0x10
(
%
rax
%
r9
4
)
%
xmm3
%
xmm3
.
byte
196
161
122
16
36
144
/
/
vmovss
(
%
rax
%
r10
4
)
%
xmm4
.
byte
196
227
97
33
220
32
/
/
vinsertps
0x20
%
xmm4
%
xmm3
%
xmm3
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
196
161
122
16
36
128
/
/
vmovss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
196
227
97
33
220
48
/
/
vinsertps
0x30
%
xmm4
%
xmm3
%
xmm3
.
byte
196
227
101
24
210
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm3
%
ymm2
.
byte
197
252
92
193
/
/
vsubps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
156
89
192
/
/
vmulps
%
ymm0
%
ymm12
%
ymm0
.
byte
197
244
88
192
/
/
vaddps
%
ymm0
%
ymm1
%
ymm0
.
byte
197
196
92
205
/
/
vsubps
%
ymm5
%
ymm7
%
ymm1
.
byte
197
156
89
201
/
/
vmulps
%
ymm1
%
ymm12
%
ymm1
.
byte
197
212
88
201
/
/
vaddps
%
ymm1
%
ymm5
%
ymm1
.
byte
196
193
108
92
214
/
/
vsubps
%
ymm14
%
ymm2
%
ymm2
.
byte
197
156
89
210
/
/
vmulps
%
ymm2
%
ymm12
%
ymm2
.
byte
197
140
88
210
/
/
vaddps
%
ymm2
%
ymm14
%
ymm2
.
byte
197
252
16
164
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
252
16
156
36
192
0
0
0
/
/
vmovups
0xc0
(
%
rsp
)
%
ymm3
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
196
193
116
92
207
/
/
vsubps
%
ymm15
%
ymm1
%
ymm1
.
byte
197
228
89
201
/
/
vmulps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
132
88
201
/
/
vaddps
%
ymm1
%
ymm15
%
ymm1
.
byte
196
193
108
92
211
/
/
vsubps
%
ymm11
%
ymm2
%
ymm2
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
164
88
210
/
/
vaddps
%
ymm2
%
ymm11
%
ymm2
.
byte
197
252
16
36
36
/
/
vmovups
(
%
rsp
)
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
252
16
92
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm3
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
16
164
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm4
.
byte
197
244
92
204
/
/
vsubps
%
ymm4
%
ymm1
%
ymm1
.
byte
197
228
89
201
/
/
vmulps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
220
88
201
/
/
vaddps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
252
16
164
36
96
1
0
0
/
/
vmovups
0x160
(
%
rsp
)
%
ymm4
.
byte
197
236
92
212
/
/
vsubps
%
ymm4
%
ymm2
%
ymm2
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
220
88
210
/
/
vaddps
%
ymm2
%
ymm4
%
ymm2
.
byte
197
252
91
156
36
192
1
0
0
/
/
vcvtdq2ps
0x1c0
(
%
rsp
)
%
ymm3
.
byte
197
252
16
164
36
224
1
0
0
/
/
vmovups
0x1e0
(
%
rsp
)
%
ymm4
.
byte
197
220
92
219
/
/
vsubps
%
ymm3
%
ymm4
%
ymm3
.
byte
197
252
16
164
36
0
1
0
0
/
/
vmovups
0x100
(
%
rsp
)
%
ymm4
.
byte
197
252
92
196
/
/
vsubps
%
ymm4
%
ymm0
%
ymm0
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
197
220
88
192
/
/
vaddps
%
ymm0
%
ymm4
%
ymm0
.
byte
197
252
16
164
36
64
1
0
0
/
/
vmovups
0x140
(
%
rsp
)
%
ymm4
.
byte
197
244
92
204
/
/
vsubps
%
ymm4
%
ymm1
%
ymm1
.
byte
197
228
89
201
/
/
vmulps
%
ymm1
%
ymm3
%
ymm1
.
byte
197
220
88
201
/
/
vaddps
%
ymm1
%
ymm4
%
ymm1
.
byte
197
252
16
164
36
128
0
0
0
/
/
vmovups
0x80
(
%
rsp
)
%
ymm4
.
byte
197
236
92
212
/
/
vsubps
%
ymm4
%
ymm2
%
ymm2
.
byte
197
228
89
210
/
/
vmulps
%
ymm2
%
ymm3
%
ymm2
.
byte
197
220
88
210
/
/
vaddps
%
ymm2
%
ymm4
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
136
252
1
0
/
/
vbroadcastss
0x1fc88
(
%
rip
)
%
ymm3
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
16
164
36
64
2
0
0
/
/
vmovups
0x240
(
%
rsp
)
%
ymm4
.
byte
197
252
16
172
36
96
2
0
0
/
/
vmovups
0x260
(
%
rsp
)
%
ymm5
.
byte
197
252
16
180
36
128
2
0
0
/
/
vmovups
0x280
(
%
rsp
)
%
ymm6
.
byte
197
252
16
188
36
160
2
0
0
/
/
vmovups
0x2a0
(
%
rsp
)
%
ymm7
.
byte
72
129
196
216
2
0
0
/
/
add
0x2d8
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gauss_a_to_rgba_avx
.
globl
_sk_gauss_a_to_rgba_avx
FUNCTION
(
_sk_gauss_a_to_rgba_avx
)
_sk_gauss_a_to_rgba_avx
:
.
byte
196
226
125
24
5
191
253
1
0
/
/
vbroadcastss
0x1fdbf
(
%
rip
)
%
ymm0
#
3c670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x424
>
.
byte
197
228
89
192
/
/
vmulps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
226
125
24
13
182
253
1
0
/
/
vbroadcastss
0x1fdb6
(
%
rip
)
%
ymm1
#
3c674
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x428
>
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
169
253
1
0
/
/
vbroadcastss
0x1fda9
(
%
rip
)
%
ymm1
#
3c678
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x42c
>
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
156
253
1
0
/
/
vbroadcastss
0x1fd9c
(
%
rip
)
%
ymm1
#
3c67c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x430
>
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
252
89
195
/
/
vmulps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
125
24
13
143
253
1
0
/
/
vbroadcastss
0x1fd8f
(
%
rip
)
%
ymm1
#
3c680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x434
>
.
byte
197
252
88
193
/
/
vaddps
%
ymm1
%
ymm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
197
252
40
208
/
/
vmovaps
%
ymm0
%
ymm2
.
byte
197
252
40
216
/
/
vmovaps
%
ymm0
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilerp_clamp_8888_avx
.
globl
_sk_bilerp_clamp_8888_avx
FUNCTION
(
_sk_bilerp_clamp_8888_avx
)
_sk_bilerp_clamp_8888_avx
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
129
236
120
1
0
0
/
/
sub
0x178
%
rsp
.
byte
197
254
127
124
36
32
/
/
vmovdqu
%
ymm7
0x20
(
%
rsp
)
.
byte
197
252
17
52
36
/
/
vmovups
%
ymm6
(
%
rsp
)
.
byte
197
252
17
108
36
224
/
/
vmovups
%
ymm5
-
0x20
(
%
rsp
)
.
byte
197
252
17
100
36
192
/
/
vmovups
%
ymm4
-
0x40
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
29
195
251
1
0
/
/
vbroadcastss
0x1fbc3
(
%
rip
)
%
ymm3
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
252
17
132
36
64
1
0
0
/
/
vmovups
%
ymm0
0x140
(
%
rsp
)
.
byte
197
252
88
195
/
/
vaddps
%
ymm3
%
ymm0
%
ymm0
.
byte
196
227
125
8
208
1
/
/
vroundps
0x1
%
ymm0
%
ymm2
.
byte
197
252
92
194
/
/
vsubps
%
ymm2
%
ymm0
%
ymm0
.
byte
197
252
17
140
36
192
0
0
0
/
/
vmovups
%
ymm1
0xc0
(
%
rsp
)
.
byte
197
244
88
203
/
/
vaddps
%
ymm3
%
ymm1
%
ymm1
.
byte
196
227
125
8
209
1
/
/
vroundps
0x1
%
ymm1
%
ymm2
.
byte
197
244
92
210
/
/
vsubps
%
ymm2
%
ymm1
%
ymm2
.
byte
196
226
125
24
13
144
251
1
0
/
/
vbroadcastss
0x1fb90
(
%
rip
)
%
ymm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
252
17
132
36
32
1
0
0
/
/
vmovups
%
ymm0
0x120
(
%
rsp
)
.
byte
197
244
92
192
/
/
vsubps
%
ymm0
%
ymm1
%
ymm0
.
byte
197
252
17
132
36
224
0
0
0
/
/
vmovups
%
ymm0
0xe0
(
%
rsp
)
.
byte
197
252
17
148
36
160
0
0
0
/
/
vmovups
%
ymm2
0xa0
(
%
rsp
)
.
byte
197
244
92
194
/
/
vsubps
%
ymm2
%
ymm1
%
ymm0
.
byte
197
252
17
68
36
64
/
/
vmovups
%
ymm0
0x40
(
%
rsp
)
.
byte
196
226
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm1
.
byte
196
227
125
25
202
1
/
/
vextractf128
0x1
%
ymm1
%
xmm2
.
byte
197
225
118
219
/
/
vpcmpeqd
%
xmm3
%
xmm3
%
xmm3
.
byte
197
233
254
211
/
/
vpaddd
%
xmm3
%
xmm2
%
xmm2
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
117
24
194
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm0
.
byte
197
252
17
132
36
0
1
0
0
/
/
vmovups
%
ymm0
0x100
(
%
rsp
)
.
byte
196
226
125
24
72
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm1
.
byte
196
227
125
25
202
1
/
/
vextractf128
0x1
%
ymm1
%
xmm2
.
byte
197
233
254
211
/
/
vpaddd
%
xmm3
%
xmm2
%
xmm2
.
byte
197
241
254
203
/
/
vpaddd
%
xmm3
%
xmm1
%
xmm1
.
byte
196
227
117
24
194
1
/
/
vinsertf128
0x1
%
xmm2
%
ymm1
%
ymm0
.
byte
197
252
17
132
36
128
0
0
0
/
/
vmovups
%
ymm0
0x80
(
%
rsp
)
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
196
226
125
24
64
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm0
.
byte
197
252
17
68
36
96
/
/
vmovups
%
ymm0
0x60
(
%
rsp
)
.
byte
197
250
16
5
90
252
1
0
/
/
vmovss
0x1fc5a
(
%
rip
)
%
xmm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
197
124
40
53
2
2
2
0
/
/
vmovaps
0x20202
(
%
rip
)
%
ymm14
#
3cc00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x9b4
>
.
byte
196
98
125
24
45
109
251
1
0
/
/
vbroadcastss
0x1fb6d
(
%
rip
)
%
ymm13
#
3c574
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x328
>
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
248
40
224
/
/
vmovaps
%
xmm0
%
xmm4
.
byte
196
227
121
4
204
0
/
/
vpermilps
0x0
%
xmm4
%
xmm1
.
byte
196
227
117
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm1
.
byte
197
244
88
140
36
192
0
0
0
/
/
vaddps
0xc0
(
%
rsp
)
%
ymm1
%
ymm1
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
252
95
201
/
/
vmaxps
%
ymm1
%
ymm0
%
ymm1
.
byte
197
248
41
100
36
144
/
/
vmovaps
%
xmm4
-
0x70
(
%
rsp
)
.
byte
197
248
46
37
68
252
1
0
/
/
vucomiss
0x1fc44
(
%
rip
)
%
xmm4
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
197
252
16
132
36
160
0
0
0
/
/
vmovups
0xa0
(
%
rsp
)
%
ymm0
.
byte
197
252
17
68
36
160
/
/
vmovups
%
ymm0
-
0x60
(
%
rsp
)
.
byte
119
12
/
/
ja
1ca65
<
_sk_bilerp_clamp_8888_avx
+
0x160
>
.
byte
197
252
16
68
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm0
.
byte
197
252
17
68
36
160
/
/
vmovups
%
ymm0
-
0x60
(
%
rsp
)
.
byte
197
244
93
140
36
128
0
0
0
/
/
vminps
0x80
(
%
rsp
)
%
ymm1
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
227
125
25
205
1
/
/
vextractf128
0x1
%
ymm1
%
xmm5
.
byte
197
254
111
68
36
96
/
/
vmovdqu
0x60
(
%
rsp
)
%
ymm0
.
byte
196
227
125
25
198
1
/
/
vextractf128
0x1
%
ymm0
%
xmm6
.
byte
196
226
73
64
237
/
/
vpmulld
%
xmm5
%
xmm6
%
xmm5
.
byte
196
226
121
64
201
/
/
vpmulld
%
xmm1
%
xmm0
%
xmm1
.
byte
196
99
117
24
253
1
/
/
vinsertf128
0x1
%
xmm5
%
ymm1
%
ymm15
.
byte
197
122
16
37
180
251
1
0
/
/
vmovss
0x1fbb4
(
%
rip
)
%
xmm12
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
196
195
121
4
204
0
/
/
vpermilps
0x0
%
xmm12
%
xmm1
.
byte
196
227
117
24
201
1
/
/
vinsertf128
0x1
%
xmm1
%
ymm1
%
ymm1
.
byte
197
244
88
140
36
64
1
0
0
/
/
vaddps
0x140
(
%
rsp
)
%
ymm1
%
ymm1
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
252
95
201
/
/
vmaxps
%
ymm1
%
ymm0
%
ymm1
.
byte
197
244
93
140
36
0
1
0
0
/
/
vminps
0x100
(
%
rsp
)
%
ymm1
%
ymm1
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
196
227
125
25
205
1
/
/
vextractf128
0x1
%
ymm1
%
xmm5
.
byte
196
99
125
25
254
1
/
/
vextractf128
0x1
%
ymm15
%
xmm6
.
byte
197
201
254
237
/
/
vpaddd
%
xmm5
%
xmm6
%
xmm5
.
byte
196
227
249
22
232
1
/
/
vpextrq
0x1
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
196
193
249
126
234
/
/
vmovq
%
xmm5
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
197
129
254
201
/
/
vpaddd
%
xmm1
%
xmm15
%
xmm1
.
byte
196
227
249
22
203
1
/
/
vpextrq
0x1
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
196
193
249
126
207
/
/
vmovq
%
xmm1
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
196
129
121
110
12
160
/
/
vmovd
(
%
r8
%
r12
4
)
%
xmm1
.
byte
196
131
113
34
12
184
1
/
/
vpinsrd
0x1
(
%
r8
%
r15
4
)
%
xmm1
%
xmm1
.
byte
196
131
113
34
12
176
2
/
/
vpinsrd
0x2
(
%
r8
%
r14
4
)
%
xmm1
%
xmm1
.
byte
196
195
113
34
12
152
3
/
/
vpinsrd
0x3
(
%
r8
%
rbx
4
)
%
xmm1
%
xmm1
.
byte
196
129
121
110
44
152
/
/
vmovd
(
%
r8
%
r11
4
)
%
xmm5
.
byte
196
131
81
34
44
144
1
/
/
vpinsrd
0x1
(
%
r8
%
r10
4
)
%
xmm5
%
xmm5
.
byte
196
131
81
34
44
136
2
/
/
vpinsrd
0x2
(
%
r8
%
r9
4
)
%
xmm5
%
xmm5
.
byte
196
195
81
34
4
128
3
/
/
vpinsrd
0x3
(
%
r8
%
rax
4
)
%
xmm5
%
xmm0
.
byte
196
227
117
24
232
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm5
.
byte
196
193
84
84
238
/
/
vandps
%
ymm14
%
ymm5
%
ymm5
.
byte
197
252
91
237
/
/
vcvtdq2ps
%
ymm5
%
ymm5
.
byte
196
193
84
89
245
/
/
vmulps
%
ymm13
%
ymm5
%
ymm6
.
byte
197
209
114
209
8
/
/
vpsrld
0x8
%
xmm1
%
xmm5
.
byte
197
217
114
208
8
/
/
vpsrld
0x8
%
xmm0
%
xmm4
.
byte
196
227
85
24
228
1
/
/
vinsertf128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
196
193
92
84
230
/
/
vandps
%
ymm14
%
ymm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
193
92
89
237
/
/
vmulps
%
ymm13
%
ymm4
%
ymm5
.
byte
197
217
114
209
16
/
/
vpsrld
0x10
%
xmm1
%
xmm4
.
byte
197
193
114
208
16
/
/
vpsrld
0x10
%
xmm0
%
xmm7
.
byte
196
227
93
24
231
1
/
/
vinsertf128
0x1
%
xmm7
%
ymm4
%
ymm4
.
byte
196
193
92
84
230
/
/
vandps
%
ymm14
%
ymm4
%
ymm4
.
byte
197
252
91
228
/
/
vcvtdq2ps
%
ymm4
%
ymm4
.
byte
196
65
92
89
221
/
/
vmulps
%
ymm13
%
ymm4
%
ymm11
.
byte
197
241
114
209
24
/
/
vpsrld
0x18
%
xmm1
%
xmm1
.
byte
197
249
114
208
24
/
/
vpsrld
0x18
%
xmm0
%
xmm0
.
byte
196
227
117
24
192
1
/
/
vinsertf128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
196
193
124
89
205
/
/
vmulps
%
ymm13
%
ymm0
%
ymm1
.
byte
197
120
46
37
217
250
1
0
/
/
vucomiss
0x1fad9
(
%
rip
)
%
xmm12
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
197
124
16
148
36
32
1
0
0
/
/
vmovups
0x120
(
%
rsp
)
%
ymm10
.
byte
119
9
/
/
ja
1cbc7
<
_sk_bilerp_clamp_8888_avx
+
0x2c2
>
.
byte
197
124
16
148
36
224
0
0
0
/
/
vmovups
0xe0
(
%
rsp
)
%
ymm10
.
byte
197
172
89
68
36
160
/
/
vmulps
-
0x60
(
%
rsp
)
%
ymm10
%
ymm0
.
byte
197
252
89
230
/
/
vmulps
%
ymm6
%
ymm0
%
ymm4
.
byte
197
52
88
204
/
/
vaddps
%
ymm4
%
ymm9
%
ymm9
.
byte
197
252
89
229
/
/
vmulps
%
ymm5
%
ymm0
%
ymm4
.
byte
197
60
88
196
/
/
vaddps
%
ymm4
%
ymm8
%
ymm8
.
byte
196
193
124
89
227
/
/
vmulps
%
ymm11
%
ymm0
%
ymm4
.
byte
197
236
88
212
/
/
vaddps
%
ymm4
%
ymm2
%
ymm2
.
byte
197
252
89
193
/
/
vmulps
%
ymm1
%
ymm0
%
ymm0
.
byte
197
228
88
216
/
/
vaddps
%
ymm0
%
ymm3
%
ymm3
.
byte
197
26
88
37
6
249
1
0
/
/
vaddss
0x1f906
(
%
rip
)
%
xmm12
%
xmm12
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
250
16
5
250
248
1
0
/
/
vmovss
0x1f8fa
(
%
rip
)
%
xmm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
120
46
196
/
/
vucomiss
%
xmm12
%
xmm0
.
byte
15
131
147
254
255
255
/
/
jae
1ca9c
<
_sk_bilerp_clamp_8888_avx
+
0x197
>
.
byte
197
248
40
100
36
144
/
/
vmovaps
-
0x70
(
%
rsp
)
%
xmm4
.
byte
197
218
88
37
229
248
1
0
/
/
vaddss
0x1f8e5
(
%
rip
)
%
xmm4
%
xmm4
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
250
16
5
217
248
1
0
/
/
vmovss
0x1f8d9
(
%
rip
)
%
xmm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
248
46
196
/
/
vucomiss
%
xmm4
%
xmm0
.
byte
15
131
244
253
255
255
/
/
jae
1ca1d
<
_sk_bilerp_clamp_8888_avx
+
0x118
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
200
/
/
vmovaps
%
ymm9
%
ymm0
.
byte
197
124
41
193
/
/
vmovaps
%
ymm8
%
ymm1
.
byte
197
252
16
100
36
192
/
/
vmovups
-
0x40
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
224
/
/
vmovups
-
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
16
52
36
/
/
vmovups
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm7
.
byte
72
129
196
120
1
0
0
/
/
add
0x178
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_start_pipeline_sse41
.
globl
_sk_start_pipeline_sse41
FUNCTION
(
_sk_start_pipeline_sse41
)
_sk_start_pipeline_sse41
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
131
0
0
0
/
/
jae
1cd10
<
_sk_start_pipeline_sse41
+
0xb6
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
4
/
/
lea
0x4
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
119
59
/
/
ja
1ccde
<
_sk_start_pipeline_sse41
+
0x84
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
4
/
/
lea
0x4
(
%
r12
)
%
rdx
.
byte
73
131
196
8
/
/
add
0x8
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
201
/
/
jbe
1cca7
<
_sk_start_pipeline_sse41
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
33
/
/
je
1cd07
<
_sk_start_pipeline_sse41
+
0xad
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
255
195
/
/
inc
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
117
137
/
/
jne
1cc99
<
_sk_start_pipeline_sse41
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_sse41
.
globl
_sk_just_return_sse41
FUNCTION
(
_sk_just_return_sse41
)
_sk_just_return_sse41
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_sse41
.
globl
_sk_seed_shader_sse41
FUNCTION
(
_sk_seed_shader_sse41
)
_sk_seed_shader_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm1
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
102
15
110
201
/
/
movd
%
ecx
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
88
13
185
1
2
0
/
/
addps
0x201b9
(
%
rip
)
%
xmm1
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
21
192
1
2
0
/
/
movaps
0x201c0
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dither_sse41
.
globl
_sk_dither_sse41
FUNCTION
(
_sk_dither_sse41
)
_sk_dither_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
68
15
110
194
/
/
movd
%
edx
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
102
68
15
254
5
105
1
2
0
/
/
paddd
0x20169
(
%
rip
)
%
xmm8
#
3cee0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc94
>
.
byte
102
68
15
110
201
/
/
movd
%
ecx
%
xmm9
.
byte
102
69
15
112
201
0
/
/
pshufd
0x0
%
xmm9
%
xmm9
.
byte
102
69
15
239
200
/
/
pxor
%
xmm8
%
xmm9
.
byte
102
68
15
111
21
144
1
2
0
/
/
movdqa
0x20190
(
%
rip
)
%
xmm10
#
3cf20
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcd4
>
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
69
15
219
218
/
/
pand
%
xmm10
%
xmm11
.
byte
102
65
15
114
243
5
/
/
pslld
0x5
%
xmm11
.
byte
102
69
15
219
208
/
/
pand
%
xmm8
%
xmm10
.
byte
102
65
15
114
242
4
/
/
pslld
0x4
%
xmm10
.
byte
102
68
15
111
37
124
1
2
0
/
/
movdqa
0x2017c
(
%
rip
)
%
xmm12
#
3cf30
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xce4
>
.
byte
102
68
15
111
45
131
1
2
0
/
/
movdqa
0x20183
(
%
rip
)
%
xmm13
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
69
15
111
241
/
/
movdqa
%
xmm9
%
xmm14
.
byte
102
69
15
219
245
/
/
pand
%
xmm13
%
xmm14
.
byte
102
65
15
114
246
2
/
/
pslld
0x2
%
xmm14
.
byte
102
69
15
235
243
/
/
por
%
xmm11
%
xmm14
.
byte
102
69
15
219
232
/
/
pand
%
xmm8
%
xmm13
.
byte
102
69
15
254
237
/
/
paddd
%
xmm13
%
xmm13
.
byte
102
69
15
235
234
/
/
por
%
xmm10
%
xmm13
.
byte
102
69
15
219
204
/
/
pand
%
xmm12
%
xmm9
.
byte
102
65
15
114
209
1
/
/
psrld
0x1
%
xmm9
.
byte
102
69
15
219
196
/
/
pand
%
xmm12
%
xmm8
.
byte
102
65
15
114
208
2
/
/
psrld
0x2
%
xmm8
.
byte
102
69
15
235
197
/
/
por
%
xmm13
%
xmm8
.
byte
102
69
15
235
198
/
/
por
%
xmm14
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
62
1
2
0
/
/
mulps
0x2013e
(
%
rip
)
%
xmm8
#
3cf50
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd04
>
.
byte
68
15
88
5
70
1
2
0
/
/
addps
0x20146
(
%
rip
)
%
xmm8
#
3cf60
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd14
>
.
byte
243
68
15
16
16
/
/
movss
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
15
93
195
/
/
minps
%
xmm3
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
15
93
203
/
/
minps
%
xmm3
%
xmm1
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
68
15
95
201
/
/
maxps
%
xmm1
%
xmm9
.
byte
68
15
93
211
/
/
minps
%
xmm3
%
xmm10
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_sse41
.
globl
_sk_uniform_color_sse41
FUNCTION
(
_sk_uniform_color_sse41
)
_sk_uniform_color_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_sse41
.
globl
_sk_black_color_sse41
FUNCTION
(
_sk_black_color_sse41
)
_sk_black_color_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
124
0
2
0
/
/
movaps
0x2007c
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_sse41
.
globl
_sk_white_color_sse41
FUNCTION
(
_sk_white_color_sse41
)
_sk_white_color_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
5
104
0
2
0
/
/
movaps
0x20068
(
%
rip
)
%
xmm0
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_rgba_sse41
.
globl
_sk_load_rgba_sse41
FUNCTION
(
_sk_load_rgba_sse41
)
_sk_load_rgba_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
15
16
72
16
/
/
movups
0x10
(
%
rax
)
%
xmm1
.
byte
15
16
80
32
/
/
movups
0x20
(
%
rax
)
%
xmm2
.
byte
15
16
88
48
/
/
movups
0x30
(
%
rax
)
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_rgba_sse41
.
globl
_sk_store_rgba_sse41
FUNCTION
(
_sk_store_rgba_sse41
)
_sk_store_rgba_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
17
0
/
/
movups
%
xmm0
(
%
rax
)
.
byte
15
17
72
16
/
/
movups
%
xmm1
0x10
(
%
rax
)
.
byte
15
17
80
32
/
/
movups
%
xmm2
0x20
(
%
rax
)
.
byte
15
17
88
48
/
/
movups
%
xmm3
0x30
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_sse41
.
globl
_sk_clear_sse41
FUNCTION
(
_sk_clear_sse41
)
_sk_clear_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_sse41
.
globl
_sk_srcatop_sse41
FUNCTION
(
_sk_srcatop_sse41
)
_sk_srcatop_sse41
:
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
68
15
40
5
24
0
2
0
/
/
movaps
0x20018
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
204
/
/
mulps
%
xmm4
%
xmm9
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_sse41
.
globl
_sk_dstatop_sse41
FUNCTION
(
_sk_dstatop_sse41
)
_sk_dstatop_sse41
:
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
89
196
/
/
mulps
%
xmm4
%
xmm8
.
byte
68
15
40
13
203
255
1
0
/
/
movaps
0x1ffcb
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
207
/
/
subps
%
xmm7
%
xmm9
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
89
197
/
/
mulps
%
xmm5
%
xmm8
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
89
198
/
/
mulps
%
xmm6
%
xmm8
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_sse41
.
globl
_sk_srcin_sse41
FUNCTION
(
_sk_srcin_sse41
)
_sk_srcin_sse41
:
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_sse41
.
globl
_sk_dstin_sse41
FUNCTION
(
_sk_dstin_sse41
)
_sk_dstin_sse41
:
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_sse41
.
globl
_sk_srcout_sse41
FUNCTION
(
_sk_srcout_sse41
)
_sk_srcout_sse41
:
.
byte
68
15
40
5
95
255
1
0
/
/
movaps
0x1ff5f
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_sse41
.
globl
_sk_dstout_sse41
FUNCTION
(
_sk_dstout_sse41
)
_sk_dstout_sse41
:
.
byte
68
15
40
5
63
255
1
0
/
/
movaps
0x1ff3f
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_sse41
.
globl
_sk_srcover_sse41
FUNCTION
(
_sk_srcover_sse41
)
_sk_srcover_sse41
:
.
byte
68
15
40
5
18
255
1
0
/
/
movaps
0x1ff12
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
204
/
/
mulps
%
xmm4
%
xmm9
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_sse41
.
globl
_sk_dstover_sse41
FUNCTION
(
_sk_dstover_sse41
)
_sk_dstover_sse41
:
.
byte
68
15
40
5
214
254
1
0
/
/
movaps
0x1fed6
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_sse41
.
globl
_sk_modulate_sse41
FUNCTION
(
_sk_modulate_sse41
)
_sk_modulate_sse41
:
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_sse41
.
globl
_sk_multiply_sse41
FUNCTION
(
_sk_multiply_sse41
)
_sk_multiply_sse41
:
.
byte
68
15
40
5
154
254
1
0
/
/
movaps
0x1fe9a
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
92
207
/
/
subps
%
xmm7
%
xmm9
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
89
220
/
/
mulps
%
xmm4
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
195
/
/
addps
%
xmm11
%
xmm0
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
89
221
/
/
mulps
%
xmm5
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
89
222
/
/
mulps
%
xmm6
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__sse41
.
globl
_sk_plus__sse41
FUNCTION
(
_sk_plus__sse41
)
_sk_plus__sse41
:
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
40
5
27
254
1
0
/
/
movaps
0x1fe1b
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_sse41
.
globl
_sk_screen_sse41
FUNCTION
(
_sk_screen_sse41
)
_sk_screen_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
196
/
/
mulps
%
xmm4
%
xmm8
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
88
197
/
/
addps
%
xmm5
%
xmm8
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
68
15
92
193
/
/
subps
%
xmm1
%
xmm8
.
byte
68
15
40
202
/
/
movaps
%
xmm2
%
xmm9
.
byte
68
15
88
206
/
/
addps
%
xmm6
%
xmm9
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
92
202
/
/
subps
%
xmm2
%
xmm9
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
88
215
/
/
addps
%
xmm7
%
xmm10
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__sse41
.
globl
_sk_xor__sse41
FUNCTION
(
_sk_xor__sse41
)
_sk_xor__sse41
:
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
15
40
29
167
253
1
0
/
/
movaps
0x1fda7
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
92
207
/
/
subps
%
xmm7
%
xmm9
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
92
216
/
/
subps
%
xmm8
%
xmm3
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
89
212
/
/
mulps
%
xmm4
%
xmm10
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
89
213
/
/
mulps
%
xmm5
%
xmm10
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
65
15
88
210
/
/
addps
%
xmm10
%
xmm2
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_sse41
.
globl
_sk_darken_sse41
FUNCTION
(
_sk_darken_sse41
)
_sk_darken_sse41
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
68
15
95
201
/
/
maxps
%
xmm1
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
69
15
95
193
/
/
maxps
%
xmm9
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
95
209
/
/
maxps
%
xmm9
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
2
253
1
0
/
/
movaps
0x1fd02
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_sse41
.
globl
_sk_lighten_sse41
FUNCTION
(
_sk_lighten_sse41
)
_sk_lighten_sse41
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
68
15
93
201
/
/
minps
%
xmm1
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
69
15
93
193
/
/
minps
%
xmm9
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
93
209
/
/
minps
%
xmm9
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
151
252
1
0
/
/
movaps
0x1fc97
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_sse41
.
globl
_sk_difference_sse41
FUNCTION
(
_sk_difference_sse41
)
_sk_difference_sse41
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
68
15
93
201
/
/
minps
%
xmm1
%
xmm9
.
byte
69
15
88
201
/
/
addps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
69
15
93
193
/
/
minps
%
xmm9
%
xmm8
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
93
209
/
/
minps
%
xmm9
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
33
252
1
0
/
/
movaps
0x1fc21
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_sse41
.
globl
_sk_exclusion_sse41
FUNCTION
(
_sk_exclusion_sse41
)
_sk_exclusion_sse41
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
197
/
/
mulps
%
xmm5
%
xmm8
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
209
251
1
0
/
/
movaps
0x1fbd1
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colorburn_sse41
.
globl
_sk_colorburn_sse41
FUNCTION
(
_sk_colorburn_sse41
)
_sk_colorburn_sse41
:
.
byte
68
15
40
29
184
251
1
0
/
/
movaps
0x1fbb8
(
%
rip
)
%
xmm11
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
68
15
92
231
/
/
subps
%
xmm7
%
xmm12
.
byte
69
15
40
204
/
/
movaps
%
xmm12
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
68
15
40
199
/
/
movaps
%
xmm7
%
xmm8
.
byte
68
15
92
196
/
/
subps
%
xmm4
%
xmm8
.
byte
68
15
89
195
/
/
mulps
%
xmm3
%
xmm8
.
byte
68
15
83
208
/
/
rcpps
%
xmm0
%
xmm10
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
69
15
93
234
/
/
minps
%
xmm10
%
xmm13
.
byte
68
15
40
199
/
/
movaps
%
xmm7
%
xmm8
.
byte
69
15
92
197
/
/
subps
%
xmm13
%
xmm8
.
byte
68
15
89
195
/
/
mulps
%
xmm3
%
xmm8
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
68
15
88
204
/
/
addps
%
xmm4
%
xmm9
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
68
15
92
219
/
/
subps
%
xmm3
%
xmm11
.
byte
69
15
40
235
/
/
movaps
%
xmm11
%
xmm13
.
byte
68
15
89
236
/
/
mulps
%
xmm4
%
xmm13
.
byte
65
15
194
194
0
/
/
cmpeqps
%
xmm10
%
xmm0
.
byte
69
15
88
197
/
/
addps
%
xmm13
%
xmm8
.
byte
102
69
15
56
20
197
/
/
blendvps
%
xmm0
%
xmm13
%
xmm8
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
194
199
0
/
/
cmpeqps
%
xmm7
%
xmm0
.
byte
102
69
15
56
20
193
/
/
blendvps
%
xmm0
%
xmm9
%
xmm8
.
byte
69
15
40
236
/
/
movaps
%
xmm12
%
xmm13
.
byte
68
15
89
233
/
/
mulps
%
xmm1
%
xmm13
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
68
15
83
201
/
/
rcpps
%
xmm1
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
65
15
93
193
/
/
minps
%
xmm9
%
xmm0
.
byte
68
15
40
207
/
/
movaps
%
xmm7
%
xmm9
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
68
15
89
245
/
/
mulps
%
xmm5
%
xmm14
.
byte
65
15
194
202
0
/
/
cmpeqps
%
xmm10
%
xmm1
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
69
15
88
205
/
/
addps
%
xmm13
%
xmm9
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
102
69
15
56
20
206
/
/
blendvps
%
xmm0
%
xmm14
%
xmm9
.
byte
68
15
88
237
/
/
addps
%
xmm5
%
xmm13
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
194
199
0
/
/
cmpeqps
%
xmm7
%
xmm0
.
byte
102
69
15
56
20
205
/
/
blendvps
%
xmm0
%
xmm13
%
xmm9
.
byte
68
15
89
226
/
/
mulps
%
xmm2
%
xmm12
.
byte
68
15
194
210
0
/
/
cmpeqps
%
xmm2
%
xmm10
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
83
202
/
/
rcpps
%
xmm2
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
93
193
/
/
minps
%
xmm1
%
xmm0
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
65
15
40
203
/
/
movaps
%
xmm11
%
xmm1
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
15
56
20
209
/
/
blendvps
%
xmm0
%
xmm1
%
xmm2
.
byte
68
15
88
230
/
/
addps
%
xmm6
%
xmm12
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
194
199
0
/
/
cmpeqps
%
xmm7
%
xmm0
.
byte
102
65
15
56
20
212
/
/
blendvps
%
xmm0
%
xmm12
%
xmm2
.
byte
68
15
89
223
/
/
mulps
%
xmm7
%
xmm11
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colordodge_sse41
.
globl
_sk_colordodge_sse41
FUNCTION
(
_sk_colordodge_sse41
)
_sk_colordodge_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
68
15
40
21
130
250
1
0
/
/
movaps
0x1fa82
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
68
15
92
223
/
/
subps
%
xmm7
%
xmm11
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
68
15
40
235
/
/
movaps
%
xmm3
%
xmm13
.
byte
68
15
89
236
/
/
mulps
%
xmm4
%
xmm13
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
68
15
83
200
/
/
rcpps
%
xmm0
%
xmm9
.
byte
69
15
89
205
/
/
mulps
%
xmm13
%
xmm9
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
68
15
40
247
/
/
movaps
%
xmm7
%
xmm14
.
byte
69
15
93
241
/
/
minps
%
xmm9
%
xmm14
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
194
195
0
/
/
cmpeqps
%
xmm3
%
xmm0
.
byte
68
15
89
243
/
/
mulps
%
xmm3
%
xmm14
.
byte
69
15
88
244
/
/
addps
%
xmm12
%
xmm14
.
byte
102
69
15
56
20
240
/
/
blendvps
%
xmm0
%
xmm8
%
xmm14
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
69
15
40
194
/
/
movaps
%
xmm10
%
xmm8
.
byte
68
15
89
196
/
/
mulps
%
xmm4
%
xmm8
.
byte
69
15
88
198
/
/
addps
%
xmm14
%
xmm8
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
194
193
0
/
/
cmpeqps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
196
/
/
blendvps
%
xmm0
%
xmm12
%
xmm8
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
68
15
89
229
/
/
mulps
%
xmm5
%
xmm12
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
83
192
/
/
rcpps
%
xmm0
%
xmm0
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
68
15
89
225
/
/
mulps
%
xmm1
%
xmm12
.
byte
68
15
93
232
/
/
minps
%
xmm0
%
xmm13
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
194
195
0
/
/
cmpeqps
%
xmm3
%
xmm0
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
69
15
88
236
/
/
addps
%
xmm12
%
xmm13
.
byte
102
68
15
56
20
233
/
/
blendvps
%
xmm0
%
xmm1
%
xmm13
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
65
15
88
205
/
/
addps
%
xmm13
%
xmm1
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
65
15
194
193
0
/
/
cmpeqps
%
xmm9
%
xmm0
.
byte
102
65
15
56
20
204
/
/
blendvps
%
xmm0
%
xmm12
%
xmm1
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
68
15
89
230
/
/
mulps
%
xmm6
%
xmm12
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
83
192
/
/
rcpps
%
xmm0
%
xmm0
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
68
15
93
224
/
/
minps
%
xmm0
%
xmm12
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
194
195
0
/
/
cmpeqps
%
xmm3
%
xmm0
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
227
/
/
addps
%
xmm11
%
xmm12
.
byte
102
68
15
56
20
226
/
/
blendvps
%
xmm0
%
xmm2
%
xmm12
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
68
15
194
206
0
/
/
cmpeqps
%
xmm6
%
xmm9
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
65
15
56
20
211
/
/
blendvps
%
xmm0
%
xmm11
%
xmm2
.
byte
68
15
89
215
/
/
mulps
%
xmm7
%
xmm10
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_sse41
.
globl
_sk_hardlight_sse41
FUNCTION
(
_sk_hardlight_sse41
)
_sk_hardlight_sse41
:
.
byte
15
41
116
36
232
/
/
movaps
%
xmm6
-
0x18
(
%
rsp
)
.
byte
68
15
40
229
/
/
movaps
%
xmm5
%
xmm12
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
40
21
77
249
1
0
/
/
movaps
0x1f94d
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
40
234
/
/
movaps
%
xmm10
%
xmm5
.
byte
15
92
239
/
/
subps
%
xmm7
%
xmm5
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
68
15
92
212
/
/
subps
%
xmm4
%
xmm10
.
byte
69
15
40
194
/
/
movaps
%
xmm10
%
xmm8
.
byte
68
15
89
198
/
/
mulps
%
xmm6
%
xmm8
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
68
15
40
252
/
/
movaps
%
xmm4
%
xmm15
.
byte
69
15
92
249
/
/
subps
%
xmm9
%
xmm15
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
68
15
40
247
/
/
movaps
%
xmm7
%
xmm14
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
65
15
89
199
/
/
mulps
%
xmm15
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
68
15
40
251
/
/
movaps
%
xmm3
%
xmm15
.
byte
68
15
92
248
/
/
subps
%
xmm0
%
xmm15
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
196
2
/
/
cmpleps
%
xmm4
%
xmm0
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
69
15
88
201
/
/
addps
%
xmm9
%
xmm9
.
byte
102
69
15
56
20
249
/
/
blendvps
%
xmm0
%
xmm9
%
xmm15
.
byte
68
15
40
221
/
/
movaps
%
xmm5
%
xmm11
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
69
15
40
204
/
/
movaps
%
xmm12
%
xmm9
.
byte
69
15
92
233
/
/
subps
%
xmm9
%
xmm13
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
69
15
88
237
/
/
addps
%
xmm13
%
xmm13
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
196
2
/
/
cmpleps
%
xmm4
%
xmm0
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
69
15
40
233
/
/
movaps
%
xmm9
%
xmm13
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
102
68
15
56
20
225
/
/
blendvps
%
xmm0
%
xmm1
%
xmm12
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
69
15
40
202
/
/
movaps
%
xmm10
%
xmm9
.
byte
68
15
89
215
/
/
mulps
%
xmm7
%
xmm10
.
byte
69
15
88
199
/
/
addps
%
xmm15
%
xmm8
.
byte
65
15
89
205
/
/
mulps
%
xmm13
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
65
15
88
204
/
/
addps
%
xmm12
%
xmm1
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
68
15
40
92
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm11
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
68
15
88
205
/
/
addps
%
xmm5
%
xmm9
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
196
2
/
/
cmpleps
%
xmm4
%
xmm0
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
69
15
92
243
/
/
subps
%
xmm11
%
xmm14
.
byte
68
15
89
245
/
/
mulps
%
xmm5
%
xmm14
.
byte
69
15
88
246
/
/
addps
%
xmm14
%
xmm14
.
byte
65
15
92
222
/
/
subps
%
xmm14
%
xmm3
.
byte
102
15
56
20
218
/
/
blendvps
%
xmm0
%
xmm2
%
xmm3
.
byte
68
15
88
203
/
/
addps
%
xmm3
%
xmm9
.
byte
65
15
88
226
/
/
addps
%
xmm10
%
xmm4
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
65
15
40
237
/
/
movaps
%
xmm13
%
xmm5
.
byte
65
15
40
243
/
/
movaps
%
xmm11
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_sse41
.
globl
_sk_overlay_sse41
FUNCTION
(
_sk_overlay_sse41
)
_sk_overlay_sse41
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
240
/
/
movaps
%
xmm0
%
xmm14
.
byte
68
15
40
21
34
248
1
0
/
/
movaps
0x1f822
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
68
15
92
223
/
/
subps
%
xmm7
%
xmm11
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
89
198
/
/
mulps
%
xmm14
%
xmm0
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
69
15
40
194
/
/
movaps
%
xmm10
%
xmm8
.
byte
68
15
89
196
/
/
mulps
%
xmm4
%
xmm8
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
68
15
40
235
/
/
movaps
%
xmm3
%
xmm13
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
68
15
89
244
/
/
mulps
%
xmm4
%
xmm14
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
199
2
/
/
cmpleps
%
xmm7
%
xmm0
.
byte
69
15
88
246
/
/
addps
%
xmm14
%
xmm14
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
68
15
89
231
/
/
mulps
%
xmm7
%
xmm12
.
byte
65
15
89
205
/
/
mulps
%
xmm13
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
69
15
40
236
/
/
movaps
%
xmm12
%
xmm13
.
byte
68
15
92
233
/
/
subps
%
xmm1
%
xmm13
.
byte
102
69
15
56
20
238
/
/
blendvps
%
xmm0
%
xmm14
%
xmm13
.
byte
69
15
88
197
/
/
addps
%
xmm13
%
xmm8
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
68
15
40
235
/
/
movaps
%
xmm3
%
xmm13
.
byte
69
15
92
233
/
/
subps
%
xmm9
%
xmm13
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
68
15
40
247
/
/
movaps
%
xmm7
%
xmm14
.
byte
68
15
92
245
/
/
subps
%
xmm5
%
xmm14
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
199
2
/
/
cmpleps
%
xmm7
%
xmm0
.
byte
69
15
88
201
/
/
addps
%
xmm9
%
xmm9
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
88
246
/
/
addps
%
xmm14
%
xmm14
.
byte
69
15
40
236
/
/
movaps
%
xmm12
%
xmm13
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
102
69
15
56
20
233
/
/
blendvps
%
xmm0
%
xmm9
%
xmm13
.
byte
65
15
88
205
/
/
addps
%
xmm13
%
xmm1
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
40
202
/
/
movaps
%
xmm10
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
68
15
92
218
/
/
subps
%
xmm2
%
xmm11
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
68
15
92
238
/
/
subps
%
xmm6
%
xmm13
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
199
2
/
/
cmpleps
%
xmm7
%
xmm0
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
69
15
89
235
/
/
mulps
%
xmm11
%
xmm13
.
byte
69
15
88
237
/
/
addps
%
xmm13
%
xmm13
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
102
68
15
56
20
226
/
/
blendvps
%
xmm0
%
xmm2
%
xmm12
.
byte
69
15
88
204
/
/
addps
%
xmm12
%
xmm9
.
byte
68
15
89
215
/
/
mulps
%
xmm7
%
xmm10
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_softlight_sse41
.
globl
_sk_softlight_sse41
FUNCTION
(
_sk_softlight_sse41
)
_sk_softlight_sse41
:
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
41
84
36
232
/
/
movaps
%
xmm2
-
0x18
(
%
rsp
)
.
byte
15
41
76
36
200
/
/
movaps
%
xmm1
-
0x38
(
%
rsp
)
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
207
1
/
/
cmpltps
%
xmm7
%
xmm9
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
94
199
/
/
divps
%
xmm7
%
xmm0
.
byte
65
15
84
193
/
/
andps
%
xmm9
%
xmm0
.
byte
15
40
13
233
246
1
0
/
/
movaps
0x1f6e9
(
%
rip
)
%
xmm1
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
68
15
92
208
/
/
subps
%
xmm0
%
xmm10
.
byte
68
15
40
240
/
/
movaps
%
xmm0
%
xmm14
.
byte
68
15
40
248
/
/
movaps
%
xmm0
%
xmm15
.
byte
15
82
208
/
/
rsqrtps
%
xmm0
%
xmm2
.
byte
68
15
83
218
/
/
rcpps
%
xmm2
%
xmm11
.
byte
68
15
92
216
/
/
subps
%
xmm0
%
xmm11
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
89
210
/
/
mulps
%
xmm2
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
68
15
40
45
23
247
1
0
/
/
movaps
0x1f717
(
%
rip
)
%
xmm13
#
3cf70
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd24
>
.
byte
69
15
88
245
/
/
addps
%
xmm13
%
xmm14
.
byte
68
15
89
242
/
/
mulps
%
xmm2
%
xmm14
.
byte
68
15
40
37
23
247
1
0
/
/
movaps
0x1f717
(
%
rip
)
%
xmm12
#
3cf80
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd34
>
.
byte
69
15
89
252
/
/
mulps
%
xmm12
%
xmm15
.
byte
69
15
88
254
/
/
addps
%
xmm14
%
xmm15
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
199
2
/
/
cmpleps
%
xmm7
%
xmm0
.
byte
102
69
15
56
20
223
/
/
blendvps
%
xmm0
%
xmm15
%
xmm11
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
68
15
88
211
/
/
addps
%
xmm3
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
88
218
/
/
addps
%
xmm2
%
xmm11
.
byte
15
194
195
2
/
/
cmpleps
%
xmm3
%
xmm0
.
byte
102
69
15
56
20
218
/
/
blendvps
%
xmm0
%
xmm10
%
xmm11
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
68
15
94
215
/
/
divps
%
xmm7
%
xmm10
.
byte
69
15
84
209
/
/
andps
%
xmm9
%
xmm10
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
89
210
/
/
mulps
%
xmm2
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
65
15
88
197
/
/
addps
%
xmm13
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
65
15
82
194
/
/
rsqrtps
%
xmm10
%
xmm0
.
byte
68
15
83
240
/
/
rcpps
%
xmm0
%
xmm14
.
byte
69
15
92
242
/
/
subps
%
xmm10
%
xmm14
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
199
2
/
/
cmpleps
%
xmm7
%
xmm0
.
byte
102
68
15
56
20
242
/
/
blendvps
%
xmm0
%
xmm2
%
xmm14
.
byte
68
15
40
249
/
/
movaps
%
xmm1
%
xmm15
.
byte
69
15
92
250
/
/
subps
%
xmm10
%
xmm15
.
byte
15
40
108
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm5
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
68
15
89
250
/
/
mulps
%
xmm2
%
xmm15
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
89
242
/
/
mulps
%
xmm2
%
xmm14
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
41
100
36
184
/
/
movaps
%
xmm4
-
0x48
(
%
rsp
)
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
68
15
88
242
/
/
addps
%
xmm2
%
xmm14
.
byte
68
15
88
251
/
/
addps
%
xmm3
%
xmm15
.
byte
68
15
89
252
/
/
mulps
%
xmm4
%
xmm15
.
byte
15
194
195
2
/
/
cmpleps
%
xmm3
%
xmm0
.
byte
102
69
15
56
20
247
/
/
blendvps
%
xmm0
%
xmm15
%
xmm14
.
byte
68
15
40
249
/
/
movaps
%
xmm1
%
xmm15
.
byte
15
40
100
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm4
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
94
199
/
/
divps
%
xmm7
%
xmm0
.
byte
65
15
84
193
/
/
andps
%
xmm9
%
xmm0
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
68
15
88
232
/
/
addps
%
xmm0
%
xmm13
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
15
82
208
/
/
rsqrtps
%
xmm0
%
xmm2
.
byte
68
15
83
202
/
/
rcpps
%
xmm2
%
xmm9
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
89
210
/
/
mulps
%
xmm2
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
199
2
/
/
cmpleps
%
xmm7
%
xmm0
.
byte
102
69
15
56
20
204
/
/
blendvps
%
xmm0
%
xmm12
%
xmm9
.
byte
68
15
40
100
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm12
.
byte
65
15
40
196
/
/
movaps
%
xmm12
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
89
202
/
/
mulps
%
xmm2
%
xmm9
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
68
15
88
202
/
/
addps
%
xmm2
%
xmm9
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
194
195
2
/
/
cmpleps
%
xmm3
%
xmm0
.
byte
102
68
15
56
20
201
/
/
blendvps
%
xmm0
%
xmm1
%
xmm9
.
byte
68
15
92
255
/
/
subps
%
xmm7
%
xmm15
.
byte
69
15
89
199
/
/
mulps
%
xmm15
%
xmm8
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
65
15
89
207
/
/
mulps
%
xmm15
%
xmm1
.
byte
69
15
89
252
/
/
mulps
%
xmm12
%
xmm15
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
69
15
88
195
/
/
addps
%
xmm11
%
xmm8
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
15
40
108
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
68
15
89
215
/
/
mulps
%
xmm7
%
xmm10
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hue_sse41
.
globl
_sk_hue_sse41
FUNCTION
(
_sk_hue_sse41
)
_sk_hue_sse41
:
.
byte
15
41
124
36
152
/
/
movaps
%
xmm7
-
0x68
(
%
rsp
)
.
byte
68
15
40
246
/
/
movaps
%
xmm6
%
xmm14
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
15
41
84
36
232
/
/
movaps
%
xmm2
-
0x18
(
%
rsp
)
.
byte
15
41
76
36
216
/
/
movaps
%
xmm1
-
0x28
(
%
rsp
)
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
68
15
41
84
36
200
/
/
movaps
%
xmm10
-
0x38
(
%
rsp
)
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
68
15
40
234
/
/
movaps
%
xmm2
%
xmm13
.
byte
69
15
89
232
/
/
mulps
%
xmm8
%
xmm13
.
byte
68
15
40
205
/
/
movaps
%
xmm5
%
xmm9
.
byte
68
15
40
221
/
/
movaps
%
xmm5
%
xmm11
.
byte
15
41
108
36
184
/
/
movaps
%
xmm5
-
0x48
(
%
rsp
)
.
byte
69
15
95
222
/
/
maxps
%
xmm14
%
xmm11
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
68
15
40
230
/
/
movaps
%
xmm6
%
xmm12
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
65
15
95
211
/
/
maxps
%
xmm11
%
xmm2
.
byte
65
15
40
230
/
/
movaps
%
xmm14
%
xmm4
.
byte
15
41
100
36
168
/
/
movaps
%
xmm4
-
0x58
(
%
rsp
)
.
byte
68
15
93
204
/
/
minps
%
xmm4
%
xmm9
.
byte
65
15
93
249
/
/
minps
%
xmm9
%
xmm7
.
byte
15
92
215
/
/
subps
%
xmm7
%
xmm2
.
byte
15
40
249
/
/
movaps
%
xmm1
%
xmm7
.
byte
65
15
93
253
/
/
minps
%
xmm13
%
xmm7
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
15
93
223
/
/
minps
%
xmm7
%
xmm3
.
byte
15
40
249
/
/
movaps
%
xmm1
%
xmm7
.
byte
65
15
95
253
/
/
maxps
%
xmm13
%
xmm7
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
15
95
199
/
/
maxps
%
xmm7
%
xmm0
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
68
15
92
235
/
/
subps
%
xmm3
%
xmm13
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
68
15
94
208
/
/
divps
%
xmm0
%
xmm10
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
94
200
/
/
divps
%
xmm0
%
xmm1
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
68
15
94
232
/
/
divps
%
xmm0
%
xmm13
.
byte
15
194
195
4
/
/
cmpneqps
%
xmm3
%
xmm0
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
68
15
84
232
/
/
andps
%
xmm0
%
xmm13
.
byte
15
40
5
130
244
1
0
/
/
movaps
0x1f482
(
%
rip
)
%
xmm0
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
15
40
21
135
244
1
0
/
/
movaps
0x1f487
(
%
rip
)
%
xmm2
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
65
15
88
252
/
/
addps
%
xmm12
%
xmm7
.
byte
68
15
40
53
136
244
1
0
/
/
movaps
0x1f488
(
%
rip
)
%
xmm14
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
68
15
40
252
/
/
movaps
%
xmm4
%
xmm15
.
byte
69
15
89
254
/
/
mulps
%
xmm14
%
xmm15
.
byte
68
15
88
255
/
/
addps
%
xmm7
%
xmm15
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
249
/
/
movaps
%
xmm1
%
xmm7
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
65
15
40
221
/
/
movaps
%
xmm13
%
xmm3
.
byte
65
15
89
222
/
/
mulps
%
xmm14
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
69
15
89
248
/
/
mulps
%
xmm8
%
xmm15
.
byte
68
15
92
251
/
/
subps
%
xmm3
%
xmm15
.
byte
69
15
88
215
/
/
addps
%
xmm15
%
xmm10
.
byte
65
15
88
207
/
/
addps
%
xmm15
%
xmm1
.
byte
69
15
88
253
/
/
addps
%
xmm13
%
xmm15
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
65
15
93
223
/
/
minps
%
xmm15
%
xmm3
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
15
93
251
/
/
minps
%
xmm3
%
xmm7
.
byte
65
15
89
194
/
/
mulps
%
xmm10
%
xmm0
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
69
15
89
247
/
/
mulps
%
xmm15
%
xmm14
.
byte
68
15
88
242
/
/
addps
%
xmm2
%
xmm14
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
207
2
/
/
cmpleps
%
xmm7
%
xmm9
.
byte
65
15
40
222
/
/
movaps
%
xmm14
%
xmm3
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
69
15
40
234
/
/
movaps
%
xmm10
%
xmm13
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
68
15
94
235
/
/
divps
%
xmm3
%
xmm13
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
234
/
/
blendvps
%
xmm0
%
xmm10
%
xmm13
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
95
199
/
/
maxps
%
xmm15
%
xmm0
.
byte
68
15
95
208
/
/
maxps
%
xmm0
%
xmm10
.
byte
65
15
40
248
/
/
movaps
%
xmm8
%
xmm7
.
byte
15
40
108
36
152
/
/
movaps
-
0x68
(
%
rsp
)
%
xmm5
.
byte
15
89
253
/
/
mulps
%
xmm5
%
xmm7
.
byte
15
40
231
/
/
movaps
%
xmm7
%
xmm4
.
byte
65
15
194
226
1
/
/
cmpltps
%
xmm10
%
xmm4
.
byte
65
15
40
213
/
/
movaps
%
xmm13
%
xmm2
.
byte
65
15
92
214
/
/
subps
%
xmm14
%
xmm2
.
byte
68
15
40
223
/
/
movaps
%
xmm7
%
xmm11
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
69
15
92
214
/
/
subps
%
xmm14
%
xmm10
.
byte
65
15
94
210
/
/
divps
%
xmm10
%
xmm2
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
102
68
15
56
20
234
/
/
blendvps
%
xmm0
%
xmm2
%
xmm13
.
byte
68
15
40
225
/
/
movaps
%
xmm1
%
xmm12
.
byte
69
15
92
230
/
/
subps
%
xmm14
%
xmm12
.
byte
69
15
89
230
/
/
mulps
%
xmm14
%
xmm12
.
byte
68
15
94
227
/
/
divps
%
xmm3
%
xmm12
.
byte
69
15
88
230
/
/
addps
%
xmm14
%
xmm12
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
68
15
56
20
225
/
/
blendvps
%
xmm0
%
xmm1
%
xmm12
.
byte
65
15
40
204
/
/
movaps
%
xmm12
%
xmm1
.
byte
65
15
92
206
/
/
subps
%
xmm14
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
65
15
94
202
/
/
divps
%
xmm10
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
102
68
15
56
20
225
/
/
blendvps
%
xmm0
%
xmm1
%
xmm12
.
byte
65
15
40
207
/
/
movaps
%
xmm15
%
xmm1
.
byte
65
15
92
206
/
/
subps
%
xmm14
%
xmm1
.
byte
65
15
89
206
/
/
mulps
%
xmm14
%
xmm1
.
byte
15
94
203
/
/
divps
%
xmm3
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
65
15
56
20
207
/
/
blendvps
%
xmm0
%
xmm15
%
xmm1
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
65
15
92
214
/
/
subps
%
xmm14
%
xmm2
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
65
15
94
210
/
/
divps
%
xmm10
%
xmm2
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
102
15
56
20
202
/
/
blendvps
%
xmm0
%
xmm2
%
xmm1
.
byte
68
15
40
13
156
242
1
0
/
/
movaps
0x1f29c
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
40
225
/
/
movaps
%
xmm9
%
xmm4
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
40
68
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
92
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
89
100
36
232
/
/
mulps
-
0x18
(
%
rsp
)
%
xmm4
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
88
197
/
/
addps
%
xmm5
%
xmm8
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
68
15
95
234
/
/
maxps
%
xmm2
%
xmm13
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
15
95
202
/
/
maxps
%
xmm2
%
xmm1
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
65
15
88
197
/
/
addps
%
xmm13
%
xmm0
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
15
40
108
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm5
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
92
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm3
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
68
15
88
204
/
/
addps
%
xmm4
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_saturation_sse41
.
globl
_sk_saturation_sse41
FUNCTION
(
_sk_saturation_sse41
)
_sk_saturation_sse41
:
.
byte
68
15
40
206
/
/
movaps
%
xmm6
%
xmm9
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
41
76
36
216
/
/
movaps
%
xmm1
-
0x28
(
%
rsp
)
.
byte
15
41
68
36
200
/
/
movaps
%
xmm0
-
0x38
(
%
rsp
)
.
byte
68
15
40
212
/
/
movaps
%
xmm4
%
xmm10
.
byte
68
15
89
213
/
/
mulps
%
xmm5
%
xmm10
.
byte
68
15
40
220
/
/
movaps
%
xmm4
%
xmm11
.
byte
68
15
89
222
/
/
mulps
%
xmm6
%
xmm11
.
byte
68
15
40
196
/
/
movaps
%
xmm4
%
xmm8
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
69
15
40
241
/
/
movaps
%
xmm9
%
xmm14
.
byte
68
15
41
116
36
184
/
/
movaps
%
xmm14
-
0x48
(
%
rsp
)
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
68
15
40
202
/
/
movaps
%
xmm2
%
xmm9
.
byte
68
15
41
76
36
232
/
/
movaps
%
xmm9
-
0x18
(
%
rsp
)
.
byte
65
15
95
217
/
/
maxps
%
xmm9
%
xmm3
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
95
211
/
/
maxps
%
xmm3
%
xmm2
.
byte
68
15
40
225
/
/
movaps
%
xmm1
%
xmm12
.
byte
69
15
93
225
/
/
minps
%
xmm9
%
xmm12
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
65
15
93
220
/
/
minps
%
xmm12
%
xmm3
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
41
100
36
168
/
/
movaps
%
xmm12
-
0x58
(
%
rsp
)
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
15
95
248
/
/
maxps
%
xmm0
%
xmm7
.
byte
15
92
251
/
/
subps
%
xmm3
%
xmm7
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
68
15
94
215
/
/
divps
%
xmm7
%
xmm10
.
byte
68
15
92
219
/
/
subps
%
xmm3
%
xmm11
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
68
15
94
223
/
/
divps
%
xmm7
%
xmm11
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
68
15
89
194
/
/
mulps
%
xmm2
%
xmm8
.
byte
68
15
94
199
/
/
divps
%
xmm7
%
xmm8
.
byte
15
194
248
4
/
/
cmpneqps
%
xmm0
%
xmm7
.
byte
68
15
84
215
/
/
andps
%
xmm7
%
xmm10
.
byte
68
15
84
223
/
/
andps
%
xmm7
%
xmm11
.
byte
68
15
84
199
/
/
andps
%
xmm7
%
xmm8
.
byte
15
40
21
198
241
1
0
/
/
movaps
0x1f1c6
(
%
rip
)
%
xmm2
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
40
13
201
241
1
0
/
/
movaps
0x1f1c9
(
%
rip
)
%
xmm1
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
68
15
40
45
200
241
1
0
/
/
movaps
0x1f1c8
(
%
rip
)
%
xmm13
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
68
15
88
247
/
/
addps
%
xmm7
%
xmm14
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
65
15
40
251
/
/
movaps
%
xmm11
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
68
15
89
244
/
/
mulps
%
xmm4
%
xmm14
.
byte
68
15
92
243
/
/
subps
%
xmm3
%
xmm14
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
69
15
88
222
/
/
addps
%
xmm14
%
xmm11
.
byte
69
15
88
240
/
/
addps
%
xmm8
%
xmm14
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
93
198
/
/
minps
%
xmm14
%
xmm0
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
68
15
88
233
/
/
addps
%
xmm1
%
xmm13
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
203
2
/
/
cmpleps
%
xmm3
%
xmm9
.
byte
65
15
40
253
/
/
movaps
%
xmm13
%
xmm7
.
byte
15
92
251
/
/
subps
%
xmm3
%
xmm7
.
byte
69
15
40
250
/
/
movaps
%
xmm10
%
xmm15
.
byte
69
15
92
253
/
/
subps
%
xmm13
%
xmm15
.
byte
69
15
89
253
/
/
mulps
%
xmm13
%
xmm15
.
byte
68
15
94
255
/
/
divps
%
xmm7
%
xmm15
.
byte
69
15
88
253
/
/
addps
%
xmm13
%
xmm15
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
250
/
/
blendvps
%
xmm0
%
xmm10
%
xmm15
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
95
198
/
/
maxps
%
xmm14
%
xmm0
.
byte
68
15
95
208
/
/
maxps
%
xmm0
%
xmm10
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
69
15
194
194
1
/
/
cmpltps
%
xmm10
%
xmm8
.
byte
65
15
40
223
/
/
movaps
%
xmm15
%
xmm3
.
byte
65
15
92
221
/
/
subps
%
xmm13
%
xmm3
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
65
15
92
205
/
/
subps
%
xmm13
%
xmm1
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
69
15
92
213
/
/
subps
%
xmm13
%
xmm10
.
byte
65
15
94
218
/
/
divps
%
xmm10
%
xmm3
.
byte
65
15
88
221
/
/
addps
%
xmm13
%
xmm3
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
102
68
15
56
20
251
/
/
blendvps
%
xmm0
%
xmm3
%
xmm15
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
68
15
94
231
/
/
divps
%
xmm7
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
227
/
/
blendvps
%
xmm0
%
xmm11
%
xmm12
.
byte
65
15
40
220
/
/
movaps
%
xmm12
%
xmm3
.
byte
65
15
92
221
/
/
subps
%
xmm13
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
65
15
94
218
/
/
divps
%
xmm10
%
xmm3
.
byte
65
15
88
221
/
/
addps
%
xmm13
%
xmm3
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
102
68
15
56
20
227
/
/
blendvps
%
xmm0
%
xmm3
%
xmm12
.
byte
69
15
40
222
/
/
movaps
%
xmm14
%
xmm11
.
byte
69
15
92
221
/
/
subps
%
xmm13
%
xmm11
.
byte
69
15
89
221
/
/
mulps
%
xmm13
%
xmm11
.
byte
68
15
94
223
/
/
divps
%
xmm7
%
xmm11
.
byte
69
15
88
221
/
/
addps
%
xmm13
%
xmm11
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
222
/
/
blendvps
%
xmm0
%
xmm14
%
xmm11
.
byte
65
15
40
251
/
/
movaps
%
xmm11
%
xmm7
.
byte
65
15
92
253
/
/
subps
%
xmm13
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
65
15
94
250
/
/
divps
%
xmm10
%
xmm7
.
byte
65
15
88
253
/
/
addps
%
xmm13
%
xmm7
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
102
68
15
56
20
223
/
/
blendvps
%
xmm0
%
xmm7
%
xmm11
.
byte
68
15
40
13
222
239
1
0
/
/
movaps
0x1efde
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
92
204
/
/
subps
%
xmm4
%
xmm9
.
byte
15
40
124
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm7
.
byte
15
88
231
/
/
addps
%
xmm7
%
xmm4
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
68
15
95
248
/
/
maxps
%
xmm0
%
xmm15
.
byte
68
15
95
224
/
/
maxps
%
xmm0
%
xmm12
.
byte
68
15
95
216
/
/
maxps
%
xmm0
%
xmm11
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
15
40
68
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
40
84
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
68
15
89
68
36
232
/
/
mulps
-
0x18
(
%
rsp
)
%
xmm8
.
byte
15
40
84
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm2
.
byte
68
15
89
202
/
/
mulps
%
xmm2
%
xmm9
.
byte
69
15
88
200
/
/
addps
%
xmm8
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_color_sse41
.
globl
_sk_color_sse41
FUNCTION
(
_sk_color_sse41
)
_sk_color_sse41
:
.
byte
68
15
40
230
/
/
movaps
%
xmm6
%
xmm12
.
byte
68
15
41
100
36
200
/
/
movaps
%
xmm12
-
0x38
(
%
rsp
)
.
byte
68
15
40
221
/
/
movaps
%
xmm5
%
xmm11
.
byte
68
15
41
92
36
216
/
/
movaps
%
xmm11
-
0x28
(
%
rsp
)
.
byte
68
15
40
212
/
/
movaps
%
xmm4
%
xmm10
.
byte
68
15
41
84
36
232
/
/
movaps
%
xmm10
-
0x18
(
%
rsp
)
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
41
84
36
184
/
/
movaps
%
xmm2
-
0x48
(
%
rsp
)
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
69
15
40
240
/
/
movaps
%
xmm8
%
xmm14
.
byte
15
40
231
/
/
movaps
%
xmm7
%
xmm4
.
byte
68
15
89
244
/
/
mulps
%
xmm4
%
xmm14
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
68
15
40
13
153
239
1
0
/
/
movaps
0x1ef99
(
%
rip
)
%
xmm9
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
65
15
89
249
/
/
mulps
%
xmm9
%
xmm7
.
byte
68
15
40
21
153
239
1
0
/
/
movaps
0x1ef99
(
%
rip
)
%
xmm10
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
65
15
40
219
/
/
movaps
%
xmm11
%
xmm3
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
68
15
40
29
150
239
1
0
/
/
movaps
0x1ef96
(
%
rip
)
%
xmm11
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
69
15
40
236
/
/
movaps
%
xmm12
%
xmm13
.
byte
69
15
89
235
/
/
mulps
%
xmm11
%
xmm13
.
byte
68
15
88
235
/
/
addps
%
xmm3
%
xmm13
.
byte
65
15
40
222
/
/
movaps
%
xmm14
%
xmm3
.
byte
65
15
89
217
/
/
mulps
%
xmm9
%
xmm3
.
byte
15
40
249
/
/
movaps
%
xmm1
%
xmm7
.
byte
65
15
89
250
/
/
mulps
%
xmm10
%
xmm7
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
65
15
89
219
/
/
mulps
%
xmm11
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
68
15
89
238
/
/
mulps
%
xmm6
%
xmm13
.
byte
68
15
92
235
/
/
subps
%
xmm3
%
xmm13
.
byte
69
15
88
245
/
/
addps
%
xmm13
%
xmm14
.
byte
65
15
88
205
/
/
addps
%
xmm13
%
xmm1
.
byte
68
15
88
232
/
/
addps
%
xmm0
%
xmm13
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
93
197
/
/
minps
%
xmm13
%
xmm0
.
byte
65
15
40
222
/
/
movaps
%
xmm14
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
69
15
89
206
/
/
mulps
%
xmm14
%
xmm9
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
69
15
88
209
/
/
addps
%
xmm9
%
xmm10
.
byte
69
15
89
221
/
/
mulps
%
xmm13
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
203
2
/
/
cmpleps
%
xmm3
%
xmm9
.
byte
69
15
40
230
/
/
movaps
%
xmm14
%
xmm12
.
byte
69
15
92
227
/
/
subps
%
xmm11
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
65
15
40
251
/
/
movaps
%
xmm11
%
xmm7
.
byte
15
92
251
/
/
subps
%
xmm3
%
xmm7
.
byte
68
15
94
231
/
/
divps
%
xmm7
%
xmm12
.
byte
69
15
88
227
/
/
addps
%
xmm11
%
xmm12
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
230
/
/
blendvps
%
xmm0
%
xmm14
%
xmm12
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
95
197
/
/
maxps
%
xmm13
%
xmm0
.
byte
65
15
40
214
/
/
movaps
%
xmm14
%
xmm2
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
194
210
1
/
/
cmpltps
%
xmm2
%
xmm10
.
byte
69
15
40
244
/
/
movaps
%
xmm12
%
xmm14
.
byte
69
15
92
243
/
/
subps
%
xmm11
%
xmm14
.
byte
68
15
40
251
/
/
movaps
%
xmm3
%
xmm15
.
byte
69
15
92
251
/
/
subps
%
xmm11
%
xmm15
.
byte
69
15
89
247
/
/
mulps
%
xmm15
%
xmm14
.
byte
65
15
92
211
/
/
subps
%
xmm11
%
xmm2
.
byte
68
15
94
242
/
/
divps
%
xmm2
%
xmm14
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
69
15
56
20
230
/
/
blendvps
%
xmm0
%
xmm14
%
xmm12
.
byte
68
15
40
241
/
/
movaps
%
xmm1
%
xmm14
.
byte
69
15
92
243
/
/
subps
%
xmm11
%
xmm14
.
byte
69
15
89
243
/
/
mulps
%
xmm11
%
xmm14
.
byte
68
15
94
247
/
/
divps
%
xmm7
%
xmm14
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
68
15
56
20
241
/
/
blendvps
%
xmm0
%
xmm1
%
xmm14
.
byte
65
15
40
206
/
/
movaps
%
xmm14
%
xmm1
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
65
15
89
207
/
/
mulps
%
xmm15
%
xmm1
.
byte
15
94
202
/
/
divps
%
xmm2
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
68
15
56
20
241
/
/
blendvps
%
xmm0
%
xmm1
%
xmm14
.
byte
65
15
40
205
/
/
movaps
%
xmm13
%
xmm1
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
15
94
207
/
/
divps
%
xmm7
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
65
15
56
20
205
/
/
blendvps
%
xmm0
%
xmm13
%
xmm1
.
byte
15
40
249
/
/
movaps
%
xmm1
%
xmm7
.
byte
65
15
92
251
/
/
subps
%
xmm11
%
xmm7
.
byte
65
15
89
255
/
/
mulps
%
xmm15
%
xmm7
.
byte
15
94
250
/
/
divps
%
xmm2
%
xmm7
.
byte
65
15
88
251
/
/
addps
%
xmm11
%
xmm7
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
15
56
20
207
/
/
blendvps
%
xmm0
%
xmm7
%
xmm1
.
byte
68
15
40
13
162
237
1
0
/
/
movaps
0x1eda2
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
89
68
36
184
/
/
mulps
-
0x48
(
%
rsp
)
%
xmm0
.
byte
68
15
92
206
/
/
subps
%
xmm6
%
xmm9
.
byte
15
88
244
/
/
addps
%
xmm4
%
xmm6
.
byte
15
40
252
/
/
movaps
%
xmm4
%
xmm7
.
byte
15
92
243
/
/
subps
%
xmm3
%
xmm6
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
68
15
95
242
/
/
maxps
%
xmm2
%
xmm14
.
byte
15
95
202
/
/
maxps
%
xmm2
%
xmm1
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
15
40
100
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm4
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
68
15
88
194
/
/
addps
%
xmm2
%
xmm8
.
byte
69
15
88
196
/
/
addps
%
xmm12
%
xmm8
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
15
40
116
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm6
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
15
40
84
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm2
.
byte
68
15
89
202
/
/
mulps
%
xmm2
%
xmm9
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminosity_sse41
.
globl
_sk_luminosity_sse41
FUNCTION
(
_sk_luminosity_sse41
)
_sk_luminosity_sse41
:
.
byte
15
41
116
36
200
/
/
movaps
%
xmm6
-
0x38
(
%
rsp
)
.
byte
15
41
108
36
232
/
/
movaps
%
xmm5
-
0x18
(
%
rsp
)
.
byte
68
15
40
196
/
/
movaps
%
xmm4
%
xmm8
.
byte
68
15
41
68
36
216
/
/
movaps
%
xmm8
-
0x28
(
%
rsp
)
.
byte
15
41
84
36
184
/
/
movaps
%
xmm2
-
0x48
(
%
rsp
)
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
68
15
40
5
106
237
1
0
/
/
movaps
0x1ed6a
(
%
rip
)
%
xmm8
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
68
15
40
21
110
237
1
0
/
/
movaps
0x1ed6e
(
%
rip
)
%
xmm10
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
65
15
89
234
/
/
mulps
%
xmm10
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
68
15
40
37
108
237
1
0
/
/
movaps
0x1ed6c
(
%
rip
)
%
xmm12
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
68
15
40
242
/
/
movaps
%
xmm2
%
xmm14
.
byte
69
15
89
244
/
/
mulps
%
xmm12
%
xmm14
.
byte
68
15
88
245
/
/
addps
%
xmm5
%
xmm14
.
byte
65
15
40
235
/
/
movaps
%
xmm11
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
65
15
89
236
/
/
mulps
%
xmm12
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
68
15
89
247
/
/
mulps
%
xmm7
%
xmm14
.
byte
68
15
92
245
/
/
subps
%
xmm5
%
xmm14
.
byte
69
15
88
222
/
/
addps
%
xmm14
%
xmm11
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
68
15
88
240
/
/
addps
%
xmm0
%
xmm14
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
93
198
/
/
minps
%
xmm14
%
xmm0
.
byte
65
15
40
235
/
/
movaps
%
xmm11
%
xmm5
.
byte
15
93
232
/
/
minps
%
xmm0
%
xmm5
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
208
/
/
addps
%
xmm8
%
xmm10
.
byte
69
15
89
230
/
/
mulps
%
xmm14
%
xmm12
.
byte
69
15
88
226
/
/
addps
%
xmm10
%
xmm12
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
68
15
194
213
2
/
/
cmpleps
%
xmm5
%
xmm10
.
byte
69
15
40
235
/
/
movaps
%
xmm11
%
xmm13
.
byte
69
15
92
236
/
/
subps
%
xmm12
%
xmm13
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
65
15
40
244
/
/
movaps
%
xmm12
%
xmm6
.
byte
15
92
245
/
/
subps
%
xmm5
%
xmm6
.
byte
68
15
94
238
/
/
divps
%
xmm6
%
xmm13
.
byte
69
15
88
236
/
/
addps
%
xmm12
%
xmm13
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
69
15
56
20
235
/
/
blendvps
%
xmm0
%
xmm11
%
xmm13
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
95
198
/
/
maxps
%
xmm14
%
xmm0
.
byte
65
15
40
211
/
/
movaps
%
xmm11
%
xmm2
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
68
15
40
221
/
/
movaps
%
xmm5
%
xmm11
.
byte
68
15
194
218
1
/
/
cmpltps
%
xmm2
%
xmm11
.
byte
69
15
40
197
/
/
movaps
%
xmm13
%
xmm8
.
byte
69
15
92
196
/
/
subps
%
xmm12
%
xmm8
.
byte
68
15
40
253
/
/
movaps
%
xmm5
%
xmm15
.
byte
69
15
92
252
/
/
subps
%
xmm12
%
xmm15
.
byte
69
15
89
199
/
/
mulps
%
xmm15
%
xmm8
.
byte
65
15
92
212
/
/
subps
%
xmm12
%
xmm2
.
byte
68
15
94
194
/
/
divps
%
xmm2
%
xmm8
.
byte
69
15
88
196
/
/
addps
%
xmm12
%
xmm8
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
102
69
15
56
20
232
/
/
blendvps
%
xmm0
%
xmm8
%
xmm13
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
69
15
92
196
/
/
subps
%
xmm12
%
xmm8
.
byte
69
15
89
196
/
/
mulps
%
xmm12
%
xmm8
.
byte
68
15
94
198
/
/
divps
%
xmm6
%
xmm8
.
byte
69
15
88
196
/
/
addps
%
xmm12
%
xmm8
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
69
15
56
20
193
/
/
blendvps
%
xmm0
%
xmm9
%
xmm8
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
69
15
92
204
/
/
subps
%
xmm12
%
xmm9
.
byte
69
15
89
207
/
/
mulps
%
xmm15
%
xmm9
.
byte
68
15
94
202
/
/
divps
%
xmm2
%
xmm9
.
byte
69
15
88
204
/
/
addps
%
xmm12
%
xmm9
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
102
69
15
56
20
193
/
/
blendvps
%
xmm0
%
xmm9
%
xmm8
.
byte
69
15
40
206
/
/
movaps
%
xmm14
%
xmm9
.
byte
69
15
92
204
/
/
subps
%
xmm12
%
xmm9
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
68
15
94
206
/
/
divps
%
xmm6
%
xmm9
.
byte
69
15
88
204
/
/
addps
%
xmm12
%
xmm9
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
69
15
56
20
206
/
/
blendvps
%
xmm0
%
xmm14
%
xmm9
.
byte
65
15
40
241
/
/
movaps
%
xmm9
%
xmm6
.
byte
65
15
92
244
/
/
subps
%
xmm12
%
xmm6
.
byte
65
15
89
247
/
/
mulps
%
xmm15
%
xmm6
.
byte
15
94
242
/
/
divps
%
xmm2
%
xmm6
.
byte
65
15
88
244
/
/
addps
%
xmm12
%
xmm6
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
102
68
15
56
20
206
/
/
blendvps
%
xmm0
%
xmm6
%
xmm9
.
byte
15
40
5
114
235
1
0
/
/
movaps
0x1eb72
(
%
rip
)
%
xmm0
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
92
215
/
/
subps
%
xmm7
%
xmm2
.
byte
15
89
226
/
/
mulps
%
xmm2
%
xmm4
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
89
84
36
184
/
/
mulps
-
0x48
(
%
rsp
)
%
xmm2
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
68
15
95
237
/
/
maxps
%
xmm5
%
xmm13
.
byte
68
15
95
197
/
/
maxps
%
xmm5
%
xmm8
.
byte
68
15
95
205
/
/
maxps
%
xmm5
%
xmm9
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
68
15
40
84
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm10
.
byte
65
15
89
234
/
/
mulps
%
xmm10
%
xmm5
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
65
15
88
229
/
/
addps
%
xmm13
%
xmm4
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
40
108
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm5
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
15
40
116
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm6
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
65
15
40
226
/
/
movaps
%
xmm10
%
xmm4
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_sse41
.
globl
_sk_srcover_rgba_8888_sse41
FUNCTION
(
_sk_srcover_rgba_8888_sse41
)
_sk_srcover_rgba_8888_sse41
:
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
0
1
0
0
/
/
jne
1e52a
<
_sk_srcover_rgba_8888_sse41
+
0x11f
>
.
byte
243
65
15
111
4
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
102
15
111
37
133
235
1
0
/
/
movdqa
0x1eb85
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
224
/
/
pand
%
xmm0
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
56
0
45
129
235
1
0
/
/
pshufb
0x1eb81
(
%
rip
)
%
xmm5
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
56
0
53
129
235
1
0
/
/
pshufb
0x1eb81
(
%
rip
)
%
xmm6
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
102
15
114
208
24
/
/
psrld
0x18
%
xmm0
.
byte
15
91
248
/
/
cvtdq2ps
%
xmm0
%
xmm7
.
byte
68
15
40
5
154
234
1
0
/
/
movaps
0x1ea9a
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
68
15
40
37
110
235
1
0
/
/
movaps
0x1eb6e
(
%
rip
)
%
xmm12
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
65
15
89
204
/
/
mulps
%
xmm12
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
65
15
89
220
/
/
mulps
%
xmm12
%
xmm3
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
88
195
/
/
addps
%
xmm3
%
xmm8
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
95
200
/
/
maxps
%
xmm0
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
201
/
/
cvtps2dq
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
65
15
95
209
/
/
maxps
%
xmm9
%
xmm2
.
byte
65
15
93
212
/
/
minps
%
xmm12
%
xmm2
.
byte
102
15
91
210
/
/
cvtps2dq
%
xmm2
%
xmm2
.
byte
102
15
114
242
8
/
/
pslld
0x8
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
217
/
/
cvtps2dq
%
xmm1
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
69
15
95
216
/
/
maxps
%
xmm8
%
xmm11
.
byte
69
15
93
220
/
/
minps
%
xmm12
%
xmm11
.
byte
102
65
15
91
203
/
/
cvtps2dq
%
xmm11
%
xmm1
.
byte
102
15
114
241
24
/
/
pslld
0x18
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
117
75
/
/
jne
1e55f
<
_sk_srcover_rgba_8888_sse41
+
0x154
>
.
byte
243
65
15
127
12
144
/
/
movdqu
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
78
/
/
je
1e580
<
_sk_srcover_rgba_8888_sse41
+
0x175
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
20
/
/
je
1e54e
<
_sk_srcover_rgba_8888_sse41
+
0x143
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
238
254
255
255
/
/
jne
1e430
<
_sk_srcover_rgba_8888_sse41
+
0x25
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
243
65
15
126
36
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
58
14
196
15
/
/
pblendw
0xf
%
xmm4
%
xmm0
.
byte
233
209
254
255
255
/
/
jmpq
1e430
<
_sk_srcover_rgba_8888_sse41
+
0x25
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
36
/
/
je
1e58b
<
_sk_srcover_rgba_8888_sse41
+
0x180
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
1e578
<
_sk_srcover_rgba_8888_sse41
+
0x16d
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
171
/
/
jne
1e51a
<
_sk_srcover_rgba_8888_sse41
+
0x10f
>
.
byte
102
65
15
58
22
76
144
8
2
/
/
pextrd
0x2
%
xmm1
0x8
(
%
r8
%
rdx
4
)
.
byte
102
65
15
214
12
144
/
/
movq
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
154
/
/
jmp
1e51a
<
_sk_srcover_rgba_8888_sse41
+
0x10f
>
.
byte
102
65
15
110
4
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
233
165
254
255
255
/
/
jmpq
1e430
<
_sk_srcover_rgba_8888_sse41
+
0x25
>
.
byte
102
65
15
126
12
144
/
/
movd
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
135
/
/
jmp
1e51a
<
_sk_srcover_rgba_8888_sse41
+
0x10f
>
HIDDEN
_sk_srcover_bgra_8888_sse41
.
globl
_sk_srcover_bgra_8888_sse41
FUNCTION
(
_sk_srcover_bgra_8888_sse41
)
_sk_srcover_bgra_8888_sse41
:
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
0
1
0
0
/
/
jne
1e6b2
<
_sk_srcover_bgra_8888_sse41
+
0x11f
>
.
byte
243
65
15
111
4
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
102
15
111
37
253
233
1
0
/
/
movdqa
0x1e9fd
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
224
/
/
pand
%
xmm0
%
xmm4
.
byte
15
91
244
/
/
cvtdq2ps
%
xmm4
%
xmm6
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
56
0
37
249
233
1
0
/
/
pshufb
0x1e9f9
(
%
rip
)
%
xmm4
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
236
/
/
cvtdq2ps
%
xmm4
%
xmm5
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
56
0
37
249
233
1
0
/
/
pshufb
0x1e9f9
(
%
rip
)
%
xmm4
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
102
15
114
208
24
/
/
psrld
0x18
%
xmm0
.
byte
15
91
248
/
/
cvtdq2ps
%
xmm0
%
xmm7
.
byte
68
15
40
5
18
233
1
0
/
/
movaps
0x1e912
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
68
15
40
37
230
233
1
0
/
/
movaps
0x1e9e6
(
%
rip
)
%
xmm12
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
65
15
89
204
/
/
mulps
%
xmm12
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
65
15
89
220
/
/
mulps
%
xmm12
%
xmm3
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
88
195
/
/
addps
%
xmm3
%
xmm8
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
201
/
/
cvtps2dq
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
65
15
95
209
/
/
maxps
%
xmm9
%
xmm2
.
byte
65
15
93
212
/
/
minps
%
xmm12
%
xmm2
.
byte
102
15
91
210
/
/
cvtps2dq
%
xmm2
%
xmm2
.
byte
102
15
114
242
8
/
/
pslld
0x8
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
15
95
200
/
/
maxps
%
xmm0
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
217
/
/
cvtps2dq
%
xmm1
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
69
15
95
216
/
/
maxps
%
xmm8
%
xmm11
.
byte
69
15
93
220
/
/
minps
%
xmm12
%
xmm11
.
byte
102
65
15
91
203
/
/
cvtps2dq
%
xmm11
%
xmm1
.
byte
102
15
114
241
24
/
/
pslld
0x18
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
117
75
/
/
jne
1e6e7
<
_sk_srcover_bgra_8888_sse41
+
0x154
>
.
byte
243
65
15
127
12
144
/
/
movdqu
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
78
/
/
je
1e708
<
_sk_srcover_bgra_8888_sse41
+
0x175
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
20
/
/
je
1e6d6
<
_sk_srcover_bgra_8888_sse41
+
0x143
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
238
254
255
255
/
/
jne
1e5b8
<
_sk_srcover_bgra_8888_sse41
+
0x25
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
243
65
15
126
36
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
58
14
196
15
/
/
pblendw
0xf
%
xmm4
%
xmm0
.
byte
233
209
254
255
255
/
/
jmpq
1e5b8
<
_sk_srcover_bgra_8888_sse41
+
0x25
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
36
/
/
je
1e713
<
_sk_srcover_bgra_8888_sse41
+
0x180
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
1e700
<
_sk_srcover_bgra_8888_sse41
+
0x16d
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
171
/
/
jne
1e6a2
<
_sk_srcover_bgra_8888_sse41
+
0x10f
>
.
byte
102
65
15
58
22
76
144
8
2
/
/
pextrd
0x2
%
xmm1
0x8
(
%
r8
%
rdx
4
)
.
byte
102
65
15
214
12
144
/
/
movq
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
154
/
/
jmp
1e6a2
<
_sk_srcover_bgra_8888_sse41
+
0x10f
>
.
byte
102
65
15
110
4
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
233
165
254
255
255
/
/
jmpq
1e5b8
<
_sk_srcover_bgra_8888_sse41
+
0x25
>
.
byte
102
65
15
126
12
144
/
/
movd
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
135
/
/
jmp
1e6a2
<
_sk_srcover_bgra_8888_sse41
+
0x10f
>
HIDDEN
_sk_clamp_0_sse41
.
globl
_sk_clamp_0_sse41
FUNCTION
(
_sk_clamp_0_sse41
)
_sk_clamp_0_sse41
:
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
95
208
/
/
maxps
%
xmm8
%
xmm2
.
byte
65
15
95
216
/
/
maxps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_1_sse41
.
globl
_sk_clamp_1_sse41
FUNCTION
(
_sk_clamp_1_sse41
)
_sk_clamp_1_sse41
:
.
byte
68
15
40
5
213
231
1
0
/
/
movaps
0x1e7d5
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_sse41
.
globl
_sk_clamp_a_sse41
FUNCTION
(
_sk_clamp_a_sse41
)
_sk_clamp_a_sse41
:
.
byte
15
93
29
186
231
1
0
/
/
minps
0x1e7ba
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
93
195
/
/
minps
%
xmm3
%
xmm0
.
byte
15
93
203
/
/
minps
%
xmm3
%
xmm1
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_sse41
.
globl
_sk_clamp_a_dst_sse41
FUNCTION
(
_sk_clamp_a_dst_sse41
)
_sk_clamp_a_dst_sse41
:
.
byte
15
93
61
166
231
1
0
/
/
minps
0x1e7a6
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
93
231
/
/
minps
%
xmm7
%
xmm4
.
byte
15
93
239
/
/
minps
%
xmm7
%
xmm5
.
byte
15
93
247
/
/
minps
%
xmm7
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_sse41
.
globl
_sk_set_rgb_sse41
FUNCTION
(
_sk_set_rgb_sse41
)
_sk_set_rgb_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_sse41
.
globl
_sk_swap_rb_sse41
FUNCTION
(
_sk_swap_rb_sse41
)
_sk_swap_rb_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_sse41
.
globl
_sk_invert_sse41
FUNCTION
(
_sk_invert_sse41
)
_sk_invert_sse41
:
.
byte
68
15
40
5
98
231
1
0
/
/
movaps
0x1e762
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
92
209
/
/
subps
%
xmm1
%
xmm10
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
92
218
/
/
subps
%
xmm2
%
xmm11
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
40
211
/
/
movaps
%
xmm11
%
xmm2
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_sse41
.
globl
_sk_move_src_dst_sse41
FUNCTION
(
_sk_move_src_dst_sse41
)
_sk_move_src_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_sse41
.
globl
_sk_move_dst_src_sse41
FUNCTION
(
_sk_move_dst_src_sse41
)
_sk_move_dst_src_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_sse41
.
globl
_sk_premul_sse41
FUNCTION
(
_sk_premul_sse41
)
_sk_premul_sse41
:
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_sse41
.
globl
_sk_premul_dst_sse41
FUNCTION
(
_sk_premul_dst_sse41
)
_sk_premul_dst_sse41
:
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_unpremul_sse41
.
globl
_sk_unpremul_sse41
FUNCTION
(
_sk_unpremul_sse41
)
_sk_unpremul_sse41
:
.
byte
68
15
40
5
240
230
1
0
/
/
movaps
0x1e6f0
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
94
195
/
/
divps
%
xmm3
%
xmm8
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
194
13
207
231
1
0
1
/
/
cmpltps
0x1e7cf
(
%
rip
)
%
xmm9
#
3d000
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdb4
>
.
byte
69
15
84
200
/
/
andps
%
xmm8
%
xmm9
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_sse41
.
globl
_sk_force_opaque_sse41
FUNCTION
(
_sk_force_opaque_sse41
)
_sk_force_opaque_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
194
230
1
0
/
/
movaps
0x1e6c2
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_sse41
.
globl
_sk_force_opaque_dst_sse41
FUNCTION
(
_sk_force_opaque_dst_sse41
)
_sk_force_opaque_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
183
230
1
0
/
/
movaps
0x1e6b7
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_sse41
.
globl
_sk_from_srgb_sse41
FUNCTION
(
_sk_from_srgb_sse41
)
_sk_from_srgb_sse41
:
.
byte
68
15
40
29
173
231
1
0
/
/
movaps
0x1e7ad
(
%
rip
)
%
xmm11
#
3d010
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdc4
>
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
69
15
89
210
/
/
mulps
%
xmm10
%
xmm10
.
byte
68
15
40
37
21
231
1
0
/
/
movaps
0x1e715
(
%
rip
)
%
xmm12
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
69
15
89
196
/
/
mulps
%
xmm12
%
xmm8
.
byte
68
15
40
45
149
231
1
0
/
/
movaps
0x1e795
(
%
rip
)
%
xmm13
#
3d020
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdd4
>
.
byte
69
15
88
197
/
/
addps
%
xmm13
%
xmm8
.
byte
69
15
89
194
/
/
mulps
%
xmm10
%
xmm8
.
byte
68
15
40
53
149
231
1
0
/
/
movaps
0x1e795
(
%
rip
)
%
xmm14
#
3d030
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xde4
>
.
byte
69
15
88
198
/
/
addps
%
xmm14
%
xmm8
.
byte
68
15
40
61
153
231
1
0
/
/
movaps
0x1e799
(
%
rip
)
%
xmm15
#
3d040
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdf4
>
.
byte
65
15
194
199
1
/
/
cmpltps
%
xmm15
%
xmm0
.
byte
102
69
15
56
20
193
/
/
blendvps
%
xmm0
%
xmm9
%
xmm8
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
69
15
89
211
/
/
mulps
%
xmm11
%
xmm10
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
69
15
88
205
/
/
addps
%
xmm13
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
65
15
194
207
1
/
/
cmpltps
%
xmm15
%
xmm1
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
102
69
15
56
20
202
/
/
blendvps
%
xmm0
%
xmm10
%
xmm9
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
68
15
40
210
/
/
movaps
%
xmm2
%
xmm10
.
byte
69
15
89
210
/
/
mulps
%
xmm10
%
xmm10
.
byte
68
15
89
226
/
/
mulps
%
xmm2
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
212
/
/
mulps
%
xmm12
%
xmm10
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
65
15
194
215
1
/
/
cmpltps
%
xmm15
%
xmm2
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
102
69
15
56
20
211
/
/
blendvps
%
xmm0
%
xmm11
%
xmm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_dst_sse41
.
globl
_sk_from_srgb_dst_sse41
FUNCTION
(
_sk_from_srgb_dst_sse41
)
_sk_from_srgb_dst_sse41
:
.
byte
68
15
40
204
/
/
movaps
%
xmm4
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
68
15
40
29
228
230
1
0
/
/
movaps
0x1e6e4
(
%
rip
)
%
xmm11
#
3d010
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdc4
>
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
69
15
89
211
/
/
mulps
%
xmm11
%
xmm10
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
37
77
230
1
0
/
/
movaps
0x1e64d
(
%
rip
)
%
xmm12
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
65
15
89
228
/
/
mulps
%
xmm12
%
xmm4
.
byte
68
15
40
45
209
230
1
0
/
/
movaps
0x1e6d1
(
%
rip
)
%
xmm13
#
3d020
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdd4
>
.
byte
65
15
88
229
/
/
addps
%
xmm13
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
68
15
40
53
210
230
1
0
/
/
movaps
0x1e6d2
(
%
rip
)
%
xmm14
#
3d030
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xde4
>
.
byte
65
15
88
230
/
/
addps
%
xmm14
%
xmm4
.
byte
68
15
40
61
214
230
1
0
/
/
movaps
0x1e6d6
(
%
rip
)
%
xmm15
#
3d040
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdf4
>
.
byte
69
15
194
207
1
/
/
cmpltps
%
xmm15
%
xmm9
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
65
15
56
20
226
/
/
blendvps
%
xmm0
%
xmm10
%
xmm4
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
69
15
89
211
/
/
mulps
%
xmm11
%
xmm10
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
205
/
/
movaps
%
xmm5
%
xmm9
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
69
15
88
205
/
/
addps
%
xmm13
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
65
15
194
239
1
/
/
cmpltps
%
xmm15
%
xmm5
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
102
69
15
56
20
202
/
/
blendvps
%
xmm0
%
xmm10
%
xmm9
.
byte
68
15
89
222
/
/
mulps
%
xmm6
%
xmm11
.
byte
68
15
40
214
/
/
movaps
%
xmm6
%
xmm10
.
byte
69
15
89
210
/
/
mulps
%
xmm10
%
xmm10
.
byte
68
15
89
230
/
/
mulps
%
xmm6
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
212
/
/
mulps
%
xmm12
%
xmm10
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
65
15
194
247
1
/
/
cmpltps
%
xmm15
%
xmm6
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
102
69
15
56
20
211
/
/
blendvps
%
xmm0
%
xmm11
%
xmm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
233
/
/
movaps
%
xmm9
%
xmm5
.
byte
65
15
40
242
/
/
movaps
%
xmm10
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_to_srgb_sse41
.
globl
_sk_to_srgb_sse41
FUNCTION
(
_sk_to_srgb_sse41
)
_sk_to_srgb_sse41
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
68
15
82
192
/
/
rsqrtps
%
xmm0
%
xmm8
.
byte
68
15
40
29
74
230
1
0
/
/
movaps
0x1e64a
(
%
rip
)
%
xmm11
#
3d050
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe04
>
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
68
15
40
37
74
230
1
0
/
/
movaps
0x1e64a
(
%
rip
)
%
xmm12
#
3d060
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe14
>
.
byte
69
15
40
248
/
/
movaps
%
xmm8
%
xmm15
.
byte
69
15
89
252
/
/
mulps
%
xmm12
%
xmm15
.
byte
68
15
40
21
74
230
1
0
/
/
movaps
0x1e64a
(
%
rip
)
%
xmm10
#
3d070
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe24
>
.
byte
69
15
88
250
/
/
addps
%
xmm10
%
xmm15
.
byte
69
15
89
248
/
/
mulps
%
xmm8
%
xmm15
.
byte
68
15
40
45
74
230
1
0
/
/
movaps
0x1e64a
(
%
rip
)
%
xmm13
#
3d080
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe34
>
.
byte
69
15
88
253
/
/
addps
%
xmm13
%
xmm15
.
byte
68
15
40
53
78
230
1
0
/
/
movaps
0x1e64e
(
%
rip
)
%
xmm14
#
3d090
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe44
>
.
byte
69
15
88
198
/
/
addps
%
xmm14
%
xmm8
.
byte
69
15
83
192
/
/
rcpps
%
xmm8
%
xmm8
.
byte
69
15
89
199
/
/
mulps
%
xmm15
%
xmm8
.
byte
68
15
40
61
74
230
1
0
/
/
movaps
0x1e64a
(
%
rip
)
%
xmm15
#
3d0a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe54
>
.
byte
65
15
194
199
1
/
/
cmpltps
%
xmm15
%
xmm0
.
byte
102
69
15
56
20
193
/
/
blendvps
%
xmm0
%
xmm9
%
xmm8
.
byte
68
15
82
202
/
/
rsqrtps
%
xmm2
%
xmm9
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
88
197
/
/
addps
%
xmm13
%
xmm0
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
69
15
83
201
/
/
rcpps
%
xmm9
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
65
15
194
215
1
/
/
cmpltps
%
xmm15
%
xmm2
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
102
68
15
56
20
201
/
/
blendvps
%
xmm0
%
xmm1
%
xmm9
.
byte
15
82
195
/
/
rsqrtps
%
xmm3
%
xmm0
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
69
15
88
226
/
/
addps
%
xmm10
%
xmm12
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
65
15
88
198
/
/
addps
%
xmm14
%
xmm0
.
byte
68
15
83
208
/
/
rcpps
%
xmm0
%
xmm10
.
byte
69
15
89
212
/
/
mulps
%
xmm12
%
xmm10
.
byte
68
15
89
219
/
/
mulps
%
xmm3
%
xmm11
.
byte
65
15
194
223
1
/
/
cmpltps
%
xmm15
%
xmm3
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
102
69
15
56
20
211
/
/
blendvps
%
xmm0
%
xmm11
%
xmm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_rgb_to_hsl_sse41
.
globl
_sk_rgb_to_hsl_sse41
FUNCTION
(
_sk_rgb_to_hsl_sse41
)
_sk_rgb_to_hsl_sse41
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
95
194
/
/
maxps
%
xmm2
%
xmm0
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
95
216
/
/
maxps
%
xmm0
%
xmm11
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
93
194
/
/
minps
%
xmm2
%
xmm0
.
byte
69
15
40
224
/
/
movaps
%
xmm8
%
xmm12
.
byte
68
15
93
224
/
/
minps
%
xmm0
%
xmm12
.
byte
65
15
40
203
/
/
movaps
%
xmm11
%
xmm1
.
byte
65
15
92
204
/
/
subps
%
xmm12
%
xmm1
.
byte
68
15
40
53
241
227
1
0
/
/
movaps
0x1e3f1
(
%
rip
)
%
xmm14
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
94
241
/
/
divps
%
xmm1
%
xmm14
.
byte
69
15
40
211
/
/
movaps
%
xmm11
%
xmm10
.
byte
69
15
194
208
0
/
/
cmpeqps
%
xmm8
%
xmm10
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
65
15
89
198
/
/
mulps
%
xmm14
%
xmm0
.
byte
69
15
40
249
/
/
movaps
%
xmm9
%
xmm15
.
byte
68
15
194
250
1
/
/
cmpltps
%
xmm2
%
xmm15
.
byte
68
15
84
61
104
229
1
0
/
/
andps
0x1e568
(
%
rip
)
%
xmm15
#
3d0b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe64
>
.
byte
68
15
88
248
/
/
addps
%
xmm0
%
xmm15
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
194
193
0
/
/
cmpeqps
%
xmm9
%
xmm0
.
byte
65
15
92
208
/
/
subps
%
xmm8
%
xmm2
.
byte
65
15
89
214
/
/
mulps
%
xmm14
%
xmm2
.
byte
68
15
40
45
91
229
1
0
/
/
movaps
0x1e55b
(
%
rip
)
%
xmm13
#
3d0c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe74
>
.
byte
65
15
88
213
/
/
addps
%
xmm13
%
xmm2
.
byte
69
15
92
193
/
/
subps
%
xmm9
%
xmm8
.
byte
69
15
89
198
/
/
mulps
%
xmm14
%
xmm8
.
byte
68
15
88
5
87
229
1
0
/
/
addps
0x1e557
(
%
rip
)
%
xmm8
#
3d0d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe84
>
.
byte
102
68
15
56
20
194
/
/
blendvps
%
xmm0
%
xmm2
%
xmm8
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
69
15
56
20
199
/
/
blendvps
%
xmm0
%
xmm15
%
xmm8
.
byte
68
15
89
5
79
229
1
0
/
/
mulps
0x1e54f
(
%
rip
)
%
xmm8
#
3d0e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe94
>
.
byte
69
15
40
203
/
/
movaps
%
xmm11
%
xmm9
.
byte
69
15
194
204
4
/
/
cmpneqps
%
xmm12
%
xmm9
.
byte
69
15
84
193
/
/
andps
%
xmm9
%
xmm8
.
byte
69
15
92
235
/
/
subps
%
xmm11
%
xmm13
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
15
40
5
83
227
1
0
/
/
movaps
0x1e353
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
65
15
40
211
/
/
movaps
%
xmm11
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
194
194
1
/
/
cmpltps
%
xmm2
%
xmm0
.
byte
69
15
92
236
/
/
subps
%
xmm12
%
xmm13
.
byte
102
69
15
56
20
221
/
/
blendvps
%
xmm0
%
xmm13
%
xmm11
.
byte
65
15
94
203
/
/
divps
%
xmm11
%
xmm1
.
byte
65
15
84
201
/
/
andps
%
xmm9
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hsl_to_rgb_sse41
.
globl
_sk_hsl_to_rgb_sse41
FUNCTION
(
_sk_hsl_to_rgb_sse41
)
_sk_hsl_to_rgb_sse41
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
15
41
108
36
200
/
/
movaps
%
xmm5
-
0x38
(
%
rsp
)
.
byte
15
41
100
36
184
/
/
movaps
%
xmm4
-
0x48
(
%
rsp
)
.
byte
15
41
92
36
168
/
/
movaps
%
xmm3
-
0x58
(
%
rsp
)
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
68
15
40
13
9
227
1
0
/
/
movaps
0x1e309
(
%
rip
)
%
xmm9
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
194
194
2
/
/
cmpleps
%
xmm2
%
xmm0
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
102
15
56
20
235
/
/
blendvps
%
xmm0
%
xmm3
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
15
41
84
36
152
/
/
movaps
%
xmm2
-
0x68
(
%
rsp
)
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
68
15
92
197
/
/
subps
%
xmm5
%
xmm8
.
byte
68
15
40
53
196
228
1
0
/
/
movaps
0x1e4c4
(
%
rip
)
%
xmm14
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
69
15
88
242
/
/
addps
%
xmm10
%
xmm14
.
byte
102
65
15
58
8
198
1
/
/
roundps
0x1
%
xmm14
%
xmm0
.
byte
68
15
92
240
/
/
subps
%
xmm0
%
xmm14
.
byte
68
15
40
29
189
228
1
0
/
/
movaps
0x1e4bd
(
%
rip
)
%
xmm11
#
3d100
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xeb4
>
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
194
198
2
/
/
cmpleps
%
xmm14
%
xmm0
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
65
15
92
240
/
/
subps
%
xmm8
%
xmm6
.
byte
15
40
61
86
228
1
0
/
/
movaps
0x1e456
(
%
rip
)
%
xmm7
#
3d0b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe64
>
.
byte
69
15
40
238
/
/
movaps
%
xmm14
%
xmm13
.
byte
68
15
89
239
/
/
mulps
%
xmm7
%
xmm13
.
byte
15
40
29
103
228
1
0
/
/
movaps
0x1e467
(
%
rip
)
%
xmm3
#
3d0d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe84
>
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
68
15
89
230
/
/
mulps
%
xmm6
%
xmm12
.
byte
69
15
88
224
/
/
addps
%
xmm8
%
xmm12
.
byte
102
69
15
56
20
224
/
/
blendvps
%
xmm0
%
xmm8
%
xmm12
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
194
198
2
/
/
cmpleps
%
xmm14
%
xmm0
.
byte
68
15
40
253
/
/
movaps
%
xmm5
%
xmm15
.
byte
102
69
15
56
20
252
/
/
blendvps
%
xmm0
%
xmm12
%
xmm15
.
byte
68
15
40
37
70
228
1
0
/
/
movaps
0x1e446
(
%
rip
)
%
xmm12
#
3d0e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe94
>
.
byte
65
15
40
196
/
/
movaps
%
xmm12
%
xmm0
.
byte
65
15
194
198
2
/
/
cmpleps
%
xmm14
%
xmm0
.
byte
68
15
89
238
/
/
mulps
%
xmm6
%
xmm13
.
byte
69
15
88
232
/
/
addps
%
xmm8
%
xmm13
.
byte
102
69
15
56
20
239
/
/
blendvps
%
xmm0
%
xmm15
%
xmm13
.
byte
69
15
87
246
/
/
xorps
%
xmm14
%
xmm14
.
byte
68
15
194
241
0
/
/
cmpeqps
%
xmm1
%
xmm14
.
byte
65
15
40
198
/
/
movaps
%
xmm14
%
xmm0
.
byte
102
68
15
56
20
234
/
/
blendvps
%
xmm0
%
xmm2
%
xmm13
.
byte
102
65
15
58
8
194
1
/
/
roundps
0x1
%
xmm10
%
xmm0
.
byte
69
15
40
250
/
/
movaps
%
xmm10
%
xmm15
.
byte
68
15
92
248
/
/
subps
%
xmm0
%
xmm15
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
194
199
2
/
/
cmpleps
%
xmm15
%
xmm0
.
byte
65
15
40
207
/
/
movaps
%
xmm15
%
xmm1
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
92
209
/
/
subps
%
xmm1
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
102
65
15
56
20
208
/
/
blendvps
%
xmm0
%
xmm8
%
xmm2
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
194
199
2
/
/
cmpleps
%
xmm15
%
xmm0
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
102
15
56
20
226
/
/
blendvps
%
xmm0
%
xmm2
%
xmm4
.
byte
65
15
40
196
/
/
movaps
%
xmm12
%
xmm0
.
byte
65
15
194
199
2
/
/
cmpleps
%
xmm15
%
xmm0
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
102
15
56
20
204
/
/
blendvps
%
xmm0
%
xmm4
%
xmm1
.
byte
65
15
40
198
/
/
movaps
%
xmm14
%
xmm0
.
byte
15
40
84
36
152
/
/
movaps
-
0x68
(
%
rsp
)
%
xmm2
.
byte
102
15
56
20
202
/
/
blendvps
%
xmm0
%
xmm2
%
xmm1
.
byte
68
15
88
21
222
227
1
0
/
/
addps
0x1e3de
(
%
rip
)
%
xmm10
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
102
65
15
58
8
194
1
/
/
roundps
0x1
%
xmm10
%
xmm0
.
byte
68
15
92
208
/
/
subps
%
xmm0
%
xmm10
.
byte
69
15
194
218
2
/
/
cmpleps
%
xmm10
%
xmm11
.
byte
65
15
89
250
/
/
mulps
%
xmm10
%
xmm7
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
102
65
15
56
20
216
/
/
blendvps
%
xmm0
%
xmm8
%
xmm3
.
byte
69
15
194
202
2
/
/
cmpleps
%
xmm10
%
xmm9
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
15
56
20
235
/
/
blendvps
%
xmm0
%
xmm3
%
xmm5
.
byte
69
15
194
226
2
/
/
cmpleps
%
xmm10
%
xmm12
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
68
15
88
199
/
/
addps
%
xmm7
%
xmm8
.
byte
65
15
40
196
/
/
movaps
%
xmm12
%
xmm0
.
byte
102
68
15
56
20
197
/
/
blendvps
%
xmm0
%
xmm5
%
xmm8
.
byte
65
15
40
198
/
/
movaps
%
xmm14
%
xmm0
.
byte
102
68
15
56
20
194
/
/
blendvps
%
xmm0
%
xmm2
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
197
/
/
movaps
%
xmm13
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
40
92
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_sse41
.
globl
_sk_scale_1_float_sse41
FUNCTION
(
_sk_scale_1_float_sse41
)
_sk_scale_1_float_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_sse41
.
globl
_sk_scale_u8_sse41
FUNCTION
(
_sk_scale_u8_sse41
)
_sk_scale_u8_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
52
/
/
jne
1ee13
<
_sk_scale_u8_sse41
+
0x46
>
.
byte
102
69
15
56
49
4
16
/
/
pmovzxbd
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
68
15
219
5
209
225
1
0
/
/
pand
0x1e1d1
(
%
rip
)
%
xmm8
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
37
227
1
0
/
/
mulps
0x1e325
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
68
15
89
195
/
/
mulps
%
xmm3
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
55
/
/
je
1ee52
<
_sk_scale_u8_sse41
+
0x85
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
21
/
/
je
1ee39
<
_sk_scale_u8_sse41
+
0x6c
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
190
/
/
jne
1ede6
<
_sk_scale_u8_sse41
+
0x19
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
69
/
/
pshufd
0x45
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
102
69
15
56
49
201
/
/
pmovzxbd
%
xmm9
%
xmm9
.
byte
102
69
15
58
14
193
15
/
/
pblendw
0xf
%
xmm9
%
xmm8
.
byte
235
148
/
/
jmp
1ede6
<
_sk_scale_u8_sse41
+
0x19
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
136
/
/
jmp
1ede6
<
_sk_scale_u8_sse41
+
0x19
>
HIDDEN
_sk_scale_565_sse41
.
globl
_sk_scale_565_sse41
FUNCTION
(
_sk_scale_565_sse41
)
_sk_scale_565_sse41
:
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
159
0
0
0
/
/
jne
1ef1b
<
_sk_scale_565_sse41
+
0xbd
>
.
byte
102
69
15
56
51
28
80
/
/
pmovzxwd
(
%
r8
%
rdx
2
)
%
xmm11
.
byte
102
15
111
5
165
226
1
0
/
/
movdqa
0x1e2a5
(
%
rip
)
%
xmm0
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
65
15
219
195
/
/
pand
%
xmm11
%
xmm0
.
byte
68
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm9
.
byte
68
15
89
13
164
226
1
0
/
/
mulps
0x1e2a4
(
%
rip
)
%
xmm9
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
5
172
226
1
0
/
/
movdqa
0x1e2ac
(
%
rip
)
%
xmm0
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
65
15
219
195
/
/
pand
%
xmm11
%
xmm0
.
byte
68
15
91
208
/
/
cvtdq2ps
%
xmm0
%
xmm10
.
byte
68
15
89
21
171
226
1
0
/
/
mulps
0x1e2ab
(
%
rip
)
%
xmm10
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
68
15
219
29
178
226
1
0
/
/
pand
0x1e2b2
(
%
rip
)
%
xmm11
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
69
15
91
219
/
/
cvtdq2ps
%
xmm11
%
xmm11
.
byte
68
15
89
29
182
226
1
0
/
/
mulps
0x1e2b6
(
%
rip
)
%
xmm11
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
194
199
1
/
/
cmpltps
%
xmm7
%
xmm0
.
byte
69
15
40
226
/
/
movaps
%
xmm10
%
xmm12
.
byte
69
15
93
227
/
/
minps
%
xmm11
%
xmm12
.
byte
69
15
40
233
/
/
movaps
%
xmm9
%
xmm13
.
byte
69
15
93
236
/
/
minps
%
xmm12
%
xmm13
.
byte
69
15
40
242
/
/
movaps
%
xmm10
%
xmm14
.
byte
69
15
95
243
/
/
maxps
%
xmm11
%
xmm14
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
69
15
95
230
/
/
maxps
%
xmm14
%
xmm12
.
byte
102
69
15
56
20
229
/
/
blendvps
%
xmm0
%
xmm13
%
xmm12
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
40
211
/
/
movaps
%
xmm11
%
xmm2
.
byte
65
15
40
220
/
/
movaps
%
xmm12
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
56
/
/
je
1ef5b
<
_sk_scale_565_sse41
+
0xfd
>
.
byte
102
69
15
239
219
/
/
pxor
%
xmm11
%
xmm11
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
24
/
/
je
1ef44
<
_sk_scale_565_sse41
+
0xe6
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
79
255
255
255
/
/
jne
1ee83
<
_sk_scale_565_sse41
+
0x25
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
68
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm11
.
byte
102
65
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
15
56
51
192
/
/
pmovzxwd
%
xmm0
%
xmm0
.
byte
102
68
15
58
14
216
15
/
/
pblendw
0xf
%
xmm0
%
xmm11
.
byte
233
40
255
255
255
/
/
jmpq
1ee83
<
_sk_scale_565_sse41
+
0x25
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
216
/
/
movd
%
eax
%
xmm11
.
byte
233
25
255
255
255
/
/
jmpq
1ee83
<
_sk_scale_565_sse41
+
0x25
>
HIDDEN
_sk_lerp_1_float_sse41
.
globl
_sk_lerp_1_float_sse41
FUNCTION
(
_sk_lerp_1_float_sse41
)
_sk_lerp_1_float_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_u8_sse41
.
globl
_sk_lerp_u8_sse41
FUNCTION
(
_sk_lerp_u8_sse41
)
_sk_lerp_u8_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
72
/
/
jne
1effc
<
_sk_lerp_u8_sse41
+
0x5a
>
.
byte
102
69
15
56
49
4
16
/
/
pmovzxbd
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
68
15
219
5
252
223
1
0
/
/
pand
0x1dffc
(
%
rip
)
%
xmm8
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
80
225
1
0
/
/
mulps
0x1e150
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
55
/
/
je
1f03b
<
_sk_lerp_u8_sse41
+
0x99
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
21
/
/
je
1f022
<
_sk_lerp_u8_sse41
+
0x80
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
170
/
/
jne
1efbb
<
_sk_lerp_u8_sse41
+
0x19
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
69
/
/
pshufd
0x45
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
102
69
15
56
49
201
/
/
pmovzxbd
%
xmm9
%
xmm9
.
byte
102
69
15
58
14
193
15
/
/
pblendw
0xf
%
xmm9
%
xmm8
.
byte
235
128
/
/
jmp
1efbb
<
_sk_lerp_u8_sse41
+
0x19
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
233
113
255
255
255
/
/
jmpq
1efbb
<
_sk_lerp_u8_sse41
+
0x19
>
HIDDEN
_sk_lerp_565_sse41
.
globl
_sk_lerp_565_sse41
FUNCTION
(
_sk_lerp_565_sse41
)
_sk_lerp_565_sse41
:
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
165
0
0
0
/
/
jne
1f10d
<
_sk_lerp_565_sse41
+
0xc3
>
.
byte
102
69
15
56
51
20
80
/
/
pmovzxwd
(
%
r8
%
rdx
2
)
%
xmm10
.
byte
102
15
111
5
185
224
1
0
/
/
movdqa
0x1e0b9
(
%
rip
)
%
xmm0
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
65
15
219
194
/
/
pand
%
xmm10
%
xmm0
.
byte
68
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm9
.
byte
68
15
89
13
184
224
1
0
/
/
mulps
0x1e0b8
(
%
rip
)
%
xmm9
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
5
192
224
1
0
/
/
movdqa
0x1e0c0
(
%
rip
)
%
xmm0
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
65
15
219
194
/
/
pand
%
xmm10
%
xmm0
.
byte
68
15
91
216
/
/
cvtdq2ps
%
xmm0
%
xmm11
.
byte
68
15
89
29
191
224
1
0
/
/
mulps
0x1e0bf
(
%
rip
)
%
xmm11
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
68
15
219
21
198
224
1
0
/
/
pand
0x1e0c6
(
%
rip
)
%
xmm10
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
69
15
91
210
/
/
cvtdq2ps
%
xmm10
%
xmm10
.
byte
68
15
89
21
202
224
1
0
/
/
mulps
0x1e0ca
(
%
rip
)
%
xmm10
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
194
199
1
/
/
cmpltps
%
xmm7
%
xmm0
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
69
15
93
218
/
/
minps
%
xmm10
%
xmm11
.
byte
69
15
40
233
/
/
movaps
%
xmm9
%
xmm13
.
byte
68
15
92
196
/
/
subps
%
xmm4
%
xmm8
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
69
15
93
203
/
/
minps
%
xmm11
%
xmm9
.
byte
69
15
95
226
/
/
maxps
%
xmm10
%
xmm12
.
byte
69
15
95
236
/
/
maxps
%
xmm12
%
xmm13
.
byte
102
69
15
56
20
233
/
/
blendvps
%
xmm0
%
xmm9
%
xmm13
.
byte
68
15
88
196
/
/
addps
%
xmm4
%
xmm8
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
56
/
/
je
1f14d
<
_sk_lerp_565_sse41
+
0x103
>
.
byte
102
69
15
239
210
/
/
pxor
%
xmm10
%
xmm10
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
24
/
/
je
1f136
<
_sk_lerp_565_sse41
+
0xec
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
73
255
255
255
/
/
jne
1f06f
<
_sk_lerp_565_sse41
+
0x25
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
68
15
112
208
69
/
/
pshufd
0x45
%
xmm0
%
xmm10
.
byte
102
65
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
15
56
51
192
/
/
pmovzxwd
%
xmm0
%
xmm0
.
byte
102
68
15
58
14
208
15
/
/
pblendw
0xf
%
xmm0
%
xmm10
.
byte
233
34
255
255
255
/
/
jmpq
1f06f
<
_sk_lerp_565_sse41
+
0x25
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
208
/
/
movd
%
eax
%
xmm10
.
byte
233
19
255
255
255
/
/
jmpq
1f06f
<
_sk_lerp_565_sse41
+
0x25
>
HIDDEN
_sk_load_tables_sse41
.
globl
_sk_load_tables_sse41
FUNCTION
(
_sk_load_tables_sse41
)
_sk_load_tables_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
20
1
0
0
/
/
jne
1f27e
<
_sk_load_tables_sse41
+
0x122
>
.
byte
243
69
15
111
4
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
15
111
5
69
222
1
0
/
/
movdqa
0x1de45
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
219
192
/
/
pand
%
xmm8
%
xmm0
.
byte
102
73
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
r8
.
byte
102
73
15
126
193
/
/
movq
%
xmm0
%
r9
.
byte
69
15
182
209
/
/
movzbl
%
r9b
%
r10d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
69
15
182
216
/
/
movzbl
%
r8b
%
r11d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
66
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm0
.
byte
102
66
15
58
33
4
11
16
/
/
insertps
0x10
(
%
rbx
%
r9
1
)
%
xmm0
.
byte
102
66
15
58
33
4
155
32
/
/
insertps
0x20
(
%
rbx
%
r11
4
)
%
xmm0
.
byte
102
66
15
58
33
4
3
48
/
/
insertps
0x30
(
%
rbx
%
r8
1
)
%
xmm0
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
15
56
0
13
0
222
1
0
/
/
pshufb
0x1de00
(
%
rip
)
%
xmm1
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
102
73
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
r8
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
68
15
182
203
/
/
movzbl
%
bl
%
r9d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
69
15
182
208
/
/
movzbl
%
r8b
%
r10d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
102
65
15
58
33
12
30
16
/
/
insertps
0x10
(
%
r14
%
rbx
1
)
%
xmm1
.
byte
243
67
15
16
20
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm2
.
byte
102
15
58
33
202
32
/
/
insertps
0x20
%
xmm2
%
xmm1
.
byte
243
67
15
16
20
6
/
/
movss
(
%
r14
%
r8
1
)
%
xmm2
.
byte
102
15
58
33
202
48
/
/
insertps
0x30
%
xmm2
%
xmm1
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
15
56
0
21
188
221
1
0
/
/
pshufb
0x1ddbc
(
%
rip
)
%
xmm2
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
102
72
15
58
22
211
1
/
/
pextrq
0x1
%
xmm2
%
rbx
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
68
15
182
200
/
/
movzbl
%
al
%
r9d
.
byte
72
193
232
30
/
/
shr
0x1e
%
rax
.
byte
68
15
182
211
/
/
movzbl
%
bl
%
r10d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
243
67
15
16
20
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
102
65
15
58
33
20
0
16
/
/
insertps
0x10
(
%
r8
%
rax
1
)
%
xmm2
.
byte
243
67
15
16
28
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm3
.
byte
102
15
58
33
211
32
/
/
insertps
0x20
%
xmm3
%
xmm2
.
byte
243
65
15
16
28
24
/
/
movss
(
%
r8
%
rbx
1
)
%
xmm3
.
byte
102
15
58
33
211
48
/
/
insertps
0x30
%
xmm3
%
xmm2
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
65
15
91
216
/
/
cvtdq2ps
%
xmm8
%
xmm3
.
byte
15
89
29
169
222
1
0
/
/
mulps
0x1dea9
(
%
rip
)
%
xmm3
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
137
249
/
/
mov
%
edi
%
r9d
.
byte
65
128
225
3
/
/
and
0x3
%
r9b
.
byte
65
128
249
1
/
/
cmp
0x1
%
r9b
.
byte
116
52
/
/
je
1f2bf
<
_sk_load_tables_sse41
+
0x163
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
65
128
249
2
/
/
cmp
0x2
%
r9b
.
byte
116
23
/
/
je
1f2ad
<
_sk_load_tables_sse41
+
0x151
>
.
byte
65
128
249
3
/
/
cmp
0x3
%
r9b
.
byte
15
133
208
254
255
255
/
/
jne
1f170
<
_sk_load_tables_sse41
+
0x14
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
243
65
15
126
4
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
58
14
192
15
/
/
pblendw
0xf
%
xmm0
%
xmm8
.
byte
233
177
254
255
255
/
/
jmpq
1f170
<
_sk_load_tables_sse41
+
0x14
>
.
byte
102
69
15
110
4
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
233
166
254
255
255
/
/
jmpq
1f170
<
_sk_load_tables_sse41
+
0x14
>
HIDDEN
_sk_load_tables_u16_be_sse41
.
globl
_sk_load_tables_u16_be_sse41
FUNCTION
(
_sk_load_tables_u16_be_sse41
)
_sk_load_tables_u16_be_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
95
1
0
0
/
/
jne
1f43f
<
_sk_load_tables_u16_be_sse41
+
0x175
>
.
byte
102
67
15
16
4
72
/
/
movupd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
243
67
15
111
76
72
16
/
/
movdqu
0x10
(
%
r8
%
r9
2
)
%
xmm1
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
68
15
40
200
/
/
movapd
%
xmm0
%
xmm9
.
byte
102
68
15
97
201
/
/
punpcklwd
%
xmm1
%
xmm9
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
68
15
105
200
/
/
punpckhwd
%
xmm0
%
xmm9
.
byte
102
68
15
111
5
123
222
1
0
/
/
movdqa
0x1de7b
(
%
rip
)
%
xmm8
#
3d190
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf44
>
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
219
192
/
/
pand
%
xmm8
%
xmm0
.
byte
102
15
56
51
192
/
/
pmovzxwd
%
xmm0
%
xmm0
.
byte
102
73
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
r8
.
byte
102
73
15
126
193
/
/
movq
%
xmm0
%
r9
.
byte
69
15
182
209
/
/
movzbl
%
r9b
%
r10d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
69
15
182
216
/
/
movzbl
%
r8b
%
r11d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
66
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm0
.
byte
102
66
15
58
33
4
11
16
/
/
insertps
0x10
(
%
rbx
%
r9
1
)
%
xmm0
.
byte
243
66
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
102
15
58
33
194
32
/
/
insertps
0x20
%
xmm2
%
xmm0
.
byte
243
66
15
16
20
3
/
/
movss
(
%
rbx
%
r8
1
)
%
xmm2
.
byte
102
15
58
33
194
48
/
/
insertps
0x30
%
xmm2
%
xmm0
.
byte
102
15
56
0
13
42
222
1
0
/
/
pshufb
0x1de2a
(
%
rip
)
%
xmm1
#
3d1a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf54
>
.
byte
102
15
56
51
201
/
/
pmovzxwd
%
xmm1
%
xmm1
.
byte
102
73
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
r8
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
68
15
182
203
/
/
movzbl
%
bl
%
r9d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
69
15
182
208
/
/
movzbl
%
r8b
%
r10d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
102
65
15
58
33
12
30
16
/
/
insertps
0x10
(
%
r14
%
rbx
1
)
%
xmm1
.
byte
243
67
15
16
20
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm2
.
byte
102
15
58
33
202
32
/
/
insertps
0x20
%
xmm2
%
xmm1
.
byte
243
67
15
16
20
6
/
/
movss
(
%
r14
%
r8
1
)
%
xmm2
.
byte
102
15
58
33
202
48
/
/
insertps
0x30
%
xmm2
%
xmm1
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
102
69
15
219
193
/
/
pand
%
xmm9
%
xmm8
.
byte
102
65
15
56
51
208
/
/
pmovzxwd
%
xmm8
%
xmm2
.
byte
102
72
15
58
22
211
1
/
/
pextrq
0x1
%
xmm2
%
rbx
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
68
15
182
200
/
/
movzbl
%
al
%
r9d
.
byte
72
193
232
30
/
/
shr
0x1e
%
rax
.
byte
68
15
182
211
/
/
movzbl
%
bl
%
r10d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
243
67
15
16
20
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
102
65
15
58
33
20
0
16
/
/
insertps
0x10
(
%
r8
%
rax
1
)
%
xmm2
.
byte
243
67
15
16
28
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm3
.
byte
102
15
58
33
211
32
/
/
insertps
0x20
%
xmm3
%
xmm2
.
byte
243
65
15
16
28
24
/
/
movss
(
%
r8
%
rbx
1
)
%
xmm3
.
byte
102
15
58
33
211
48
/
/
insertps
0x30
%
xmm3
%
xmm2
.
byte
102
65
15
112
217
78
/
/
pshufd
0x4e
%
xmm9
%
xmm3
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
65
15
113
240
8
/
/
psllw
0x8
%
xmm8
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
65
15
235
216
/
/
por
%
xmm8
%
xmm3
.
byte
102
15
56
51
219
/
/
pmovzxwd
%
xmm3
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
29
120
221
1
0
/
/
mulps
0x1dd78
(
%
rip
)
%
xmm3
#
3d1b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
67
15
16
4
72
/
/
movsd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
1f458
<
_sk_load_tables_u16_be_sse41
+
0x18e
>
.
byte
243
15
126
192
/
/
movq
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
149
254
255
255
/
/
jmpq
1f2ed
<
_sk_load_tables_u16_be_sse41
+
0x23
>
.
byte
102
67
15
22
68
72
8
/
/
movhpd
0x8
(
%
r8
%
r9
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
128
254
255
255
/
/
jb
1f2ed
<
_sk_load_tables_u16_be_sse41
+
0x23
>
.
byte
243
67
15
126
76
72
16
/
/
movq
0x10
(
%
r8
%
r9
2
)
%
xmm1
.
byte
233
116
254
255
255
/
/
jmpq
1f2ed
<
_sk_load_tables_u16_be_sse41
+
0x23
>
HIDDEN
_sk_load_tables_rgb_u16_be_sse41
.
globl
_sk_load_tables_rgb_u16_be_sse41
FUNCTION
(
_sk_load_tables_rgb_u16_be_sse41
)
_sk_load_tables_rgb_u16_be_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
79
1
0
0
/
/
jne
1f5da
<
_sk_load_tables_rgb_u16_be_sse41
+
0x161
>
.
byte
243
67
15
111
20
72
/
/
movdqu
(
%
r8
%
r9
2
)
%
xmm2
.
byte
243
67
15
111
76
72
8
/
/
movdqu
0x8
(
%
r8
%
r9
2
)
%
xmm1
.
byte
102
15
115
217
4
/
/
psrldq
0x4
%
xmm1
.
byte
102
68
15
111
202
/
/
movdqa
%
xmm2
%
xmm9
.
byte
102
65
15
115
217
6
/
/
psrldq
0x6
%
xmm9
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
115
216
6
/
/
psrldq
0x6
%
xmm0
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
68
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm9
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
65
15
97
201
/
/
punpcklwd
%
xmm9
%
xmm1
.
byte
102
68
15
111
5
193
220
1
0
/
/
movdqa
0x1dcc1
(
%
rip
)
%
xmm8
#
3d190
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf44
>
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
219
192
/
/
pand
%
xmm8
%
xmm0
.
byte
102
15
56
51
192
/
/
pmovzxwd
%
xmm0
%
xmm0
.
byte
102
73
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
r8
.
byte
102
73
15
126
193
/
/
movq
%
xmm0
%
r9
.
byte
69
15
182
209
/
/
movzbl
%
r9b
%
r10d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
69
15
182
216
/
/
movzbl
%
r8b
%
r11d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
66
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm0
.
byte
102
66
15
58
33
4
11
16
/
/
insertps
0x10
(
%
rbx
%
r9
1
)
%
xmm0
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
102
15
58
33
195
32
/
/
insertps
0x20
%
xmm3
%
xmm0
.
byte
243
66
15
16
28
3
/
/
movss
(
%
rbx
%
r8
1
)
%
xmm3
.
byte
102
15
58
33
195
48
/
/
insertps
0x30
%
xmm3
%
xmm0
.
byte
102
15
56
0
13
112
220
1
0
/
/
pshufb
0x1dc70
(
%
rip
)
%
xmm1
#
3d1a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf54
>
.
byte
102
15
56
51
201
/
/
pmovzxwd
%
xmm1
%
xmm1
.
byte
102
73
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
r8
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
68
15
182
203
/
/
movzbl
%
bl
%
r9d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
69
15
182
208
/
/
movzbl
%
r8b
%
r10d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
102
65
15
58
33
12
30
16
/
/
insertps
0x10
(
%
r14
%
rbx
1
)
%
xmm1
.
byte
243
67
15
16
28
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm3
.
byte
102
15
58
33
203
32
/
/
insertps
0x20
%
xmm3
%
xmm1
.
byte
243
67
15
16
28
6
/
/
movss
(
%
r14
%
r8
1
)
%
xmm3
.
byte
102
15
58
33
203
48
/
/
insertps
0x30
%
xmm3
%
xmm1
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
102
65
15
105
209
/
/
punpckhwd
%
xmm9
%
xmm2
.
byte
102
65
15
219
208
/
/
pand
%
xmm8
%
xmm2
.
byte
102
15
56
51
210
/
/
pmovzxwd
%
xmm2
%
xmm2
.
byte
102
72
15
58
22
211
1
/
/
pextrq
0x1
%
xmm2
%
rbx
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
68
15
182
200
/
/
movzbl
%
al
%
r9d
.
byte
72
193
232
30
/
/
shr
0x1e
%
rax
.
byte
68
15
182
211
/
/
movzbl
%
bl
%
r10d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
243
67
15
16
20
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
102
65
15
58
33
20
0
16
/
/
insertps
0x10
(
%
r8
%
rax
1
)
%
xmm2
.
byte
243
67
15
16
28
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm3
.
byte
102
15
58
33
211
32
/
/
insertps
0x20
%
xmm3
%
xmm2
.
byte
243
65
15
16
28
24
/
/
movss
(
%
r8
%
rbx
1
)
%
xmm3
.
byte
102
15
58
33
211
48
/
/
insertps
0x30
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
59
217
1
0
/
/
movaps
0x1d93b
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
67
15
110
20
72
/
/
movd
(
%
r8
%
r9
2
)
%
xmm2
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
67
15
196
84
72
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
r9
2
)
%
xmm2
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
14
/
/
jne
1f600
<
_sk_load_tables_rgb_u16_be_sse41
+
0x187
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
233
177
254
255
255
/
/
jmpq
1f4b1
<
_sk_load_tables_rgb_u16_be_sse41
+
0x38
>
.
byte
102
71
15
110
76
72
6
/
/
movd
0x6
(
%
r8
%
r9
2
)
%
xmm9
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
71
15
196
76
72
10
2
/
/
pinsrw
0x2
0xa
(
%
r8
%
r9
2
)
%
xmm9
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
24
/
/
jb
1f631
<
_sk_load_tables_rgb_u16_be_sse41
+
0x1b8
>
.
byte
102
67
15
110
76
72
12
/
/
movd
0xc
(
%
r8
%
r9
2
)
%
xmm1
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
67
15
196
76
72
16
2
/
/
pinsrw
0x2
0x10
(
%
r8
%
r9
2
)
%
xmm1
.
byte
233
128
254
255
255
/
/
jmpq
1f4b1
<
_sk_load_tables_rgb_u16_be_sse41
+
0x38
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
119
254
255
255
/
/
jmpq
1f4b1
<
_sk_load_tables_rgb_u16_be_sse41
+
0x38
>
HIDDEN
_sk_byte_tables_sse41
.
globl
_sk_byte_tables_sse41
FUNCTION
(
_sk_byte_tables_sse41
)
_sk_byte_tables_sse41
:
.
byte
85
/
/
push
%
rbp
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
68
15
95
216
/
/
maxps
%
xmm0
%
xmm11
.
byte
68
15
40
13
188
216
1
0
/
/
movaps
0x1d8bc
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
217
/
/
minps
%
xmm9
%
xmm11
.
byte
68
15
40
21
144
217
1
0
/
/
movaps
0x1d990
(
%
rip
)
%
xmm10
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
102
65
15
91
195
/
/
cvtps2dq
%
xmm11
%
xmm0
.
byte
102
73
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
77
137
194
/
/
mov
%
r8
%
r10
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
69
137
195
/
/
mov
%
r8d
%
r11d
.
byte
77
137
198
/
/
mov
%
r8
%
r14
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
70
15
182
52
51
/
/
movzbl
(
%
rbx
%
r14
1
)
%
r14d
.
byte
66
15
182
44
27
/
/
movzbl
(
%
rbx
%
r11
1
)
%
ebp
.
byte
102
15
110
197
/
/
movd
%
ebp
%
xmm0
.
byte
102
65
15
58
32
198
1
/
/
pinsrb
0x1
%
r14d
%
xmm0
.
byte
66
15
182
44
11
/
/
movzbl
(
%
rbx
%
r9
1
)
%
ebp
.
byte
102
15
58
32
197
2
/
/
pinsrb
0x2
%
ebp
%
xmm0
.
byte
66
15
182
44
19
/
/
movzbl
(
%
rbx
%
r10
1
)
%
ebp
.
byte
102
15
58
32
197
3
/
/
pinsrb
0x3
%
ebp
%
xmm0
.
byte
102
15
56
49
192
/
/
pmovzxbd
%
xmm0
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
29
85
218
1
0
/
/
movaps
0x1da55
(
%
rip
)
%
xmm11
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
225
/
/
minps
%
xmm9
%
xmm12
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
102
65
15
91
204
/
/
cvtps2dq
%
xmm12
%
xmm1
.
byte
102
72
15
58
22
205
1
/
/
pextrq
0x1
%
xmm1
%
rbp
.
byte
65
137
233
/
/
mov
%
ebp
%
r9d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
218
/
/
mov
%
ebx
%
r10d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
r11d
.
byte
67
15
182
28
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
ebx
.
byte
102
15
110
203
/
/
movd
%
ebx
%
xmm1
.
byte
102
65
15
58
32
203
1
/
/
pinsrb
0x1
%
r11d
%
xmm1
.
byte
67
15
182
28
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
ebx
.
byte
102
15
58
32
203
2
/
/
pinsrb
0x2
%
ebx
%
xmm1
.
byte
65
15
182
44
40
/
/
movzbl
(
%
r8
%
rbp
1
)
%
ebp
.
byte
102
15
58
32
205
3
/
/
pinsrb
0x3
%
ebp
%
xmm1
.
byte
102
15
56
49
201
/
/
pmovzxbd
%
xmm1
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
69
15
93
225
/
/
minps
%
xmm9
%
xmm12
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
102
65
15
91
212
/
/
cvtps2dq
%
xmm12
%
xmm2
.
byte
102
72
15
58
22
211
1
/
/
pextrq
0x1
%
xmm2
%
rbx
.
byte
65
137
216
/
/
mov
%
ebx
%
r8d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
72
15
126
213
/
/
movq
%
xmm2
%
rbp
.
byte
65
137
234
/
/
mov
%
ebp
%
r10d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
69
15
182
28
41
/
/
movzbl
(
%
r9
%
rbp
1
)
%
r11d
.
byte
67
15
182
44
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
ebp
.
byte
102
15
110
213
/
/
movd
%
ebp
%
xmm2
.
byte
102
65
15
58
32
211
1
/
/
pinsrb
0x1
%
r11d
%
xmm2
.
byte
67
15
182
44
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
ebp
.
byte
102
15
58
32
213
2
/
/
pinsrb
0x2
%
ebp
%
xmm2
.
byte
65
15
182
44
25
/
/
movzbl
(
%
r9
%
rbx
1
)
%
ebp
.
byte
102
15
58
32
213
3
/
/
pinsrb
0x3
%
ebp
%
xmm2
.
byte
102
15
56
49
210
/
/
pmovzxbd
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
193
/
/
minps
%
xmm9
%
xmm8
.
byte
69
15
89
194
/
/
mulps
%
xmm10
%
xmm8
.
byte
102
65
15
91
216
/
/
cvtps2dq
%
xmm8
%
xmm3
.
byte
102
72
15
58
22
221
1
/
/
pextrq
0x1
%
xmm3
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
65
137
217
/
/
mov
%
ebx
%
r9d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
68
15
182
20
24
/
/
movzbl
(
%
rax
%
rbx
1
)
%
r10d
.
byte
66
15
182
28
8
/
/
movzbl
(
%
rax
%
r9
1
)
%
ebx
.
byte
102
15
110
219
/
/
movd
%
ebx
%
xmm3
.
byte
102
65
15
58
32
218
1
/
/
pinsrb
0x1
%
r10d
%
xmm3
.
byte
66
15
182
28
0
/
/
movzbl
(
%
rax
%
r8
1
)
%
ebx
.
byte
102
15
58
32
219
2
/
/
pinsrb
0x2
%
ebx
%
xmm3
.
byte
15
182
4
40
/
/
movzbl
(
%
rax
%
rbp
1
)
%
eax
.
byte
102
15
58
32
216
3
/
/
pinsrb
0x3
%
eax
%
xmm3
.
byte
102
15
56
49
219
/
/
pmovzxbd
%
xmm3
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
219
/
/
mulps
%
xmm11
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_byte_tables_rgb_sse41
.
globl
_sk_byte_tables_rgb_sse41
FUNCTION
(
_sk_byte_tables_rgb_sse41
)
_sk_byte_tables_rgb_sse41
:
.
byte
85
/
/
push
%
rbp
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
102
69
15
110
192
/
/
movd
%
r8d
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
68
15
95
216
/
/
maxps
%
xmm0
%
xmm11
.
byte
68
15
40
21
212
214
1
0
/
/
movaps
0x1d6d4
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
218
/
/
minps
%
xmm10
%
xmm11
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
102
65
15
91
195
/
/
cvtps2dq
%
xmm11
%
xmm0
.
byte
102
73
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
77
137
194
/
/
mov
%
r8
%
r10
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
69
137
195
/
/
mov
%
r8d
%
r11d
.
byte
77
137
198
/
/
mov
%
r8
%
r14
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
70
15
182
52
51
/
/
movzbl
(
%
rbx
%
r14
1
)
%
r14d
.
byte
66
15
182
44
27
/
/
movzbl
(
%
rbx
%
r11
1
)
%
ebp
.
byte
102
15
110
197
/
/
movd
%
ebp
%
xmm0
.
byte
102
65
15
58
32
198
1
/
/
pinsrb
0x1
%
r14d
%
xmm0
.
byte
66
15
182
44
11
/
/
movzbl
(
%
rbx
%
r9
1
)
%
ebp
.
byte
102
15
58
32
197
2
/
/
pinsrb
0x2
%
ebp
%
xmm0
.
byte
66
15
182
44
19
/
/
movzbl
(
%
rbx
%
r10
1
)
%
ebp
.
byte
102
15
58
32
197
3
/
/
pinsrb
0x3
%
ebp
%
xmm0
.
byte
102
15
56
49
192
/
/
pmovzxbd
%
xmm0
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
29
117
216
1
0
/
/
movaps
0x1d875
(
%
rip
)
%
xmm11
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
102
65
15
91
204
/
/
cvtps2dq
%
xmm12
%
xmm1
.
byte
102
72
15
58
22
205
1
/
/
pextrq
0x1
%
xmm1
%
rbp
.
byte
65
137
233
/
/
mov
%
ebp
%
r9d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
218
/
/
mov
%
ebx
%
r10d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
r11d
.
byte
67
15
182
28
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
ebx
.
byte
102
15
110
203
/
/
movd
%
ebx
%
xmm1
.
byte
102
65
15
58
32
203
1
/
/
pinsrb
0x1
%
r11d
%
xmm1
.
byte
67
15
182
28
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
ebx
.
byte
102
15
58
32
203
2
/
/
pinsrb
0x2
%
ebx
%
xmm1
.
byte
65
15
182
44
40
/
/
movzbl
(
%
r8
%
rbp
1
)
%
ebp
.
byte
102
15
58
32
205
3
/
/
pinsrb
0x3
%
ebp
%
xmm1
.
byte
102
15
56
49
201
/
/
pmovzxbd
%
xmm1
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
72
139
64
16
/
/
mov
0x10
(
%
rax
)
%
rax
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
65
15
91
209
/
/
cvtps2dq
%
xmm9
%
xmm2
.
byte
102
72
15
58
22
213
1
/
/
pextrq
0x1
%
xmm2
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
72
15
126
211
/
/
movq
%
xmm2
%
rbx
.
byte
65
137
217
/
/
mov
%
ebx
%
r9d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
68
15
182
20
24
/
/
movzbl
(
%
rax
%
rbx
1
)
%
r10d
.
byte
66
15
182
28
8
/
/
movzbl
(
%
rax
%
r9
1
)
%
ebx
.
byte
102
15
110
211
/
/
movd
%
ebx
%
xmm2
.
byte
102
65
15
58
32
210
1
/
/
pinsrb
0x1
%
r10d
%
xmm2
.
byte
66
15
182
28
0
/
/
movzbl
(
%
rax
%
r8
1
)
%
ebx
.
byte
102
15
58
32
211
2
/
/
pinsrb
0x2
%
ebx
%
xmm2
.
byte
15
182
4
40
/
/
movzbl
(
%
rax
%
rbp
1
)
%
eax
.
byte
102
15
58
32
208
3
/
/
pinsrb
0x3
%
eax
%
xmm2
.
byte
102
15
56
49
210
/
/
pmovzxbd
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_r_sse41
.
globl
_sk_table_r_sse41
FUNCTION
(
_sk_table_r_sse41
)
_sk_table_r_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
13
101
213
1
0
/
/
minps
0x1d565
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
65
15
91
193
/
/
cvtps2dq
%
xmm9
%
xmm0
.
byte
102
72
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
194
/
/
movq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
67
15
16
4
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm0
.
byte
102
67
15
58
33
4
144
16
/
/
insertps
0x10
(
%
r8
%
r10
4
)
%
xmm0
.
byte
243
71
15
16
4
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm8
.
byte
102
65
15
58
33
192
32
/
/
insertps
0x20
%
xmm8
%
xmm0
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
102
65
15
58
33
192
48
/
/
insertps
0x30
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_g_sse41
.
globl
_sk_table_g_sse41
FUNCTION
(
_sk_table_g_sse41
)
_sk_table_g_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
201
/
/
maxps
%
xmm1
%
xmm9
.
byte
68
15
93
13
237
212
1
0
/
/
minps
0x1d4ed
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
65
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
67
15
16
12
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm1
.
byte
102
67
15
58
33
12
144
16
/
/
insertps
0x10
(
%
r8
%
r10
4
)
%
xmm1
.
byte
243
71
15
16
4
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm8
.
byte
102
65
15
58
33
200
32
/
/
insertps
0x20
%
xmm8
%
xmm1
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
102
65
15
58
33
200
48
/
/
insertps
0x30
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_b_sse41
.
globl
_sk_table_b_sse41
FUNCTION
(
_sk_table_b_sse41
)
_sk_table_b_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
68
15
93
13
117
212
1
0
/
/
minps
0x1d475
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
65
15
91
209
/
/
cvtps2dq
%
xmm9
%
xmm2
.
byte
102
72
15
58
22
208
1
/
/
pextrq
0x1
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
210
/
/
movq
%
xmm2
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
67
15
16
20
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm2
.
byte
102
67
15
58
33
20
144
16
/
/
insertps
0x10
(
%
r8
%
r10
4
)
%
xmm2
.
byte
243
71
15
16
4
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm8
.
byte
102
65
15
58
33
208
32
/
/
insertps
0x20
%
xmm8
%
xmm2
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
102
65
15
58
33
208
48
/
/
insertps
0x30
%
xmm8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_a_sse41
.
globl
_sk_table_a_sse41
FUNCTION
(
_sk_table_a_sse41
)
_sk_table_a_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
203
/
/
maxps
%
xmm3
%
xmm9
.
byte
68
15
93
13
253
211
1
0
/
/
minps
0x1d3fd
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
65
15
91
217
/
/
cvtps2dq
%
xmm9
%
xmm3
.
byte
102
72
15
58
22
216
1
/
/
pextrq
0x1
%
xmm3
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
218
/
/
movq
%
xmm3
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
67
15
16
28
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm3
.
byte
102
67
15
58
33
28
144
16
/
/
insertps
0x10
(
%
r8
%
r10
4
)
%
xmm3
.
byte
243
71
15
16
4
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm8
.
byte
102
65
15
58
33
216
32
/
/
insertps
0x20
%
xmm8
%
xmm3
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
102
65
15
58
33
216
48
/
/
insertps
0x30
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_r_sse41
.
globl
_sk_parametric_r_sse41
FUNCTION
(
_sk_parametric_r_sse41
)
_sk_parametric_r_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
243
68
15
16
80
4
/
/
movss
0x4
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
65
15
194
192
2
/
/
cmpleps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
24
/
/
movss
0x18
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
69
15
88
200
/
/
addps
%
xmm8
%
xmm9
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
68
15
16
64
8
/
/
movss
0x8
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
69
15
88
208
/
/
addps
%
xmm8
%
xmm10
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
69
15
91
194
/
/
cvtdq2ps
%
xmm10
%
xmm8
.
byte
68
15
89
5
246
213
1
0
/
/
mulps
0x1d5f6
(
%
rip
)
%
xmm8
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
254
213
1
0
/
/
movaps
0x1d5fe
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
234
/
/
andps
%
xmm10
%
xmm13
.
byte
68
15
86
45
34
211
1
0
/
/
orps
0x1d322
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
5
250
213
1
0
/
/
addps
0x1d5fa
(
%
rip
)
%
xmm8
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
53
2
214
1
0
/
/
movaps
0x1d602
(
%
rip
)
%
xmm14
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
92
198
/
/
subps
%
xmm14
%
xmm8
.
byte
68
15
88
45
2
214
1
0
/
/
addps
0x1d602
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
68
15
40
53
10
214
1
0
/
/
movaps
0x1d60a
(
%
rip
)
%
xmm14
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
92
198
/
/
subps
%
xmm14
%
xmm8
.
byte
69
15
89
196
/
/
mulps
%
xmm12
%
xmm8
.
byte
102
69
15
58
8
224
1
/
/
roundps
0x1
%
xmm8
%
xmm12
.
byte
69
15
40
232
/
/
movaps
%
xmm8
%
xmm13
.
byte
69
15
92
236
/
/
subps
%
xmm12
%
xmm13
.
byte
68
15
88
5
247
213
1
0
/
/
addps
0x1d5f7
(
%
rip
)
%
xmm8
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
68
15
40
37
255
213
1
0
/
/
movaps
0x1d5ff
(
%
rip
)
%
xmm12
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
69
15
92
196
/
/
subps
%
xmm12
%
xmm8
.
byte
68
15
40
37
255
213
1
0
/
/
movaps
0x1d5ff
(
%
rip
)
%
xmm12
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
68
15
40
45
3
214
1
0
/
/
movaps
0x1d603
(
%
rip
)
%
xmm13
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
69
15
94
236
/
/
divps
%
xmm12
%
xmm13
.
byte
69
15
88
232
/
/
addps
%
xmm8
%
xmm13
.
byte
68
15
89
45
3
214
1
0
/
/
mulps
0x1d603
(
%
rip
)
%
xmm13
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
69
15
91
197
/
/
cvtps2dq
%
xmm13
%
xmm8
.
byte
69
15
194
211
4
/
/
cmpneqps
%
xmm11
%
xmm10
.
byte
69
15
84
208
/
/
andps
%
xmm8
%
xmm10
.
byte
243
68
15
16
64
20
/
/
movss
0x14
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
69
15
88
194
/
/
addps
%
xmm10
%
xmm8
.
byte
102
69
15
56
20
193
/
/
blendvps
%
xmm0
%
xmm9
%
xmm8
.
byte
69
15
95
195
/
/
maxps
%
xmm11
%
xmm8
.
byte
68
15
93
5
132
210
1
0
/
/
minps
0x1d284
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_g_sse41
.
globl
_sk_parametric_g_sse41
FUNCTION
(
_sk_parametric_g_sse41
)
_sk_parametric_g_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
88
16
/
/
movss
0x10
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
243
68
15
16
80
4
/
/
movss
0x4
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
194
195
2
/
/
cmpleps
%
xmm11
%
xmm0
.
byte
243
15
16
72
24
/
/
movss
0x18
(
%
rax
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
15
16
72
8
/
/
movss
0x8
(
%
rax
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
68
15
88
209
/
/
addps
%
xmm1
%
xmm10
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
69
15
91
242
/
/
cvtdq2ps
%
xmm10
%
xmm14
.
byte
68
15
89
53
193
212
1
0
/
/
mulps
0x1d4c1
(
%
rip
)
%
xmm14
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
201
212
1
0
/
/
movaps
0x1d4c9
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
234
/
/
andps
%
xmm10
%
xmm13
.
byte
68
15
86
45
237
209
1
0
/
/
orps
0x1d1ed
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
53
197
212
1
0
/
/
addps
0x1d4c5
(
%
rip
)
%
xmm14
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
15
40
13
206
212
1
0
/
/
movaps
0x1d4ce
(
%
rip
)
%
xmm1
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
65
15
89
205
/
/
mulps
%
xmm13
%
xmm1
.
byte
68
15
92
241
/
/
subps
%
xmm1
%
xmm14
.
byte
68
15
88
45
206
212
1
0
/
/
addps
0x1d4ce
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
15
40
13
215
212
1
0
/
/
movaps
0x1d4d7
(
%
rip
)
%
xmm1
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
65
15
94
205
/
/
divps
%
xmm13
%
xmm1
.
byte
68
15
92
241
/
/
subps
%
xmm1
%
xmm14
.
byte
69
15
89
244
/
/
mulps
%
xmm12
%
xmm14
.
byte
102
69
15
58
8
230
1
/
/
roundps
0x1
%
xmm14
%
xmm12
.
byte
69
15
40
238
/
/
movaps
%
xmm14
%
xmm13
.
byte
69
15
92
236
/
/
subps
%
xmm12
%
xmm13
.
byte
68
15
88
53
196
212
1
0
/
/
addps
0x1d4c4
(
%
rip
)
%
xmm14
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
13
205
212
1
0
/
/
movaps
0x1d4cd
(
%
rip
)
%
xmm1
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
65
15
89
205
/
/
mulps
%
xmm13
%
xmm1
.
byte
68
15
92
241
/
/
subps
%
xmm1
%
xmm14
.
byte
68
15
40
37
205
212
1
0
/
/
movaps
0x1d4cd
(
%
rip
)
%
xmm12
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
15
40
13
210
212
1
0
/
/
movaps
0x1d4d2
(
%
rip
)
%
xmm1
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
65
15
94
204
/
/
divps
%
xmm12
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
15
89
13
211
212
1
0
/
/
mulps
0x1d4d3
(
%
rip
)
%
xmm1
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
15
91
201
/
/
cvtps2dq
%
xmm1
%
xmm1
.
byte
69
15
194
211
4
/
/
cmpneqps
%
xmm11
%
xmm10
.
byte
68
15
84
209
/
/
andps
%
xmm1
%
xmm10
.
byte
243
15
16
72
20
/
/
movss
0x14
(
%
rax
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
102
65
15
56
20
201
/
/
blendvps
%
xmm0
%
xmm9
%
xmm1
.
byte
65
15
95
203
/
/
maxps
%
xmm11
%
xmm1
.
byte
15
93
13
88
209
1
0
/
/
minps
0x1d158
(
%
rip
)
%
xmm1
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_b_sse41
.
globl
_sk_parametric_b_sse41
FUNCTION
(
_sk_parametric_b_sse41
)
_sk_parametric_b_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
88
16
/
/
movss
0x10
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
202
/
/
mulps
%
xmm2
%
xmm9
.
byte
243
68
15
16
80
4
/
/
movss
0x4
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
65
15
194
195
2
/
/
cmpleps
%
xmm11
%
xmm0
.
byte
243
15
16
80
24
/
/
movss
0x18
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
68
15
88
202
/
/
addps
%
xmm2
%
xmm9
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
69
15
91
242
/
/
cvtdq2ps
%
xmm10
%
xmm14
.
byte
68
15
89
53
149
211
1
0
/
/
mulps
0x1d395
(
%
rip
)
%
xmm14
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
157
211
1
0
/
/
movaps
0x1d39d
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
234
/
/
andps
%
xmm10
%
xmm13
.
byte
68
15
86
45
193
208
1
0
/
/
orps
0x1d0c1
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
53
153
211
1
0
/
/
addps
0x1d399
(
%
rip
)
%
xmm14
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
15
40
21
162
211
1
0
/
/
movaps
0x1d3a2
(
%
rip
)
%
xmm2
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
68
15
92
242
/
/
subps
%
xmm2
%
xmm14
.
byte
68
15
88
45
162
211
1
0
/
/
addps
0x1d3a2
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
15
40
21
171
211
1
0
/
/
movaps
0x1d3ab
(
%
rip
)
%
xmm2
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
65
15
94
213
/
/
divps
%
xmm13
%
xmm2
.
byte
68
15
92
242
/
/
subps
%
xmm2
%
xmm14
.
byte
69
15
89
244
/
/
mulps
%
xmm12
%
xmm14
.
byte
102
69
15
58
8
230
1
/
/
roundps
0x1
%
xmm14
%
xmm12
.
byte
69
15
40
238
/
/
movaps
%
xmm14
%
xmm13
.
byte
69
15
92
236
/
/
subps
%
xmm12
%
xmm13
.
byte
68
15
88
53
152
211
1
0
/
/
addps
0x1d398
(
%
rip
)
%
xmm14
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
21
161
211
1
0
/
/
movaps
0x1d3a1
(
%
rip
)
%
xmm2
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
68
15
92
242
/
/
subps
%
xmm2
%
xmm14
.
byte
68
15
40
37
161
211
1
0
/
/
movaps
0x1d3a1
(
%
rip
)
%
xmm12
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
15
40
21
166
211
1
0
/
/
movaps
0x1d3a6
(
%
rip
)
%
xmm2
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
65
15
94
212
/
/
divps
%
xmm12
%
xmm2
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
15
89
21
167
211
1
0
/
/
mulps
0x1d3a7
(
%
rip
)
%
xmm2
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
15
91
210
/
/
cvtps2dq
%
xmm2
%
xmm2
.
byte
69
15
194
211
4
/
/
cmpneqps
%
xmm11
%
xmm10
.
byte
68
15
84
210
/
/
andps
%
xmm2
%
xmm10
.
byte
243
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
65
15
88
210
/
/
addps
%
xmm10
%
xmm2
.
byte
102
65
15
56
20
209
/
/
blendvps
%
xmm0
%
xmm9
%
xmm2
.
byte
65
15
95
211
/
/
maxps
%
xmm11
%
xmm2
.
byte
15
93
21
44
208
1
0
/
/
minps
0x1d02c
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_a_sse41
.
globl
_sk_parametric_a_sse41
FUNCTION
(
_sk_parametric_a_sse41
)
_sk_parametric_a_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
88
16
/
/
movss
0x10
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
243
68
15
16
80
4
/
/
movss
0x4
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
65
15
194
195
2
/
/
cmpleps
%
xmm11
%
xmm0
.
byte
243
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
68
15
88
203
/
/
addps
%
xmm3
%
xmm9
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
15
16
88
8
/
/
movss
0x8
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
68
15
88
211
/
/
addps
%
xmm3
%
xmm10
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
69
15
91
242
/
/
cvtdq2ps
%
xmm10
%
xmm14
.
byte
68
15
89
53
105
210
1
0
/
/
mulps
0x1d269
(
%
rip
)
%
xmm14
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
113
210
1
0
/
/
movaps
0x1d271
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
234
/
/
andps
%
xmm10
%
xmm13
.
byte
68
15
86
45
149
207
1
0
/
/
orps
0x1cf95
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
53
109
210
1
0
/
/
addps
0x1d26d
(
%
rip
)
%
xmm14
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
15
40
29
118
210
1
0
/
/
movaps
0x1d276
(
%
rip
)
%
xmm3
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
68
15
92
243
/
/
subps
%
xmm3
%
xmm14
.
byte
68
15
88
45
118
210
1
0
/
/
addps
0x1d276
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
15
40
29
127
210
1
0
/
/
movaps
0x1d27f
(
%
rip
)
%
xmm3
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
65
15
94
221
/
/
divps
%
xmm13
%
xmm3
.
byte
68
15
92
243
/
/
subps
%
xmm3
%
xmm14
.
byte
69
15
89
244
/
/
mulps
%
xmm12
%
xmm14
.
byte
102
69
15
58
8
230
1
/
/
roundps
0x1
%
xmm14
%
xmm12
.
byte
69
15
40
238
/
/
movaps
%
xmm14
%
xmm13
.
byte
69
15
92
236
/
/
subps
%
xmm12
%
xmm13
.
byte
68
15
88
53
108
210
1
0
/
/
addps
0x1d26c
(
%
rip
)
%
xmm14
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
29
117
210
1
0
/
/
movaps
0x1d275
(
%
rip
)
%
xmm3
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
68
15
92
243
/
/
subps
%
xmm3
%
xmm14
.
byte
68
15
40
37
117
210
1
0
/
/
movaps
0x1d275
(
%
rip
)
%
xmm12
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
15
40
29
122
210
1
0
/
/
movaps
0x1d27a
(
%
rip
)
%
xmm3
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
65
15
94
220
/
/
divps
%
xmm12
%
xmm3
.
byte
65
15
88
222
/
/
addps
%
xmm14
%
xmm3
.
byte
15
89
29
123
210
1
0
/
/
mulps
0x1d27b
(
%
rip
)
%
xmm3
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
15
91
219
/
/
cvtps2dq
%
xmm3
%
xmm3
.
byte
69
15
194
211
4
/
/
cmpneqps
%
xmm11
%
xmm10
.
byte
68
15
84
211
/
/
andps
%
xmm3
%
xmm10
.
byte
243
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
102
65
15
56
20
217
/
/
blendvps
%
xmm0
%
xmm9
%
xmm3
.
byte
65
15
95
219
/
/
maxps
%
xmm11
%
xmm3
.
byte
15
93
29
0
207
1
0
/
/
minps
0x1cf00
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_sse41
.
globl
_sk_gamma_sse41
FUNCTION
(
_sk_gamma_sse41
)
_sk_gamma_sse41
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
15
41
108
36
200
/
/
movaps
%
xmm5
-
0x38
(
%
rsp
)
.
byte
15
41
100
36
184
/
/
movaps
%
xmm4
-
0x48
(
%
rsp
)
.
byte
15
41
92
36
168
/
/
movaps
%
xmm3
-
0x58
(
%
rsp
)
.
byte
68
15
40
226
/
/
movaps
%
xmm2
%
xmm12
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
91
244
/
/
cvtdq2ps
%
xmm4
%
xmm6
.
byte
15
40
5
122
209
1
0
/
/
movaps
0x1d17a
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
68
15
40
5
124
209
1
0
/
/
movaps
0x1d17c
(
%
rip
)
%
xmm8
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
15
86
5
158
206
1
0
/
/
orps
0x1ce9e
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
88
53
119
209
1
0
/
/
addps
0x1d177
(
%
rip
)
%
xmm6
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
29
127
209
1
0
/
/
movaps
0x1d17f
(
%
rip
)
%
xmm11
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
68
15
40
61
125
209
1
0
/
/
movaps
0x1d17d
(
%
rip
)
%
xmm15
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
68
15
40
53
129
209
1
0
/
/
movaps
0x1d181
(
%
rip
)
%
xmm14
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
65
15
40
206
/
/
movaps
%
xmm14
%
xmm1
.
byte
15
94
200
/
/
divps
%
xmm0
%
xmm1
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
16
/
/
movss
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
102
15
58
8
198
1
/
/
roundps
0x1
%
xmm6
%
xmm0
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
88
53
103
209
1
0
/
/
addps
0x1d167
(
%
rip
)
%
xmm6
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
29
128
209
1
0
/
/
movaps
0x1d180
(
%
rip
)
%
xmm3
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
13
94
209
1
0
/
/
movaps
0x1d15e
(
%
rip
)
%
xmm1
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
68
15
40
45
112
209
1
0
/
/
movaps
0x1d170
(
%
rip
)
%
xmm13
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
65
15
40
197
/
/
movaps
%
xmm13
%
xmm0
.
byte
65
15
94
193
/
/
divps
%
xmm9
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
68
15
40
13
109
209
1
0
/
/
movaps
0x1d16d
(
%
rip
)
%
xmm9
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
194
230
4
/
/
cmpneqps
%
xmm6
%
xmm4
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
65
15
91
194
/
/
cvtdq2ps
%
xmm10
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
65
15
84
248
/
/
andps
%
xmm8
%
xmm7
.
byte
15
40
53
229
205
1
0
/
/
movaps
0x1cde5
(
%
rip
)
%
xmm6
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
86
254
/
/
orps
%
xmm6
%
xmm7
.
byte
15
88
5
187
208
1
0
/
/
addps
0x1d0bb
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
15
40
239
/
/
movaps
%
xmm7
%
xmm5
.
byte
65
15
89
235
/
/
mulps
%
xmm11
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
65
15
88
255
/
/
addps
%
xmm15
%
xmm7
.
byte
65
15
40
238
/
/
movaps
%
xmm14
%
xmm5
.
byte
15
94
239
/
/
divps
%
xmm7
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
102
15
58
8
232
1
/
/
roundps
0x1
%
xmm0
%
xmm5
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
92
253
/
/
subps
%
xmm5
%
xmm7
.
byte
68
15
40
61
204
208
1
0
/
/
movaps
0x1d0cc
(
%
rip
)
%
xmm15
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
92
239
/
/
subps
%
xmm7
%
xmm5
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
65
15
40
253
/
/
movaps
%
xmm13
%
xmm7
.
byte
15
94
253
/
/
divps
%
xmm5
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
65
15
89
249
/
/
mulps
%
xmm9
%
xmm7
.
byte
102
15
91
199
/
/
cvtps2dq
%
xmm7
%
xmm0
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
68
15
194
215
4
/
/
cmpneqps
%
xmm7
%
xmm10
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
65
15
91
196
/
/
cvtdq2ps
%
xmm12
%
xmm0
.
byte
15
89
5
51
208
1
0
/
/
mulps
0x1d033
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
68
15
86
198
/
/
orps
%
xmm6
%
xmm8
.
byte
15
88
5
68
208
1
0
/
/
addps
0x1d044
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
68
15
88
5
84
208
1
0
/
/
addps
0x1d054
(
%
rip
)
%
xmm8
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
69
15
94
240
/
/
divps
%
xmm8
%
xmm14
.
byte
65
15
92
198
/
/
subps
%
xmm14
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
102
15
58
8
208
1
/
/
roundps
0x1
%
xmm0
%
xmm2
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
68
15
94
235
/
/
divps
%
xmm3
%
xmm13
.
byte
68
15
88
232
/
/
addps
%
xmm0
%
xmm13
.
byte
69
15
89
233
/
/
mulps
%
xmm9
%
xmm13
.
byte
68
15
194
231
4
/
/
cmpneqps
%
xmm7
%
xmm12
.
byte
102
65
15
91
197
/
/
cvtps2dq
%
xmm13
%
xmm0
.
byte
68
15
84
224
/
/
andps
%
xmm0
%
xmm12
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
40
212
/
/
movaps
%
xmm12
%
xmm2
.
byte
15
40
92
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_dst_sse41
.
globl
_sk_gamma_dst_sse41
FUNCTION
(
_sk_gamma_dst_sse41
)
_sk_gamma_dst_sse41
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
68
15
40
230
/
/
movaps
%
xmm6
%
xmm12
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
41
92
36
216
/
/
movaps
%
xmm3
-
0x28
(
%
rsp
)
.
byte
15
41
84
36
200
/
/
movaps
%
xmm2
-
0x38
(
%
rsp
)
.
byte
15
41
76
36
184
/
/
movaps
%
xmm1
-
0x48
(
%
rsp
)
.
byte
15
41
68
36
168
/
/
movaps
%
xmm0
-
0x58
(
%
rsp
)
.
byte
15
91
221
/
/
cvtdq2ps
%
xmm5
%
xmm3
.
byte
15
40
5
128
207
1
0
/
/
movaps
0x1cf80
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
68
15
40
5
130
207
1
0
/
/
movaps
0x1cf82
(
%
rip
)
%
xmm8
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
15
86
5
164
204
1
0
/
/
orps
0x1cca4
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
88
29
125
207
1
0
/
/
addps
0x1cf7d
(
%
rip
)
%
xmm3
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
29
133
207
1
0
/
/
movaps
0x1cf85
(
%
rip
)
%
xmm11
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
68
15
40
61
131
207
1
0
/
/
movaps
0x1cf83
(
%
rip
)
%
xmm15
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
68
15
40
53
135
207
1
0
/
/
movaps
0x1cf87
(
%
rip
)
%
xmm14
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
65
15
40
206
/
/
movaps
%
xmm14
%
xmm1
.
byte
15
94
200
/
/
divps
%
xmm0
%
xmm1
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
16
/
/
movss
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
102
15
58
8
195
1
/
/
roundps
0x1
%
xmm3
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
88
29
109
207
1
0
/
/
addps
0x1cf6d
(
%
rip
)
%
xmm3
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
37
134
207
1
0
/
/
movaps
0x1cf86
(
%
rip
)
%
xmm4
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
68
15
40
204
/
/
movaps
%
xmm4
%
xmm9
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
13
100
207
1
0
/
/
movaps
0x1cf64
(
%
rip
)
%
xmm1
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
68
15
40
45
118
207
1
0
/
/
movaps
0x1cf76
(
%
rip
)
%
xmm13
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
65
15
40
197
/
/
movaps
%
xmm13
%
xmm0
.
byte
65
15
94
193
/
/
divps
%
xmm9
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
68
15
40
13
115
207
1
0
/
/
movaps
0x1cf73
(
%
rip
)
%
xmm9
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
194
235
4
/
/
cmpneqps
%
xmm3
%
xmm5
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
65
15
91
194
/
/
cvtdq2ps
%
xmm10
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
65
15
84
248
/
/
andps
%
xmm8
%
xmm7
.
byte
15
40
29
235
203
1
0
/
/
movaps
0x1cbeb
(
%
rip
)
%
xmm3
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
86
251
/
/
orps
%
xmm3
%
xmm7
.
byte
15
88
5
193
206
1
0
/
/
addps
0x1cec1
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
65
15
89
243
/
/
mulps
%
xmm11
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
65
15
88
255
/
/
addps
%
xmm15
%
xmm7
.
byte
65
15
40
246
/
/
movaps
%
xmm14
%
xmm6
.
byte
15
94
247
/
/
divps
%
xmm7
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
102
15
58
8
240
1
/
/
roundps
0x1
%
xmm0
%
xmm6
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
92
254
/
/
subps
%
xmm6
%
xmm7
.
byte
68
15
40
61
210
206
1
0
/
/
movaps
0x1ced2
(
%
rip
)
%
xmm15
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
65
15
40
253
/
/
movaps
%
xmm13
%
xmm7
.
byte
15
94
254
/
/
divps
%
xmm6
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
65
15
89
249
/
/
mulps
%
xmm9
%
xmm7
.
byte
102
15
91
199
/
/
cvtps2dq
%
xmm7
%
xmm0
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
68
15
194
215
4
/
/
cmpneqps
%
xmm7
%
xmm10
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
65
15
91
196
/
/
cvtdq2ps
%
xmm12
%
xmm0
.
byte
15
89
5
57
206
1
0
/
/
mulps
0x1ce39
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
68
15
86
195
/
/
orps
%
xmm3
%
xmm8
.
byte
15
88
5
74
206
1
0
/
/
addps
0x1ce4a
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
68
15
88
5
90
206
1
0
/
/
addps
0x1ce5a
(
%
rip
)
%
xmm8
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
69
15
94
240
/
/
divps
%
xmm8
%
xmm14
.
byte
65
15
92
198
/
/
subps
%
xmm14
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
102
15
58
8
208
1
/
/
roundps
0x1
%
xmm0
%
xmm2
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
92
242
/
/
subps
%
xmm2
%
xmm6
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
92
230
/
/
subps
%
xmm6
%
xmm4
.
byte
68
15
94
236
/
/
divps
%
xmm4
%
xmm13
.
byte
68
15
88
232
/
/
addps
%
xmm0
%
xmm13
.
byte
69
15
89
233
/
/
mulps
%
xmm9
%
xmm13
.
byte
68
15
194
231
4
/
/
cmpneqps
%
xmm7
%
xmm12
.
byte
102
65
15
91
197
/
/
cvtps2dq
%
xmm13
%
xmm0
.
byte
68
15
84
224
/
/
andps
%
xmm0
%
xmm12
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
68
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm0
.
byte
15
40
76
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm1
.
byte
15
40
84
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm2
.
byte
15
40
92
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm3
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
65
15
40
234
/
/
movaps
%
xmm10
%
xmm5
.
byte
65
15
40
244
/
/
movaps
%
xmm12
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lab_to_xyz_sse41
.
globl
_sk_lab_to_xyz_sse41
FUNCTION
(
_sk_lab_to_xyz_sse41
)
_sk_lab_to_xyz_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
68
15
89
5
88
206
1
0
/
/
mulps
0x1ce58
(
%
rip
)
%
xmm8
#
3d270
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1024
>
.
byte
68
15
40
13
208
203
1
0
/
/
movaps
0x1cbd0
(
%
rip
)
%
xmm9
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
15
40
5
85
206
1
0
/
/
movaps
0x1ce55
(
%
rip
)
%
xmm0
#
3d280
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1034
>
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
68
15
88
5
83
206
1
0
/
/
addps
0x1ce53
(
%
rip
)
%
xmm8
#
3d290
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1044
>
.
byte
68
15
89
5
91
206
1
0
/
/
mulps
0x1ce5b
(
%
rip
)
%
xmm8
#
3d2a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1054
>
.
byte
15
89
13
100
206
1
0
/
/
mulps
0x1ce64
(
%
rip
)
%
xmm1
#
3d2b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1064
>
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
15
89
21
105
206
1
0
/
/
mulps
0x1ce69
(
%
rip
)
%
xmm2
#
3d2c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1074
>
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
92
210
/
/
subps
%
xmm2
%
xmm10
.
byte
68
15
40
217
/
/
movaps
%
xmm1
%
xmm11
.
byte
69
15
89
219
/
/
mulps
%
xmm11
%
xmm11
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
68
15
40
13
93
206
1
0
/
/
movaps
0x1ce5d
(
%
rip
)
%
xmm9
#
3d2d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1084
>
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
194
195
1
/
/
cmpltps
%
xmm11
%
xmm0
.
byte
15
40
21
93
206
1
0
/
/
movaps
0x1ce5d
(
%
rip
)
%
xmm2
#
3d2e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1094
>
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
68
15
40
37
98
206
1
0
/
/
movaps
0x1ce62
(
%
rip
)
%
xmm12
#
3d2f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10a4
>
.
byte
65
15
89
204
/
/
mulps
%
xmm12
%
xmm1
.
byte
102
65
15
56
20
203
/
/
blendvps
%
xmm0
%
xmm11
%
xmm1
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
69
15
89
219
/
/
mulps
%
xmm11
%
xmm11
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
194
195
1
/
/
cmpltps
%
xmm11
%
xmm0
.
byte
68
15
88
194
/
/
addps
%
xmm2
%
xmm8
.
byte
69
15
89
196
/
/
mulps
%
xmm12
%
xmm8
.
byte
102
69
15
56
20
195
/
/
blendvps
%
xmm0
%
xmm11
%
xmm8
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
69
15
89
219
/
/
mulps
%
xmm11
%
xmm11
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
69
15
194
203
1
/
/
cmpltps
%
xmm11
%
xmm9
.
byte
65
15
88
210
/
/
addps
%
xmm10
%
xmm2
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
65
15
56
20
211
/
/
blendvps
%
xmm0
%
xmm11
%
xmm2
.
byte
15
89
13
27
206
1
0
/
/
mulps
0x1ce1b
(
%
rip
)
%
xmm1
#
3d300
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10b4
>
.
byte
15
89
21
36
206
1
0
/
/
mulps
0x1ce24
(
%
rip
)
%
xmm2
#
3d310
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10c4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_sse41
.
globl
_sk_load_a8_sse41
FUNCTION
(
_sk_load_a8_sse41
)
_sk_load_a8_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
39
/
/
jne
20530
<
_sk_load_a8_sse41
+
0x39
>
.
byte
102
65
15
56
49
4
16
/
/
pmovzxbd
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
102
15
219
5
168
202
1
0
/
/
pand
0x1caa8
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
216
/
/
cvtdq2ps
%
xmm0
%
xmm3
.
byte
15
89
29
254
203
1
0
/
/
mulps
0x1cbfe
(
%
rip
)
%
xmm3
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
49
/
/
je
20569
<
_sk_load_a8_sse41
+
0x72
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
20553
<
_sk_load_a8_sse41
+
0x5c
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
204
/
/
jne
20510
<
_sk_load_a8_sse41
+
0x19
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
200
/
/
movd
%
eax
%
xmm1
.
byte
102
15
56
49
201
/
/
pmovzxbd
%
xmm1
%
xmm1
.
byte
102
15
58
14
193
15
/
/
pblendw
0xf
%
xmm1
%
xmm0
.
byte
235
167
/
/
jmp
20510
<
_sk_load_a8_sse41
+
0x19
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
156
/
/
jmp
20510
<
_sk_load_a8_sse41
+
0x19
>
HIDDEN
_sk_load_a8_dst_sse41
.
globl
_sk_load_a8_dst_sse41
FUNCTION
(
_sk_load_a8_dst_sse41
)
_sk_load_a8_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
39
/
/
jne
205ad
<
_sk_load_a8_dst_sse41
+
0x39
>
.
byte
102
65
15
56
49
36
16
/
/
pmovzxbd
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
102
15
219
37
43
202
1
0
/
/
pand
0x1ca2b
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
252
/
/
cvtdq2ps
%
xmm4
%
xmm7
.
byte
15
89
61
129
203
1
0
/
/
mulps
0x1cb81
(
%
rip
)
%
xmm7
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
49
/
/
je
205e6
<
_sk_load_a8_dst_sse41
+
0x72
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
205d0
<
_sk_load_a8_dst_sse41
+
0x5c
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
204
/
/
jne
2058d
<
_sk_load_a8_dst_sse41
+
0x19
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
102
15
56
49
237
/
/
pmovzxbd
%
xmm5
%
xmm5
.
byte
102
15
58
14
229
15
/
/
pblendw
0xf
%
xmm5
%
xmm4
.
byte
235
167
/
/
jmp
2058d
<
_sk_load_a8_dst_sse41
+
0x19
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
156
/
/
jmp
2058d
<
_sk_load_a8_dst_sse41
+
0x19
>
HIDDEN
_sk_gather_a8_sse41
.
globl
_sk_gather_a8_sse41
FUNCTION
(
_sk_gather_a8_sse41
)
_sk_gather_a8_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
194
/
/
movq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
102
65
15
110
195
/
/
movd
%
r11d
%
xmm0
.
byte
102
65
15
58
32
194
1
/
/
pinsrb
0x1
%
r10d
%
xmm0
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
102
65
15
58
32
193
2
/
/
pinsrb
0x2
%
r9d
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
102
15
58
32
192
3
/
/
pinsrb
0x3
%
eax
%
xmm0
.
byte
102
15
56
49
192
/
/
pmovzxbd
%
xmm0
%
xmm0
.
byte
15
91
216
/
/
cvtdq2ps
%
xmm0
%
xmm3
.
byte
15
89
29
130
202
1
0
/
/
mulps
0x1ca82
(
%
rip
)
%
xmm3
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_a8_sse41
.
globl
_sk_store_a8_sse41
FUNCTION
(
_sk_store_a8_sse41
)
_sk_store_a8_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
68
15
93
5
71
200
1
0
/
/
minps
0x1c847
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
89
5
31
201
1
0
/
/
mulps
0x1c91f
(
%
rip
)
%
xmm8
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
68
15
56
43
192
/
/
packusdw
%
xmm0
%
xmm8
.
byte
102
68
15
103
192
/
/
packuswb
%
xmm0
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
13
/
/
jne
206f3
<
_sk_store_a8_sse41
+
0x47
>
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
65
137
4
16
/
/
mov
%
eax
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
56
49
192
/
/
pmovzxbd
%
xmm8
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
20726
<
_sk_store_a8_sse41
+
0x7a
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
20712
<
_sk_store_a8_sse41
+
0x66
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
230
/
/
jne
206ef
<
_sk_store_a8_sse41
+
0x43
>
.
byte
102
69
15
58
20
68
16
2
8
/
/
pextrb
0x8
%
xmm8
0x2
(
%
r8
%
rdx
1
)
.
byte
102
68
15
56
0
5
4
204
1
0
/
/
pshufb
0x1cc04
(
%
rip
)
%
xmm8
#
3d320
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10d4
>
.
byte
102
69
15
58
21
4
16
0
/
/
pextrw
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
201
/
/
jmp
206ef
<
_sk_store_a8_sse41
+
0x43
>
.
byte
102
69
15
58
20
4
16
0
/
/
pextrb
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
191
/
/
jmp
206ef
<
_sk_store_a8_sse41
+
0x43
>
HIDDEN
_sk_load_g8_sse41
.
globl
_sk_load_g8_sse41
FUNCTION
(
_sk_load_g8_sse41
)
_sk_load_g8_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
42
/
/
jne
2076c
<
_sk_load_g8_sse41
+
0x3c
>
.
byte
102
65
15
56
49
4
16
/
/
pmovzxbd
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
102
15
219
5
111
200
1
0
/
/
pand
0x1c86f
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
197
201
1
0
/
/
mulps
0x1c9c5
(
%
rip
)
%
xmm0
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
172
199
1
0
/
/
movaps
0x1c7ac
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
49
/
/
je
207a5
<
_sk_load_g8_sse41
+
0x75
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
2078f
<
_sk_load_g8_sse41
+
0x5f
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
201
/
/
jne
20749
<
_sk_load_g8_sse41
+
0x19
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
200
/
/
movd
%
eax
%
xmm1
.
byte
102
15
56
49
201
/
/
pmovzxbd
%
xmm1
%
xmm1
.
byte
102
15
58
14
193
15
/
/
pblendw
0xf
%
xmm1
%
xmm0
.
byte
235
164
/
/
jmp
20749
<
_sk_load_g8_sse41
+
0x19
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
153
/
/
jmp
20749
<
_sk_load_g8_sse41
+
0x19
>
HIDDEN
_sk_load_g8_dst_sse41
.
globl
_sk_load_g8_dst_sse41
FUNCTION
(
_sk_load_g8_dst_sse41
)
_sk_load_g8_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
42
/
/
jne
207ec
<
_sk_load_g8_dst_sse41
+
0x3c
>
.
byte
102
65
15
56
49
36
16
/
/
pmovzxbd
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
102
15
219
37
239
199
1
0
/
/
pand
0x1c7ef
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
37
69
201
1
0
/
/
mulps
0x1c945
(
%
rip
)
%
xmm4
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
44
199
1
0
/
/
movaps
0x1c72c
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
49
/
/
je
20825
<
_sk_load_g8_dst_sse41
+
0x75
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
2080f
<
_sk_load_g8_dst_sse41
+
0x5f
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
201
/
/
jne
207c9
<
_sk_load_g8_dst_sse41
+
0x19
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
102
15
56
49
237
/
/
pmovzxbd
%
xmm5
%
xmm5
.
byte
102
15
58
14
229
15
/
/
pblendw
0xf
%
xmm5
%
xmm4
.
byte
235
164
/
/
jmp
207c9
<
_sk_load_g8_dst_sse41
+
0x19
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
153
/
/
jmp
207c9
<
_sk_load_g8_dst_sse41
+
0x19
>
HIDDEN
_sk_gather_g8_sse41
.
globl
_sk_gather_g8_sse41
FUNCTION
(
_sk_gather_g8_sse41
)
_sk_gather_g8_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
194
/
/
movq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
102
65
15
110
195
/
/
movd
%
r11d
%
xmm0
.
byte
102
65
15
58
32
194
1
/
/
pinsrb
0x1
%
r10d
%
xmm0
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
102
65
15
58
32
193
2
/
/
pinsrb
0x2
%
r9d
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
102
15
58
32
192
3
/
/
pinsrb
0x3
%
eax
%
xmm0
.
byte
102
15
56
49
192
/
/
pmovzxbd
%
xmm0
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
67
200
1
0
/
/
mulps
0x1c843
(
%
rip
)
%
xmm0
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
42
198
1
0
/
/
movaps
0x1c62a
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_sse41
.
globl
_sk_load_565_sse41
FUNCTION
(
_sk_load_565_sse41
)
_sk_load_565_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
80
/
/
jne
20953
<
_sk_load_565_sse41
+
0x65
>
.
byte
102
65
15
56
51
20
80
/
/
pmovzxwd
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
102
15
111
5
30
200
1
0
/
/
movdqa
0x1c81e
(
%
rip
)
%
xmm0
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
32
200
1
0
/
/
mulps
0x1c820
(
%
rip
)
%
xmm0
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
13
40
200
1
0
/
/
movdqa
0x1c828
(
%
rip
)
%
xmm1
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
42
200
1
0
/
/
mulps
0x1c82a
(
%
rip
)
%
xmm1
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
15
219
21
50
200
1
0
/
/
pand
0x1c832
(
%
rip
)
%
xmm2
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
56
200
1
0
/
/
mulps
0x1c838
(
%
rip
)
%
xmm2
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
191
197
1
0
/
/
movaps
0x1c5bf
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
46
/
/
je
20989
<
_sk_load_565_sse41
+
0x9b
>
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
20976
<
_sk_load_565_sse41
+
0x88
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
163
/
/
jne
2090a
<
_sk_load_565_sse41
+
0x1c
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
208
69
/
/
pshufd
0x45
%
xmm0
%
xmm2
.
byte
102
65
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
15
56
51
192
/
/
pmovzxwd
%
xmm0
%
xmm0
.
byte
102
15
58
14
208
15
/
/
pblendw
0xf
%
xmm0
%
xmm2
.
byte
235
129
/
/
jmp
2090a
<
_sk_load_565_sse41
+
0x1c
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
233
115
255
255
255
/
/
jmpq
2090a
<
_sk_load_565_sse41
+
0x1c
>
HIDDEN
_sk_load_565_dst_sse41
.
globl
_sk_load_565_dst_sse41
FUNCTION
(
_sk_load_565_dst_sse41
)
_sk_load_565_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
80
/
/
jne
209fc
<
_sk_load_565_dst_sse41
+
0x65
>
.
byte
102
65
15
56
51
52
80
/
/
pmovzxwd
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
102
15
111
37
117
199
1
0
/
/
movdqa
0x1c775
(
%
rip
)
%
xmm4
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
37
119
199
1
0
/
/
mulps
0x1c777
(
%
rip
)
%
xmm4
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
45
127
199
1
0
/
/
movdqa
0x1c77f
(
%
rip
)
%
xmm5
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
45
129
199
1
0
/
/
mulps
0x1c781
(
%
rip
)
%
xmm5
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
15
219
53
137
199
1
0
/
/
pand
0x1c789
(
%
rip
)
%
xmm6
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
53
143
199
1
0
/
/
mulps
0x1c78f
(
%
rip
)
%
xmm6
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
22
197
1
0
/
/
movaps
0x1c516
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
46
/
/
je
20a32
<
_sk_load_565_dst_sse41
+
0x9b
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
20a1f
<
_sk_load_565_dst_sse41
+
0x88
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
163
/
/
jne
209b3
<
_sk_load_565_dst_sse41
+
0x1c
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
244
69
/
/
pshufd
0x45
%
xmm4
%
xmm6
.
byte
102
65
15
110
36
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
15
56
51
228
/
/
pmovzxwd
%
xmm4
%
xmm4
.
byte
102
15
58
14
244
15
/
/
pblendw
0xf
%
xmm4
%
xmm6
.
byte
235
129
/
/
jmp
209b3
<
_sk_load_565_dst_sse41
+
0x1c
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
240
/
/
movd
%
eax
%
xmm6
.
byte
233
115
255
255
255
/
/
jmpq
209b3
<
_sk_load_565_dst_sse41
+
0x1c
>
HIDDEN
_sk_gather_565_sse41
.
globl
_sk_gather_565_sse41
FUNCTION
(
_sk_gather_565_sse41
)
_sk_gather_565_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
194
/
/
movq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
102
65
15
110
195
/
/
movd
%
r11d
%
xmm0
.
byte
102
65
15
196
194
1
/
/
pinsrw
0x1
%
r10d
%
xmm0
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
102
65
15
196
193
2
/
/
pinsrw
0x2
%
r9d
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
102
15
196
192
3
/
/
pinsrw
0x3
%
eax
%
xmm0
.
byte
102
15
56
51
208
/
/
pmovzxwd
%
xmm0
%
xmm2
.
byte
102
15
111
5
72
198
1
0
/
/
movdqa
0x1c648
(
%
rip
)
%
xmm0
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
74
198
1
0
/
/
mulps
0x1c64a
(
%
rip
)
%
xmm0
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
13
82
198
1
0
/
/
movdqa
0x1c652
(
%
rip
)
%
xmm1
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
84
198
1
0
/
/
mulps
0x1c654
(
%
rip
)
%
xmm1
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
15
219
21
92
198
1
0
/
/
pand
0x1c65c
(
%
rip
)
%
xmm2
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
98
198
1
0
/
/
mulps
0x1c662
(
%
rip
)
%
xmm2
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
233
195
1
0
/
/
movaps
0x1c3e9
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_565_sse41
.
globl
_sk_store_565_sse41
FUNCTION
(
_sk_store_565_sse41
)
_sk_store_565_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
195
195
1
0
/
/
movaps
0x1c3c3
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
215
199
1
0
/
/
movaps
0x1c7d7
(
%
rip
)
%
xmm11
#
3d330
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10e4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
11
/
/
pslld
0xb
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
68
15
89
37
196
199
1
0
/
/
mulps
0x1c7c4
(
%
rip
)
%
xmm12
#
3d340
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10f4
>
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
5
/
/
pslld
0x5
%
xmm12
.
byte
68
15
95
194
/
/
maxps
%
xmm2
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
69
15
86
193
/
/
orpd
%
xmm9
%
xmm8
.
byte
102
69
15
86
196
/
/
orpd
%
xmm12
%
xmm8
.
byte
102
68
15
56
43
192
/
/
packusdw
%
xmm0
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
20bb7
<
_sk_store_565_sse41
+
0x8e
>
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
56
51
192
/
/
pmovzxwd
%
xmm8
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
31
/
/
je
20be4
<
_sk_store_565_sse41
+
0xbb
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
20bd6
<
_sk_store_565_sse41
+
0xad
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
230
/
/
jne
20bb3
<
_sk_store_565_sse41
+
0x8a
>
.
byte
102
69
15
58
21
68
80
4
4
/
/
pextrw
0x4
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
242
69
15
112
192
232
/
/
pshuflw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
207
/
/
jmp
20bb3
<
_sk_store_565_sse41
+
0x8a
>
.
byte
102
69
15
58
21
4
80
0
/
/
pextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
197
/
/
jmp
20bb3
<
_sk_store_565_sse41
+
0x8a
>
HIDDEN
_sk_load_4444_sse41
.
globl
_sk_load_4444_sse41
FUNCTION
(
_sk_load_4444_sse41
)
_sk_load_4444_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
95
/
/
jne
20c62
<
_sk_load_4444_sse41
+
0x74
>
.
byte
102
65
15
56
51
28
80
/
/
pmovzxwd
(
%
r8
%
rdx
2
)
%
xmm3
.
byte
102
15
111
5
62
199
1
0
/
/
movdqa
0x1c73e
(
%
rip
)
%
xmm0
#
3d350
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1104
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
64
199
1
0
/
/
mulps
0x1c740
(
%
rip
)
%
xmm0
#
3d360
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1114
>
.
byte
102
15
111
13
72
199
1
0
/
/
movdqa
0x1c748
(
%
rip
)
%
xmm1
#
3d370
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1124
>
.
byte
102
15
219
203
/
/
pand
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
74
199
1
0
/
/
mulps
0x1c74a
(
%
rip
)
%
xmm1
#
3d380
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1134
>
.
byte
102
15
111
21
82
199
1
0
/
/
movdqa
0x1c752
(
%
rip
)
%
xmm2
#
3d390
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1144
>
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
84
199
1
0
/
/
mulps
0x1c754
(
%
rip
)
%
xmm2
#
3d3a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1154
>
.
byte
102
15
219
29
92
199
1
0
/
/
pand
0x1c75c
(
%
rip
)
%
xmm3
#
3d3b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1164
>
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
29
98
199
1
0
/
/
mulps
0x1c762
(
%
rip
)
%
xmm3
#
3d3c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1174
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
49
/
/
je
20c9b
<
_sk_load_4444_sse41
+
0xad
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
20c85
<
_sk_load_4444_sse41
+
0x97
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
148
/
/
jne
20c0a
<
_sk_load_4444_sse41
+
0x1c
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
65
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
15
56
51
192
/
/
pmovzxwd
%
xmm0
%
xmm0
.
byte
102
15
58
14
216
15
/
/
pblendw
0xf
%
xmm0
%
xmm3
.
byte
233
111
255
255
255
/
/
jmpq
20c0a
<
_sk_load_4444_sse41
+
0x1c
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
233
97
255
255
255
/
/
jmpq
20c0a
<
_sk_load_4444_sse41
+
0x1c
>
HIDDEN
_sk_load_4444_dst_sse41
.
globl
_sk_load_4444_dst_sse41
FUNCTION
(
_sk_load_4444_dst_sse41
)
_sk_load_4444_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
95
/
/
jne
20d1d
<
_sk_load_4444_dst_sse41
+
0x74
>
.
byte
102
65
15
56
51
60
80
/
/
pmovzxwd
(
%
r8
%
rdx
2
)
%
xmm7
.
byte
102
15
111
37
131
198
1
0
/
/
movdqa
0x1c683
(
%
rip
)
%
xmm4
#
3d350
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1104
>
.
byte
102
15
219
231
/
/
pand
%
xmm7
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
37
133
198
1
0
/
/
mulps
0x1c685
(
%
rip
)
%
xmm4
#
3d360
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1114
>
.
byte
102
15
111
45
141
198
1
0
/
/
movdqa
0x1c68d
(
%
rip
)
%
xmm5
#
3d370
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1124
>
.
byte
102
15
219
239
/
/
pand
%
xmm7
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
45
143
198
1
0
/
/
mulps
0x1c68f
(
%
rip
)
%
xmm5
#
3d380
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1134
>
.
byte
102
15
111
53
151
198
1
0
/
/
movdqa
0x1c697
(
%
rip
)
%
xmm6
#
3d390
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1144
>
.
byte
102
15
219
247
/
/
pand
%
xmm7
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
53
153
198
1
0
/
/
mulps
0x1c699
(
%
rip
)
%
xmm6
#
3d3a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1154
>
.
byte
102
15
219
61
161
198
1
0
/
/
pand
0x1c6a1
(
%
rip
)
%
xmm7
#
3d3b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1164
>
.
byte
15
91
255
/
/
cvtdq2ps
%
xmm7
%
xmm7
.
byte
15
89
61
167
198
1
0
/
/
mulps
0x1c6a7
(
%
rip
)
%
xmm7
#
3d3c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1174
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
49
/
/
je
20d56
<
_sk_load_4444_dst_sse41
+
0xad
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
20d40
<
_sk_load_4444_dst_sse41
+
0x97
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
148
/
/
jne
20cc5
<
_sk_load_4444_dst_sse41
+
0x1c
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
252
69
/
/
pshufd
0x45
%
xmm4
%
xmm7
.
byte
102
65
15
110
36
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
15
56
51
228
/
/
pmovzxwd
%
xmm4
%
xmm4
.
byte
102
15
58
14
252
15
/
/
pblendw
0xf
%
xmm4
%
xmm7
.
byte
233
111
255
255
255
/
/
jmpq
20cc5
<
_sk_load_4444_dst_sse41
+
0x1c
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
248
/
/
movd
%
eax
%
xmm7
.
byte
233
97
255
255
255
/
/
jmpq
20cc5
<
_sk_load_4444_dst_sse41
+
0x1c
>
HIDDEN
_sk_gather_4444_sse41
.
globl
_sk_gather_4444_sse41
FUNCTION
(
_sk_gather_4444_sse41
)
_sk_gather_4444_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
194
/
/
movq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
102
65
15
110
195
/
/
movd
%
r11d
%
xmm0
.
byte
102
65
15
196
194
1
/
/
pinsrw
0x1
%
r10d
%
xmm0
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
102
65
15
196
193
2
/
/
pinsrw
0x2
%
r9d
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
102
15
196
192
3
/
/
pinsrw
0x3
%
eax
%
xmm0
.
byte
102
15
56
51
216
/
/
pmovzxwd
%
xmm0
%
xmm3
.
byte
102
15
111
5
68
197
1
0
/
/
movdqa
0x1c544
(
%
rip
)
%
xmm0
#
3d350
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1104
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
70
197
1
0
/
/
mulps
0x1c546
(
%
rip
)
%
xmm0
#
3d360
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1114
>
.
byte
102
15
111
13
78
197
1
0
/
/
movdqa
0x1c54e
(
%
rip
)
%
xmm1
#
3d370
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1124
>
.
byte
102
15
219
203
/
/
pand
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
80
197
1
0
/
/
mulps
0x1c550
(
%
rip
)
%
xmm1
#
3d380
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1134
>
.
byte
102
15
111
21
88
197
1
0
/
/
movdqa
0x1c558
(
%
rip
)
%
xmm2
#
3d390
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1144
>
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
90
197
1
0
/
/
mulps
0x1c55a
(
%
rip
)
%
xmm2
#
3d3a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1154
>
.
byte
102
15
219
29
98
197
1
0
/
/
pand
0x1c562
(
%
rip
)
%
xmm3
#
3d3b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1164
>
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
29
104
197
1
0
/
/
mulps
0x1c568
(
%
rip
)
%
xmm3
#
3d3c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1174
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_4444_sse41
.
globl
_sk_store_4444_sse41
FUNCTION
(
_sk_store_4444_sse41
)
_sk_store_4444_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
144
192
1
0
/
/
movaps
0x1c090
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
68
197
1
0
/
/
movaps
0x1c544
(
%
rip
)
%
xmm11
#
3d3d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1184
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
12
/
/
pslld
0xc
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
8
/
/
pslld
0x8
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
4
/
/
pslld
0x4
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
69
15
86
193
/
/
orpd
%
xmm9
%
xmm8
.
byte
102
69
15
86
196
/
/
orpd
%
xmm12
%
xmm8
.
byte
102
68
15
56
43
192
/
/
packusdw
%
xmm0
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
20f07
<
_sk_store_4444_sse41
+
0xab
>
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
56
51
192
/
/
pmovzxwd
%
xmm8
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
31
/
/
je
20f34
<
_sk_store_4444_sse41
+
0xd8
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
20f26
<
_sk_store_4444_sse41
+
0xca
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
230
/
/
jne
20f03
<
_sk_store_4444_sse41
+
0xa7
>
.
byte
102
69
15
58
21
68
80
4
4
/
/
pextrw
0x4
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
242
69
15
112
192
232
/
/
pshuflw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
207
/
/
jmp
20f03
<
_sk_store_4444_sse41
+
0xa7
>
.
byte
102
69
15
58
21
4
80
0
/
/
pextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
197
/
/
jmp
20f03
<
_sk_store_4444_sse41
+
0xa7
>
HIDDEN
_sk_load_8888_sse41
.
globl
_sk_load_8888_sse41
FUNCTION
(
_sk_load_8888_sse41
)
_sk_load_8888_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
89
/
/
jne
20fad
<
_sk_load_8888_sse41
+
0x6f
>
.
byte
243
65
15
111
28
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
102
15
111
5
94
192
1
0
/
/
movdqa
0x1c05e
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
175
193
1
0
/
/
movaps
0x1c1af
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
56
0
13
78
192
1
0
/
/
pshufb
0x1c04e
(
%
rip
)
%
xmm1
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
56
0
21
74
192
1
0
/
/
pshufb
0x1c04a
(
%
rip
)
%
xmm2
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
41
/
/
je
20fde
<
_sk_load_8888_sse41
+
0xa0
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
20fcd
<
_sk_load_8888_sse41
+
0x8f
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
153
/
/
jne
20f5a
<
_sk_load_8888_sse41
+
0x1c
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
243
65
15
126
4
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
58
14
216
15
/
/
pblendw
0xf
%
xmm0
%
xmm3
.
byte
233
124
255
255
255
/
/
jmpq
20f5a
<
_sk_load_8888_sse41
+
0x1c
>
.
byte
102
65
15
110
28
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
113
255
255
255
/
/
jmpq
20f5a
<
_sk_load_8888_sse41
+
0x1c
>
HIDDEN
_sk_load_8888_dst_sse41
.
globl
_sk_load_8888_dst_sse41
FUNCTION
(
_sk_load_8888_dst_sse41
)
_sk_load_8888_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
89
/
/
jne
21058
<
_sk_load_8888_dst_sse41
+
0x6f
>
.
byte
243
65
15
111
60
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
15
111
37
179
191
1
0
/
/
movdqa
0x1bfb3
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
231
/
/
pand
%
xmm7
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
68
15
40
5
4
193
1
0
/
/
movaps
0x1c104
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
224
/
/
mulps
%
xmm8
%
xmm4
.
byte
102
15
111
239
/
/
movdqa
%
xmm7
%
xmm5
.
byte
102
15
56
0
45
163
191
1
0
/
/
pshufb
0x1bfa3
(
%
rip
)
%
xmm5
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
102
15
111
247
/
/
movdqa
%
xmm7
%
xmm6
.
byte
102
15
56
0
53
159
191
1
0
/
/
pshufb
0x1bf9f
(
%
rip
)
%
xmm6
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
65
15
89
240
/
/
mulps
%
xmm8
%
xmm6
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
15
91
255
/
/
cvtdq2ps
%
xmm7
%
xmm7
.
byte
65
15
89
248
/
/
mulps
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
41
/
/
je
21089
<
_sk_load_8888_dst_sse41
+
0xa0
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
21078
<
_sk_load_8888_dst_sse41
+
0x8f
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
153
/
/
jne
21005
<
_sk_load_8888_dst_sse41
+
0x1c
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
112
252
69
/
/
pshufd
0x45
%
xmm4
%
xmm7
.
byte
243
65
15
126
36
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
58
14
252
15
/
/
pblendw
0xf
%
xmm4
%
xmm7
.
byte
233
124
255
255
255
/
/
jmpq
21005
<
_sk_load_8888_dst_sse41
+
0x1c
>
.
byte
102
65
15
110
60
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
113
255
255
255
/
/
jmpq
21005
<
_sk_load_8888_dst_sse41
+
0x1c
>
HIDDEN
_sk_gather_8888_sse41
.
globl
_sk_gather_8888_sse41
FUNCTION
(
_sk_gather_8888_sse41
)
_sk_gather_8888_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
58
22
194
1
/
/
pextrq
0x1
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
67
15
110
28
136
/
/
movd
(
%
r8
%
r9
4
)
%
xmm3
.
byte
102
65
15
58
34
28
128
1
/
/
pinsrd
0x1
(
%
r8
%
rax
4
)
%
xmm3
.
byte
102
67
15
58
34
28
152
2
/
/
pinsrd
0x2
(
%
r8
%
r11
4
)
%
xmm3
.
byte
102
67
15
58
34
28
144
3
/
/
pinsrd
0x3
(
%
r8
%
r10
4
)
%
xmm3
.
byte
102
15
111
5
149
190
1
0
/
/
movdqa
0x1be95
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
230
191
1
0
/
/
movaps
0x1bfe6
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
56
0
13
133
190
1
0
/
/
pshufb
0x1be85
(
%
rip
)
%
xmm1
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
56
0
21
129
190
1
0
/
/
pshufb
0x1be81
(
%
rip
)
%
xmm2
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_8888_sse41
.
globl
_sk_store_8888_sse41
FUNCTION
(
_sk_store_8888_sse41
)
_sk_store_8888_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
117
189
1
0
/
/
movaps
0x1bd75
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
73
190
1
0
/
/
movaps
0x1be49
(
%
rip
)
%
xmm11
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
8
/
/
pslld
0x8
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
87
201
/
/
xorpd
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
24
/
/
pslld
0x18
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
2121c
<
_sk_store_8888_sse41
+
0xa6
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
25
/
/
je
2123d
<
_sk_store_8888_sse41
+
0xc7
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
21235
<
_sk_store_8888_sse41
+
0xbf
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
236
/
/
jne
21218
<
_sk_store_8888_sse41
+
0xa2
>
.
byte
102
69
15
58
22
68
144
8
2
/
/
pextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
219
/
/
jmp
21218
<
_sk_store_8888_sse41
+
0xa2
>
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
211
/
/
jmp
21218
<
_sk_store_8888_sse41
+
0xa2
>
HIDDEN
_sk_load_bgra_sse41
.
globl
_sk_load_bgra_sse41
FUNCTION
(
_sk_load_bgra_sse41
)
_sk_load_bgra_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
89
/
/
jne
212b4
<
_sk_load_bgra_sse41
+
0x6f
>
.
byte
243
65
15
111
28
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
102
15
111
5
87
189
1
0
/
/
movdqa
0x1bd57
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
208
/
/
cvtdq2ps
%
xmm0
%
xmm2
.
byte
68
15
40
5
168
190
1
0
/
/
movaps
0x1bea8
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
56
0
5
71
189
1
0
/
/
pshufb
0x1bd47
(
%
rip
)
%
xmm0
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
56
0
5
67
189
1
0
/
/
pshufb
0x1bd43
(
%
rip
)
%
xmm0
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
41
/
/
je
212e5
<
_sk_load_bgra_sse41
+
0xa0
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
212d4
<
_sk_load_bgra_sse41
+
0x8f
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
153
/
/
jne
21261
<
_sk_load_bgra_sse41
+
0x1c
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
243
65
15
126
4
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
58
14
216
15
/
/
pblendw
0xf
%
xmm0
%
xmm3
.
byte
233
124
255
255
255
/
/
jmpq
21261
<
_sk_load_bgra_sse41
+
0x1c
>
.
byte
102
65
15
110
28
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
233
113
255
255
255
/
/
jmpq
21261
<
_sk_load_bgra_sse41
+
0x1c
>
HIDDEN
_sk_load_bgra_dst_sse41
.
globl
_sk_load_bgra_dst_sse41
FUNCTION
(
_sk_load_bgra_dst_sse41
)
_sk_load_bgra_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
89
/
/
jne
2135f
<
_sk_load_bgra_dst_sse41
+
0x6f
>
.
byte
243
65
15
111
60
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
15
111
37
172
188
1
0
/
/
movdqa
0x1bcac
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
231
/
/
pand
%
xmm7
%
xmm4
.
byte
15
91
244
/
/
cvtdq2ps
%
xmm4
%
xmm6
.
byte
68
15
40
5
253
189
1
0
/
/
movaps
0x1bdfd
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
240
/
/
mulps
%
xmm8
%
xmm6
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
56
0
37
156
188
1
0
/
/
pshufb
0x1bc9c
(
%
rip
)
%
xmm4
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
236
/
/
cvtdq2ps
%
xmm4
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
56
0
37
152
188
1
0
/
/
pshufb
0x1bc98
(
%
rip
)
%
xmm4
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
65
15
89
224
/
/
mulps
%
xmm8
%
xmm4
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
15
91
255
/
/
cvtdq2ps
%
xmm7
%
xmm7
.
byte
65
15
89
248
/
/
mulps
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
41
/
/
je
21390
<
_sk_load_bgra_dst_sse41
+
0xa0
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
2137f
<
_sk_load_bgra_dst_sse41
+
0x8f
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
153
/
/
jne
2130c
<
_sk_load_bgra_dst_sse41
+
0x1c
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
112
252
69
/
/
pshufd
0x45
%
xmm4
%
xmm7
.
byte
243
65
15
126
36
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
58
14
252
15
/
/
pblendw
0xf
%
xmm4
%
xmm7
.
byte
233
124
255
255
255
/
/
jmpq
2130c
<
_sk_load_bgra_dst_sse41
+
0x1c
>
.
byte
102
65
15
110
60
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
233
113
255
255
255
/
/
jmpq
2130c
<
_sk_load_bgra_dst_sse41
+
0x1c
>
HIDDEN
_sk_gather_bgra_sse41
.
globl
_sk_gather_bgra_sse41
FUNCTION
(
_sk_gather_bgra_sse41
)
_sk_gather_bgra_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
58
22
194
1
/
/
pextrq
0x1
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
67
15
110
28
136
/
/
movd
(
%
r8
%
r9
4
)
%
xmm3
.
byte
102
65
15
58
34
28
128
1
/
/
pinsrd
0x1
(
%
r8
%
rax
4
)
%
xmm3
.
byte
102
67
15
58
34
28
152
2
/
/
pinsrd
0x2
(
%
r8
%
r11
4
)
%
xmm3
.
byte
102
67
15
58
34
28
144
3
/
/
pinsrd
0x3
(
%
r8
%
r10
4
)
%
xmm3
.
byte
102
15
111
5
142
187
1
0
/
/
movdqa
0x1bb8e
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
208
/
/
cvtdq2ps
%
xmm0
%
xmm2
.
byte
68
15
40
5
223
188
1
0
/
/
movaps
0x1bcdf
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
56
0
5
126
187
1
0
/
/
pshufb
0x1bb7e
(
%
rip
)
%
xmm0
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
56
0
5
122
187
1
0
/
/
pshufb
0x1bb7a
(
%
rip
)
%
xmm0
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_bgra_sse41
.
globl
_sk_store_bgra_sse41
FUNCTION
(
_sk_store_bgra_sse41
)
_sk_store_bgra_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
68
15
40
21
110
186
1
0
/
/
movaps
0x1ba6e
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
66
187
1
0
/
/
movaps
0x1bb42
(
%
rip
)
%
xmm11
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
8
/
/
pslld
0x8
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
87
201
/
/
xorpd
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
24
/
/
pslld
0x18
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
21523
<
_sk_store_bgra_sse41
+
0xa6
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
25
/
/
je
21544
<
_sk_store_bgra_sse41
+
0xc7
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
2153c
<
_sk_store_bgra_sse41
+
0xbf
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
236
/
/
jne
2151f
<
_sk_store_bgra_sse41
+
0xa2
>
.
byte
102
69
15
58
22
68
144
8
2
/
/
pextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
219
/
/
jmp
2151f
<
_sk_store_bgra_sse41
+
0xa2
>
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
211
/
/
jmp
2151f
<
_sk_store_bgra_sse41
+
0xa2
>
HIDDEN
_sk_load_1010102_sse41
.
globl
_sk_load_1010102_sse41
FUNCTION
(
_sk_load_1010102_sse41
)
_sk_load_1010102_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
215c7
<
_sk_load_1010102_sse41
+
0x7b
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
21
112
190
1
0
/
/
movdqa
0x1be70
(
%
rip
)
%
xmm2
#
3d3e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1194
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
108
190
1
0
/
/
movaps
0x1be6c
(
%
rip
)
%
xmm8
#
3d3f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11a4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
10
/
/
psrld
0xa
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
20
/
/
psrld
0x14
%
xmm3
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
15
91
211
/
/
cvtdq2ps
%
xmm3
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
114
209
30
/
/
psrld
0x1e
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
15
89
29
45
187
1
0
/
/
mulps
0x1bb2d
(
%
rip
)
%
xmm3
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
44
/
/
je
215fb
<
_sk_load_1010102_sse41
+
0xaf
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
215e9
<
_sk_load_1010102_sse41
+
0x9d
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
140
/
/
jne
21568
<
_sk_load_1010102_sse41
+
0x1c
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
200
69
/
/
pshufd
0x45
%
xmm0
%
xmm9
.
byte
243
65
15
126
4
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
58
14
200
15
/
/
pblendw
0xf
%
xmm0
%
xmm9
.
byte
233
109
255
255
255
/
/
jmpq
21568
<
_sk_load_1010102_sse41
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
98
255
255
255
/
/
jmpq
21568
<
_sk_load_1010102_sse41
+
0x1c
>
HIDDEN
_sk_load_1010102_dst_sse41
.
globl
_sk_load_1010102_dst_sse41
FUNCTION
(
_sk_load_1010102_dst_sse41
)
_sk_load_1010102_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
21681
<
_sk_load_1010102_dst_sse41
+
0x7b
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
53
182
189
1
0
/
/
movdqa
0x1bdb6
(
%
rip
)
%
xmm6
#
3d3e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1194
>
.
byte
102
65
15
111
225
/
/
movdqa
%
xmm9
%
xmm4
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
68
15
40
5
178
189
1
0
/
/
movaps
0x1bdb2
(
%
rip
)
%
xmm8
#
3d3f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11a4
>
.
byte
65
15
89
224
/
/
mulps
%
xmm8
%
xmm4
.
byte
102
65
15
111
233
/
/
movdqa
%
xmm9
%
xmm5
.
byte
102
15
114
213
10
/
/
psrld
0xa
%
xmm5
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
102
65
15
111
249
/
/
movdqa
%
xmm9
%
xmm7
.
byte
102
15
114
215
20
/
/
psrld
0x14
%
xmm7
.
byte
102
15
219
254
/
/
pand
%
xmm6
%
xmm7
.
byte
15
91
247
/
/
cvtdq2ps
%
xmm7
%
xmm6
.
byte
65
15
89
240
/
/
mulps
%
xmm8
%
xmm6
.
byte
102
65
15
114
209
30
/
/
psrld
0x1e
%
xmm9
.
byte
65
15
91
249
/
/
cvtdq2ps
%
xmm9
%
xmm7
.
byte
15
89
61
115
186
1
0
/
/
mulps
0x1ba73
(
%
rip
)
%
xmm7
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
44
/
/
je
216b5
<
_sk_load_1010102_dst_sse41
+
0xaf
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
216a3
<
_sk_load_1010102_dst_sse41
+
0x9d
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
140
/
/
jne
21622
<
_sk_load_1010102_dst_sse41
+
0x1c
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
204
69
/
/
pshufd
0x45
%
xmm4
%
xmm9
.
byte
243
65
15
126
36
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
58
14
204
15
/
/
pblendw
0xf
%
xmm4
%
xmm9
.
byte
233
109
255
255
255
/
/
jmpq
21622
<
_sk_load_1010102_dst_sse41
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
98
255
255
255
/
/
jmpq
21622
<
_sk_load_1010102_dst_sse41
+
0x1c
>
HIDDEN
_sk_gather_1010102_sse41
.
globl
_sk_gather_1010102_sse41
FUNCTION
(
_sk_gather_1010102_sse41
)
_sk_gather_1010102_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
58
22
194
1
/
/
pextrq
0x1
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
71
15
110
12
136
/
/
movd
(
%
r8
%
r9
4
)
%
xmm9
.
byte
102
69
15
58
34
12
128
1
/
/
pinsrd
0x1
(
%
r8
%
rax
4
)
%
xmm9
.
byte
102
71
15
58
34
12
152
2
/
/
pinsrd
0x2
(
%
r8
%
r11
4
)
%
xmm9
.
byte
102
71
15
58
34
12
144
3
/
/
pinsrd
0x3
(
%
r8
%
r10
4
)
%
xmm9
.
byte
102
15
111
21
137
188
1
0
/
/
movdqa
0x1bc89
(
%
rip
)
%
xmm2
#
3d3e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1194
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
133
188
1
0
/
/
movaps
0x1bc85
(
%
rip
)
%
xmm8
#
3d3f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11a4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
10
/
/
psrld
0xa
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
20
/
/
psrld
0x14
%
xmm3
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
15
91
211
/
/
cvtdq2ps
%
xmm3
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
114
209
30
/
/
psrld
0x1e
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
15
89
29
70
185
1
0
/
/
mulps
0x1b946
(
%
rip
)
%
xmm3
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_1010102_sse41
.
globl
_sk_store_1010102_sse41
FUNCTION
(
_sk_store_1010102_sse41
)
_sk_store_1010102_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
61
183
1
0
/
/
movaps
0x1b73d
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
33
188
1
0
/
/
movaps
0x1bc21
(
%
rip
)
%
xmm11
#
3d400
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11b4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
10
/
/
pslld
0xa
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
87
201
/
/
xorpd
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
20
/
/
pslld
0x14
%
xmm9
.
byte
102
69
15
235
204
/
/
por
%
xmm12
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
68
15
89
5
215
187
1
0
/
/
mulps
0x1bbd7
(
%
rip
)
%
xmm8
#
3d410
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11c4
>
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
30
/
/
pslld
0x1e
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
21858
<
_sk_store_1010102_sse41
+
0xaa
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
25
/
/
je
21879
<
_sk_store_1010102_sse41
+
0xcb
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
13
/
/
je
21871
<
_sk_store_1010102_sse41
+
0xc3
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
236
/
/
jne
21854
<
_sk_store_1010102_sse41
+
0xa6
>
.
byte
102
69
15
58
22
68
144
8
2
/
/
pextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
219
/
/
jmp
21854
<
_sk_store_1010102_sse41
+
0xa6
>
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
211
/
/
jmp
21854
<
_sk_store_1010102_sse41
+
0xa6
>
HIDDEN
_sk_load_f16_sse41
.
globl
_sk_load_f16_sse41
FUNCTION
(
_sk_load_f16_sse41
)
_sk_load_f16_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
68
1
0
0
/
/
jne
219df
<
_sk_load_f16_sse41
+
0x15e
>
.
byte
102
65
15
16
4
208
/
/
movupd
(
%
r8
%
rdx
8
)
%
xmm0
.
byte
243
65
15
111
76
208
16
/
/
movdqu
0x10
(
%
r8
%
rdx
8
)
%
xmm1
.
byte
102
68
15
40
200
/
/
movapd
%
xmm0
%
xmm9
.
byte
102
68
15
97
201
/
/
punpcklwd
%
xmm1
%
xmm9
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
69
15
111
225
/
/
movdqa
%
xmm9
%
xmm12
.
byte
102
68
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm12
.
byte
102
68
15
105
200
/
/
punpckhwd
%
xmm0
%
xmm9
.
byte
102
65
15
56
51
212
/
/
pmovzxwd
%
xmm12
%
xmm2
.
byte
102
68
15
111
5
76
187
1
0
/
/
movdqa
0x1bb4c
(
%
rip
)
%
xmm8
#
3d420
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11d4
>
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
65
15
219
200
/
/
pand
%
xmm8
%
xmm1
.
byte
102
68
15
111
21
74
187
1
0
/
/
movdqa
0x1bb4a
(
%
rip
)
%
xmm10
#
3d430
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11e4
>
.
byte
102
65
15
219
210
/
/
pand
%
xmm10
%
xmm2
.
byte
102
15
111
29
77
187
1
0
/
/
movdqa
0x1bb4d
(
%
rip
)
%
xmm3
#
3d440
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11f4
>
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
56
63
195
/
/
pmaxud
%
xmm3
%
xmm0
.
byte
102
15
118
194
/
/
pcmpeqd
%
xmm2
%
xmm0
.
byte
102
15
114
242
13
/
/
pslld
0xd
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
68
15
111
29
57
187
1
0
/
/
movdqa
0x1bb39
(
%
rip
)
%
xmm11
#
3d450
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1204
>
.
byte
102
65
15
254
211
/
/
paddd
%
xmm11
%
xmm2
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
102
65
15
112
204
238
/
/
pshufd
0xee
%
xmm12
%
xmm1
.
byte
102
15
56
51
209
/
/
pmovzxwd
%
xmm1
%
xmm2
.
byte
102
68
15
111
226
/
/
movdqa
%
xmm2
%
xmm12
.
byte
102
69
15
219
224
/
/
pand
%
xmm8
%
xmm12
.
byte
102
65
15
219
210
/
/
pand
%
xmm10
%
xmm2
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
56
63
203
/
/
pmaxud
%
xmm3
%
xmm1
.
byte
102
15
118
202
/
/
pcmpeqd
%
xmm2
%
xmm1
.
byte
102
15
114
242
13
/
/
pslld
0xd
%
xmm2
.
byte
102
65
15
235
212
/
/
por
%
xmm12
%
xmm2
.
byte
102
65
15
254
211
/
/
paddd
%
xmm11
%
xmm2
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
69
15
56
51
225
/
/
pmovzxwd
%
xmm9
%
xmm12
.
byte
102
69
15
111
236
/
/
movdqa
%
xmm12
%
xmm13
.
byte
102
69
15
219
232
/
/
pand
%
xmm8
%
xmm13
.
byte
102
69
15
219
226
/
/
pand
%
xmm10
%
xmm12
.
byte
102
65
15
114
245
16
/
/
pslld
0x10
%
xmm13
.
byte
102
65
15
111
212
/
/
movdqa
%
xmm12
%
xmm2
.
byte
102
15
56
63
211
/
/
pmaxud
%
xmm3
%
xmm2
.
byte
102
65
15
118
212
/
/
pcmpeqd
%
xmm12
%
xmm2
.
byte
102
65
15
114
244
13
/
/
pslld
0xd
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
102
69
15
254
227
/
/
paddd
%
xmm11
%
xmm12
.
byte
102
65
15
219
212
/
/
pand
%
xmm12
%
xmm2
.
byte
102
69
15
112
201
238
/
/
pshufd
0xee
%
xmm9
%
xmm9
.
byte
102
69
15
56
51
201
/
/
pmovzxwd
%
xmm9
%
xmm9
.
byte
102
69
15
219
193
/
/
pand
%
xmm9
%
xmm8
.
byte
102
69
15
219
202
/
/
pand
%
xmm10
%
xmm9
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
56
63
217
/
/
pmaxud
%
xmm9
%
xmm3
.
byte
102
65
15
118
217
/
/
pcmpeqd
%
xmm9
%
xmm3
.
byte
102
65
15
114
241
13
/
/
pslld
0xd
%
xmm9
.
byte
102
69
15
235
200
/
/
por
%
xmm8
%
xmm9
.
byte
102
69
15
254
203
/
/
paddd
%
xmm11
%
xmm9
.
byte
102
65
15
219
217
/
/
pand
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
65
15
16
4
208
/
/
movsd
(
%
r8
%
rdx
8
)
%
xmm0
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
219f8
<
_sk_load_f16_sse41
+
0x177
>
.
byte
243
15
126
192
/
/
movq
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
176
254
255
255
/
/
jmpq
218a8
<
_sk_load_f16_sse41
+
0x27
>
.
byte
102
65
15
22
68
208
8
/
/
movhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
155
254
255
255
/
/
jb
218a8
<
_sk_load_f16_sse41
+
0x27
>
.
byte
243
65
15
126
76
208
16
/
/
movq
0x10
(
%
r8
%
rdx
8
)
%
xmm1
.
byte
233
143
254
255
255
/
/
jmpq
218a8
<
_sk_load_f16_sse41
+
0x27
>
HIDDEN
_sk_load_f16_dst_sse41
.
globl
_sk_load_f16_dst_sse41
FUNCTION
(
_sk_load_f16_dst_sse41
)
_sk_load_f16_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
68
1
0
0
/
/
jne
21b77
<
_sk_load_f16_dst_sse41
+
0x15e
>
.
byte
102
65
15
16
36
208
/
/
movupd
(
%
r8
%
rdx
8
)
%
xmm4
.
byte
243
65
15
111
108
208
16
/
/
movdqu
0x10
(
%
r8
%
rdx
8
)
%
xmm5
.
byte
102
68
15
40
204
/
/
movapd
%
xmm4
%
xmm9
.
byte
102
68
15
97
205
/
/
punpcklwd
%
xmm5
%
xmm9
.
byte
102
15
105
229
/
/
punpckhwd
%
xmm5
%
xmm4
.
byte
102
69
15
111
225
/
/
movdqa
%
xmm9
%
xmm12
.
byte
102
68
15
97
228
/
/
punpcklwd
%
xmm4
%
xmm12
.
byte
102
68
15
105
204
/
/
punpckhwd
%
xmm4
%
xmm9
.
byte
102
65
15
56
51
244
/
/
pmovzxwd
%
xmm12
%
xmm6
.
byte
102
68
15
111
5
180
185
1
0
/
/
movdqa
0x1b9b4
(
%
rip
)
%
xmm8
#
3d420
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11d4
>
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
65
15
219
232
/
/
pand
%
xmm8
%
xmm5
.
byte
102
68
15
111
21
178
185
1
0
/
/
movdqa
0x1b9b2
(
%
rip
)
%
xmm10
#
3d430
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11e4
>
.
byte
102
65
15
219
242
/
/
pand
%
xmm10
%
xmm6
.
byte
102
15
111
61
181
185
1
0
/
/
movdqa
0x1b9b5
(
%
rip
)
%
xmm7
#
3d440
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11f4
>
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
111
230
/
/
movdqa
%
xmm6
%
xmm4
.
byte
102
15
56
63
231
/
/
pmaxud
%
xmm7
%
xmm4
.
byte
102
15
118
230
/
/
pcmpeqd
%
xmm6
%
xmm4
.
byte
102
15
114
246
13
/
/
pslld
0xd
%
xmm6
.
byte
102
15
235
245
/
/
por
%
xmm5
%
xmm6
.
byte
102
68
15
111
29
161
185
1
0
/
/
movdqa
0x1b9a1
(
%
rip
)
%
xmm11
#
3d450
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1204
>
.
byte
102
65
15
254
243
/
/
paddd
%
xmm11
%
xmm6
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
102
65
15
112
236
238
/
/
pshufd
0xee
%
xmm12
%
xmm5
.
byte
102
15
56
51
245
/
/
pmovzxwd
%
xmm5
%
xmm6
.
byte
102
68
15
111
230
/
/
movdqa
%
xmm6
%
xmm12
.
byte
102
69
15
219
224
/
/
pand
%
xmm8
%
xmm12
.
byte
102
65
15
219
242
/
/
pand
%
xmm10
%
xmm6
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
56
63
239
/
/
pmaxud
%
xmm7
%
xmm5
.
byte
102
15
118
238
/
/
pcmpeqd
%
xmm6
%
xmm5
.
byte
102
15
114
246
13
/
/
pslld
0xd
%
xmm6
.
byte
102
65
15
235
244
/
/
por
%
xmm12
%
xmm6
.
byte
102
65
15
254
243
/
/
paddd
%
xmm11
%
xmm6
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
102
69
15
56
51
225
/
/
pmovzxwd
%
xmm9
%
xmm12
.
byte
102
69
15
111
236
/
/
movdqa
%
xmm12
%
xmm13
.
byte
102
69
15
219
232
/
/
pand
%
xmm8
%
xmm13
.
byte
102
69
15
219
226
/
/
pand
%
xmm10
%
xmm12
.
byte
102
65
15
114
245
16
/
/
pslld
0x10
%
xmm13
.
byte
102
65
15
111
244
/
/
movdqa
%
xmm12
%
xmm6
.
byte
102
15
56
63
247
/
/
pmaxud
%
xmm7
%
xmm6
.
byte
102
65
15
118
244
/
/
pcmpeqd
%
xmm12
%
xmm6
.
byte
102
65
15
114
244
13
/
/
pslld
0xd
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
102
69
15
254
227
/
/
paddd
%
xmm11
%
xmm12
.
byte
102
65
15
219
244
/
/
pand
%
xmm12
%
xmm6
.
byte
102
69
15
112
201
238
/
/
pshufd
0xee
%
xmm9
%
xmm9
.
byte
102
69
15
56
51
201
/
/
pmovzxwd
%
xmm9
%
xmm9
.
byte
102
69
15
219
193
/
/
pand
%
xmm9
%
xmm8
.
byte
102
69
15
219
202
/
/
pand
%
xmm10
%
xmm9
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
56
63
249
/
/
pmaxud
%
xmm9
%
xmm7
.
byte
102
65
15
118
249
/
/
pcmpeqd
%
xmm9
%
xmm7
.
byte
102
65
15
114
241
13
/
/
pslld
0xd
%
xmm9
.
byte
102
69
15
235
200
/
/
por
%
xmm8
%
xmm9
.
byte
102
69
15
254
203
/
/
paddd
%
xmm11
%
xmm9
.
byte
102
65
15
219
249
/
/
pand
%
xmm9
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
65
15
16
36
208
/
/
movsd
(
%
r8
%
rdx
8
)
%
xmm4
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
21b90
<
_sk_load_f16_dst_sse41
+
0x177
>
.
byte
243
15
126
228
/
/
movq
%
xmm4
%
xmm4
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
233
176
254
255
255
/
/
jmpq
21a40
<
_sk_load_f16_dst_sse41
+
0x27
>
.
byte
102
65
15
22
100
208
8
/
/
movhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm4
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
155
254
255
255
/
/
jb
21a40
<
_sk_load_f16_dst_sse41
+
0x27
>
.
byte
243
65
15
126
108
208
16
/
/
movq
0x10
(
%
r8
%
rdx
8
)
%
xmm5
.
byte
233
143
254
255
255
/
/
jmpq
21a40
<
_sk_load_f16_dst_sse41
+
0x27
>
HIDDEN
_sk_gather_f16_sse41
.
globl
_sk_gather_f16_sse41
FUNCTION
(
_sk_gather_f16_sse41
)
_sk_gather_f16_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
56
64
200
/
/
pmulld
%
xmm0
%
xmm1
.
byte
243
65
15
91
193
/
/
cvttps2dq
%
xmm9
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
58
22
194
1
/
/
pextrq
0x1
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
67
15
126
4
208
/
/
movq
(
%
r8
%
r10
8
)
%
xmm0
.
byte
243
67
15
126
12
216
/
/
movq
(
%
r8
%
r11
8
)
%
xmm1
.
byte
102
15
108
200
/
/
punpcklqdq
%
xmm0
%
xmm1
.
byte
243
65
15
126
4
192
/
/
movq
(
%
r8
%
rax
8
)
%
xmm0
.
byte
243
67
15
126
20
200
/
/
movq
(
%
r8
%
r9
8
)
%
xmm2
.
byte
102
15
108
208
/
/
punpcklqdq
%
xmm0
%
xmm2
.
byte
102
68
15
111
202
/
/
movdqa
%
xmm2
%
xmm9
.
byte
102
68
15
97
201
/
/
punpcklwd
%
xmm1
%
xmm9
.
byte
102
15
105
209
/
/
punpckhwd
%
xmm1
%
xmm2
.
byte
102
69
15
111
225
/
/
movdqa
%
xmm9
%
xmm12
.
byte
102
68
15
97
226
/
/
punpcklwd
%
xmm2
%
xmm12
.
byte
102
68
15
105
202
/
/
punpckhwd
%
xmm2
%
xmm9
.
byte
102
65
15
56
51
212
/
/
pmovzxwd
%
xmm12
%
xmm2
.
byte
102
68
15
111
5
178
183
1
0
/
/
movdqa
0x1b7b2
(
%
rip
)
%
xmm8
#
3d420
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11d4
>
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
65
15
219
200
/
/
pand
%
xmm8
%
xmm1
.
byte
102
68
15
111
21
176
183
1
0
/
/
movdqa
0x1b7b0
(
%
rip
)
%
xmm10
#
3d430
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11e4
>
.
byte
102
65
15
219
210
/
/
pand
%
xmm10
%
xmm2
.
byte
102
15
111
29
179
183
1
0
/
/
movdqa
0x1b7b3
(
%
rip
)
%
xmm3
#
3d440
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11f4
>
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
56
63
195
/
/
pmaxud
%
xmm3
%
xmm0
.
byte
102
15
118
194
/
/
pcmpeqd
%
xmm2
%
xmm0
.
byte
102
15
114
242
13
/
/
pslld
0xd
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
68
15
111
29
159
183
1
0
/
/
movdqa
0x1b79f
(
%
rip
)
%
xmm11
#
3d450
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1204
>
.
byte
102
65
15
254
211
/
/
paddd
%
xmm11
%
xmm2
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
102
65
15
112
204
238
/
/
pshufd
0xee
%
xmm12
%
xmm1
.
byte
102
15
56
51
209
/
/
pmovzxwd
%
xmm1
%
xmm2
.
byte
102
68
15
111
226
/
/
movdqa
%
xmm2
%
xmm12
.
byte
102
69
15
219
224
/
/
pand
%
xmm8
%
xmm12
.
byte
102
65
15
219
210
/
/
pand
%
xmm10
%
xmm2
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
56
63
203
/
/
pmaxud
%
xmm3
%
xmm1
.
byte
102
15
118
202
/
/
pcmpeqd
%
xmm2
%
xmm1
.
byte
102
15
114
242
13
/
/
pslld
0xd
%
xmm2
.
byte
102
65
15
235
212
/
/
por
%
xmm12
%
xmm2
.
byte
102
65
15
254
211
/
/
paddd
%
xmm11
%
xmm2
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
69
15
56
51
225
/
/
pmovzxwd
%
xmm9
%
xmm12
.
byte
102
69
15
111
236
/
/
movdqa
%
xmm12
%
xmm13
.
byte
102
69
15
219
232
/
/
pand
%
xmm8
%
xmm13
.
byte
102
69
15
219
226
/
/
pand
%
xmm10
%
xmm12
.
byte
102
65
15
114
245
16
/
/
pslld
0x10
%
xmm13
.
byte
102
65
15
111
212
/
/
movdqa
%
xmm12
%
xmm2
.
byte
102
15
56
63
211
/
/
pmaxud
%
xmm3
%
xmm2
.
byte
102
65
15
118
212
/
/
pcmpeqd
%
xmm12
%
xmm2
.
byte
102
65
15
114
244
13
/
/
pslld
0xd
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
102
69
15
254
227
/
/
paddd
%
xmm11
%
xmm12
.
byte
102
65
15
219
212
/
/
pand
%
xmm12
%
xmm2
.
byte
102
69
15
112
201
238
/
/
pshufd
0xee
%
xmm9
%
xmm9
.
byte
102
69
15
56
51
201
/
/
pmovzxwd
%
xmm9
%
xmm9
.
byte
102
69
15
219
193
/
/
pand
%
xmm9
%
xmm8
.
byte
102
69
15
219
202
/
/
pand
%
xmm10
%
xmm9
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
56
63
217
/
/
pmaxud
%
xmm9
%
xmm3
.
byte
102
65
15
118
217
/
/
pcmpeqd
%
xmm9
%
xmm3
.
byte
102
65
15
114
241
13
/
/
pslld
0xd
%
xmm9
.
byte
102
69
15
235
200
/
/
por
%
xmm8
%
xmm9
.
byte
102
69
15
254
203
/
/
paddd
%
xmm11
%
xmm9
.
byte
102
65
15
219
217
/
/
pand
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_f16_sse41
.
globl
_sk_store_f16_sse41
FUNCTION
(
_sk_store_f16_sse41
)
_sk_store_f16_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
13
205
182
1
0
/
/
movdqa
0x1b6cd
(
%
rip
)
%
xmm9
#
3d460
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1214
>
.
byte
102
68
15
111
224
/
/
movdqa
%
xmm0
%
xmm12
.
byte
102
69
15
219
225
/
/
pand
%
xmm9
%
xmm12
.
byte
102
68
15
111
29
202
182
1
0
/
/
movdqa
0x1b6ca
(
%
rip
)
%
xmm11
#
3d470
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1224
>
.
byte
102
68
15
111
232
/
/
movdqa
%
xmm0
%
xmm13
.
byte
102
69
15
219
235
/
/
pand
%
xmm11
%
xmm13
.
byte
102
68
15
111
21
199
182
1
0
/
/
movdqa
0x1b6c7
(
%
rip
)
%
xmm10
#
3d480
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1234
>
.
byte
102
65
15
114
212
16
/
/
psrld
0x10
%
xmm12
.
byte
102
69
15
111
197
/
/
movdqa
%
xmm13
%
xmm8
.
byte
102
69
15
56
63
194
/
/
pmaxud
%
xmm10
%
xmm8
.
byte
102
69
15
118
197
/
/
pcmpeqd
%
xmm13
%
xmm8
.
byte
102
65
15
114
213
13
/
/
psrld
0xd
%
xmm13
.
byte
102
69
15
254
236
/
/
paddd
%
xmm12
%
xmm13
.
byte
102
68
15
111
37
173
182
1
0
/
/
movdqa
0x1b6ad
(
%
rip
)
%
xmm12
#
3d490
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1244
>
.
byte
102
69
15
254
236
/
/
paddd
%
xmm12
%
xmm13
.
byte
102
69
15
219
197
/
/
pand
%
xmm13
%
xmm8
.
byte
102
68
15
56
43
192
/
/
packusdw
%
xmm0
%
xmm8
.
byte
102
68
15
111
233
/
/
movdqa
%
xmm1
%
xmm13
.
byte
102
69
15
219
233
/
/
pand
%
xmm9
%
xmm13
.
byte
102
68
15
111
241
/
/
movdqa
%
xmm1
%
xmm14
.
byte
102
69
15
219
243
/
/
pand
%
xmm11
%
xmm14
.
byte
102
65
15
114
213
16
/
/
psrld
0x10
%
xmm13
.
byte
102
69
15
111
254
/
/
movdqa
%
xmm14
%
xmm15
.
byte
102
69
15
56
63
250
/
/
pmaxud
%
xmm10
%
xmm15
.
byte
102
69
15
118
254
/
/
pcmpeqd
%
xmm14
%
xmm15
.
byte
102
65
15
114
214
13
/
/
psrld
0xd
%
xmm14
.
byte
102
69
15
254
245
/
/
paddd
%
xmm13
%
xmm14
.
byte
102
69
15
254
244
/
/
paddd
%
xmm12
%
xmm14
.
byte
102
69
15
219
254
/
/
pand
%
xmm14
%
xmm15
.
byte
102
68
15
56
43
248
/
/
packusdw
%
xmm0
%
xmm15
.
byte
102
69
15
97
199
/
/
punpcklwd
%
xmm15
%
xmm8
.
byte
102
68
15
111
242
/
/
movdqa
%
xmm2
%
xmm14
.
byte
102
69
15
219
241
/
/
pand
%
xmm9
%
xmm14
.
byte
102
68
15
111
250
/
/
movdqa
%
xmm2
%
xmm15
.
byte
102
69
15
219
251
/
/
pand
%
xmm11
%
xmm15
.
byte
102
65
15
114
214
16
/
/
psrld
0x10
%
xmm14
.
byte
102
69
15
111
239
/
/
movdqa
%
xmm15
%
xmm13
.
byte
102
69
15
56
63
234
/
/
pmaxud
%
xmm10
%
xmm13
.
byte
102
69
15
118
239
/
/
pcmpeqd
%
xmm15
%
xmm13
.
byte
102
65
15
114
215
13
/
/
psrld
0xd
%
xmm15
.
byte
102
69
15
254
254
/
/
paddd
%
xmm14
%
xmm15
.
byte
102
69
15
254
252
/
/
paddd
%
xmm12
%
xmm15
.
byte
102
69
15
219
239
/
/
pand
%
xmm15
%
xmm13
.
byte
102
68
15
56
43
232
/
/
packusdw
%
xmm0
%
xmm13
.
byte
102
68
15
219
203
/
/
pand
%
xmm3
%
xmm9
.
byte
102
68
15
219
219
/
/
pand
%
xmm3
%
xmm11
.
byte
102
65
15
114
209
16
/
/
psrld
0x10
%
xmm9
.
byte
102
69
15
56
63
211
/
/
pmaxud
%
xmm11
%
xmm10
.
byte
102
69
15
118
211
/
/
pcmpeqd
%
xmm11
%
xmm10
.
byte
102
65
15
114
211
13
/
/
psrld
0xd
%
xmm11
.
byte
102
69
15
254
217
/
/
paddd
%
xmm9
%
xmm11
.
byte
102
69
15
254
220
/
/
paddd
%
xmm12
%
xmm11
.
byte
102
69
15
219
211
/
/
pand
%
xmm11
%
xmm10
.
byte
102
68
15
56
43
208
/
/
packusdw
%
xmm0
%
xmm10
.
byte
102
69
15
97
234
/
/
punpcklwd
%
xmm10
%
xmm13
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
69
15
98
205
/
/
punpckldq
%
xmm13
%
xmm9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
21
/
/
jne
21ee1
<
_sk_store_f16_sse41
+
0x168
>
.
byte
69
15
17
12
208
/
/
movups
%
xmm9
(
%
r8
%
rdx
8
)
.
byte
102
69
15
106
197
/
/
punpckhdq
%
xmm13
%
xmm8
.
byte
243
69
15
127
68
208
16
/
/
movdqu
%
xmm8
0x10
(
%
r8
%
rdx
8
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
214
12
208
/
/
movq
%
xmm9
(
%
r8
%
rdx
8
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
21edd
<
_sk_store_f16_sse41
+
0x164
>
.
byte
102
69
15
23
76
208
8
/
/
movhpd
%
xmm9
0x8
(
%
r8
%
rdx
8
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
21edd
<
_sk_store_f16_sse41
+
0x164
>
.
byte
102
69
15
106
197
/
/
punpckhdq
%
xmm13
%
xmm8
.
byte
102
69
15
214
68
208
16
/
/
movq
%
xmm8
0x10
(
%
r8
%
rdx
8
)
.
byte
235
213
/
/
jmp
21edd
<
_sk_store_f16_sse41
+
0x164
>
HIDDEN
_sk_load_u16_be_sse41
.
globl
_sk_load_u16_be_sse41
FUNCTION
(
_sk_load_u16_be_sse41
)
_sk_load_u16_be_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
185
0
0
0
/
/
jne
21fe2
<
_sk_load_u16_be_sse41
+
0xda
>
.
byte
102
67
15
16
4
65
/
/
movupd
(
%
r9
%
r8
2
)
%
xmm0
.
byte
243
67
15
111
76
65
16
/
/
movdqu
0x10
(
%
r9
%
r8
2
)
%
xmm1
.
byte
102
15
40
208
/
/
movapd
%
xmm0
%
xmm2
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
15
56
51
193
/
/
pmovzxwd
%
xmm1
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
59
178
1
0
/
/
movaps
0x1b23b
(
%
rip
)
%
xmm8
#
3d1b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf64
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
235
217
/
/
por
%
xmm1
%
xmm3
.
byte
102
15
56
51
203
/
/
pmovzxwd
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
68
15
111
202
/
/
movdqa
%
xmm2
%
xmm9
.
byte
102
65
15
113
241
8
/
/
psllw
0x8
%
xmm9
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
65
15
235
209
/
/
por
%
xmm9
%
xmm2
.
byte
102
15
56
51
210
/
/
pmovzxwd
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
68
15
111
203
/
/
movdqa
%
xmm3
%
xmm9
.
byte
102
65
15
113
241
8
/
/
psllw
0x8
%
xmm9
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
65
15
235
217
/
/
por
%
xmm9
%
xmm3
.
byte
102
15
56
51
219
/
/
pmovzxwd
%
xmm3
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
67
15
16
4
65
/
/
movsd
(
%
r9
%
r8
2
)
%
xmm0
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
21ffb
<
_sk_load_u16_be_sse41
+
0xf3
>
.
byte
243
15
126
192
/
/
movq
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
59
255
255
255
/
/
jmpq
21f36
<
_sk_load_u16_be_sse41
+
0x2e
>
.
byte
102
67
15
22
68
65
8
/
/
movhpd
0x8
(
%
r9
%
r8
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
38
255
255
255
/
/
jb
21f36
<
_sk_load_u16_be_sse41
+
0x2e
>
.
byte
243
67
15
126
76
65
16
/
/
movq
0x10
(
%
r9
%
r8
2
)
%
xmm1
.
byte
233
26
255
255
255
/
/
jmpq
21f36
<
_sk_load_u16_be_sse41
+
0x2e
>
HIDDEN
_sk_load_rgb_u16_be_sse41
.
globl
_sk_load_rgb_u16_be_sse41
FUNCTION
(
_sk_load_rgb_u16_be_sse41
)
_sk_load_rgb_u16_be_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
170
0
0
0
/
/
jne
220e3
<
_sk_load_rgb_u16_be_sse41
+
0xc7
>
.
byte
243
67
15
111
20
65
/
/
movdqu
(
%
r9
%
r8
2
)
%
xmm2
.
byte
243
67
15
111
92
65
8
/
/
movdqu
0x8
(
%
r9
%
r8
2
)
%
xmm3
.
byte
102
15
115
219
4
/
/
psrldq
0x4
%
xmm3
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
115
216
6
/
/
psrldq
0x6
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
115
217
6
/
/
psrldq
0x6
%
xmm1
.
byte
102
15
97
193
/
/
punpcklwd
%
xmm1
%
xmm0
.
byte
102
15
97
211
/
/
punpcklwd
%
xmm3
%
xmm2
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
15
56
51
193
/
/
pmovzxwd
%
xmm1
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
24
177
1
0
/
/
movaps
0x1b118
(
%
rip
)
%
xmm8
#
3d1b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf64
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
235
217
/
/
por
%
xmm1
%
xmm3
.
byte
102
15
56
51
203
/
/
pmovzxwd
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
15
56
51
210
/
/
pmovzxwd
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
47
174
1
0
/
/
movaps
0x1ae2f
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
67
15
110
20
65
/
/
movd
(
%
r9
%
r8
2
)
%
xmm2
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
67
15
196
84
65
4
2
/
/
pinsrw
0x2
0x4
(
%
r9
%
r8
2
)
%
xmm2
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
22108
<
_sk_load_rgb_u16_be_sse41
+
0xec
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
233
85
255
255
255
/
/
jmpq
2205d
<
_sk_load_rgb_u16_be_sse41
+
0x41
>
.
byte
102
67
15
110
68
65
6
/
/
movd
0x6
(
%
r9
%
r8
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
67
15
196
68
65
10
2
/
/
pinsrw
0x2
0xa
(
%
r9
%
r8
2
)
%
xmm0
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
24
/
/
jb
22139
<
_sk_load_rgb_u16_be_sse41
+
0x11d
>
.
byte
102
67
15
110
92
65
12
/
/
movd
0xc
(
%
r9
%
r8
2
)
%
xmm3
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
67
15
196
92
65
16
2
/
/
pinsrw
0x2
0x10
(
%
r9
%
r8
2
)
%
xmm3
.
byte
233
36
255
255
255
/
/
jmpq
2205d
<
_sk_load_rgb_u16_be_sse41
+
0x41
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
233
27
255
255
255
/
/
jmpq
2205d
<
_sk_load_rgb_u16_be_sse41
+
0x41
>
HIDDEN
_sk_store_u16_be_sse41
.
globl
_sk_store_u16_be_sse41
FUNCTION
(
_sk_store_u16_be_sse41
)
_sk_store_u16_be_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
68
15
40
21
162
173
1
0
/
/
movaps
0x1ada2
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
68
15
40
29
38
179
1
0
/
/
movaps
0x1b326
(
%
rip
)
%
xmm11
#
3d4a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1254
>
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
68
15
56
43
192
/
/
packusdw
%
xmm0
%
xmm8
.
byte
102
69
15
111
224
/
/
movdqa
%
xmm8
%
xmm12
.
byte
102
65
15
113
244
8
/
/
psllw
0x8
%
xmm12
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
68
15
56
43
224
/
/
packusdw
%
xmm0
%
xmm12
.
byte
102
69
15
111
236
/
/
movdqa
%
xmm12
%
xmm13
.
byte
102
65
15
113
245
8
/
/
psllw
0x8
%
xmm13
.
byte
102
65
15
113
212
8
/
/
psrlw
0x8
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
102
69
15
97
196
/
/
punpcklwd
%
xmm12
%
xmm8
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
68
15
56
43
224
/
/
packusdw
%
xmm0
%
xmm12
.
byte
102
69
15
111
236
/
/
movdqa
%
xmm12
%
xmm13
.
byte
102
65
15
113
245
8
/
/
psllw
0x8
%
xmm13
.
byte
102
65
15
113
212
8
/
/
psrlw
0x8
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
68
15
95
203
/
/
maxps
%
xmm3
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
68
15
56
43
200
/
/
packusdw
%
xmm0
%
xmm9
.
byte
102
69
15
111
209
/
/
movdqa
%
xmm9
%
xmm10
.
byte
102
65
15
113
242
8
/
/
psllw
0x8
%
xmm10
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
69
15
235
202
/
/
por
%
xmm10
%
xmm9
.
byte
102
69
15
97
225
/
/
punpcklwd
%
xmm9
%
xmm12
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
69
15
98
204
/
/
punpckldq
%
xmm12
%
xmm9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
21
/
/
jne
2225e
<
_sk_store_u16_be_sse41
+
0x11c
>
.
byte
71
15
17
12
65
/
/
movups
%
xmm9
(
%
r9
%
r8
2
)
.
byte
102
69
15
106
196
/
/
punpckhdq
%
xmm12
%
xmm8
.
byte
243
71
15
127
68
65
16
/
/
movdqu
%
xmm8
0x10
(
%
r9
%
r8
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
71
15
214
12
65
/
/
movq
%
xmm9
(
%
r9
%
r8
2
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
2225a
<
_sk_store_u16_be_sse41
+
0x118
>
.
byte
102
71
15
23
76
65
8
/
/
movhpd
%
xmm9
0x8
(
%
r9
%
r8
2
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
2225a
<
_sk_store_u16_be_sse41
+
0x118
>
.
byte
102
69
15
106
196
/
/
punpckhdq
%
xmm12
%
xmm8
.
byte
102
71
15
214
68
65
16
/
/
movq
%
xmm8
0x10
(
%
r9
%
r8
2
)
.
byte
235
213
/
/
jmp
2225a
<
_sk_store_u16_be_sse41
+
0x118
>
HIDDEN
_sk_load_f32_sse41
.
globl
_sk_load_f32_sse41
FUNCTION
(
_sk_load_f32_sse41
)
_sk_load_f32_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
137
208
/
/
mov
%
rdx
%
rax
.
byte
72
193
224
4
/
/
shl
0x4
%
rax
.
byte
70
15
16
4
8
/
/
movups
(
%
rax
%
r9
1
)
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
66
/
/
jne
222f1
<
_sk_load_f32_sse41
+
0x6c
>
.
byte
67
15
16
68
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm0
.
byte
67
15
16
92
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm3
.
byte
71
15
16
76
129
48
/
/
movups
0x30
(
%
r9
%
r8
4
)
%
xmm9
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
20
208
/
/
unpcklps
%
xmm0
%
xmm2
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
65
15
20
201
/
/
unpcklps
%
xmm9
%
xmm1
.
byte
68
15
21
192
/
/
unpckhps
%
xmm0
%
xmm8
.
byte
65
15
21
217
/
/
unpckhps
%
xmm9
%
xmm3
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
15
18
202
/
/
movhlps
%
xmm2
%
xmm1
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
65
15
18
216
/
/
movhlps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
8
/
/
jne
22303
<
_sk_load_f32_sse41
+
0x7e
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
235
190
/
/
jmp
222c1
<
_sk_load_f32_sse41
+
0x3c
>
.
byte
67
15
16
68
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm0
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
8
/
/
jb
22317
<
_sk_load_f32_sse41
+
0x92
>
.
byte
67
15
16
92
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm3
.
byte
235
170
/
/
jmp
222c1
<
_sk_load_f32_sse41
+
0x3c
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
235
165
/
/
jmp
222c1
<
_sk_load_f32_sse41
+
0x3c
>
HIDDEN
_sk_load_f32_dst_sse41
.
globl
_sk_load_f32_dst_sse41
FUNCTION
(
_sk_load_f32_dst_sse41
)
_sk_load_f32_dst_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
137
208
/
/
mov
%
rdx
%
rax
.
byte
72
193
224
4
/
/
shl
0x4
%
rax
.
byte
70
15
16
4
8
/
/
movups
(
%
rax
%
r9
1
)
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
66
/
/
jne
22388
<
_sk_load_f32_dst_sse41
+
0x6c
>
.
byte
67
15
16
100
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm4
.
byte
67
15
16
124
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm7
.
byte
71
15
16
76
129
48
/
/
movups
0x30
(
%
r9
%
r8
4
)
%
xmm9
.
byte
65
15
40
240
/
/
movaps
%
xmm8
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
15
40
239
/
/
movaps
%
xmm7
%
xmm5
.
byte
65
15
20
233
/
/
unpcklps
%
xmm9
%
xmm5
.
byte
68
15
21
196
/
/
unpckhps
%
xmm4
%
xmm8
.
byte
65
15
21
249
/
/
unpckhps
%
xmm9
%
xmm7
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
102
15
20
229
/
/
unpcklpd
%
xmm5
%
xmm4
.
byte
15
18
238
/
/
movhlps
%
xmm6
%
xmm5
.
byte
65
15
40
240
/
/
movaps
%
xmm8
%
xmm6
.
byte
102
15
20
247
/
/
unpcklpd
%
xmm7
%
xmm6
.
byte
65
15
18
248
/
/
movhlps
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
8
/
/
jne
2239a
<
_sk_load_f32_dst_sse41
+
0x7e
>
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
235
190
/
/
jmp
22358
<
_sk_load_f32_dst_sse41
+
0x3c
>
.
byte
67
15
16
100
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm4
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
8
/
/
jb
223ae
<
_sk_load_f32_dst_sse41
+
0x92
>
.
byte
67
15
16
124
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm7
.
byte
235
170
/
/
jmp
22358
<
_sk_load_f32_dst_sse41
+
0x3c
>
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
235
165
/
/
jmp
22358
<
_sk_load_f32_dst_sse41
+
0x3c
>
HIDDEN
_sk_store_f32_sse41
.
globl
_sk_store_f32_sse41
FUNCTION
(
_sk_store_f32_sse41
)
_sk_store_f32_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
137
208
/
/
mov
%
rdx
%
rax
.
byte
72
193
224
4
/
/
shl
0x4
%
rax
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
20
201
/
/
unpcklps
%
xmm1
%
xmm9
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
20
195
/
/
unpcklps
%
xmm3
%
xmm8
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
68
15
21
209
/
/
unpckhps
%
xmm1
%
xmm10
.
byte
68
15
40
218
/
/
movaps
%
xmm2
%
xmm11
.
byte
68
15
21
219
/
/
unpckhps
%
xmm3
%
xmm11
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
102
69
15
20
224
/
/
unpcklpd
%
xmm8
%
xmm12
.
byte
69
15
18
193
/
/
movhlps
%
xmm9
%
xmm8
.
byte
69
15
40
202
/
/
movaps
%
xmm10
%
xmm9
.
byte
102
69
15
20
203
/
/
unpcklpd
%
xmm11
%
xmm9
.
byte
102
70
15
17
36
8
/
/
movupd
%
xmm12
(
%
rax
%
r9
1
)
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
29
/
/
jne
22431
<
_sk_store_f32_sse41
+
0x7e
>
.
byte
102
69
15
21
211
/
/
unpckhpd
%
xmm11
%
xmm10
.
byte
71
15
17
68
129
16
/
/
movups
%
xmm8
0x10
(
%
r9
%
r8
4
)
.
byte
102
71
15
17
76
129
32
/
/
movupd
%
xmm9
0x20
(
%
r9
%
r8
4
)
.
byte
102
71
15
17
84
129
48
/
/
movupd
%
xmm10
0x30
(
%
r9
%
r8
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
246
/
/
je
2242d
<
_sk_store_f32_sse41
+
0x7a
>
.
byte
71
15
17
68
129
16
/
/
movups
%
xmm8
0x10
(
%
r9
%
r8
4
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
234
/
/
jb
2242d
<
_sk_store_f32_sse41
+
0x7a
>
.
byte
102
71
15
17
76
129
32
/
/
movupd
%
xmm9
0x20
(
%
r9
%
r8
4
)
.
byte
235
225
/
/
jmp
2242d
<
_sk_store_f32_sse41
+
0x7a
>
HIDDEN
_sk_repeat_x_sse41
.
globl
_sk_repeat_x_sse41
FUNCTION
(
_sk_repeat_x_sse41
)
_sk_repeat_x_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
102
69
15
58
8
201
1
/
/
roundps
0x1
%
xmm9
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_y_sse41
.
globl
_sk_repeat_y_sse41
FUNCTION
(
_sk_repeat_y_sse41
)
_sk_repeat_y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
102
69
15
58
8
201
1
/
/
roundps
0x1
%
xmm9
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_sse41
.
globl
_sk_mirror_x_sse41
FUNCTION
(
_sk_mirror_x_sse41
)
_sk_mirror_x_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
92
194
/
/
subps
%
xmm10
%
xmm0
.
byte
243
69
15
88
192
/
/
addss
%
xmm8
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
89
13
35
160
1
0
/
/
mulss
0x1a023
(
%
rip
)
%
xmm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
102
69
15
58
8
201
1
/
/
roundps
0x1
%
xmm9
%
xmm9
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
92
194
/
/
subps
%
xmm10
%
xmm0
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
92
192
/
/
subps
%
xmm0
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_y_sse41
.
globl
_sk_mirror_y_sse41
FUNCTION
(
_sk_mirror_y_sse41
)
_sk_mirror_y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
92
202
/
/
subps
%
xmm10
%
xmm1
.
byte
243
69
15
88
192
/
/
addss
%
xmm8
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
89
13
202
159
1
0
/
/
mulss
0x19fca
(
%
rip
)
%
xmm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
102
69
15
58
8
201
1
/
/
roundps
0x1
%
xmm9
%
xmm9
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
65
15
92
201
/
/
subps
%
xmm9
%
xmm1
.
byte
65
15
92
202
/
/
subps
%
xmm10
%
xmm1
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
92
193
/
/
subps
%
xmm1
%
xmm8
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_x_1_sse41
.
globl
_sk_clamp_x_1_sse41
FUNCTION
(
_sk_clamp_x_1_sse41
)
_sk_clamp_x_1_sse41
:
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
68
15
93
5
166
169
1
0
/
/
minps
0x1a9a6
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_sse41
.
globl
_sk_repeat_x_1_sse41
FUNCTION
(
_sk_repeat_x_1_sse41
)
_sk_repeat_x_1_sse41
:
.
byte
102
68
15
58
8
192
1
/
/
roundps
0x1
%
xmm0
%
xmm8
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
68
15
93
5
131
169
1
0
/
/
minps
0x1a983
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_sse41
.
globl
_sk_mirror_x_1_sse41
FUNCTION
(
_sk_mirror_x_1_sse41
)
_sk_mirror_x_1_sse41
:
.
byte
68
15
40
5
211
169
1
0
/
/
movaps
0x1a9d3
(
%
rip
)
%
xmm8
#
3cf70
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd24
>
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
13
87
169
1
0
/
/
movaps
0x1a957
(
%
rip
)
%
xmm9
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
102
69
15
58
8
201
1
/
/
roundps
0x1
%
xmm9
%
xmm9
.
byte
69
15
88
201
/
/
addps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
68
15
84
200
/
/
andps
%
xmm0
%
xmm9
.
byte
69
15
95
193
/
/
maxps
%
xmm9
%
xmm8
.
byte
68
15
93
5
52
169
1
0
/
/
minps
0x1a934
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_sse41
.
globl
_sk_decal_x_sse41
FUNCTION
(
_sk_decal_x_sse41
)
_sk_decal_x_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
194
192
2
/
/
cmpleps
%
xmm0
%
xmm8
.
byte
243
68
15
16
72
64
/
/
movss
0x40
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
69
15
194
209
1
/
/
cmpltps
%
xmm9
%
xmm10
.
byte
69
15
84
208
/
/
andps
%
xmm8
%
xmm10
.
byte
68
15
17
16
/
/
movups
%
xmm10
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_sse41
.
globl
_sk_decal_y_sse41
FUNCTION
(
_sk_decal_y_sse41
)
_sk_decal_y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
194
193
2
/
/
cmpleps
%
xmm1
%
xmm8
.
byte
243
68
15
16
72
68
/
/
movss
0x44
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
69
15
194
209
1
/
/
cmpltps
%
xmm9
%
xmm10
.
byte
69
15
84
208
/
/
andps
%
xmm8
%
xmm10
.
byte
68
15
17
16
/
/
movups
%
xmm10
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_sse41
.
globl
_sk_decal_x_and_y_sse41
FUNCTION
(
_sk_decal_x_and_y_sse41
)
_sk_decal_x_and_y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
200
2
/
/
cmpleps
%
xmm0
%
xmm9
.
byte
243
68
15
16
80
64
/
/
movss
0x40
(
%
rax
)
%
xmm10
.
byte
243
68
15
16
88
68
/
/
movss
0x44
(
%
rax
)
%
xmm11
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
40
224
/
/
movaps
%
xmm0
%
xmm12
.
byte
69
15
194
226
1
/
/
cmpltps
%
xmm10
%
xmm12
.
byte
68
15
194
193
2
/
/
cmpleps
%
xmm1
%
xmm8
.
byte
69
15
84
193
/
/
andps
%
xmm9
%
xmm8
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
194
203
1
/
/
cmpltps
%
xmm11
%
xmm9
.
byte
69
15
84
200
/
/
andps
%
xmm8
%
xmm9
.
byte
68
15
17
8
/
/
movups
%
xmm9
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_sse41
.
globl
_sk_check_decal_mask_sse41
FUNCTION
(
_sk_check_decal_mask_sse41
)
_sk_check_decal_mask_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
0
/
/
movups
(
%
rax
)
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
65
15
84
208
/
/
andps
%
xmm8
%
xmm2
.
byte
65
15
84
216
/
/
andps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminance_to_alpha_sse41
.
globl
_sk_luminance_to_alpha_sse41
FUNCTION
(
_sk_luminance_to_alpha_sse41
)
_sk_luminance_to_alpha_sse41
:
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
89
5
2
174
1
0
/
/
mulps
0x1ae02
(
%
rip
)
%
xmm0
#
3d4b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1264
>
.
byte
15
89
13
11
174
1
0
/
/
mulps
0x1ae0b
(
%
rip
)
%
xmm1
#
3d4c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1274
>
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
89
29
17
174
1
0
/
/
mulps
0x1ae11
(
%
rip
)
%
xmm3
#
3d4d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1284
>
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_sse41
.
globl
_sk_matrix_translate_sse41
FUNCTION
(
_sk_matrix_translate_sse41
)
_sk_matrix_translate_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_sse41
.
globl
_sk_matrix_scale_translate_sse41
FUNCTION
(
_sk_matrix_scale_translate_sse41
)
_sk_matrix_scale_translate_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_sse41
.
globl
_sk_matrix_2x3_sse41
FUNCTION
(
_sk_matrix_2x3_sse41
)
_sk_matrix_2x3_sse41
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
16
/
/
movss
0x10
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
68
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_3x4_sse41
.
globl
_sk_matrix_3x4_sse41
FUNCTION
(
_sk_matrix_3x4_sse41
)
_sk_matrix_3x4_sse41
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
36
/
/
movss
0x24
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
28
/
/
movss
0x1c
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
40
/
/
movss
0x28
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
32
/
/
movss
0x20
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
44
/
/
movss
0x2c
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
89
226
/
/
mulps
%
xmm2
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
217
/
/
mulps
%
xmm9
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x5_sse41
.
globl
_sk_matrix_4x5_sse41
FUNCTION
(
_sk_matrix_4x5_sse41
)
_sk_matrix_4x5_sse41
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
32
/
/
movss
0x20
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
48
/
/
movss
0x30
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
64
/
/
movss
0x40
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
68
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
36
/
/
movss
0x24
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
52
/
/
movss
0x34
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
68
/
/
movss
0x44
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
40
/
/
movss
0x28
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
56
/
/
movss
0x38
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
243
68
15
16
112
72
/
/
movss
0x48
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
68
15
89
226
/
/
mulps
%
xmm2
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
217
/
/
mulps
%
xmm9
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
243
68
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
28
/
/
movss
0x1c
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
44
/
/
movss
0x2c
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
243
68
15
16
112
60
/
/
movss
0x3c
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
243
68
15
16
120
76
/
/
movss
0x4c
(
%
rax
)
%
xmm15
.
byte
69
15
198
255
0
/
/
shufps
0x0
%
xmm15
%
xmm15
.
byte
68
15
89
243
/
/
mulps
%
xmm3
%
xmm14
.
byte
69
15
88
247
/
/
addps
%
xmm15
%
xmm14
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
69
15
89
225
/
/
mulps
%
xmm9
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
219
/
/
movaps
%
xmm11
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x3_sse41
.
globl
_sk_matrix_4x3_sse41
FUNCTION
(
_sk_matrix_4x3_sse41
)
_sk_matrix_4x3_sse41
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
32
/
/
movss
0x20
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
36
/
/
movss
0x24
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
80
40
/
/
movss
0x28
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
89
217
/
/
mulps
%
xmm9
%
xmm3
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
80
28
/
/
movss
0x1c
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
44
/
/
movss
0x2c
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_sse41
.
globl
_sk_matrix_perspective_sse41
FUNCTION
(
_sk_matrix_perspective_sse41
)
_sk_matrix_perspective_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
243
68
15
16
80
24
/
/
movss
0x18
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
28
/
/
movss
0x1c
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
32
/
/
movss
0x20
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
83
202
/
/
rcpps
%
xmm10
%
xmm1
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_sse41
.
globl
_sk_evenly_spaced_gradient_sse41
FUNCTION
(
_sk_evenly_spaced_gradient_sse41
)
_sk_evenly_spaced_gradient_sse41
:
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
76
139
112
8
/
/
mov
0x8
(
%
rax
)
%
r14
.
byte
72
255
203
/
/
dec
%
rbx
.
byte
120
7
/
/
js
22b5f
<
_sk_evenly_spaced_gradient_sse41
+
0x18
>
.
byte
243
72
15
42
203
/
/
cvtsi2ss
%
rbx
%
xmm1
.
byte
235
21
/
/
jmp
22b74
<
_sk_evenly_spaced_gradient_sse41
+
0x2d
>
.
byte
73
137
216
/
/
mov
%
rbx
%
r8
.
byte
73
209
232
/
/
shr
%
r8
.
byte
131
227
1
/
/
and
0x1
%
ebx
.
byte
76
9
195
/
/
or
%
r8
%
rbx
.
byte
243
72
15
42
203
/
/
cvtsi2ss
%
rbx
%
xmm1
.
byte
243
15
88
201
/
/
addss
%
xmm1
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
73
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
71
15
16
4
158
/
/
movss
(
%
r14
%
r11
4
)
%
xmm8
.
byte
102
71
15
58
33
4
150
16
/
/
insertps
0x10
(
%
r14
%
r10
4
)
%
xmm8
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
102
68
15
58
33
193
32
/
/
insertps
0x20
%
xmm1
%
xmm8
.
byte
243
67
15
16
12
134
/
/
movss
(
%
r14
%
r8
4
)
%
xmm1
.
byte
102
68
15
58
33
193
48
/
/
insertps
0x30
%
xmm1
%
xmm8
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
243
70
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm9
.
byte
102
70
15
58
33
12
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm9
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
102
68
15
58
33
201
32
/
/
insertps
0x20
%
xmm1
%
xmm9
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
102
68
15
58
33
201
48
/
/
insertps
0x30
%
xmm1
%
xmm9
.
byte
72
139
88
16
/
/
mov
0x10
(
%
rax
)
%
rbx
.
byte
243
66
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm1
.
byte
102
66
15
58
33
12
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
102
15
58
33
202
32
/
/
insertps
0x20
%
xmm2
%
xmm1
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
102
15
58
33
202
48
/
/
insertps
0x30
%
xmm2
%
xmm1
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
243
70
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm10
.
byte
102
70
15
58
33
20
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm10
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
102
68
15
58
33
210
32
/
/
insertps
0x20
%
xmm2
%
xmm10
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
102
68
15
58
33
210
48
/
/
insertps
0x30
%
xmm2
%
xmm10
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
243
66
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
102
66
15
58
33
20
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
102
15
58
33
211
32
/
/
insertps
0x20
%
xmm3
%
xmm2
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
102
15
58
33
211
48
/
/
insertps
0x30
%
xmm3
%
xmm2
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
243
70
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm11
.
byte
102
70
15
58
33
28
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm11
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
102
68
15
58
33
219
32
/
/
insertps
0x20
%
xmm3
%
xmm11
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
102
68
15
58
33
219
48
/
/
insertps
0x30
%
xmm3
%
xmm11
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
102
66
15
58
33
28
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
243
70
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm12
.
byte
102
65
15
58
33
220
32
/
/
insertps
0x20
%
xmm12
%
xmm3
.
byte
243
70
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm12
.
byte
102
65
15
58
33
220
48
/
/
insertps
0x30
%
xmm12
%
xmm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
70
15
16
36
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm12
.
byte
102
70
15
58
33
36
144
16
/
/
insertps
0x10
(
%
rax
%
r10
4
)
%
xmm12
.
byte
243
70
15
16
44
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm13
.
byte
102
69
15
58
33
229
32
/
/
insertps
0x20
%
xmm13
%
xmm12
.
byte
243
70
15
16
44
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm13
.
byte
102
69
15
58
33
229
48
/
/
insertps
0x30
%
xmm13
%
xmm12
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_sse41
.
globl
_sk_gradient_sse41
FUNCTION
(
_sk_gradient_sse41
)
_sk_gradient_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
73
131
248
2
/
/
cmp
0x2
%
r8
.
byte
114
41
/
/
jb
22d51
<
_sk_gradient_sse41
+
0x38
>
.
byte
76
139
72
72
/
/
mov
0x48
(
%
rax
)
%
r9
.
byte
73
255
200
/
/
dec
%
r8
.
byte
73
131
193
4
/
/
add
0x4
%
r9
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
243
65
15
16
17
/
/
movss
(
%
r9
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
194
208
2
/
/
cmpleps
%
xmm0
%
xmm2
.
byte
102
15
250
202
/
/
psubd
%
xmm2
%
xmm1
.
byte
73
131
193
4
/
/
add
0x4
%
r9
.
byte
73
255
200
/
/
dec
%
r8
.
byte
117
230
/
/
jne
22d37
<
_sk_gradient_sse41
+
0x1e
>
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
73
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
70
15
16
4
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm8
.
byte
102
70
15
58
33
4
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm8
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
102
68
15
58
33
193
32
/
/
insertps
0x20
%
xmm1
%
xmm8
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
102
68
15
58
33
193
48
/
/
insertps
0x30
%
xmm1
%
xmm8
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
243
70
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm9
.
byte
102
70
15
58
33
12
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm9
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
102
68
15
58
33
201
32
/
/
insertps
0x20
%
xmm1
%
xmm9
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
102
68
15
58
33
201
48
/
/
insertps
0x30
%
xmm1
%
xmm9
.
byte
243
67
15
16
12
158
/
/
movss
(
%
r14
%
r11
4
)
%
xmm1
.
byte
102
67
15
58
33
12
150
16
/
/
insertps
0x10
(
%
r14
%
r10
4
)
%
xmm1
.
byte
243
67
15
16
20
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm2
.
byte
102
15
58
33
202
32
/
/
insertps
0x20
%
xmm2
%
xmm1
.
byte
243
67
15
16
20
134
/
/
movss
(
%
r14
%
r8
4
)
%
xmm2
.
byte
102
15
58
33
202
48
/
/
insertps
0x30
%
xmm2
%
xmm1
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
243
70
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm10
.
byte
102
70
15
58
33
20
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm10
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
102
68
15
58
33
210
32
/
/
insertps
0x20
%
xmm2
%
xmm10
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
102
68
15
58
33
210
48
/
/
insertps
0x30
%
xmm2
%
xmm10
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
243
66
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
102
66
15
58
33
20
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
102
15
58
33
211
32
/
/
insertps
0x20
%
xmm3
%
xmm2
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
102
15
58
33
211
48
/
/
insertps
0x30
%
xmm3
%
xmm2
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
243
70
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm11
.
byte
102
70
15
58
33
28
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm11
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
102
68
15
58
33
219
32
/
/
insertps
0x20
%
xmm3
%
xmm11
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
102
68
15
58
33
219
48
/
/
insertps
0x30
%
xmm3
%
xmm11
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
102
66
15
58
33
28
147
16
/
/
insertps
0x10
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
243
70
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm12
.
byte
102
65
15
58
33
220
32
/
/
insertps
0x20
%
xmm12
%
xmm3
.
byte
243
70
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm12
.
byte
102
65
15
58
33
220
48
/
/
insertps
0x30
%
xmm12
%
xmm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
70
15
16
36
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm12
.
byte
102
70
15
58
33
36
144
16
/
/
insertps
0x10
(
%
rax
%
r10
4
)
%
xmm12
.
byte
243
70
15
16
44
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm13
.
byte
102
69
15
58
33
229
32
/
/
insertps
0x20
%
xmm13
%
xmm12
.
byte
243
70
15
16
44
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm13
.
byte
102
69
15
58
33
229
48
/
/
insertps
0x30
%
xmm13
%
xmm12
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_sse41
.
globl
_sk_evenly_spaced_2_stop_gradient_sse41
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_sse41
)
_sk_evenly_spaced_2_stop_gradient_sse41
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
72
28
/
/
movss
0x1c
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_sse41
.
globl
_sk_xy_to_unit_angle_sse41
FUNCTION
(
_sk_xy_to_unit_angle_sse41
)
_sk_xy_to_unit_angle_sse41
:
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
84
193
/
/
andps
%
xmm9
%
xmm0
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
68
15
92
217
/
/
subps
%
xmm1
%
xmm11
.
byte
68
15
84
217
/
/
andps
%
xmm1
%
xmm11
.
byte
68
15
40
224
/
/
movaps
%
xmm0
%
xmm12
.
byte
69
15
93
227
/
/
minps
%
xmm11
%
xmm12
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
69
15
95
211
/
/
maxps
%
xmm11
%
xmm10
.
byte
69
15
94
226
/
/
divps
%
xmm10
%
xmm12
.
byte
69
15
40
236
/
/
movaps
%
xmm12
%
xmm13
.
byte
69
15
89
237
/
/
mulps
%
xmm13
%
xmm13
.
byte
68
15
40
21
59
165
1
0
/
/
movaps
0x1a53b
(
%
rip
)
%
xmm10
#
3d4e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1294
>
.
byte
69
15
89
213
/
/
mulps
%
xmm13
%
xmm10
.
byte
68
15
88
21
63
165
1
0
/
/
addps
0x1a53f
(
%
rip
)
%
xmm10
#
3d4f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12a4
>
.
byte
69
15
89
213
/
/
mulps
%
xmm13
%
xmm10
.
byte
68
15
88
21
67
165
1
0
/
/
addps
0x1a543
(
%
rip
)
%
xmm10
#
3d500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12b4
>
.
byte
69
15
89
213
/
/
mulps
%
xmm13
%
xmm10
.
byte
68
15
88
21
71
165
1
0
/
/
addps
0x1a547
(
%
rip
)
%
xmm10
#
3d510
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12c4
>
.
byte
69
15
89
212
/
/
mulps
%
xmm12
%
xmm10
.
byte
65
15
194
195
1
/
/
cmpltps
%
xmm11
%
xmm0
.
byte
68
15
40
29
70
165
1
0
/
/
movaps
0x1a546
(
%
rip
)
%
xmm11
#
3d520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12d4
>
.
byte
69
15
92
218
/
/
subps
%
xmm10
%
xmm11
.
byte
102
69
15
56
20
211
/
/
blendvps
%
xmm0
%
xmm11
%
xmm10
.
byte
69
15
194
200
1
/
/
cmpltps
%
xmm8
%
xmm9
.
byte
68
15
40
29
15
159
1
0
/
/
movaps
0x19f0f
(
%
rip
)
%
xmm11
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
92
218
/
/
subps
%
xmm10
%
xmm11
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
211
/
/
blendvps
%
xmm0
%
xmm11
%
xmm10
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
194
192
1
/
/
cmpltps
%
xmm8
%
xmm0
.
byte
68
15
40
13
1
159
1
0
/
/
movaps
0x19f01
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
202
/
/
subps
%
xmm10
%
xmm9
.
byte
102
69
15
56
20
209
/
/
blendvps
%
xmm0
%
xmm9
%
xmm10
.
byte
69
15
194
194
7
/
/
cmpordps
%
xmm10
%
xmm8
.
byte
69
15
84
194
/
/
andps
%
xmm10
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_sse41
.
globl
_sk_xy_to_radius_sse41
FUNCTION
(
_sk_xy_to_radius_sse41
)
_sk_xy_to_radius_sse41
:
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
65
15
81
192
/
/
sqrtps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_negate_x_sse41
.
globl
_sk_negate_x_sse41
FUNCTION
(
_sk_negate_x_sse41
)
_sk_negate_x_sse41
:
.
byte
15
87
5
24
164
1
0
/
/
xorps
0x1a418
(
%
rip
)
%
xmm0
#
3d460
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1214
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_strip_sse41
.
globl
_sk_xy_to_2pt_conical_strip_sse41
FUNCTION
(
_sk_xy_to_2pt_conical_strip_sse41
)
_sk_xy_to_2pt_conical_strip_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
69
15
92
193
/
/
subps
%
xmm9
%
xmm8
.
byte
69
15
81
192
/
/
sqrtps
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_focal_on_circle_sse41
.
globl
_sk_xy_to_2pt_conical_focal_on_circle_sse41
FUNCTION
(
_sk_xy_to_2pt_conical_focal_on_circle_sse41
)
_sk_xy_to_2pt_conical_focal_on_circle_sse41
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
94
192
/
/
divps
%
xmm0
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_well_behaved_sse41
.
globl
_sk_xy_to_2pt_conical_well_behaved_sse41
FUNCTION
(
_sk_xy_to_2pt_conical_well_behaved_sse41
)
_sk_xy_to_2pt_conical_well_behaved_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
65
15
81
193
/
/
sqrtps
%
xmm9
%
xmm0
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_greater_sse41
.
globl
_sk_xy_to_2pt_conical_greater_sse41
FUNCTION
(
_sk_xy_to_2pt_conical_greater_sse41
)
_sk_xy_to_2pt_conical_greater_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
15
81
192
/
/
sqrtps
%
xmm0
%
xmm0
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_smaller_sse41
.
globl
_sk_xy_to_2pt_conical_smaller_sse41
FUNCTION
(
_sk_xy_to_2pt_conical_smaller_sse41
)
_sk_xy_to_2pt_conical_smaller_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
15
81
192
/
/
sqrtps
%
xmm0
%
xmm0
.
byte
15
87
5
90
163
1
0
/
/
xorps
0x1a35a
(
%
rip
)
%
xmm0
#
3d460
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1214
>
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_compensate_focal_sse41
.
globl
_sk_alter_2pt_conical_compensate_focal_sse41
FUNCTION
(
_sk_alter_2pt_conical_compensate_focal_sse41
)
_sk_alter_2pt_conical_compensate_focal_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
68
/
/
movss
0x44
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_unswap_sse41
.
globl
_sk_alter_2pt_conical_unswap_sse41
FUNCTION
(
_sk_alter_2pt_conical_unswap_sse41
)
_sk_alter_2pt_conical_unswap_sse41
:
.
byte
68
15
40
5
229
157
1
0
/
/
movaps
0x19de5
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
192
/
/
subps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_nan_sse41
.
globl
_sk_mask_2pt_conical_nan_sse41
FUNCTION
(
_sk_mask_2pt_conical_nan_sse41
)
_sk_mask_2pt_conical_nan_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
194
192
7
/
/
cmpordps
%
xmm0
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
68
15
17
0
/
/
movups
%
xmm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_degenerates_sse41
.
globl
_sk_mask_2pt_conical_degenerates_sse41
FUNCTION
(
_sk_mask_2pt_conical_degenerates_sse41
)
_sk_mask_2pt_conical_degenerates_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
69
15
95
193
/
/
maxps
%
xmm9
%
xmm8
.
byte
68
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm9
.
byte
68
15
17
8
/
/
movups
%
xmm9
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_apply_vector_mask_sse41
.
globl
_sk_apply_vector_mask_sse41
FUNCTION
(
_sk_apply_vector_mask_sse41
)
_sk_apply_vector_mask_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
0
/
/
movups
(
%
rax
)
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
65
15
84
208
/
/
andps
%
xmm8
%
xmm2
.
byte
65
15
84
216
/
/
andps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_save_xy_sse41
.
globl
_sk_save_xy_sse41
FUNCTION
(
_sk_save_xy_sse41
)
_sk_save_xy_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
40
5
111
157
1
0
/
/
movaps
0x19d6f
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
17
0
/
/
movups
%
xmm0
(
%
rax
)
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
69
15
88
200
/
/
addps
%
xmm8
%
xmm9
.
byte
102
69
15
58
8
209
1
/
/
roundps
0x1
%
xmm9
%
xmm10
.
byte
69
15
92
202
/
/
subps
%
xmm10
%
xmm9
.
byte
68
15
88
193
/
/
addps
%
xmm1
%
xmm8
.
byte
102
69
15
58
8
208
1
/
/
roundps
0x1
%
xmm8
%
xmm10
.
byte
69
15
92
194
/
/
subps
%
xmm10
%
xmm8
.
byte
15
17
72
64
/
/
movups
%
xmm1
0x40
(
%
rax
)
.
byte
68
15
17
136
128
0
0
0
/
/
movups
%
xmm9
0x80
(
%
rax
)
.
byte
68
15
17
128
192
0
0
0
/
/
movups
%
xmm8
0xc0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_accumulate_sse41
.
globl
_sk_accumulate_sse41
FUNCTION
(
_sk_accumulate_sse41
)
_sk_accumulate_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
128
0
1
0
0
/
/
movups
0x100
(
%
rax
)
%
xmm8
.
byte
68
15
16
136
64
1
0
0
/
/
movups
0x140
(
%
rax
)
%
xmm9
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
65
15
88
224
/
/
addps
%
xmm8
%
xmm4
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
89
193
/
/
mulps
%
xmm1
%
xmm8
.
byte
65
15
88
232
/
/
addps
%
xmm8
%
xmm5
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
89
194
/
/
mulps
%
xmm2
%
xmm8
.
byte
65
15
88
240
/
/
addps
%
xmm8
%
xmm6
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
65
15
88
249
/
/
addps
%
xmm9
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_nx_sse41
.
globl
_sk_bilinear_nx_sse41
FUNCTION
(
_sk_bilinear_nx_sse41
)
_sk_bilinear_nx_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
8
163
1
0
/
/
addps
0x1a308
(
%
rip
)
%
xmm0
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
224
156
1
0
/
/
movaps
0x19ce0
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
17
136
0
1
0
0
/
/
movups
%
xmm9
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_px_sse41
.
globl
_sk_bilinear_px_sse41
FUNCTION
(
_sk_bilinear_px_sse41
)
_sk_bilinear_px_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
172
156
1
0
/
/
addps
0x19cac
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
17
128
0
1
0
0
/
/
movups
%
xmm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_ny_sse41
.
globl
_sk_bilinear_ny_sse41
FUNCTION
(
_sk_bilinear_ny_sse41
)
_sk_bilinear_ny_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
187
162
1
0
/
/
addps
0x1a2bb
(
%
rip
)
%
xmm1
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
147
156
1
0
/
/
movaps
0x19c93
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
17
136
64
1
0
0
/
/
movups
%
xmm9
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_py_sse41
.
globl
_sk_bilinear_py_sse41
FUNCTION
(
_sk_bilinear_py_sse41
)
_sk_bilinear_py_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
94
156
1
0
/
/
addps
0x19c5e
(
%
rip
)
%
xmm1
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
17
128
64
1
0
0
/
/
movups
%
xmm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3x_sse41
.
globl
_sk_bicubic_n3x_sse41
FUNCTION
(
_sk_bicubic_n3x_sse41
)
_sk_bicubic_n3x_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
126
162
1
0
/
/
addps
0x1a27e
(
%
rip
)
%
xmm0
#
3d540
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12f4
>
.
byte
68
15
40
13
70
156
1
0
/
/
movaps
0x19c46
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
89
13
114
162
1
0
/
/
mulps
0x1a272
(
%
rip
)
%
xmm9
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
13
42
158
1
0
/
/
addps
0x19e2a
(
%
rip
)
%
xmm9
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
68
15
17
136
0
1
0
0
/
/
movups
%
xmm9
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1x_sse41
.
globl
_sk_bicubic_n1x_sse41
FUNCTION
(
_sk_bicubic_n1x_sse41
)
_sk_bicubic_n1x_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
38
162
1
0
/
/
addps
0x1a226
(
%
rip
)
%
xmm0
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
254
155
1
0
/
/
movaps
0x19bfe
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
40
5
66
162
1
0
/
/
movaps
0x1a242
(
%
rip
)
%
xmm8
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
70
162
1
0
/
/
addps
0x1a246
(
%
rip
)
%
xmm8
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
202
155
1
0
/
/
addps
0x19bca
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
62
162
1
0
/
/
addps
0x1a23e
(
%
rip
)
%
xmm8
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
128
0
1
0
0
/
/
movups
%
xmm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1x_sse41
.
globl
_sk_bicubic_p1x_sse41
FUNCTION
(
_sk_bicubic_p1x_sse41
)
_sk_bicubic_p1x_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
40
5
168
155
1
0
/
/
movaps
0x19ba8
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
136
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm9
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
21
241
161
1
0
/
/
movaps
0x1a1f1
(
%
rip
)
%
xmm10
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
245
161
1
0
/
/
addps
0x1a1f5
(
%
rip
)
%
xmm10
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
208
/
/
addps
%
xmm8
%
xmm10
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
241
161
1
0
/
/
addps
0x1a1f1
(
%
rip
)
%
xmm10
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
144
0
1
0
0
/
/
movups
%
xmm10
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3x_sse41
.
globl
_sk_bicubic_p3x_sse41
FUNCTION
(
_sk_bicubic_p3x_sse41
)
_sk_bicubic_p3x_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
193
161
1
0
/
/
addps
0x1a1c1
(
%
rip
)
%
xmm0
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
89
5
145
161
1
0
/
/
mulps
0x1a191
(
%
rip
)
%
xmm8
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
5
73
157
1
0
/
/
addps
0x19d49
(
%
rip
)
%
xmm8
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
17
128
0
1
0
0
/
/
movups
%
xmm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3y_sse41
.
globl
_sk_bicubic_n3y_sse41
FUNCTION
(
_sk_bicubic_n3y_sse41
)
_sk_bicubic_n3y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
84
161
1
0
/
/
addps
0x1a154
(
%
rip
)
%
xmm1
#
3d540
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12f4
>
.
byte
68
15
40
13
28
155
1
0
/
/
movaps
0x19b1c
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
89
13
72
161
1
0
/
/
mulps
0x1a148
(
%
rip
)
%
xmm9
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
13
0
157
1
0
/
/
addps
0x19d00
(
%
rip
)
%
xmm9
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
68
15
17
136
64
1
0
0
/
/
movups
%
xmm9
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1y_sse41
.
globl
_sk_bicubic_n1y_sse41
FUNCTION
(
_sk_bicubic_n1y_sse41
)
_sk_bicubic_n1y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
251
160
1
0
/
/
addps
0x1a0fb
(
%
rip
)
%
xmm1
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
211
154
1
0
/
/
movaps
0x19ad3
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
40
5
23
161
1
0
/
/
movaps
0x1a117
(
%
rip
)
%
xmm8
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
27
161
1
0
/
/
addps
0x1a11b
(
%
rip
)
%
xmm8
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
159
154
1
0
/
/
addps
0x19a9f
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
19
161
1
0
/
/
addps
0x1a113
(
%
rip
)
%
xmm8
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
128
64
1
0
0
/
/
movups
%
xmm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1y_sse41
.
globl
_sk_bicubic_p1y_sse41
FUNCTION
(
_sk_bicubic_p1y_sse41
)
_sk_bicubic_p1y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
40
5
125
154
1
0
/
/
movaps
0x19a7d
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
136
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm9
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
68
15
40
21
197
160
1
0
/
/
movaps
0x1a0c5
(
%
rip
)
%
xmm10
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
201
160
1
0
/
/
addps
0x1a0c9
(
%
rip
)
%
xmm10
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
208
/
/
addps
%
xmm8
%
xmm10
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
197
160
1
0
/
/
addps
0x1a0c5
(
%
rip
)
%
xmm10
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
144
64
1
0
0
/
/
movups
%
xmm10
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3y_sse41
.
globl
_sk_bicubic_p3y_sse41
FUNCTION
(
_sk_bicubic_p3y_sse41
)
_sk_bicubic_p3y_sse41
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
148
160
1
0
/
/
addps
0x1a094
(
%
rip
)
%
xmm1
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
89
5
100
160
1
0
/
/
mulps
0x1a064
(
%
rip
)
%
xmm8
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
5
28
156
1
0
/
/
addps
0x19c1c
(
%
rip
)
%
xmm8
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
17
128
64
1
0
0
/
/
movups
%
xmm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_callback_sse41
.
globl
_sk_callback_sse41
FUNCTION
(
_sk_callback_sse41
)
_sk_callback_sse41
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
72
/
/
sub
0x48
%
rsp
.
byte
15
41
125
144
/
/
movaps
%
xmm7
-
0x70
(
%
rbp
)
.
byte
15
41
117
160
/
/
movaps
%
xmm6
-
0x60
(
%
rbp
)
.
byte
15
41
109
176
/
/
movaps
%
xmm5
-
0x50
(
%
rbp
)
.
byte
15
41
101
192
/
/
movaps
%
xmm4
-
0x40
(
%
rbp
)
.
byte
73
137
206
/
/
mov
%
rcx
%
r14
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
73
137
253
/
/
mov
%
rdi
%
r13
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
137
195
/
/
mov
%
rax
%
rbx
.
byte
73
137
244
/
/
mov
%
rsi
%
r12
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
15
21
193
/
/
unpckhps
%
xmm1
%
xmm0
.
byte
15
21
211
/
/
unpckhps
%
xmm3
%
xmm2
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
15
18
236
/
/
movhlps
%
xmm4
%
xmm5
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
102
15
20
218
/
/
unpcklpd
%
xmm2
%
xmm3
.
byte
102
15
17
75
8
/
/
movupd
%
xmm1
0x8
(
%
rbx
)
.
byte
15
18
208
/
/
movhlps
%
xmm0
%
xmm2
.
byte
15
17
107
24
/
/
movups
%
xmm5
0x18
(
%
rbx
)
.
byte
102
15
17
91
40
/
/
movupd
%
xmm3
0x28
(
%
rbx
)
.
byte
15
17
83
56
/
/
movups
%
xmm2
0x38
(
%
rbx
)
.
byte
77
133
237
/
/
test
%
r13
%
r13
.
byte
190
4
0
0
0
/
/
mov
0x4
%
esi
.
byte
65
15
69
245
/
/
cmovne
%
r13d
%
esi
.
byte
72
137
223
/
/
mov
%
rbx
%
rdi
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
72
139
131
8
1
0
0
/
/
mov
0x108
(
%
rbx
)
%
rax
.
byte
15
16
32
/
/
movups
(
%
rax
)
%
xmm4
.
byte
15
16
64
16
/
/
movups
0x10
(
%
rax
)
%
xmm0
.
byte
15
16
88
32
/
/
movups
0x20
(
%
rax
)
%
xmm3
.
byte
15
16
80
48
/
/
movups
0x30
(
%
rax
)
%
xmm2
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
15
21
224
/
/
unpckhps
%
xmm0
%
xmm4
.
byte
15
21
218
/
/
unpckhps
%
xmm2
%
xmm3
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
15
18
205
/
/
movhlps
%
xmm5
%
xmm1
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
15
18
220
/
/
movhlps
%
xmm4
%
xmm3
.
byte
76
137
230
/
/
mov
%
r12
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
137
239
/
/
mov
%
r13
%
rdi
.
byte
76
137
250
/
/
mov
%
r15
%
rdx
.
byte
76
137
241
/
/
mov
%
r14
%
rcx
.
byte
15
40
101
192
/
/
movaps
-
0x40
(
%
rbp
)
%
xmm4
.
byte
15
40
109
176
/
/
movaps
-
0x50
(
%
rbp
)
%
xmm5
.
byte
15
40
117
160
/
/
movaps
-
0x60
(
%
rbp
)
%
xmm6
.
byte
15
40
125
144
/
/
movaps
-
0x70
(
%
rbp
)
%
xmm7
.
byte
72
131
196
72
/
/
add
0x48
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_3D_sse41
.
globl
_sk_clut_3D_sse41
FUNCTION
(
_sk_clut_3D_sse41
)
_sk_clut_3D_sse41
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
56
/
/
sub
0x38
%
rsp
.
byte
15
41
124
36
32
/
/
movaps
%
xmm7
0x20
(
%
rsp
)
.
byte
15
41
116
36
16
/
/
movaps
%
xmm6
0x10
(
%
rsp
)
.
byte
15
41
44
36
/
/
movaps
%
xmm5
(
%
rsp
)
.
byte
15
41
100
36
240
/
/
movaps
%
xmm4
-
0x10
(
%
rsp
)
.
byte
15
41
92
36
224
/
/
movaps
%
xmm3
-
0x20
(
%
rsp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
72
137
76
36
136
/
/
mov
%
rcx
-
0x78
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
193
/
/
movd
%
r9d
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
41
68
36
208
/
/
movaps
%
xmm0
-
0x30
(
%
rsp
)
.
byte
243
15
91
240
/
/
cvttps2dq
%
xmm0
%
xmm6
.
byte
15
40
37
79
159
1
0
/
/
movaps
0x19f4f
(
%
rip
)
%
xmm4
#
3d590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1344
>
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
41
68
36
176
/
/
movaps
%
xmm0
-
0x50
(
%
rsp
)
.
byte
102
65
15
110
208
/
/
movd
%
r8d
%
xmm2
.
byte
102
68
15
112
202
0
/
/
pshufd
0x0
%
xmm2
%
xmm9
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
233
/
/
movd
%
r9d
%
xmm5
.
byte
102
15
112
237
0
/
/
pshufd
0x0
%
xmm5
%
xmm5
.
byte
15
91
253
/
/
cvtdq2ps
%
xmm5
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
41
124
36
144
/
/
movaps
%
xmm7
-
0x70
(
%
rsp
)
.
byte
243
15
91
239
/
/
cvttps2dq
%
xmm7
%
xmm5
.
byte
102
15
127
108
36
160
/
/
movdqa
%
xmm5
-
0x60
(
%
rsp
)
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
243
68
15
91
209
/
/
cvttps2dq
%
xmm1
%
xmm10
.
byte
102
69
15
56
64
209
/
/
pmulld
%
xmm9
%
xmm10
.
byte
102
68
15
56
64
205
/
/
pmulld
%
xmm5
%
xmm9
.
byte
102
65
15
110
192
/
/
movd
%
r8d
%
xmm0
.
byte
102
15
56
64
194
/
/
pmulld
%
xmm2
%
xmm0
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
68
15
91
217
/
/
cvtdq2ps
%
xmm1
%
xmm11
.
byte
68
15
89
219
/
/
mulps
%
xmm3
%
xmm11
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
127
108
36
192
/
/
movdqa
%
xmm5
-
0x40
(
%
rsp
)
.
byte
102
15
254
213
/
/
paddd
%
xmm5
%
xmm2
.
byte
102
68
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm8
.
byte
65
15
88
227
/
/
addps
%
xmm11
%
xmm4
.
byte
243
68
15
91
244
/
/
cvttps2dq
%
xmm4
%
xmm14
.
byte
102
69
15
56
64
240
/
/
pmulld
%
xmm8
%
xmm14
.
byte
243
65
15
91
195
/
/
cvttps2dq
%
xmm11
%
xmm0
.
byte
102
68
15
56
64
192
/
/
pmulld
%
xmm0
%
xmm8
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
254
218
/
/
paddd
%
xmm2
%
xmm3
.
byte
102
68
15
111
37
166
158
1
0
/
/
movdqa
0x19ea6
(
%
rip
)
%
xmm12
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
65
15
56
64
220
/
/
pmulld
%
xmm12
%
xmm3
.
byte
102
15
118
228
/
/
pcmpeqd
%
xmm4
%
xmm4
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
250
204
/
/
psubd
%
xmm4
%
xmm1
.
byte
102
69
15
118
237
/
/
pcmpeqd
%
xmm13
%
xmm13
.
byte
102
73
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
r8
.
byte
102
73
15
126
201
/
/
movq
%
xmm1
%
r9
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
195
/
/
mov
%
r8d
%
r11d
.
byte
243
66
15
16
12
144
/
/
movss
(
%
rax
%
r10
4
)
%
xmm1
.
byte
102
66
15
58
33
12
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm1
.
byte
243
66
15
16
36
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm4
.
byte
102
15
58
33
204
32
/
/
insertps
0x20
%
xmm4
%
xmm1
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
102
15
58
33
204
48
/
/
insertps
0x30
%
xmm4
%
xmm1
.
byte
102
65
15
126
216
/
/
movd
%
xmm3
%
r8d
.
byte
102
65
15
58
22
217
1
/
/
pextrd
0x1
%
xmm3
%
r9d
.
byte
102
65
15
58
22
218
2
/
/
pextrd
0x2
%
xmm3
%
r10d
.
byte
102
65
15
58
22
219
3
/
/
pextrd
0x3
%
xmm3
%
r11d
.
byte
102
15
111
37
202
151
1
0
/
/
movdqa
0x197ca
(
%
rip
)
%
xmm4
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
254
220
/
/
paddd
%
xmm4
%
xmm3
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
73
15
58
22
222
1
/
/
pextrq
0x1
%
xmm3
%
r14
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
65
137
223
/
/
mov
%
ebx
%
r15d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
244
/
/
mov
%
r14d
%
r12d
.
byte
243
66
15
16
36
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm4
.
byte
102
15
58
33
36
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
243
66
15
16
28
160
/
/
movss
(
%
rax
%
r12
4
)
%
xmm3
.
byte
102
15
58
33
227
32
/
/
insertps
0x20
%
xmm3
%
xmm4
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
243
66
15
16
28
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
102
15
58
33
227
48
/
/
insertps
0x30
%
xmm3
%
xmm4
.
byte
102
65
15
254
214
/
/
paddd
%
xmm14
%
xmm2
.
byte
102
65
15
56
64
212
/
/
pmulld
%
xmm12
%
xmm2
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
65
15
250
221
/
/
psubd
%
xmm13
%
xmm3
.
byte
102
69
15
118
255
/
/
pcmpeqd
%
xmm15
%
xmm15
.
byte
102
73
15
58
22
222
1
/
/
pextrq
0x1
%
xmm3
%
r14
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
65
137
223
/
/
mov
%
ebx
%
r15d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
244
/
/
mov
%
r14d
%
r12d
.
byte
243
66
15
16
28
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm3
.
byte
102
15
58
33
28
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
243
66
15
16
52
160
/
/
movss
(
%
rax
%
r12
4
)
%
xmm6
.
byte
102
15
58
33
222
32
/
/
insertps
0x20
%
xmm6
%
xmm3
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
243
66
15
16
52
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
102
15
58
33
222
48
/
/
insertps
0x30
%
xmm6
%
xmm3
.
byte
102
65
15
126
213
/
/
movd
%
xmm2
%
r13d
.
byte
102
65
15
58
22
214
1
/
/
pextrd
0x1
%
xmm2
%
r14d
.
byte
102
65
15
58
22
215
2
/
/
pextrd
0x2
%
xmm2
%
r15d
.
byte
102
65
15
58
22
212
3
/
/
pextrd
0x3
%
xmm2
%
r12d
.
byte
102
15
254
215
/
/
paddd
%
xmm7
%
xmm2
.
byte
102
68
15
111
239
/
/
movdqa
%
xmm7
%
xmm13
.
byte
102
72
15
58
22
211
1
/
/
pextrq
0x1
%
xmm2
%
rbx
.
byte
102
72
15
126
213
/
/
movq
%
xmm2
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
15
16
60
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm7
.
byte
102
15
58
33
60
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm7
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
250
32
/
/
insertps
0x20
%
xmm2
%
xmm7
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
20
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
102
15
58
33
250
48
/
/
insertps
0x30
%
xmm2
%
xmm7
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
92
216
/
/
subps
%
xmm0
%
xmm11
.
byte
243
66
15
16
20
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm2
.
byte
102
66
15
58
33
20
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
.
byte
102
66
15
58
33
20
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm2
.
byte
102
66
15
58
33
20
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm2
.
byte
243
66
15
16
4
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm0
.
byte
102
66
15
58
33
4
176
16
/
/
insertps
0x10
(
%
rax
%
r14
4
)
%
xmm0
.
byte
102
66
15
58
33
4
184
32
/
/
insertps
0x20
(
%
rax
%
r15
4
)
%
xmm0
.
byte
102
66
15
58
33
4
160
48
/
/
insertps
0x30
(
%
rax
%
r12
4
)
%
xmm0
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
65
15
89
219
/
/
mulps
%
xmm11
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
92
252
/
/
subps
%
xmm4
%
xmm7
.
byte
65
15
89
251
/
/
mulps
%
xmm11
%
xmm7
.
byte
15
88
252
/
/
addps
%
xmm4
%
xmm7
.
byte
102
65
15
111
226
/
/
movdqa
%
xmm10
%
xmm4
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
65
15
56
64
204
/
/
pmulld
%
xmm12
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
58
22
205
1
/
/
pextrd
0x1
%
xmm1
%
ebp
.
byte
102
65
15
58
22
200
2
/
/
pextrd
0x2
%
xmm1
%
r8d
.
byte
102
15
58
22
203
3
/
/
pextrd
0x3
%
xmm1
%
ebx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
20
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm2
.
byte
102
66
15
58
33
20
128
32
/
/
insertps
0x20
(
%
rax
%
r8
4
)
%
xmm2
.
byte
243
15
16
52
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
102
15
58
33
214
48
/
/
insertps
0x30
%
xmm6
%
xmm2
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
65
15
250
247
/
/
psubd
%
xmm15
%
xmm6
.
byte
102
69
15
118
255
/
/
pcmpeqd
%
xmm15
%
xmm15
.
byte
102
72
15
58
22
241
1
/
/
pextrq
0x1
%
xmm6
%
rcx
.
byte
102
72
15
126
245
/
/
movq
%
xmm6
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
66
15
16
52
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
102
15
58
33
52
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm6
.
byte
243
15
16
44
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
102
15
58
33
245
32
/
/
insertps
0x20
%
xmm5
%
xmm6
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
44
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm5
.
byte
102
15
58
33
245
48
/
/
insertps
0x30
%
xmm5
%
xmm6
.
byte
102
65
15
254
205
/
/
paddd
%
xmm13
%
xmm1
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
205
/
/
movq
%
xmm1
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
66
15
16
12
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm1
.
byte
102
15
58
33
12
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm1
.
byte
243
15
16
44
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
102
15
58
33
205
32
/
/
insertps
0x20
%
xmm5
%
xmm1
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
44
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm5
.
byte
102
15
58
33
205
48
/
/
insertps
0x30
%
xmm5
%
xmm1
.
byte
102
65
15
254
230
/
/
paddd
%
xmm14
%
xmm4
.
byte
102
65
15
56
64
228
/
/
pmulld
%
xmm12
%
xmm4
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
65
15
250
239
/
/
psubd
%
xmm15
%
xmm5
.
byte
102
72
15
58
22
233
1
/
/
pextrq
0x1
%
xmm5
%
rcx
.
byte
102
72
15
126
237
/
/
movq
%
xmm5
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
70
15
16
36
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm12
.
byte
102
68
15
58
33
36
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm12
.
byte
243
15
16
44
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
102
68
15
58
33
229
32
/
/
insertps
0x20
%
xmm5
%
xmm12
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
44
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm5
.
byte
102
68
15
58
33
229
48
/
/
insertps
0x30
%
xmm5
%
xmm12
.
byte
102
65
15
126
224
/
/
movd
%
xmm4
%
r8d
.
byte
102
65
15
58
22
227
1
/
/
pextrd
0x1
%
xmm4
%
r11d
.
byte
102
65
15
58
22
226
2
/
/
pextrd
0x2
%
xmm4
%
r10d
.
byte
102
65
15
58
22
225
3
/
/
pextrd
0x3
%
xmm4
%
r9d
.
byte
102
65
15
254
229
/
/
paddd
%
xmm13
%
xmm4
.
byte
102
72
15
58
22
225
1
/
/
pextrq
0x1
%
xmm4
%
rcx
.
byte
102
72
15
126
227
/
/
movq
%
xmm4
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
137
205
/
/
mov
%
ecx
%
ebp
.
byte
243
70
15
16
60
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm15
.
byte
102
68
15
58
33
60
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm15
.
byte
243
15
16
36
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm4
.
byte
102
68
15
58
33
252
32
/
/
insertps
0x20
%
xmm4
%
xmm15
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
36
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm4
.
byte
102
68
15
58
33
252
48
/
/
insertps
0x30
%
xmm4
%
xmm15
.
byte
243
66
15
16
36
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
102
66
15
58
33
36
152
16
/
/
insertps
0x10
(
%
rax
%
r11
4
)
%
xmm4
.
byte
102
66
15
58
33
36
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm4
.
byte
102
66
15
58
33
36
136
48
/
/
insertps
0x30
(
%
rax
%
r9
4
)
%
xmm4
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
65
15
89
227
/
/
mulps
%
xmm11
%
xmm4
.
byte
15
88
226
/
/
addps
%
xmm2
%
xmm4
.
byte
68
15
92
230
/
/
subps
%
xmm6
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
68
15
88
230
/
/
addps
%
xmm6
%
xmm12
.
byte
68
15
92
249
/
/
subps
%
xmm1
%
xmm15
.
byte
69
15
89
251
/
/
mulps
%
xmm11
%
xmm15
.
byte
68
15
88
249
/
/
addps
%
xmm1
%
xmm15
.
byte
15
91
76
36
160
/
/
cvtdq2ps
-
0x60
(
%
rsp
)
%
xmm1
.
byte
68
15
40
108
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm13
.
byte
68
15
92
233
/
/
subps
%
xmm1
%
xmm13
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
65
15
89
229
/
/
mulps
%
xmm13
%
xmm4
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
68
15
92
227
/
/
subps
%
xmm3
%
xmm12
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
68
15
88
227
/
/
addps
%
xmm3
%
xmm12
.
byte
68
15
92
255
/
/
subps
%
xmm7
%
xmm15
.
byte
69
15
89
253
/
/
mulps
%
xmm13
%
xmm15
.
byte
68
15
88
255
/
/
addps
%
xmm7
%
xmm15
.
byte
243
15
91
84
36
176
/
/
cvttps2dq
-
0x50
(
%
rsp
)
%
xmm2
.
byte
102
68
15
254
202
/
/
paddd
%
xmm2
%
xmm9
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
65
15
254
193
/
/
paddd
%
xmm9
%
xmm0
.
byte
102
15
111
45
181
154
1
0
/
/
movdqa
0x19ab5
(
%
rip
)
%
xmm5
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
56
64
197
/
/
pmulld
%
xmm5
%
xmm0
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
118
246
/
/
pcmpeqd
%
xmm6
%
xmm6
.
byte
102
15
250
206
/
/
psubd
%
xmm6
%
xmm1
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
205
/
/
movq
%
xmm1
%
rbp
.
byte
137
235
/
/
mov
%
ebp
%
ebx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
65
137
200
/
/
mov
%
ecx
%
r8d
.
byte
243
15
16
12
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
102
15
58
33
12
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm1
.
byte
243
66
15
16
28
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm3
.
byte
102
15
58
33
203
32
/
/
insertps
0x20
%
xmm3
%
xmm1
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
28
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm3
.
byte
102
15
58
33
203
48
/
/
insertps
0x30
%
xmm3
%
xmm1
.
byte
102
65
15
126
192
/
/
movd
%
xmm0
%
r8d
.
byte
102
65
15
58
22
193
1
/
/
pextrd
0x1
%
xmm0
%
r9d
.
byte
102
65
15
58
22
194
2
/
/
pextrd
0x2
%
xmm0
%
r10d
.
byte
102
65
15
58
22
195
3
/
/
pextrd
0x3
%
xmm0
%
r11d
.
byte
102
15
111
61
230
147
1
0
/
/
movdqa
0x193e6
(
%
rip
)
%
xmm7
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
254
199
/
/
paddd
%
xmm7
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
4
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm0
.
byte
102
15
58
33
4
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
243
66
15
16
28
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
102
15
58
33
195
32
/
/
insertps
0x20
%
xmm3
%
xmm0
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
28
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm3
.
byte
102
15
58
33
195
48
/
/
insertps
0x30
%
xmm3
%
xmm0
.
byte
102
69
15
254
206
/
/
paddd
%
xmm14
%
xmm9
.
byte
102
68
15
56
64
205
/
/
pmulld
%
xmm5
%
xmm9
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
250
222
/
/
psubd
%
xmm6
%
xmm3
.
byte
102
72
15
58
22
217
1
/
/
pextrq
0x1
%
xmm3
%
rcx
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
52
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm6
.
byte
102
15
58
33
52
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
243
66
15
16
28
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
102
15
58
33
243
32
/
/
insertps
0x20
%
xmm3
%
xmm6
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
28
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm3
.
byte
102
15
58
33
243
48
/
/
insertps
0x30
%
xmm3
%
xmm6
.
byte
102
69
15
126
205
/
/
movd
%
xmm9
%
r13d
.
byte
102
69
15
58
22
207
1
/
/
pextrd
0x1
%
xmm9
%
r15d
.
byte
102
69
15
58
22
204
2
/
/
pextrd
0x2
%
xmm9
%
r12d
.
byte
102
69
15
58
22
206
3
/
/
pextrd
0x3
%
xmm9
%
r14d
.
byte
102
68
15
254
207
/
/
paddd
%
xmm7
%
xmm9
.
byte
102
76
15
58
22
203
1
/
/
pextrq
0x1
%
xmm9
%
rbx
.
byte
102
76
15
126
205
/
/
movq
%
xmm9
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
15
16
60
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm7
.
byte
102
15
58
33
60
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm7
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
28
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm3
.
byte
102
15
58
33
251
32
/
/
insertps
0x20
%
xmm3
%
xmm7
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
28
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
102
15
58
33
251
48
/
/
insertps
0x30
%
xmm3
%
xmm7
.
byte
243
66
15
16
44
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
102
66
15
58
33
44
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
.
byte
102
66
15
58
33
44
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm5
.
byte
102
66
15
58
33
44
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm5
.
byte
243
66
15
16
28
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm3
.
byte
102
66
15
58
33
28
184
16
/
/
insertps
0x10
(
%
rax
%
r15
4
)
%
xmm3
.
byte
102
66
15
58
33
28
160
32
/
/
insertps
0x20
(
%
rax
%
r12
4
)
%
xmm3
.
byte
102
66
15
58
33
28
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm3
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
65
15
89
219
/
/
mulps
%
xmm11
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
65
15
89
243
/
/
mulps
%
xmm11
%
xmm6
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
15
92
248
/
/
subps
%
xmm0
%
xmm7
.
byte
65
15
89
251
/
/
mulps
%
xmm11
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
102
68
15
254
210
/
/
paddd
%
xmm2
%
xmm10
.
byte
102
69
15
254
194
/
/
paddd
%
xmm10
%
xmm8
.
byte
102
15
111
13
241
152
1
0
/
/
movdqa
0x198f1
(
%
rip
)
%
xmm1
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
68
15
56
64
193
/
/
pmulld
%
xmm1
%
xmm8
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
15
118
237
/
/
pcmpeqd
%
xmm5
%
xmm5
.
byte
102
15
250
197
/
/
psubd
%
xmm5
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
197
/
/
movq
%
xmm0
%
rbp
.
byte
137
235
/
/
mov
%
ebp
%
ebx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
65
137
200
/
/
mov
%
ecx
%
r8d
.
byte
243
68
15
16
12
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm9
.
byte
102
68
15
58
33
12
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm9
.
byte
243
66
15
16
4
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm0
.
byte
102
68
15
58
33
200
32
/
/
insertps
0x20
%
xmm0
%
xmm9
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
68
15
58
33
200
48
/
/
insertps
0x30
%
xmm0
%
xmm9
.
byte
102
69
15
126
192
/
/
movd
%
xmm8
%
r8d
.
byte
102
69
15
58
22
193
1
/
/
pextrd
0x1
%
xmm8
%
r9d
.
byte
102
69
15
58
22
194
2
/
/
pextrd
0x2
%
xmm8
%
r10d
.
byte
102
69
15
58
22
195
3
/
/
pextrd
0x3
%
xmm8
%
r11d
.
byte
102
15
111
21
28
146
1
0
/
/
movdqa
0x1921c
(
%
rip
)
%
xmm2
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
68
15
254
194
/
/
paddd
%
xmm2
%
xmm8
.
byte
102
76
15
58
22
193
1
/
/
pextrq
0x1
%
xmm8
%
rcx
.
byte
102
76
15
126
195
/
/
movq
%
xmm8
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
68
15
16
4
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm8
.
byte
102
68
15
58
33
4
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm8
.
byte
243
66
15
16
4
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm0
.
byte
102
68
15
58
33
192
32
/
/
insertps
0x20
%
xmm0
%
xmm8
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
68
15
58
33
192
48
/
/
insertps
0x30
%
xmm0
%
xmm8
.
byte
102
69
15
254
214
/
/
paddd
%
xmm14
%
xmm10
.
byte
102
68
15
56
64
209
/
/
pmulld
%
xmm1
%
xmm10
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
15
250
197
/
/
psubd
%
xmm5
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
12
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm1
.
byte
102
15
58
33
12
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
243
66
15
16
4
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm0
.
byte
102
15
58
33
200
32
/
/
insertps
0x20
%
xmm0
%
xmm1
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
15
58
33
200
48
/
/
insertps
0x30
%
xmm0
%
xmm1
.
byte
102
69
15
126
213
/
/
movd
%
xmm10
%
r13d
.
byte
102
69
15
58
22
215
1
/
/
pextrd
0x1
%
xmm10
%
r15d
.
byte
102
69
15
58
22
212
2
/
/
pextrd
0x2
%
xmm10
%
r12d
.
byte
102
69
15
58
22
214
3
/
/
pextrd
0x3
%
xmm10
%
r14d
.
byte
102
68
15
254
210
/
/
paddd
%
xmm2
%
xmm10
.
byte
102
76
15
58
22
211
1
/
/
pextrq
0x1
%
xmm10
%
rbx
.
byte
102
76
15
126
213
/
/
movq
%
xmm10
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
20
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm2
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
15
58
33
208
32
/
/
insertps
0x20
%
xmm0
%
xmm2
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
4
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
102
15
58
33
208
48
/
/
insertps
0x30
%
xmm0
%
xmm2
.
byte
243
66
15
16
44
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
102
66
15
58
33
44
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
.
byte
102
66
15
58
33
44
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm5
.
byte
102
66
15
58
33
44
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm5
.
byte
243
66
15
16
4
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm0
.
byte
102
66
15
58
33
4
184
16
/
/
insertps
0x10
(
%
rax
%
r15
4
)
%
xmm0
.
byte
102
66
15
58
33
4
160
32
/
/
insertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
.
byte
102
66
15
58
33
4
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm0
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
65
15
92
201
/
/
subps
%
xmm9
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
65
15
92
208
/
/
subps
%
xmm8
%
xmm2
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
65
15
89
197
/
/
mulps
%
xmm13
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
65
15
89
205
/
/
mulps
%
xmm13
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
92
215
/
/
subps
%
xmm7
%
xmm2
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
91
92
36
192
/
/
cvtdq2ps
-
0x40
(
%
rsp
)
%
xmm3
.
byte
15
40
108
36
208
/
/
movaps
-
0x30
(
%
rsp
)
%
xmm5
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
65
15
92
204
/
/
subps
%
xmm12
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
65
15
88
204
/
/
addps
%
xmm12
%
xmm1
.
byte
65
15
92
215
/
/
subps
%
xmm15
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
65
15
88
215
/
/
addps
%
xmm15
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
139
76
36
136
/
/
mov
-
0x78
(
%
rsp
)
%
rcx
.
byte
15
40
92
36
224
/
/
movaps
-
0x20
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm4
.
byte
15
40
44
36
/
/
movaps
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
16
/
/
movaps
0x10
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
32
/
/
movaps
0x20
(
%
rsp
)
%
xmm7
.
byte
72
131
196
56
/
/
add
0x38
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_4D_sse41
.
globl
_sk_clut_4D_sse41
FUNCTION
(
_sk_clut_4D_sse41
)
_sk_clut_4D_sse41
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
129
236
136
0
0
0
/
/
sub
0x88
%
rsp
.
byte
15
41
124
36
112
/
/
movaps
%
xmm7
0x70
(
%
rsp
)
.
byte
15
41
116
36
96
/
/
movaps
%
xmm6
0x60
(
%
rsp
)
.
byte
15
41
108
36
80
/
/
movaps
%
xmm5
0x50
(
%
rsp
)
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
rsp
)
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
72
137
76
36
248
/
/
mov
%
rcx
-
0x8
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
20
/
/
mov
0x14
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
201
/
/
movd
%
r9d
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
41
76
36
48
/
/
movaps
%
xmm1
0x30
(
%
rsp
)
.
byte
243
15
91
249
/
/
cvttps2dq
%
xmm1
%
xmm7
.
byte
15
40
29
79
150
1
0
/
/
movaps
0x1964f
(
%
rip
)
%
xmm3
#
3d590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1344
>
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
41
76
36
160
/
/
movaps
%
xmm1
-
0x60
(
%
rsp
)
.
byte
102
65
15
110
232
/
/
movd
%
r8d
%
xmm5
.
byte
102
68
15
112
237
0
/
/
pshufd
0x0
%
xmm5
%
xmm13
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
201
/
/
movd
%
r9d
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
41
76
36
224
/
/
movaps
%
xmm1
-
0x20
(
%
rsp
)
.
byte
243
15
91
209
/
/
cvttps2dq
%
xmm1
%
xmm2
.
byte
102
15
127
84
36
32
/
/
movdqa
%
xmm2
0x20
(
%
rsp
)
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
56
64
205
/
/
pmulld
%
xmm13
%
xmm1
.
byte
102
15
127
76
36
144
/
/
movdqa
%
xmm1
-
0x70
(
%
rsp
)
.
byte
102
68
15
56
64
234
/
/
pmulld
%
xmm2
%
xmm13
.
byte
102
65
15
110
240
/
/
movd
%
r8d
%
xmm6
.
byte
102
15
56
64
245
/
/
pmulld
%
xmm5
%
xmm6
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
201
/
/
movd
%
r9d
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
15
91
209
/
/
cvtdq2ps
%
xmm1
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
68
139
72
8
/
/
mov
0x8
(
%
rax
)
%
r9d
.
byte
65
255
201
/
/
dec
%
r9d
.
byte
102
65
15
110
201
/
/
movd
%
r9d
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
68
15
91
225
/
/
cvtdq2ps
%
xmm1
%
xmm12
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
102
65
15
111
229
/
/
movdqa
%
xmm13
%
xmm4
.
byte
102
15
127
124
36
16
/
/
movdqa
%
xmm7
0x10
(
%
rsp
)
.
byte
102
15
254
231
/
/
paddd
%
xmm7
%
xmm4
.
byte
102
65
15
110
232
/
/
movd
%
r8d
%
xmm5
.
byte
102
15
56
64
238
/
/
pmulld
%
xmm6
%
xmm5
.
byte
102
68
15
112
246
0
/
/
pshufd
0x0
%
xmm6
%
xmm14
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
243
68
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm8
.
byte
102
69
15
56
64
198
/
/
pmulld
%
xmm14
%
xmm8
.
byte
102
68
15
127
68
36
208
/
/
movdqa
%
xmm8
-
0x30
(
%
rsp
)
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
127
68
36
128
/
/
movdqa
%
xmm0
-
0x80
(
%
rsp
)
.
byte
102
68
15
56
64
240
/
/
pmulld
%
xmm0
%
xmm14
.
byte
102
65
15
111
206
/
/
movdqa
%
xmm14
%
xmm1
.
byte
102
15
254
204
/
/
paddd
%
xmm4
%
xmm1
.
byte
102
68
15
112
253
0
/
/
pshufd
0x0
%
xmm5
%
xmm15
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
243
68
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm9
.
byte
102
69
15
56
64
207
/
/
pmulld
%
xmm15
%
xmm9
.
byte
243
65
15
91
236
/
/
cvttps2dq
%
xmm12
%
xmm5
.
byte
102
68
15
56
64
253
/
/
pmulld
%
xmm5
%
xmm15
.
byte
102
65
15
111
223
/
/
movdqa
%
xmm15
%
xmm3
.
byte
102
15
254
217
/
/
paddd
%
xmm1
%
xmm3
.
byte
102
68
15
111
29
78
149
1
0
/
/
movdqa
0x1954e
(
%
rip
)
%
xmm11
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
65
15
56
64
219
/
/
pmulld
%
xmm11
%
xmm3
.
byte
102
15
118
192
/
/
pcmpeqd
%
xmm0
%
xmm0
.
byte
102
15
111
243
/
/
movdqa
%
xmm3
%
xmm6
.
byte
102
15
250
240
/
/
psubd
%
xmm0
%
xmm6
.
byte
102
73
15
58
22
240
1
/
/
pextrq
0x1
%
xmm6
%
r8
.
byte
102
73
15
126
241
/
/
movq
%
xmm6
%
r9
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
69
137
195
/
/
mov
%
r8d
%
r11d
.
byte
243
66
15
16
60
144
/
/
movss
(
%
rax
%
r10
4
)
%
xmm7
.
byte
102
66
15
58
33
60
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
.
byte
243
66
15
16
52
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm6
.
byte
102
15
58
33
254
32
/
/
insertps
0x20
%
xmm6
%
xmm7
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
52
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
102
15
58
33
254
48
/
/
insertps
0x30
%
xmm6
%
xmm7
.
byte
102
65
15
126
216
/
/
movd
%
xmm3
%
r8d
.
byte
102
65
15
58
22
217
1
/
/
pextrd
0x1
%
xmm3
%
r9d
.
byte
102
65
15
58
22
218
2
/
/
pextrd
0x2
%
xmm3
%
r10d
.
byte
102
65
15
58
22
219
3
/
/
pextrd
0x3
%
xmm3
%
r11d
.
byte
102
68
15
111
21
118
142
1
0
/
/
movdqa
0x18e76
(
%
rip
)
%
xmm10
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
65
15
254
218
/
/
paddd
%
xmm10
%
xmm3
.
byte
102
73
15
58
22
222
1
/
/
pextrq
0x1
%
xmm3
%
r14
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
65
137
223
/
/
mov
%
ebx
%
r15d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
244
/
/
mov
%
r14d
%
r12d
.
byte
243
66
15
16
4
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm0
.
byte
102
15
58
33
4
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
243
66
15
16
28
160
/
/
movss
(
%
rax
%
r12
4
)
%
xmm3
.
byte
102
15
58
33
195
32
/
/
insertps
0x20
%
xmm3
%
xmm0
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
243
66
15
16
28
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm3
.
byte
102
15
58
33
195
48
/
/
insertps
0x30
%
xmm3
%
xmm0
.
byte
102
68
15
127
76
36
176
/
/
movdqa
%
xmm9
-
0x50
(
%
rsp
)
.
byte
102
65
15
254
201
/
/
paddd
%
xmm9
%
xmm1
.
byte
102
65
15
56
64
203
/
/
pmulld
%
xmm11
%
xmm1
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
15
250
29
132
148
1
0
/
/
psubd
0x19484
(
%
rip
)
%
xmm3
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
73
15
58
22
222
1
/
/
pextrq
0x1
%
xmm3
%
r14
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
65
137
223
/
/
mov
%
ebx
%
r15d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
69
137
244
/
/
mov
%
r14d
%
r12d
.
byte
243
66
15
16
28
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm3
.
byte
102
15
58
33
28
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
243
66
15
16
52
160
/
/
movss
(
%
rax
%
r12
4
)
%
xmm6
.
byte
102
15
58
33
222
32
/
/
insertps
0x20
%
xmm6
%
xmm3
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
243
66
15
16
52
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
102
15
58
33
222
48
/
/
insertps
0x30
%
xmm6
%
xmm3
.
byte
102
65
15
126
205
/
/
movd
%
xmm1
%
r13d
.
byte
102
65
15
58
22
206
1
/
/
pextrd
0x1
%
xmm1
%
r14d
.
byte
102
65
15
58
22
207
2
/
/
pextrd
0x2
%
xmm1
%
r15d
.
byte
102
65
15
58
22
204
3
/
/
pextrd
0x3
%
xmm1
%
r12d
.
byte
102
65
15
254
202
/
/
paddd
%
xmm10
%
xmm1
.
byte
102
72
15
58
22
203
1
/
/
pextrq
0x1
%
xmm1
%
rbx
.
byte
102
72
15
126
205
/
/
movq
%
xmm1
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
15
58
33
12
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm1
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
52
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm6
.
byte
102
15
58
33
206
32
/
/
insertps
0x20
%
xmm6
%
xmm1
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
52
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
102
15
58
33
206
48
/
/
insertps
0x30
%
xmm6
%
xmm1
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
68
15
92
229
/
/
subps
%
xmm5
%
xmm12
.
byte
243
66
15
16
44
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm5
.
byte
102
66
15
58
33
44
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm5
.
byte
102
66
15
58
33
44
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm5
.
byte
102
66
15
58
33
44
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm5
.
byte
243
66
15
16
52
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm6
.
byte
102
66
15
58
33
52
176
16
/
/
insertps
0x10
(
%
rax
%
r14
4
)
%
xmm6
.
byte
102
66
15
58
33
52
184
32
/
/
insertps
0x20
(
%
rax
%
r15
4
)
%
xmm6
.
byte
102
66
15
58
33
52
160
48
/
/
insertps
0x30
(
%
rax
%
r12
4
)
%
xmm6
.
byte
15
92
245
/
/
subps
%
xmm5
%
xmm6
.
byte
65
15
89
244
/
/
mulps
%
xmm12
%
xmm6
.
byte
15
88
245
/
/
addps
%
xmm5
%
xmm6
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
65
15
89
220
/
/
mulps
%
xmm12
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
65
15
89
204
/
/
mulps
%
xmm12
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
102
65
15
254
224
/
/
paddd
%
xmm8
%
xmm4
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
68
15
127
124
36
192
/
/
movdqa
%
xmm15
-
0x40
(
%
rsp
)
.
byte
102
65
15
254
199
/
/
paddd
%
xmm15
%
xmm0
.
byte
102
65
15
56
64
195
/
/
pmulld
%
xmm11
%
xmm0
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
250
45
100
147
1
0
/
/
psubd
0x19364
(
%
rip
)
%
xmm5
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
72
15
58
22
233
1
/
/
pextrq
0x1
%
xmm5
%
rcx
.
byte
102
72
15
126
237
/
/
movq
%
xmm5
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
66
15
16
60
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
102
15
58
33
60
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm7
.
byte
243
15
16
44
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
102
15
58
33
253
32
/
/
insertps
0x20
%
xmm5
%
xmm7
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
44
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm5
.
byte
102
15
58
33
253
48
/
/
insertps
0x30
%
xmm5
%
xmm7
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
65
15
58
22
195
1
/
/
pextrd
0x1
%
xmm0
%
r11d
.
byte
102
65
15
58
22
194
2
/
/
pextrd
0x2
%
xmm0
%
r10d
.
byte
102
65
15
58
22
192
3
/
/
pextrd
0x3
%
xmm0
%
r8d
.
byte
102
65
15
254
194
/
/
paddd
%
xmm10
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
137
205
/
/
mov
%
ecx
%
ebp
.
byte
243
66
15
16
4
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm0
.
byte
102
15
58
33
4
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
243
15
16
44
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm5
.
byte
102
15
58
33
197
32
/
/
insertps
0x20
%
xmm5
%
xmm0
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
44
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm5
.
byte
102
15
58
33
197
48
/
/
insertps
0x30
%
xmm5
%
xmm0
.
byte
102
65
15
254
225
/
/
paddd
%
xmm9
%
xmm4
.
byte
102
65
15
56
64
227
/
/
pmulld
%
xmm11
%
xmm4
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
65
15
250
232
/
/
psubd
%
xmm8
%
xmm5
.
byte
102
72
15
58
22
233
1
/
/
pextrq
0x1
%
xmm5
%
rcx
.
byte
102
72
15
126
235
/
/
movq
%
xmm5
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
137
205
/
/
mov
%
ecx
%
ebp
.
byte
243
70
15
16
4
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm8
.
byte
102
68
15
58
33
4
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm8
.
byte
243
15
16
44
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm5
.
byte
102
68
15
58
33
197
32
/
/
insertps
0x20
%
xmm5
%
xmm8
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
44
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm5
.
byte
102
68
15
58
33
197
48
/
/
insertps
0x30
%
xmm5
%
xmm8
.
byte
102
65
15
126
231
/
/
movd
%
xmm4
%
r15d
.
byte
102
65
15
58
22
228
1
/
/
pextrd
0x1
%
xmm4
%
r12d
.
byte
102
65
15
58
22
229
2
/
/
pextrd
0x2
%
xmm4
%
r13d
.
byte
102
65
15
58
22
230
3
/
/
pextrd
0x3
%
xmm4
%
r14d
.
byte
102
65
15
254
226
/
/
paddd
%
xmm10
%
xmm4
.
byte
102
72
15
58
22
225
1
/
/
pextrq
0x1
%
xmm4
%
rcx
.
byte
102
72
15
126
227
/
/
movq
%
xmm4
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
68
15
16
12
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm9
.
byte
102
68
15
58
33
12
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm9
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
15
16
36
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
102
68
15
58
33
204
32
/
/
insertps
0x20
%
xmm4
%
xmm9
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
36
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm4
.
byte
102
68
15
58
33
204
48
/
/
insertps
0x30
%
xmm4
%
xmm9
.
byte
243
66
15
16
36
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm4
.
byte
102
66
15
58
33
36
152
16
/
/
insertps
0x10
(
%
rax
%
r11
4
)
%
xmm4
.
byte
102
66
15
58
33
36
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm4
.
byte
102
66
15
58
33
36
128
48
/
/
insertps
0x30
(
%
rax
%
r8
4
)
%
xmm4
.
byte
243
66
15
16
44
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm5
.
byte
102
66
15
58
33
44
160
16
/
/
insertps
0x10
(
%
rax
%
r12
4
)
%
xmm5
.
byte
102
66
15
58
33
44
168
32
/
/
insertps
0x20
(
%
rax
%
r13
4
)
%
xmm5
.
byte
102
66
15
58
33
44
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
65
15
89
236
/
/
mulps
%
xmm12
%
xmm5
.
byte
15
88
236
/
/
addps
%
xmm4
%
xmm5
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
69
15
89
196
/
/
mulps
%
xmm12
%
xmm8
.
byte
68
15
88
199
/
/
addps
%
xmm7
%
xmm8
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
15
91
68
36
128
/
/
cvtdq2ps
-
0x80
(
%
rsp
)
%
xmm0
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
15
41
84
36
128
/
/
movaps
%
xmm2
-
0x80
(
%
rsp
)
.
byte
15
92
238
/
/
subps
%
xmm6
%
xmm5
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
68
15
89
194
/
/
mulps
%
xmm2
%
xmm8
.
byte
68
15
88
195
/
/
addps
%
xmm3
%
xmm8
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
68
15
89
202
/
/
mulps
%
xmm2
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
102
15
111
68
36
144
/
/
movdqa
-
0x70
(
%
rsp
)
%
xmm0
.
byte
102
15
254
68
36
16
/
/
paddd
0x10
(
%
rsp
)
%
xmm0
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
65
15
254
206
/
/
paddd
%
xmm14
%
xmm1
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
65
15
254
223
/
/
paddd
%
xmm15
%
xmm3
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
15
56
64
218
/
/
pmulld
%
xmm2
%
xmm3
.
byte
102
15
126
217
/
/
movd
%
xmm3
%
ecx
.
byte
102
15
58
22
221
1
/
/
pextrd
0x1
%
xmm3
%
ebp
.
byte
102
65
15
58
22
216
2
/
/
pextrd
0x2
%
xmm3
%
r8d
.
byte
102
15
58
22
219
3
/
/
pextrd
0x3
%
xmm3
%
ebx
.
byte
243
68
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm10
.
byte
102
68
15
58
33
20
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm10
.
byte
102
70
15
58
33
20
128
32
/
/
insertps
0x20
(
%
rax
%
r8
4
)
%
xmm10
.
byte
243
15
16
36
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
102
68
15
58
33
212
48
/
/
insertps
0x30
%
xmm4
%
xmm10
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
102
15
118
246
/
/
pcmpeqd
%
xmm6
%
xmm6
.
byte
102
15
250
230
/
/
psubd
%
xmm6
%
xmm4
.
byte
102
72
15
58
22
225
1
/
/
pextrq
0x1
%
xmm4
%
rcx
.
byte
102
72
15
126
229
/
/
movq
%
xmm4
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
70
15
16
28
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm11
.
byte
102
68
15
58
33
28
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm11
.
byte
243
15
16
36
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm4
.
byte
102
68
15
58
33
220
32
/
/
insertps
0x20
%
xmm4
%
xmm11
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
36
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm4
.
byte
102
68
15
58
33
220
48
/
/
insertps
0x30
%
xmm4
%
xmm11
.
byte
102
15
111
61
104
138
1
0
/
/
movdqa
0x18a68
(
%
rip
)
%
xmm7
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
254
223
/
/
paddd
%
xmm7
%
xmm3
.
byte
102
72
15
58
22
217
1
/
/
pextrq
0x1
%
xmm3
%
rcx
.
byte
102
72
15
126
221
/
/
movq
%
xmm3
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
66
15
16
36
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
102
15
58
33
36
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm4
.
byte
243
15
16
28
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
102
15
58
33
227
32
/
/
insertps
0x20
%
xmm3
%
xmm4
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
28
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm3
.
byte
102
15
58
33
227
48
/
/
insertps
0x30
%
xmm3
%
xmm4
.
byte
102
68
15
111
124
36
176
/
/
movdqa
-
0x50
(
%
rsp
)
%
xmm15
.
byte
102
65
15
254
207
/
/
paddd
%
xmm15
%
xmm1
.
byte
102
15
56
64
202
/
/
pmulld
%
xmm2
%
xmm1
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
15
250
222
/
/
psubd
%
xmm6
%
xmm3
.
byte
102
72
15
58
22
217
1
/
/
pextrq
0x1
%
xmm3
%
rcx
.
byte
102
72
15
126
221
/
/
movq
%
xmm3
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
66
15
16
52
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
102
15
58
33
52
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm6
.
byte
243
15
16
28
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm3
.
byte
102
15
58
33
243
32
/
/
insertps
0x20
%
xmm3
%
xmm6
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
28
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm3
.
byte
102
15
58
33
243
48
/
/
insertps
0x30
%
xmm3
%
xmm6
.
byte
102
65
15
126
201
/
/
movd
%
xmm1
%
r9d
.
byte
102
65
15
58
22
203
1
/
/
pextrd
0x1
%
xmm1
%
r11d
.
byte
102
65
15
58
22
202
2
/
/
pextrd
0x2
%
xmm1
%
r10d
.
byte
102
65
15
58
22
200
3
/
/
pextrd
0x3
%
xmm1
%
r8d
.
byte
102
15
254
207
/
/
paddd
%
xmm7
%
xmm1
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
137
205
/
/
mov
%
ecx
%
ebp
.
byte
243
66
15
16
60
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm7
.
byte
102
15
58
33
60
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
243
15
16
12
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm1
.
byte
102
15
58
33
249
32
/
/
insertps
0x20
%
xmm1
%
xmm7
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
15
58
33
249
48
/
/
insertps
0x30
%
xmm1
%
xmm7
.
byte
243
66
15
16
28
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm3
.
byte
102
66
15
58
33
28
152
16
/
/
insertps
0x10
(
%
rax
%
r11
4
)
%
xmm3
.
byte
102
66
15
58
33
28
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm3
.
byte
102
66
15
58
33
28
128
48
/
/
insertps
0x30
(
%
rax
%
r8
4
)
%
xmm3
.
byte
65
15
92
218
/
/
subps
%
xmm10
%
xmm3
.
byte
68
15
41
36
36
/
/
movaps
%
xmm12
(
%
rsp
)
.
byte
65
15
89
220
/
/
mulps
%
xmm12
%
xmm3
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
65
15
92
243
/
/
subps
%
xmm11
%
xmm6
.
byte
65
15
89
244
/
/
mulps
%
xmm12
%
xmm6
.
byte
65
15
88
243
/
/
addps
%
xmm11
%
xmm6
.
byte
15
92
252
/
/
subps
%
xmm4
%
xmm7
.
byte
65
15
89
252
/
/
mulps
%
xmm12
%
xmm7
.
byte
15
88
252
/
/
addps
%
xmm4
%
xmm7
.
byte
102
15
254
68
36
208
/
/
paddd
-
0x30
(
%
rsp
)
%
xmm0
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
254
76
36
192
/
/
paddd
-
0x40
(
%
rsp
)
%
xmm1
.
byte
102
68
15
111
218
/
/
movdqa
%
xmm2
%
xmm11
.
byte
102
65
15
56
64
203
/
/
pmulld
%
xmm11
%
xmm1
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
69
15
118
228
/
/
pcmpeqd
%
xmm12
%
xmm12
.
byte
102
65
15
250
228
/
/
psubd
%
xmm12
%
xmm4
.
byte
102
72
15
58
22
225
1
/
/
pextrq
0x1
%
xmm4
%
rcx
.
byte
102
72
15
126
229
/
/
movq
%
xmm4
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
66
15
16
36
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm4
.
byte
102
15
58
33
36
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm4
.
byte
243
15
16
20
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
102
15
58
33
226
32
/
/
insertps
0x20
%
xmm2
%
xmm4
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
226
48
/
/
insertps
0x30
%
xmm2
%
xmm4
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
65
15
58
22
201
1
/
/
pextrd
0x1
%
xmm1
%
r9d
.
byte
102
65
15
58
22
202
2
/
/
pextrd
0x2
%
xmm1
%
r10d
.
byte
102
65
15
58
22
203
3
/
/
pextrd
0x3
%
xmm1
%
r11d
.
byte
102
68
15
111
21
172
136
1
0
/
/
movdqa
0x188ac
(
%
rip
)
%
xmm10
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
65
15
254
202
/
/
paddd
%
xmm10
%
xmm1
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
137
205
/
/
mov
%
ecx
%
ebp
.
byte
243
66
15
16
12
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
102
15
58
33
12
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
243
15
16
20
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm2
.
byte
102
15
58
33
202
32
/
/
insertps
0x20
%
xmm2
%
xmm1
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
202
48
/
/
insertps
0x30
%
xmm2
%
xmm1
.
byte
102
65
15
254
199
/
/
paddd
%
xmm15
%
xmm0
.
byte
102
65
15
56
64
195
/
/
pmulld
%
xmm11
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
65
15
250
212
/
/
psubd
%
xmm12
%
xmm2
.
byte
102
72
15
58
22
209
1
/
/
pextrq
0x1
%
xmm2
%
rcx
.
byte
102
72
15
126
211
/
/
movq
%
xmm2
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
137
205
/
/
mov
%
ecx
%
ebp
.
byte
243
70
15
16
60
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm15
.
byte
102
68
15
58
33
60
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm15
.
byte
243
15
16
20
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm2
.
byte
102
68
15
58
33
250
32
/
/
insertps
0x20
%
xmm2
%
xmm15
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
68
15
58
33
250
48
/
/
insertps
0x30
%
xmm2
%
xmm15
.
byte
102
65
15
126
199
/
/
movd
%
xmm0
%
r15d
.
byte
102
65
15
58
22
196
1
/
/
pextrd
0x1
%
xmm0
%
r12d
.
byte
102
65
15
58
22
197
2
/
/
pextrd
0x2
%
xmm0
%
r13d
.
byte
102
65
15
58
22
198
3
/
/
pextrd
0x3
%
xmm0
%
r14d
.
byte
102
65
15
254
194
/
/
paddd
%
xmm10
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
68
15
16
28
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm11
.
byte
102
68
15
58
33
28
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm11
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
243
15
16
4
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
102
68
15
58
33
216
32
/
/
insertps
0x20
%
xmm0
%
xmm11
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
68
15
58
33
216
48
/
/
insertps
0x30
%
xmm0
%
xmm11
.
byte
243
66
15
16
4
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm0
.
byte
102
66
15
58
33
4
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm0
.
byte
102
66
15
58
33
4
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm0
.
byte
102
66
15
58
33
4
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm0
.
byte
243
70
15
16
20
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm10
.
byte
102
70
15
58
33
20
160
16
/
/
insertps
0x10
(
%
rax
%
r12
4
)
%
xmm10
.
byte
102
70
15
58
33
20
168
32
/
/
insertps
0x20
(
%
rax
%
r13
4
)
%
xmm10
.
byte
102
70
15
58
33
20
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm10
.
byte
68
15
92
208
/
/
subps
%
xmm0
%
xmm10
.
byte
68
15
40
36
36
/
/
movaps
(
%
rsp
)
%
xmm12
.
byte
69
15
89
212
/
/
mulps
%
xmm12
%
xmm10
.
byte
68
15
88
208
/
/
addps
%
xmm0
%
xmm10
.
byte
68
15
92
252
/
/
subps
%
xmm4
%
xmm15
.
byte
69
15
89
252
/
/
mulps
%
xmm12
%
xmm15
.
byte
68
15
88
252
/
/
addps
%
xmm4
%
xmm15
.
byte
68
15
92
217
/
/
subps
%
xmm1
%
xmm11
.
byte
69
15
89
220
/
/
mulps
%
xmm12
%
xmm11
.
byte
68
15
88
217
/
/
addps
%
xmm1
%
xmm11
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
15
40
68
36
128
/
/
movaps
-
0x80
(
%
rsp
)
%
xmm0
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
68
15
88
211
/
/
addps
%
xmm3
%
xmm10
.
byte
68
15
92
254
/
/
subps
%
xmm6
%
xmm15
.
byte
68
15
89
248
/
/
mulps
%
xmm0
%
xmm15
.
byte
68
15
88
254
/
/
addps
%
xmm6
%
xmm15
.
byte
68
15
92
223
/
/
subps
%
xmm7
%
xmm11
.
byte
68
15
89
216
/
/
mulps
%
xmm0
%
xmm11
.
byte
68
15
88
223
/
/
addps
%
xmm7
%
xmm11
.
byte
15
91
68
36
32
/
/
cvtdq2ps
0x20
(
%
rsp
)
%
xmm0
.
byte
15
40
76
36
224
/
/
movaps
-
0x20
(
%
rsp
)
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
41
76
36
224
/
/
movaps
%
xmm1
-
0x20
(
%
rsp
)
.
byte
68
15
92
213
/
/
subps
%
xmm5
%
xmm10
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
68
15
88
213
/
/
addps
%
xmm5
%
xmm10
.
byte
69
15
92
248
/
/
subps
%
xmm8
%
xmm15
.
byte
68
15
89
249
/
/
mulps
%
xmm1
%
xmm15
.
byte
69
15
88
248
/
/
addps
%
xmm8
%
xmm15
.
byte
69
15
92
217
/
/
subps
%
xmm9
%
xmm11
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
243
15
91
68
36
160
/
/
cvttps2dq
-
0x60
(
%
rsp
)
%
xmm0
.
byte
102
15
127
68
36
160
/
/
movdqa
%
xmm0
-
0x60
(
%
rsp
)
.
byte
102
68
15
254
232
/
/
paddd
%
xmm0
%
xmm13
.
byte
102
65
15
111
222
/
/
movdqa
%
xmm14
%
xmm3
.
byte
102
65
15
254
221
/
/
paddd
%
xmm13
%
xmm3
.
byte
102
15
111
124
36
192
/
/
movdqa
-
0x40
(
%
rsp
)
%
xmm7
.
byte
102
15
111
199
/
/
movdqa
%
xmm7
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
102
15
111
21
38
141
1
0
/
/
movdqa
0x18d26
(
%
rip
)
%
xmm2
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
56
64
194
/
/
pmulld
%
xmm2
%
xmm0
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
250
200
/
/
psubd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
205
/
/
movq
%
xmm1
%
rbp
.
byte
137
235
/
/
mov
%
ebp
%
ebx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
65
137
200
/
/
mov
%
ecx
%
r8d
.
byte
243
15
16
44
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
102
15
58
33
44
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm5
.
byte
243
66
15
16
12
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm1
.
byte
102
15
58
33
233
32
/
/
insertps
0x20
%
xmm1
%
xmm5
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
15
58
33
233
48
/
/
insertps
0x30
%
xmm1
%
xmm5
.
byte
102
65
15
126
192
/
/
movd
%
xmm0
%
r8d
.
byte
102
65
15
58
22
193
1
/
/
pextrd
0x1
%
xmm0
%
r9d
.
byte
102
65
15
58
22
194
2
/
/
pextrd
0x2
%
xmm0
%
r10d
.
byte
102
65
15
58
22
195
3
/
/
pextrd
0x3
%
xmm0
%
r11d
.
byte
102
15
111
37
85
134
1
0
/
/
movdqa
0x18655
(
%
rip
)
%
xmm4
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
254
196
/
/
paddd
%
xmm4
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
52
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm6
.
byte
102
15
58
33
52
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
243
66
15
16
4
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm0
.
byte
102
15
58
33
240
32
/
/
insertps
0x20
%
xmm0
%
xmm6
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
15
58
33
240
48
/
/
insertps
0x30
%
xmm0
%
xmm6
.
byte
102
68
15
111
76
36
176
/
/
movdqa
-
0x50
(
%
rsp
)
%
xmm9
.
byte
102
65
15
254
217
/
/
paddd
%
xmm9
%
xmm3
.
byte
102
15
56
64
218
/
/
pmulld
%
xmm2
%
xmm3
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
65
15
250
192
/
/
psubd
%
xmm8
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
4
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm0
.
byte
102
15
58
33
4
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
243
66
15
16
12
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
102
15
58
33
193
32
/
/
insertps
0x20
%
xmm1
%
xmm0
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
15
58
33
193
48
/
/
insertps
0x30
%
xmm1
%
xmm0
.
byte
102
65
15
126
221
/
/
movd
%
xmm3
%
r13d
.
byte
102
65
15
58
22
223
1
/
/
pextrd
0x1
%
xmm3
%
r15d
.
byte
102
65
15
58
22
220
2
/
/
pextrd
0x2
%
xmm3
%
r12d
.
byte
102
65
15
58
22
222
3
/
/
pextrd
0x3
%
xmm3
%
r14d
.
byte
102
15
254
220
/
/
paddd
%
xmm4
%
xmm3
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
102
72
15
58
22
219
1
/
/
pextrq
0x1
%
xmm3
%
rbx
.
byte
102
72
15
126
221
/
/
movq
%
xmm3
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
15
16
28
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm3
.
byte
102
15
58
33
28
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm3
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
15
58
33
217
32
/
/
insertps
0x20
%
xmm1
%
xmm3
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
12
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
102
15
58
33
217
48
/
/
insertps
0x30
%
xmm1
%
xmm3
.
byte
243
66
15
16
12
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm1
.
byte
102
66
15
58
33
12
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm1
.
byte
102
66
15
58
33
12
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm1
.
byte
102
66
15
58
33
12
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm1
.
byte
243
66
15
16
36
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm4
.
byte
102
66
15
58
33
36
184
16
/
/
insertps
0x10
(
%
rax
%
r15
4
)
%
xmm4
.
byte
102
66
15
58
33
36
160
32
/
/
insertps
0x20
(
%
rax
%
r12
4
)
%
xmm4
.
byte
102
66
15
58
33
36
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
65
15
89
228
/
/
mulps
%
xmm12
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
92
222
/
/
subps
%
xmm6
%
xmm3
.
byte
65
15
89
220
/
/
mulps
%
xmm12
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
102
68
15
254
108
36
208
/
/
paddd
-
0x30
(
%
rsp
)
%
xmm13
.
byte
102
65
15
111
205
/
/
movdqa
%
xmm13
%
xmm1
.
byte
102
15
254
207
/
/
paddd
%
xmm7
%
xmm1
.
byte
102
68
15
111
226
/
/
movdqa
%
xmm2
%
xmm12
.
byte
102
65
15
56
64
204
/
/
pmulld
%
xmm12
%
xmm1
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
250
21
81
139
1
0
/
/
psubd
0x18b51
(
%
rip
)
%
xmm2
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
72
15
58
22
209
1
/
/
pextrq
0x1
%
xmm2
%
rcx
.
byte
102
72
15
126
213
/
/
movq
%
xmm2
%
rbp
.
byte
137
235
/
/
mov
%
ebp
%
ebx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
65
137
200
/
/
mov
%
ecx
%
r8d
.
byte
243
15
16
44
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
102
15
58
33
44
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm5
.
byte
243
66
15
16
20
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm2
.
byte
102
15
58
33
234
32
/
/
insertps
0x20
%
xmm2
%
xmm5
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
234
48
/
/
insertps
0x30
%
xmm2
%
xmm5
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
65
15
58
22
201
1
/
/
pextrd
0x1
%
xmm1
%
r9d
.
byte
102
65
15
58
22
202
2
/
/
pextrd
0x2
%
xmm1
%
r10d
.
byte
102
65
15
58
22
203
3
/
/
pextrd
0x3
%
xmm1
%
r11d
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
52
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm6
.
byte
102
15
58
33
52
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
243
66
15
16
12
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
102
15
58
33
241
32
/
/
insertps
0x20
%
xmm1
%
xmm6
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
15
58
33
241
48
/
/
insertps
0x30
%
xmm1
%
xmm6
.
byte
102
69
15
254
233
/
/
paddd
%
xmm9
%
xmm13
.
byte
102
65
15
111
249
/
/
movdqa
%
xmm9
%
xmm7
.
byte
102
69
15
56
64
236
/
/
pmulld
%
xmm12
%
xmm13
.
byte
102
65
15
111
205
/
/
movdqa
%
xmm13
%
xmm1
.
byte
102
15
250
13
157
138
1
0
/
/
psubd
0x18a9d
(
%
rip
)
%
xmm1
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
68
15
16
12
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm9
.
byte
102
68
15
58
33
12
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm9
.
byte
243
66
15
16
12
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
102
68
15
58
33
201
32
/
/
insertps
0x20
%
xmm1
%
xmm9
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
68
15
58
33
201
48
/
/
insertps
0x30
%
xmm1
%
xmm9
.
byte
102
69
15
126
237
/
/
movd
%
xmm13
%
r13d
.
byte
102
69
15
58
22
239
1
/
/
pextrd
0x1
%
xmm13
%
r15d
.
byte
102
69
15
58
22
236
2
/
/
pextrd
0x2
%
xmm13
%
r12d
.
byte
102
69
15
58
22
238
3
/
/
pextrd
0x3
%
xmm13
%
r14d
.
byte
102
69
15
254
232
/
/
paddd
%
xmm8
%
xmm13
.
byte
102
76
15
58
22
235
1
/
/
pextrq
0x1
%
xmm13
%
rbx
.
byte
102
76
15
126
237
/
/
movq
%
xmm13
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
68
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm8
.
byte
102
68
15
58
33
4
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm8
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
12
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm1
.
byte
102
68
15
58
33
193
32
/
/
insertps
0x20
%
xmm1
%
xmm8
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
12
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
102
68
15
58
33
193
48
/
/
insertps
0x30
%
xmm1
%
xmm8
.
byte
243
66
15
16
12
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm1
.
byte
102
66
15
58
33
12
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm1
.
byte
102
66
15
58
33
12
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm1
.
byte
102
66
15
58
33
12
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm1
.
byte
243
70
15
16
36
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm12
.
byte
102
70
15
58
33
36
184
16
/
/
insertps
0x10
(
%
rax
%
r15
4
)
%
xmm12
.
byte
102
70
15
58
33
36
160
32
/
/
insertps
0x20
(
%
rax
%
r12
4
)
%
xmm12
.
byte
102
70
15
58
33
36
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm12
.
byte
68
15
92
225
/
/
subps
%
xmm1
%
xmm12
.
byte
68
15
40
44
36
/
/
movaps
(
%
rsp
)
%
xmm13
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
68
15
88
225
/
/
addps
%
xmm1
%
xmm12
.
byte
68
15
92
205
/
/
subps
%
xmm5
%
xmm9
.
byte
69
15
89
205
/
/
mulps
%
xmm13
%
xmm9
.
byte
68
15
88
205
/
/
addps
%
xmm5
%
xmm9
.
byte
68
15
92
198
/
/
subps
%
xmm6
%
xmm8
.
byte
69
15
89
197
/
/
mulps
%
xmm13
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
68
15
92
228
/
/
subps
%
xmm4
%
xmm12
.
byte
15
40
76
36
128
/
/
movaps
-
0x80
(
%
rsp
)
%
xmm1
.
byte
68
15
89
225
/
/
mulps
%
xmm1
%
xmm12
.
byte
68
15
88
228
/
/
addps
%
xmm4
%
xmm12
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
68
15
89
193
/
/
mulps
%
xmm1
%
xmm8
.
byte
68
15
88
195
/
/
addps
%
xmm3
%
xmm8
.
byte
102
15
111
68
36
144
/
/
movdqa
-
0x70
(
%
rsp
)
%
xmm0
.
byte
102
15
254
68
36
160
/
/
paddd
-
0x60
(
%
rsp
)
%
xmm0
.
byte
102
15
127
68
36
144
/
/
movdqa
%
xmm0
-
0x70
(
%
rsp
)
.
byte
102
68
15
254
240
/
/
paddd
%
xmm0
%
xmm14
.
byte
102
65
15
111
206
/
/
movdqa
%
xmm14
%
xmm1
.
byte
102
15
111
108
36
192
/
/
movdqa
-
0x40
(
%
rsp
)
%
xmm5
.
byte
102
15
254
205
/
/
paddd
%
xmm5
%
xmm1
.
byte
102
15
111
37
52
137
1
0
/
/
movdqa
0x18934
(
%
rip
)
%
xmm4
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
56
64
204
/
/
pmulld
%
xmm4
%
xmm1
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
118
246
/
/
pcmpeqd
%
xmm6
%
xmm6
.
byte
102
15
250
198
/
/
psubd
%
xmm6
%
xmm0
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
197
/
/
movq
%
xmm0
%
rbp
.
byte
137
235
/
/
mov
%
ebp
%
ebx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
65
137
200
/
/
mov
%
ecx
%
r8d
.
byte
243
15
16
4
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
102
15
58
33
4
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm0
.
byte
243
66
15
16
20
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm2
.
byte
102
15
58
33
194
32
/
/
insertps
0x20
%
xmm2
%
xmm0
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
194
48
/
/
insertps
0x30
%
xmm2
%
xmm0
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
65
15
58
22
201
1
/
/
pextrd
0x1
%
xmm1
%
r9d
.
byte
102
65
15
58
22
202
2
/
/
pextrd
0x2
%
xmm1
%
r10d
.
byte
102
65
15
58
22
203
3
/
/
pextrd
0x3
%
xmm1
%
r11d
.
byte
102
15
111
29
101
130
1
0
/
/
movdqa
0x18265
(
%
rip
)
%
xmm3
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
254
203
/
/
paddd
%
xmm3
%
xmm1
.
byte
102
72
15
58
22
201
1
/
/
pextrq
0x1
%
xmm1
%
rcx
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
12
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm1
.
byte
102
15
58
33
12
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
243
66
15
16
20
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
102
15
58
33
202
32
/
/
insertps
0x20
%
xmm2
%
xmm1
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
202
48
/
/
insertps
0x30
%
xmm2
%
xmm1
.
byte
102
68
15
254
247
/
/
paddd
%
xmm7
%
xmm14
.
byte
102
68
15
56
64
244
/
/
pmulld
%
xmm4
%
xmm14
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
65
15
111
214
/
/
movdqa
%
xmm14
%
xmm2
.
byte
102
15
250
214
/
/
psubd
%
xmm6
%
xmm2
.
byte
102
72
15
58
22
209
1
/
/
pextrq
0x1
%
xmm2
%
rcx
.
byte
102
72
15
126
211
/
/
movq
%
xmm2
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
52
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm6
.
byte
102
15
58
33
52
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm6
.
byte
243
66
15
16
20
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm2
.
byte
102
15
58
33
242
32
/
/
insertps
0x20
%
xmm2
%
xmm6
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
242
48
/
/
insertps
0x30
%
xmm2
%
xmm6
.
byte
102
69
15
126
245
/
/
movd
%
xmm14
%
r13d
.
byte
102
69
15
58
22
247
1
/
/
pextrd
0x1
%
xmm14
%
r15d
.
byte
102
69
15
58
22
244
2
/
/
pextrd
0x2
%
xmm14
%
r12d
.
byte
102
69
15
58
22
246
3
/
/
pextrd
0x3
%
xmm14
%
r14d
.
byte
102
68
15
254
243
/
/
paddd
%
xmm3
%
xmm14
.
byte
102
76
15
58
22
243
1
/
/
pextrq
0x1
%
xmm14
%
rbx
.
byte
102
76
15
126
245
/
/
movq
%
xmm14
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
15
16
36
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm4
.
byte
102
15
58
33
36
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm4
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
226
32
/
/
insertps
0x20
%
xmm2
%
xmm4
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
20
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm2
.
byte
102
15
58
33
226
48
/
/
insertps
0x30
%
xmm2
%
xmm4
.
byte
243
66
15
16
20
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm2
.
byte
102
66
15
58
33
20
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm2
.
byte
102
66
15
58
33
20
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm2
.
byte
102
66
15
58
33
20
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm2
.
byte
243
66
15
16
28
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm3
.
byte
102
66
15
58
33
28
184
16
/
/
insertps
0x10
(
%
rax
%
r15
4
)
%
xmm3
.
byte
102
66
15
58
33
28
160
32
/
/
insertps
0x20
(
%
rax
%
r12
4
)
%
xmm3
.
byte
102
66
15
58
33
28
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm3
.
byte
15
92
218
/
/
subps
%
xmm2
%
xmm3
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
65
15
89
245
/
/
mulps
%
xmm13
%
xmm6
.
byte
15
88
240
/
/
addps
%
xmm0
%
xmm6
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
65
15
89
229
/
/
mulps
%
xmm13
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
102
15
111
68
36
144
/
/
movdqa
-
0x70
(
%
rsp
)
%
xmm0
.
byte
102
15
254
68
36
208
/
/
paddd
-
0x30
(
%
rsp
)
%
xmm0
.
byte
102
15
254
232
/
/
paddd
%
xmm0
%
xmm5
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
56
64
239
/
/
pmulld
%
xmm7
%
xmm5
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
250
5
101
135
1
0
/
/
psubd
0x18765
(
%
rip
)
%
xmm0
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
197
/
/
movq
%
xmm0
%
rbp
.
byte
137
235
/
/
mov
%
ebp
%
ebx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
65
137
200
/
/
mov
%
ecx
%
r8d
.
byte
243
15
16
44
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm5
.
byte
102
15
58
33
44
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm5
.
byte
243
66
15
16
4
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm0
.
byte
102
15
58
33
232
32
/
/
insertps
0x20
%
xmm0
%
xmm5
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
15
58
33
232
48
/
/
insertps
0x30
%
xmm0
%
xmm5
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
126
192
/
/
movd
%
xmm0
%
r8d
.
byte
102
65
15
58
22
193
1
/
/
pextrd
0x1
%
xmm0
%
r9d
.
byte
102
65
15
58
22
194
2
/
/
pextrd
0x2
%
xmm0
%
r10d
.
byte
102
65
15
58
22
195
3
/
/
pextrd
0x3
%
xmm0
%
r11d
.
byte
102
15
254
5
147
128
1
0
/
/
paddd
0x18093
(
%
rip
)
%
xmm0
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
68
15
16
52
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm14
.
byte
102
68
15
58
33
52
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm14
.
byte
243
66
15
16
4
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm0
.
byte
102
68
15
58
33
240
32
/
/
insertps
0x20
%
xmm0
%
xmm14
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
68
15
58
33
240
48
/
/
insertps
0x30
%
xmm0
%
xmm14
.
byte
102
15
254
84
36
176
/
/
paddd
-
0x50
(
%
rsp
)
%
xmm2
.
byte
102
15
56
64
215
/
/
pmulld
%
xmm7
%
xmm2
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
250
5
172
134
1
0
/
/
psubd
0x186ac
(
%
rip
)
%
xmm0
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
72
15
58
22
193
1
/
/
pextrq
0x1
%
xmm0
%
rcx
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
137
221
/
/
mov
%
ebx
%
ebp
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
65
137
206
/
/
mov
%
ecx
%
r14d
.
byte
243
15
16
12
168
/
/
movss
(
%
rax
%
rbp
4
)
%
xmm1
.
byte
102
15
58
33
12
152
16
/
/
insertps
0x10
(
%
rax
%
rbx
4
)
%
xmm1
.
byte
243
66
15
16
4
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm0
.
byte
102
15
58
33
200
32
/
/
insertps
0x20
%
xmm0
%
xmm1
.
byte
72
193
233
32
/
/
shr
0x20
%
rcx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
15
58
33
200
48
/
/
insertps
0x30
%
xmm0
%
xmm1
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
65
15
126
197
/
/
movd
%
xmm0
%
r13d
.
byte
102
65
15
58
22
199
1
/
/
pextrd
0x1
%
xmm0
%
r15d
.
byte
102
65
15
58
22
196
2
/
/
pextrd
0x2
%
xmm0
%
r12d
.
byte
102
65
15
58
22
198
3
/
/
pextrd
0x3
%
xmm0
%
r14d
.
byte
102
15
254
5
218
127
1
0
/
/
paddd
0x17fda
(
%
rip
)
%
xmm0
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
72
15
58
22
195
1
/
/
pextrq
0x1
%
xmm0
%
rbx
.
byte
102
72
15
126
197
/
/
movq
%
xmm0
%
rbp
.
byte
137
233
/
/
mov
%
ebp
%
ecx
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
243
15
16
20
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm2
.
byte
102
15
58
33
20
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm2
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
243
15
16
4
136
/
/
movss
(
%
rax
%
rcx
4
)
%
xmm0
.
byte
102
15
58
33
208
32
/
/
insertps
0x20
%
xmm0
%
xmm2
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
243
15
16
4
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
102
15
58
33
208
48
/
/
insertps
0x30
%
xmm0
%
xmm2
.
byte
243
66
15
16
60
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
102
66
15
58
33
60
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm7
.
byte
102
66
15
58
33
60
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm7
.
byte
102
66
15
58
33
60
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm7
.
byte
243
66
15
16
4
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm0
.
byte
102
66
15
58
33
4
184
16
/
/
insertps
0x10
(
%
rax
%
r15
4
)
%
xmm0
.
byte
102
66
15
58
33
4
160
32
/
/
insertps
0x20
(
%
rax
%
r12
4
)
%
xmm0
.
byte
102
66
15
58
33
4
176
48
/
/
insertps
0x30
(
%
rax
%
r14
4
)
%
xmm0
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
65
15
89
197
/
/
mulps
%
xmm13
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
65
15
89
205
/
/
mulps
%
xmm13
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
65
15
92
214
/
/
subps
%
xmm14
%
xmm2
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
40
108
36
128
/
/
movaps
-
0x80
(
%
rsp
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
65
15
92
196
/
/
subps
%
xmm12
%
xmm0
.
byte
15
40
92
36
224
/
/
movaps
-
0x20
(
%
rsp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
65
15
88
196
/
/
addps
%
xmm12
%
xmm0
.
byte
65
15
92
201
/
/
subps
%
xmm9
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
65
15
92
208
/
/
subps
%
xmm8
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
15
91
92
36
16
/
/
cvtdq2ps
0x10
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
48
/
/
movaps
0x30
(
%
rsp
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
65
15
92
194
/
/
subps
%
xmm10
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
92
207
/
/
subps
%
xmm15
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
65
15
88
207
/
/
addps
%
xmm15
%
xmm1
.
byte
65
15
92
211
/
/
subps
%
xmm11
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
151
126
1
0
/
/
movaps
0x17e97
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
139
76
36
248
/
/
mov
-
0x8
(
%
rsp
)
%
rcx
.
byte
15
40
100
36
64
/
/
movaps
0x40
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
80
/
/
movaps
0x50
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
96
/
/
movaps
0x60
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
112
/
/
movaps
0x70
(
%
rsp
)
%
xmm7
.
byte
72
129
196
136
0
0
0
/
/
add
0x88
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gauss_a_to_rgba_sse41
.
globl
_sk_gauss_a_to_rgba_sse41
FUNCTION
(
_sk_gauss_a_to_rgba_sse41
)
_sk_gauss_a_to_rgba_sse41
:
.
byte
15
40
5
20
133
1
0
/
/
movaps
0x18514
(
%
rip
)
%
xmm0
#
3d5c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1374
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
26
133
1
0
/
/
addps
0x1851a
(
%
rip
)
%
xmm0
#
3d5d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1384
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
32
133
1
0
/
/
addps
0x18520
(
%
rip
)
%
xmm0
#
3d5e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1394
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
38
133
1
0
/
/
addps
0x18526
(
%
rip
)
%
xmm0
#
3d5f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13a4
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
44
133
1
0
/
/
addps
0x1852c
(
%
rip
)
%
xmm0
#
3d600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13b4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilerp_clamp_8888_sse41
.
globl
_sk_bilerp_clamp_8888_sse41
FUNCTION
(
_sk_bilerp_clamp_8888_sse41
)
_sk_bilerp_clamp_8888_sse41
:
.
byte
72
131
236
56
/
/
sub
0x38
%
rsp
.
byte
15
41
124
36
176
/
/
movaps
%
xmm7
-
0x50
(
%
rsp
)
.
byte
15
41
116
36
160
/
/
movaps
%
xmm6
-
0x60
(
%
rsp
)
.
byte
15
41
108
36
144
/
/
movaps
%
xmm5
-
0x70
(
%
rsp
)
.
byte
15
41
100
36
128
/
/
movaps
%
xmm4
-
0x80
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
254
125
1
0
/
/
movaps
0x17dfe
(
%
rip
)
%
xmm3
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
68
15
88
211
/
/
addps
%
xmm3
%
xmm10
.
byte
102
65
15
58
8
210
1
/
/
roundps
0x1
%
xmm10
%
xmm2
.
byte
68
15
92
210
/
/
subps
%
xmm2
%
xmm10
.
byte
15
41
12
36
/
/
movaps
%
xmm1
(
%
rsp
)
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
102
15
58
8
203
1
/
/
roundps
0x1
%
xmm3
%
xmm1
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
15
40
13
228
125
1
0
/
/
movaps
0x17de4
(
%
rip
)
%
xmm1
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
65
15
92
210
/
/
subps
%
xmm10
%
xmm2
.
byte
15
41
84
36
32
/
/
movaps
%
xmm2
0x20
(
%
rsp
)
.
byte
15
41
92
36
240
/
/
movaps
%
xmm3
-
0x10
(
%
rsp
)
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
41
76
36
192
/
/
movaps
%
xmm1
-
0x40
(
%
rsp
)
.
byte
243
68
15
16
112
12
/
/
movss
0xc
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
102
15
118
201
/
/
pcmpeqd
%
xmm1
%
xmm1
.
byte
102
68
15
254
241
/
/
paddd
%
xmm1
%
xmm14
.
byte
243
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
254
209
/
/
paddd
%
xmm1
%
xmm2
.
byte
102
15
127
84
36
224
/
/
movdqa
%
xmm2
-
0x20
(
%
rsp
)
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
127
76
36
208
/
/
movdqa
%
xmm1
-
0x30
(
%
rsp
)
.
byte
243
15
16
13
201
116
1
0
/
/
movss
0x174c9
(
%
rip
)
%
xmm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
40
37
143
127
1
0
/
/
movaps
0x17f8f
(
%
rip
)
%
xmm4
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
88
44
36
/
/
addps
(
%
rsp
)
%
xmm5
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
95
205
/
/
maxps
%
xmm5
%
xmm1
.
byte
15
41
116
36
16
/
/
movaps
%
xmm6
0x10
(
%
rsp
)
.
byte
15
46
53
207
116
1
0
/
/
ucomiss
0x174cf
(
%
rip
)
%
xmm6
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
68
15
40
124
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm15
.
byte
119
6
/
/
ja
251cb
<
_sk_bilerp_clamp_8888_sse41
+
0xea
>
.
byte
68
15
40
124
36
192
/
/
movaps
-
0x40
(
%
rsp
)
%
xmm15
.
byte
15
93
76
36
224
/
/
minps
-
0x20
(
%
rsp
)
%
xmm1
.
byte
243
68
15
91
233
/
/
cvttps2dq
%
xmm1
%
xmm13
.
byte
102
68
15
56
64
108
36
208
/
/
pmulld
-
0x30
(
%
rsp
)
%
xmm13
.
byte
243
15
16
45
107
116
1
0
/
/
movss
0x1746b
(
%
rip
)
%
xmm5
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
95
241
/
/
maxps
%
xmm1
%
xmm6
.
byte
65
15
93
246
/
/
minps
%
xmm14
%
xmm6
.
byte
243
15
91
206
/
/
cvttps2dq
%
xmm6
%
xmm1
.
byte
102
65
15
254
205
/
/
paddd
%
xmm13
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
67
15
110
12
152
/
/
movd
(
%
r8
%
r11
4
)
%
xmm1
.
byte
102
67
15
58
34
12
144
1
/
/
pinsrd
0x1
(
%
r8
%
r10
4
)
%
xmm1
.
byte
102
67
15
58
34
12
136
2
/
/
pinsrd
0x2
(
%
r8
%
r9
4
)
%
xmm1
.
byte
102
65
15
58
34
12
128
3
/
/
pinsrd
0x3
(
%
r8
%
rax
4
)
%
xmm1
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
219
53
122
125
1
0
/
/
pand
0x17d7a
(
%
rip
)
%
xmm6
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
254
/
/
cvtdq2ps
%
xmm6
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
56
0
53
119
125
1
0
/
/
pshufb
0x17d77
(
%
rip
)
%
xmm6
#
3cfd0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd84
>
.
byte
68
15
91
230
/
/
cvtdq2ps
%
xmm6
%
xmm12
.
byte
68
15
89
228
/
/
mulps
%
xmm4
%
xmm12
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
56
0
53
114
125
1
0
/
/
pshufb
0x17d72
(
%
rip
)
%
xmm6
#
3cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd94
>
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
102
15
114
209
24
/
/
psrld
0x18
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
46
45
6
116
1
0
/
/
ucomiss
0x17406
(
%
rip
)
%
xmm5
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
119
6
/
/
ja
25292
<
_sk_bilerp_clamp_8888_sse41
+
0x1b1
>
.
byte
68
15
40
92
36
32
/
/
movaps
0x20
(
%
rsp
)
%
xmm11
.
byte
69
15
89
223
/
/
mulps
%
xmm15
%
xmm11
.
byte
65
15
89
251
/
/
mulps
%
xmm11
%
xmm7
.
byte
68
15
88
207
/
/
addps
%
xmm7
%
xmm9
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
69
15
88
196
/
/
addps
%
xmm12
%
xmm8
.
byte
65
15
89
243
/
/
mulps
%
xmm11
%
xmm6
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
243
15
88
45
63
114
1
0
/
/
addss
0x1723f
(
%
rip
)
%
xmm5
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
243
15
16
13
51
114
1
0
/
/
movss
0x17233
(
%
rip
)
%
xmm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
15
46
205
/
/
ucomiss
%
xmm5
%
xmm1
.
byte
15
131
23
255
255
255
/
/
jae
251e5
<
_sk_bilerp_clamp_8888_sse41
+
0x104
>
.
byte
15
40
116
36
16
/
/
movaps
0x10
(
%
rsp
)
%
xmm6
.
byte
243
15
88
53
33
114
1
0
/
/
addss
0x17221
(
%
rip
)
%
xmm6
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
243
15
16
13
21
114
1
0
/
/
movss
0x17215
(
%
rip
)
%
xmm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
15
46
206
/
/
ucomiss
%
xmm6
%
xmm1
.
byte
15
131
180
254
255
255
/
/
jae
251a0
<
_sk_bilerp_clamp_8888_sse41
+
0xbf
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
40
100
36
128
/
/
movaps
-
0x80
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
160
/
/
movaps
-
0x60
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
176
/
/
movaps
-
0x50
(
%
rsp
)
%
xmm7
.
byte
72
131
196
56
/
/
add
0x38
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_start_pipeline_sse2
.
globl
_sk_start_pipeline_sse2
FUNCTION
(
_sk_start_pipeline_sse2
)
_sk_start_pipeline_sse2
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
131
0
0
0
/
/
jae
253c6
<
_sk_start_pipeline_sse2
+
0xb6
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
4
/
/
lea
0x4
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
119
59
/
/
ja
25394
<
_sk_start_pipeline_sse2
+
0x84
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
4
/
/
lea
0x4
(
%
r12
)
%
rdx
.
byte
73
131
196
8
/
/
add
0x8
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
201
/
/
jbe
2535d
<
_sk_start_pipeline_sse2
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
33
/
/
je
253bd
<
_sk_start_pipeline_sse2
+
0xad
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
255
195
/
/
inc
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
117
137
/
/
jne
2534f
<
_sk_start_pipeline_sse2
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_sse2
.
globl
_sk_just_return_sse2
FUNCTION
(
_sk_just_return_sse2
)
_sk_just_return_sse2
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_sse2
.
globl
_sk_seed_shader_sse2
FUNCTION
(
_sk_seed_shader_sse2
)
_sk_seed_shader_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm1
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
102
15
110
201
/
/
movd
%
ecx
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
88
13
3
123
1
0
/
/
addps
0x17b03
(
%
rip
)
%
xmm1
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
21
10
123
1
0
/
/
movaps
0x17b0a
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dither_sse2
.
globl
_sk_dither_sse2
FUNCTION
(
_sk_dither_sse2
)
_sk_dither_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
68
15
110
194
/
/
movd
%
edx
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
102
68
15
254
5
179
122
1
0
/
/
paddd
0x17ab3
(
%
rip
)
%
xmm8
#
3cee0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc94
>
.
byte
102
68
15
110
201
/
/
movd
%
ecx
%
xmm9
.
byte
102
69
15
112
201
0
/
/
pshufd
0x0
%
xmm9
%
xmm9
.
byte
102
69
15
239
200
/
/
pxor
%
xmm8
%
xmm9
.
byte
102
68
15
111
21
218
122
1
0
/
/
movdqa
0x17ada
(
%
rip
)
%
xmm10
#
3cf20
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcd4
>
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
69
15
219
218
/
/
pand
%
xmm10
%
xmm11
.
byte
102
65
15
114
243
5
/
/
pslld
0x5
%
xmm11
.
byte
102
69
15
219
208
/
/
pand
%
xmm8
%
xmm10
.
byte
102
65
15
114
242
4
/
/
pslld
0x4
%
xmm10
.
byte
102
68
15
111
37
198
122
1
0
/
/
movdqa
0x17ac6
(
%
rip
)
%
xmm12
#
3cf30
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xce4
>
.
byte
102
68
15
111
45
205
122
1
0
/
/
movdqa
0x17acd
(
%
rip
)
%
xmm13
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
69
15
111
241
/
/
movdqa
%
xmm9
%
xmm14
.
byte
102
69
15
219
245
/
/
pand
%
xmm13
%
xmm14
.
byte
102
65
15
114
246
2
/
/
pslld
0x2
%
xmm14
.
byte
102
69
15
235
243
/
/
por
%
xmm11
%
xmm14
.
byte
102
69
15
219
232
/
/
pand
%
xmm8
%
xmm13
.
byte
102
69
15
254
237
/
/
paddd
%
xmm13
%
xmm13
.
byte
102
69
15
235
234
/
/
por
%
xmm10
%
xmm13
.
byte
102
69
15
219
204
/
/
pand
%
xmm12
%
xmm9
.
byte
102
65
15
114
209
1
/
/
psrld
0x1
%
xmm9
.
byte
102
69
15
219
196
/
/
pand
%
xmm12
%
xmm8
.
byte
102
65
15
114
208
2
/
/
psrld
0x2
%
xmm8
.
byte
102
69
15
235
197
/
/
por
%
xmm13
%
xmm8
.
byte
102
69
15
235
198
/
/
por
%
xmm14
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
136
122
1
0
/
/
mulps
0x17a88
(
%
rip
)
%
xmm8
#
3cf50
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd04
>
.
byte
68
15
88
5
144
122
1
0
/
/
addps
0x17a90
(
%
rip
)
%
xmm8
#
3cf60
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd14
>
.
byte
243
68
15
16
16
/
/
movss
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
15
93
195
/
/
minps
%
xmm3
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
15
93
203
/
/
minps
%
xmm3
%
xmm1
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
68
15
95
201
/
/
maxps
%
xmm1
%
xmm9
.
byte
68
15
93
211
/
/
minps
%
xmm3
%
xmm10
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_sse2
.
globl
_sk_uniform_color_sse2
FUNCTION
(
_sk_uniform_color_sse2
)
_sk_uniform_color_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_sse2
.
globl
_sk_black_color_sse2
FUNCTION
(
_sk_black_color_sse2
)
_sk_black_color_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
198
121
1
0
/
/
movaps
0x179c6
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_sse2
.
globl
_sk_white_color_sse2
FUNCTION
(
_sk_white_color_sse2
)
_sk_white_color_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
5
178
121
1
0
/
/
movaps
0x179b2
(
%
rip
)
%
xmm0
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_rgba_sse2
.
globl
_sk_load_rgba_sse2
FUNCTION
(
_sk_load_rgba_sse2
)
_sk_load_rgba_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
15
16
72
16
/
/
movups
0x10
(
%
rax
)
%
xmm1
.
byte
15
16
80
32
/
/
movups
0x20
(
%
rax
)
%
xmm2
.
byte
15
16
88
48
/
/
movups
0x30
(
%
rax
)
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_rgba_sse2
.
globl
_sk_store_rgba_sse2
FUNCTION
(
_sk_store_rgba_sse2
)
_sk_store_rgba_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
17
0
/
/
movups
%
xmm0
(
%
rax
)
.
byte
15
17
72
16
/
/
movups
%
xmm1
0x10
(
%
rax
)
.
byte
15
17
80
32
/
/
movups
%
xmm2
0x20
(
%
rax
)
.
byte
15
17
88
48
/
/
movups
%
xmm3
0x30
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_sse2
.
globl
_sk_clear_sse2
FUNCTION
(
_sk_clear_sse2
)
_sk_clear_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_sse2
.
globl
_sk_srcatop_sse2
FUNCTION
(
_sk_srcatop_sse2
)
_sk_srcatop_sse2
:
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
68
15
40
5
98
121
1
0
/
/
movaps
0x17962
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
204
/
/
mulps
%
xmm4
%
xmm9
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_sse2
.
globl
_sk_dstatop_sse2
FUNCTION
(
_sk_dstatop_sse2
)
_sk_dstatop_sse2
:
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
89
196
/
/
mulps
%
xmm4
%
xmm8
.
byte
68
15
40
13
21
121
1
0
/
/
movaps
0x17915
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
207
/
/
subps
%
xmm7
%
xmm9
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
89
197
/
/
mulps
%
xmm5
%
xmm8
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
89
198
/
/
mulps
%
xmm6
%
xmm8
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_sse2
.
globl
_sk_srcin_sse2
FUNCTION
(
_sk_srcin_sse2
)
_sk_srcin_sse2
:
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_sse2
.
globl
_sk_dstin_sse2
FUNCTION
(
_sk_dstin_sse2
)
_sk_dstin_sse2
:
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_sse2
.
globl
_sk_srcout_sse2
FUNCTION
(
_sk_srcout_sse2
)
_sk_srcout_sse2
:
.
byte
68
15
40
5
169
120
1
0
/
/
movaps
0x178a9
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_sse2
.
globl
_sk_dstout_sse2
FUNCTION
(
_sk_dstout_sse2
)
_sk_dstout_sse2
:
.
byte
68
15
40
5
137
120
1
0
/
/
movaps
0x17889
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_sse2
.
globl
_sk_srcover_sse2
FUNCTION
(
_sk_srcover_sse2
)
_sk_srcover_sse2
:
.
byte
68
15
40
5
92
120
1
0
/
/
movaps
0x1785c
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
204
/
/
mulps
%
xmm4
%
xmm9
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_sse2
.
globl
_sk_dstover_sse2
FUNCTION
(
_sk_dstover_sse2
)
_sk_dstover_sse2
:
.
byte
68
15
40
5
32
120
1
0
/
/
movaps
0x17820
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_sse2
.
globl
_sk_modulate_sse2
FUNCTION
(
_sk_modulate_sse2
)
_sk_modulate_sse2
:
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_sse2
.
globl
_sk_multiply_sse2
FUNCTION
(
_sk_multiply_sse2
)
_sk_multiply_sse2
:
.
byte
68
15
40
5
228
119
1
0
/
/
movaps
0x177e4
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
92
207
/
/
subps
%
xmm7
%
xmm9
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
89
220
/
/
mulps
%
xmm4
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
195
/
/
addps
%
xmm11
%
xmm0
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
89
221
/
/
mulps
%
xmm5
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
89
222
/
/
mulps
%
xmm6
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__sse2
.
globl
_sk_plus__sse2
FUNCTION
(
_sk_plus__sse2
)
_sk_plus__sse2
:
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
40
5
101
119
1
0
/
/
movaps
0x17765
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_sse2
.
globl
_sk_screen_sse2
FUNCTION
(
_sk_screen_sse2
)
_sk_screen_sse2
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
196
/
/
mulps
%
xmm4
%
xmm8
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
88
197
/
/
addps
%
xmm5
%
xmm8
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
68
15
92
193
/
/
subps
%
xmm1
%
xmm8
.
byte
68
15
40
202
/
/
movaps
%
xmm2
%
xmm9
.
byte
68
15
88
206
/
/
addps
%
xmm6
%
xmm9
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
92
202
/
/
subps
%
xmm2
%
xmm9
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
88
215
/
/
addps
%
xmm7
%
xmm10
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__sse2
.
globl
_sk_xor__sse2
FUNCTION
(
_sk_xor__sse2
)
_sk_xor__sse2
:
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
15
40
29
241
118
1
0
/
/
movaps
0x176f1
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
92
207
/
/
subps
%
xmm7
%
xmm9
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
92
216
/
/
subps
%
xmm8
%
xmm3
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
89
212
/
/
mulps
%
xmm4
%
xmm10
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
89
213
/
/
mulps
%
xmm5
%
xmm10
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
65
15
88
210
/
/
addps
%
xmm10
%
xmm2
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_sse2
.
globl
_sk_darken_sse2
FUNCTION
(
_sk_darken_sse2
)
_sk_darken_sse2
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
68
15
95
201
/
/
maxps
%
xmm1
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
69
15
95
193
/
/
maxps
%
xmm9
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
95
209
/
/
maxps
%
xmm9
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
76
118
1
0
/
/
movaps
0x1764c
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_sse2
.
globl
_sk_lighten_sse2
FUNCTION
(
_sk_lighten_sse2
)
_sk_lighten_sse2
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
68
15
93
201
/
/
minps
%
xmm1
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
69
15
93
193
/
/
minps
%
xmm9
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
93
209
/
/
minps
%
xmm9
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
225
117
1
0
/
/
movaps
0x175e1
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_sse2
.
globl
_sk_difference_sse2
FUNCTION
(
_sk_difference_sse2
)
_sk_difference_sse2
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
68
15
93
201
/
/
minps
%
xmm1
%
xmm9
.
byte
69
15
88
201
/
/
addps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
69
15
93
193
/
/
minps
%
xmm9
%
xmm8
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
93
209
/
/
minps
%
xmm9
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
107
117
1
0
/
/
movaps
0x1756b
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_sse2
.
globl
_sk_exclusion_sse2
FUNCTION
(
_sk_exclusion_sse2
)
_sk_exclusion_sse2
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
68
15
89
197
/
/
mulps
%
xmm5
%
xmm8
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
68
15
92
194
/
/
subps
%
xmm2
%
xmm8
.
byte
15
40
21
27
117
1
0
/
/
movaps
0x1751b
(
%
rip
)
%
xmm2
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colorburn_sse2
.
globl
_sk_colorburn_sse2
FUNCTION
(
_sk_colorburn_sse2
)
_sk_colorburn_sse2
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
68
15
40
13
254
116
1
0
/
/
movaps
0x174fe
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
92
215
/
/
subps
%
xmm7
%
xmm10
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
69
15
83
224
/
/
rcpps
%
xmm8
%
xmm12
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
65
15
93
196
/
/
minps
%
xmm12
%
xmm0
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
92
224
/
/
subps
%
xmm0
%
xmm12
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
227
/
/
addps
%
xmm11
%
xmm12
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
68
15
92
203
/
/
subps
%
xmm3
%
xmm9
.
byte
69
15
40
233
/
/
movaps
%
xmm9
%
xmm13
.
byte
68
15
89
236
/
/
mulps
%
xmm4
%
xmm13
.
byte
69
15
194
195
0
/
/
cmpeqps
%
xmm11
%
xmm8
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
84
232
/
/
andps
%
xmm8
%
xmm13
.
byte
69
15
85
196
/
/
andnps
%
xmm12
%
xmm8
.
byte
68
15
40
228
/
/
movaps
%
xmm4
%
xmm12
.
byte
68
15
194
231
0
/
/
cmpeqps
%
xmm7
%
xmm12
.
byte
69
15
86
197
/
/
orps
%
xmm13
%
xmm8
.
byte
65
15
84
196
/
/
andps
%
xmm12
%
xmm0
.
byte
69
15
85
224
/
/
andnps
%
xmm8
%
xmm12
.
byte
65
15
86
196
/
/
orps
%
xmm12
%
xmm0
.
byte
69
15
40
194
/
/
movaps
%
xmm10
%
xmm8
.
byte
68
15
89
193
/
/
mulps
%
xmm1
%
xmm8
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
92
229
/
/
subps
%
xmm5
%
xmm12
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
68
15
83
233
/
/
rcpps
%
xmm1
%
xmm13
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
69
15
93
229
/
/
minps
%
xmm13
%
xmm12
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
69
15
92
236
/
/
subps
%
xmm12
%
xmm13
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
68
15
89
229
/
/
mulps
%
xmm5
%
xmm12
.
byte
65
15
194
203
0
/
/
cmpeqps
%
xmm11
%
xmm1
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
69
15
88
232
/
/
addps
%
xmm8
%
xmm13
.
byte
69
15
88
236
/
/
addps
%
xmm12
%
xmm13
.
byte
68
15
84
225
/
/
andps
%
xmm1
%
xmm12
.
byte
65
15
85
205
/
/
andnps
%
xmm13
%
xmm1
.
byte
68
15
88
197
/
/
addps
%
xmm5
%
xmm8
.
byte
65
15
86
204
/
/
orps
%
xmm12
%
xmm1
.
byte
68
15
40
229
/
/
movaps
%
xmm5
%
xmm12
.
byte
68
15
194
231
0
/
/
cmpeqps
%
xmm7
%
xmm12
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
68
15
85
225
/
/
andnps
%
xmm1
%
xmm12
.
byte
69
15
86
196
/
/
orps
%
xmm12
%
xmm8
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
68
15
194
218
0
/
/
cmpeqps
%
xmm2
%
xmm11
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
83
210
/
/
rcpps
%
xmm2
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
93
202
/
/
minps
%
xmm2
%
xmm1
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
92
209
/
/
subps
%
xmm1
%
xmm2
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
65
15
88
210
/
/
addps
%
xmm10
%
xmm2
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
65
15
84
203
/
/
andps
%
xmm11
%
xmm1
.
byte
68
15
85
218
/
/
andnps
%
xmm2
%
xmm11
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
68
15
86
217
/
/
orps
%
xmm1
%
xmm11
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
194
207
0
/
/
cmpeqps
%
xmm7
%
xmm1
.
byte
15
84
209
/
/
andps
%
xmm1
%
xmm2
.
byte
65
15
85
203
/
/
andnps
%
xmm11
%
xmm1
.
byte
15
86
209
/
/
orps
%
xmm1
%
xmm2
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_colordodge_sse2
.
globl
_sk_colordodge_sse2
FUNCTION
(
_sk_colordodge_sse2
)
_sk_colordodge_sse2
:
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
40
21
166
115
1
0
/
/
movaps
0x173a6
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
68
15
92
223
/
/
subps
%
xmm7
%
xmm11
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
68
15
89
228
/
/
mulps
%
xmm4
%
xmm12
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
69
15
92
193
/
/
subps
%
xmm9
%
xmm8
.
byte
69
15
83
192
/
/
rcpps
%
xmm8
%
xmm8
.
byte
69
15
89
196
/
/
mulps
%
xmm12
%
xmm8
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
69
15
93
232
/
/
minps
%
xmm8
%
xmm13
.
byte
69
15
40
241
/
/
movaps
%
xmm9
%
xmm14
.
byte
68
15
194
243
0
/
/
cmpeqps
%
xmm3
%
xmm14
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
68
15
88
232
/
/
addps
%
xmm0
%
xmm13
.
byte
69
15
84
206
/
/
andps
%
xmm14
%
xmm9
.
byte
69
15
85
245
/
/
andnps
%
xmm13
%
xmm14
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
92
211
/
/
subps
%
xmm3
%
xmm10
.
byte
69
15
86
241
/
/
orps
%
xmm9
%
xmm14
.
byte
69
15
40
202
/
/
movaps
%
xmm10
%
xmm9
.
byte
68
15
89
204
/
/
mulps
%
xmm4
%
xmm9
.
byte
69
15
88
241
/
/
addps
%
xmm9
%
xmm14
.
byte
68
15
40
204
/
/
movaps
%
xmm4
%
xmm9
.
byte
69
15
194
200
0
/
/
cmpeqps
%
xmm8
%
xmm9
.
byte
65
15
84
193
/
/
andps
%
xmm9
%
xmm0
.
byte
69
15
85
206
/
/
andnps
%
xmm14
%
xmm9
.
byte
65
15
86
193
/
/
orps
%
xmm9
%
xmm0
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
68
15
40
235
/
/
movaps
%
xmm3
%
xmm13
.
byte
68
15
92
233
/
/
subps
%
xmm1
%
xmm13
.
byte
69
15
83
237
/
/
rcpps
%
xmm13
%
xmm13
.
byte
69
15
89
233
/
/
mulps
%
xmm9
%
xmm13
.
byte
69
15
40
203
/
/
movaps
%
xmm11
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
69
15
93
229
/
/
minps
%
xmm13
%
xmm12
.
byte
68
15
40
233
/
/
movaps
%
xmm1
%
xmm13
.
byte
68
15
194
235
0
/
/
cmpeqps
%
xmm3
%
xmm13
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
225
/
/
addps
%
xmm9
%
xmm12
.
byte
65
15
84
205
/
/
andps
%
xmm13
%
xmm1
.
byte
69
15
85
236
/
/
andnps
%
xmm12
%
xmm13
.
byte
68
15
86
233
/
/
orps
%
xmm1
%
xmm13
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
68
15
88
233
/
/
addps
%
xmm1
%
xmm13
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
65
15
194
200
0
/
/
cmpeqps
%
xmm8
%
xmm1
.
byte
68
15
84
201
/
/
andps
%
xmm1
%
xmm9
.
byte
65
15
85
205
/
/
andnps
%
xmm13
%
xmm1
.
byte
68
15
86
201
/
/
orps
%
xmm1
%
xmm9
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
68
15
89
230
/
/
mulps
%
xmm6
%
xmm12
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
92
202
/
/
subps
%
xmm2
%
xmm1
.
byte
68
15
83
233
/
/
rcpps
%
xmm1
%
xmm13
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
93
229
/
/
minps
%
xmm13
%
xmm12
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
194
203
0
/
/
cmpeqps
%
xmm3
%
xmm1
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
227
/
/
addps
%
xmm11
%
xmm12
.
byte
15
84
209
/
/
andps
%
xmm1
%
xmm2
.
byte
65
15
85
204
/
/
andnps
%
xmm12
%
xmm1
.
byte
15
86
202
/
/
orps
%
xmm2
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
68
15
194
198
0
/
/
cmpeqps
%
xmm6
%
xmm8
.
byte
69
15
84
216
/
/
andps
%
xmm8
%
xmm11
.
byte
68
15
85
193
/
/
andnps
%
xmm1
%
xmm8
.
byte
69
15
86
195
/
/
orps
%
xmm11
%
xmm8
.
byte
68
15
89
215
/
/
mulps
%
xmm7
%
xmm10
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_sse2
.
globl
_sk_hardlight_sse2
FUNCTION
(
_sk_hardlight_sse2
)
_sk_hardlight_sse2
:
.
byte
15
41
116
36
232
/
/
movaps
%
xmm6
-
0x18
(
%
rsp
)
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
68
15
40
29
79
114
1
0
/
/
movaps
0x1724f
(
%
rip
)
%
xmm11
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
211
/
/
movaps
%
xmm11
%
xmm10
.
byte
68
15
92
215
/
/
subps
%
xmm7
%
xmm10
.
byte
69
15
40
194
/
/
movaps
%
xmm10
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
68
15
92
219
/
/
subps
%
xmm3
%
xmm11
.
byte
69
15
40
203
/
/
movaps
%
xmm11
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
69
15
88
200
/
/
addps
%
xmm8
%
xmm9
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
92
192
/
/
subps
%
xmm0
%
xmm8
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
68
15
40
247
/
/
movaps
%
xmm7
%
xmm14
.
byte
68
15
40
255
/
/
movaps
%
xmm7
%
xmm15
.
byte
68
15
92
253
/
/
subps
%
xmm5
%
xmm15
.
byte
69
15
89
248
/
/
mulps
%
xmm8
%
xmm15
.
byte
69
15
88
255
/
/
addps
%
xmm15
%
xmm15
.
byte
68
15
40
228
/
/
movaps
%
xmm4
%
xmm12
.
byte
69
15
92
231
/
/
subps
%
xmm15
%
xmm12
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
68
15
194
195
2
/
/
cmpleps
%
xmm3
%
xmm8
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
69
15
85
196
/
/
andnps
%
xmm12
%
xmm8
.
byte
68
15
86
192
/
/
orps
%
xmm0
%
xmm8
.
byte
69
15
40
251
/
/
movaps
%
xmm11
%
xmm15
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
68
15
89
223
/
/
mulps
%
xmm7
%
xmm11
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
68
15
89
254
/
/
mulps
%
xmm6
%
xmm15
.
byte
68
15
88
248
/
/
addps
%
xmm0
%
xmm15
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
68
15
92
238
/
/
subps
%
xmm6
%
xmm13
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
69
15
88
237
/
/
addps
%
xmm13
%
xmm13
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
92
197
/
/
subps
%
xmm13
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
88
201
/
/
addps
%
xmm9
%
xmm9
.
byte
68
15
194
203
2
/
/
cmpleps
%
xmm3
%
xmm9
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
65
15
84
201
/
/
andps
%
xmm9
%
xmm1
.
byte
68
15
85
200
/
/
andnps
%
xmm0
%
xmm9
.
byte
68
15
86
201
/
/
orps
%
xmm1
%
xmm9
.
byte
69
15
88
207
/
/
addps
%
xmm15
%
xmm9
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
68
15
40
108
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm13
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
69
15
88
226
/
/
addps
%
xmm10
%
xmm12
.
byte
68
15
40
210
/
/
movaps
%
xmm2
%
xmm10
.
byte
69
15
88
210
/
/
addps
%
xmm10
%
xmm10
.
byte
68
15
194
211
2
/
/
cmpleps
%
xmm3
%
xmm10
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
69
15
92
245
/
/
subps
%
xmm13
%
xmm14
.
byte
68
15
89
240
/
/
mulps
%
xmm0
%
xmm14
.
byte
69
15
88
246
/
/
addps
%
xmm14
%
xmm14
.
byte
65
15
92
230
/
/
subps
%
xmm14
%
xmm4
.
byte
65
15
84
210
/
/
andps
%
xmm10
%
xmm2
.
byte
68
15
85
212
/
/
andnps
%
xmm4
%
xmm10
.
byte
68
15
86
210
/
/
orps
%
xmm2
%
xmm10
.
byte
69
15
88
212
/
/
addps
%
xmm12
%
xmm10
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
65
15
40
245
/
/
movaps
%
xmm13
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_sse2
.
globl
_sk_overlay_sse2
FUNCTION
(
_sk_overlay_sse2
)
_sk_overlay_sse2
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
68
15
40
232
/
/
movaps
%
xmm0
%
xmm13
.
byte
68
15
40
13
13
113
1
0
/
/
movaps
0x1710d
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
92
215
/
/
subps
%
xmm7
%
xmm10
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
69
15
89
221
/
/
mulps
%
xmm13
%
xmm11
.
byte
68
15
92
203
/
/
subps
%
xmm3
%
xmm9
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
195
/
/
addps
%
xmm11
%
xmm0
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
68
15
89
236
/
/
mulps
%
xmm4
%
xmm13
.
byte
68
15
40
247
/
/
movaps
%
xmm7
%
xmm14
.
byte
68
15
92
244
/
/
subps
%
xmm4
%
xmm14
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
194
207
2
/
/
cmpleps
%
xmm7
%
xmm1
.
byte
69
15
88
237
/
/
addps
%
xmm13
%
xmm13
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
68
15
89
223
/
/
mulps
%
xmm7
%
xmm11
.
byte
69
15
89
244
/
/
mulps
%
xmm12
%
xmm14
.
byte
69
15
88
246
/
/
addps
%
xmm14
%
xmm14
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
69
15
92
230
/
/
subps
%
xmm14
%
xmm12
.
byte
68
15
84
233
/
/
andps
%
xmm1
%
xmm13
.
byte
65
15
85
204
/
/
andnps
%
xmm12
%
xmm1
.
byte
65
15
86
205
/
/
orps
%
xmm13
%
xmm1
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
69
15
40
226
/
/
movaps
%
xmm10
%
xmm12
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
65
15
88
204
/
/
addps
%
xmm12
%
xmm1
.
byte
68
15
40
227
/
/
movaps
%
xmm3
%
xmm12
.
byte
69
15
92
224
/
/
subps
%
xmm8
%
xmm12
.
byte
68
15
89
197
/
/
mulps
%
xmm5
%
xmm8
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
68
15
92
237
/
/
subps
%
xmm5
%
xmm13
.
byte
68
15
40
245
/
/
movaps
%
xmm5
%
xmm14
.
byte
69
15
88
246
/
/
addps
%
xmm14
%
xmm14
.
byte
68
15
194
247
2
/
/
cmpleps
%
xmm7
%
xmm14
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
69
15
88
237
/
/
addps
%
xmm13
%
xmm13
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
69
15
92
229
/
/
subps
%
xmm13
%
xmm12
.
byte
69
15
84
198
/
/
andps
%
xmm14
%
xmm8
.
byte
69
15
85
244
/
/
andnps
%
xmm12
%
xmm14
.
byte
69
15
86
240
/
/
orps
%
xmm8
%
xmm14
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
89
198
/
/
mulps
%
xmm6
%
xmm8
.
byte
69
15
88
194
/
/
addps
%
xmm10
%
xmm8
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
92
210
/
/
subps
%
xmm2
%
xmm10
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
92
230
/
/
subps
%
xmm6
%
xmm12
.
byte
68
15
40
238
/
/
movaps
%
xmm6
%
xmm13
.
byte
69
15
88
237
/
/
addps
%
xmm13
%
xmm13
.
byte
68
15
194
239
2
/
/
cmpleps
%
xmm7
%
xmm13
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
69
15
88
228
/
/
addps
%
xmm12
%
xmm12
.
byte
69
15
92
220
/
/
subps
%
xmm12
%
xmm11
.
byte
65
15
84
213
/
/
andps
%
xmm13
%
xmm2
.
byte
69
15
85
235
/
/
andnps
%
xmm11
%
xmm13
.
byte
68
15
86
234
/
/
orps
%
xmm2
%
xmm13
.
byte
69
15
88
197
/
/
addps
%
xmm13
%
xmm8
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_softlight_sse2
.
globl
_sk_softlight_sse2
FUNCTION
(
_sk_softlight_sse2
)
_sk_softlight_sse2
:
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
41
84
36
232
/
/
movaps
%
xmm2
-
0x18
(
%
rsp
)
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
194
231
1
/
/
cmpltps
%
xmm7
%
xmm12
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
68
15
94
215
/
/
divps
%
xmm7
%
xmm10
.
byte
69
15
84
212
/
/
andps
%
xmm12
%
xmm10
.
byte
68
15
40
13
186
111
1
0
/
/
movaps
0x16fba
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
249
/
/
movaps
%
xmm9
%
xmm15
.
byte
69
15
92
250
/
/
subps
%
xmm10
%
xmm15
.
byte
69
15
40
218
/
/
movaps
%
xmm10
%
xmm11
.
byte
69
15
40
234
/
/
movaps
%
xmm10
%
xmm13
.
byte
65
15
82
194
/
/
rsqrtps
%
xmm10
%
xmm0
.
byte
15
83
200
/
/
rcpps
%
xmm0
%
xmm1
.
byte
65
15
92
202
/
/
subps
%
xmm10
%
xmm1
.
byte
69
15
88
210
/
/
addps
%
xmm10
%
xmm10
.
byte
69
15
88
210
/
/
addps
%
xmm10
%
xmm10
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
68
15
40
53
228
111
1
0
/
/
movaps
0x16fe4
(
%
rip
)
%
xmm14
#
3cf70
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd24
>
.
byte
69
15
88
222
/
/
addps
%
xmm14
%
xmm11
.
byte
68
15
89
216
/
/
mulps
%
xmm0
%
xmm11
.
byte
68
15
40
21
228
111
1
0
/
/
movaps
0x16fe4
(
%
rip
)
%
xmm10
#
3cf80
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd34
>
.
byte
69
15
89
234
/
/
mulps
%
xmm10
%
xmm13
.
byte
69
15
88
235
/
/
addps
%
xmm11
%
xmm13
.
byte
15
88
228
/
/
addps
%
xmm4
%
xmm4
.
byte
15
88
228
/
/
addps
%
xmm4
%
xmm4
.
byte
15
194
231
2
/
/
cmpleps
%
xmm7
%
xmm4
.
byte
68
15
84
236
/
/
andps
%
xmm4
%
xmm13
.
byte
15
85
225
/
/
andnps
%
xmm1
%
xmm4
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
65
15
86
229
/
/
orps
%
xmm13
%
xmm4
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
68
15
89
249
/
/
mulps
%
xmm1
%
xmm15
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
69
15
40
217
/
/
movaps
%
xmm9
%
xmm11
.
byte
68
15
92
219
/
/
subps
%
xmm3
%
xmm11
.
byte
65
15
40
203
/
/
movaps
%
xmm11
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
69
15
40
233
/
/
movaps
%
xmm9
%
xmm13
.
byte
68
15
92
239
/
/
subps
%
xmm7
%
xmm13
.
byte
69
15
89
197
/
/
mulps
%
xmm13
%
xmm8
.
byte
68
15
88
193
/
/
addps
%
xmm1
%
xmm8
.
byte
68
15
88
251
/
/
addps
%
xmm3
%
xmm15
.
byte
68
15
89
253
/
/
mulps
%
xmm5
%
xmm15
.
byte
15
194
195
2
/
/
cmpleps
%
xmm3
%
xmm0
.
byte
68
15
84
248
/
/
andps
%
xmm0
%
xmm15
.
byte
15
85
196
/
/
andnps
%
xmm4
%
xmm0
.
byte
65
15
86
199
/
/
orps
%
xmm15
%
xmm0
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
198
/
/
movaps
%
xmm6
%
xmm8
.
byte
68
15
94
199
/
/
divps
%
xmm7
%
xmm8
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
89
228
/
/
mulps
%
xmm4
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
69
15
40
249
/
/
movaps
%
xmm9
%
xmm15
.
byte
69
15
92
248
/
/
subps
%
xmm8
%
xmm15
.
byte
65
15
82
224
/
/
rsqrtps
%
xmm8
%
xmm4
.
byte
15
83
228
/
/
rcpps
%
xmm4
%
xmm4
.
byte
65
15
92
224
/
/
subps
%
xmm8
%
xmm4
.
byte
69
15
89
194
/
/
mulps
%
xmm10
%
xmm8
.
byte
68
15
88
193
/
/
addps
%
xmm1
%
xmm8
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
194
207
2
/
/
cmpleps
%
xmm7
%
xmm1
.
byte
68
15
84
193
/
/
andps
%
xmm1
%
xmm8
.
byte
15
85
204
/
/
andnps
%
xmm4
%
xmm1
.
byte
65
15
86
200
/
/
orps
%
xmm8
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
69
15
88
192
/
/
addps
%
xmm8
%
xmm8
.
byte
65
15
40
224
/
/
movaps
%
xmm8
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
68
15
89
252
/
/
mulps
%
xmm4
%
xmm15
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
65
15
40
227
/
/
movaps
%
xmm11
%
xmm4
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
68
15
88
251
/
/
addps
%
xmm3
%
xmm15
.
byte
68
15
89
254
/
/
mulps
%
xmm6
%
xmm15
.
byte
68
15
194
195
2
/
/
cmpleps
%
xmm3
%
xmm8
.
byte
69
15
84
248
/
/
andps
%
xmm8
%
xmm15
.
byte
68
15
85
193
/
/
andnps
%
xmm1
%
xmm8
.
byte
69
15
86
199
/
/
orps
%
xmm15
%
xmm8
.
byte
68
15
88
194
/
/
addps
%
xmm2
%
xmm8
.
byte
68
15
40
124
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm15
.
byte
65
15
40
207
/
/
movaps
%
xmm15
%
xmm1
.
byte
15
94
207
/
/
divps
%
xmm7
%
xmm1
.
byte
65
15
84
204
/
/
andps
%
xmm12
%
xmm1
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
68
15
88
241
/
/
addps
%
xmm1
%
xmm14
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
15
82
209
/
/
rsqrtps
%
xmm1
%
xmm2
.
byte
15
83
210
/
/
rcpps
%
xmm2
%
xmm2
.
byte
15
92
209
/
/
subps
%
xmm1
%
xmm2
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
89
228
/
/
mulps
%
xmm4
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
68
15
89
244
/
/
mulps
%
xmm4
%
xmm14
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
65
15
40
207
/
/
movaps
%
xmm15
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
194
207
2
/
/
cmpleps
%
xmm7
%
xmm1
.
byte
68
15
84
209
/
/
andps
%
xmm1
%
xmm10
.
byte
15
85
202
/
/
andnps
%
xmm2
%
xmm1
.
byte
15
40
84
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm2
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
65
15
86
202
/
/
orps
%
xmm10
%
xmm1
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
68
15
89
204
/
/
mulps
%
xmm4
%
xmm9
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
65
15
89
231
/
/
mulps
%
xmm15
%
xmm4
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
65
15
40
227
/
/
movaps
%
xmm11
%
xmm4
.
byte
65
15
89
231
/
/
mulps
%
xmm15
%
xmm4
.
byte
65
15
88
229
/
/
addps
%
xmm13
%
xmm4
.
byte
68
15
88
203
/
/
addps
%
xmm3
%
xmm9
.
byte
69
15
89
207
/
/
mulps
%
xmm15
%
xmm9
.
byte
69
15
40
215
/
/
movaps
%
xmm15
%
xmm10
.
byte
15
194
211
2
/
/
cmpleps
%
xmm3
%
xmm2
.
byte
68
15
84
202
/
/
andps
%
xmm2
%
xmm9
.
byte
15
85
209
/
/
andnps
%
xmm1
%
xmm2
.
byte
65
15
86
209
/
/
orps
%
xmm9
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
68
15
89
223
/
/
mulps
%
xmm7
%
xmm11
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
65
15
40
242
/
/
movaps
%
xmm10
%
xmm6
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hue_sse2
.
globl
_sk_hue_sse2
FUNCTION
(
_sk_hue_sse2
)
_sk_hue_sse2
:
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
68
15
40
225
/
/
movaps
%
xmm1
%
xmm12
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
68
15
40
5
119
109
1
0
/
/
movaps
0x16d77
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
68
15
92
217
/
/
subps
%
xmm1
%
xmm11
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
15
41
68
36
232
/
/
movaps
%
xmm0
-
0x18
(
%
rsp
)
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
68
15
41
100
36
216
/
/
movaps
%
xmm12
-
0x28
(
%
rsp
)
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
68
15
40
229
/
/
movaps
%
xmm5
%
xmm12
.
byte
68
15
40
237
/
/
movaps
%
xmm5
%
xmm13
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
41
68
36
200
/
/
movaps
%
xmm0
-
0x38
(
%
rsp
)
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
68
15
95
239
/
/
maxps
%
xmm7
%
xmm13
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
68
15
40
245
/
/
movaps
%
xmm5
%
xmm14
.
byte
68
15
40
253
/
/
movaps
%
xmm5
%
xmm15
.
byte
69
15
95
253
/
/
maxps
%
xmm13
%
xmm15
.
byte
68
15
93
231
/
/
minps
%
xmm7
%
xmm12
.
byte
69
15
93
244
/
/
minps
%
xmm12
%
xmm14
.
byte
69
15
92
254
/
/
subps
%
xmm14
%
xmm15
.
byte
69
15
40
226
/
/
movaps
%
xmm10
%
xmm12
.
byte
68
15
93
226
/
/
minps
%
xmm2
%
xmm12
.
byte
69
15
40
233
/
/
movaps
%
xmm9
%
xmm13
.
byte
69
15
93
236
/
/
minps
%
xmm12
%
xmm13
.
byte
69
15
40
226
/
/
movaps
%
xmm10
%
xmm12
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
69
15
40
241
/
/
movaps
%
xmm9
%
xmm14
.
byte
69
15
95
244
/
/
maxps
%
xmm12
%
xmm14
.
byte
69
15
92
245
/
/
subps
%
xmm13
%
xmm14
.
byte
69
15
92
205
/
/
subps
%
xmm13
%
xmm9
.
byte
69
15
92
213
/
/
subps
%
xmm13
%
xmm10
.
byte
65
15
92
213
/
/
subps
%
xmm13
%
xmm2
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
68
15
89
251
/
/
mulps
%
xmm3
%
xmm15
.
byte
69
15
89
207
/
/
mulps
%
xmm15
%
xmm9
.
byte
69
15
89
215
/
/
mulps
%
xmm15
%
xmm10
.
byte
65
15
89
215
/
/
mulps
%
xmm15
%
xmm2
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
69
15
94
206
/
/
divps
%
xmm14
%
xmm9
.
byte
69
15
94
214
/
/
divps
%
xmm14
%
xmm10
.
byte
65
15
94
214
/
/
divps
%
xmm14
%
xmm2
.
byte
69
15
194
244
4
/
/
cmpneqps
%
xmm12
%
xmm14
.
byte
69
15
84
206
/
/
andps
%
xmm14
%
xmm9
.
byte
69
15
84
214
/
/
andps
%
xmm14
%
xmm10
.
byte
65
15
84
214
/
/
andps
%
xmm14
%
xmm2
.
byte
68
15
40
61
52
109
1
0
/
/
movaps
0x16d34
(
%
rip
)
%
xmm15
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
65
15
89
231
/
/
mulps
%
xmm15
%
xmm4
.
byte
15
40
5
57
109
1
0
/
/
movaps
0x16d39
(
%
rip
)
%
xmm0
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
88
244
/
/
addps
%
xmm4
%
xmm6
.
byte
68
15
40
53
59
109
1
0
/
/
movaps
0x16d3b
(
%
rip
)
%
xmm14
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
68
15
40
239
/
/
movaps
%
xmm7
%
xmm13
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
68
15
88
238
/
/
addps
%
xmm6
%
xmm13
.
byte
65
15
40
225
/
/
movaps
%
xmm9
%
xmm4
.
byte
65
15
89
231
/
/
mulps
%
xmm15
%
xmm4
.
byte
65
15
40
242
/
/
movaps
%
xmm10
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
88
244
/
/
addps
%
xmm4
%
xmm6
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
65
15
89
230
/
/
mulps
%
xmm14
%
xmm4
.
byte
15
88
230
/
/
addps
%
xmm6
%
xmm4
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
68
15
92
236
/
/
subps
%
xmm4
%
xmm13
.
byte
69
15
88
205
/
/
addps
%
xmm13
%
xmm9
.
byte
69
15
88
213
/
/
addps
%
xmm13
%
xmm10
.
byte
68
15
88
234
/
/
addps
%
xmm2
%
xmm13
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
93
213
/
/
minps
%
xmm13
%
xmm2
.
byte
65
15
40
241
/
/
movaps
%
xmm9
%
xmm6
.
byte
15
93
242
/
/
minps
%
xmm2
%
xmm6
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
95
213
/
/
maxps
%
xmm13
%
xmm2
.
byte
65
15
40
225
/
/
movaps
%
xmm9
%
xmm4
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
69
15
89
249
/
/
mulps
%
xmm9
%
xmm15
.
byte
65
15
89
194
/
/
mulps
%
xmm10
%
xmm0
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
68
15
88
240
/
/
addps
%
xmm0
%
xmm14
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
194
214
2
/
/
cmpleps
%
xmm6
%
xmm2
.
byte
69
15
40
254
/
/
movaps
%
xmm14
%
xmm15
.
byte
68
15
92
254
/
/
subps
%
xmm6
%
xmm15
.
byte
65
15
40
241
/
/
movaps
%
xmm9
%
xmm6
.
byte
65
15
92
246
/
/
subps
%
xmm14
%
xmm6
.
byte
65
15
89
246
/
/
mulps
%
xmm14
%
xmm6
.
byte
65
15
94
247
/
/
divps
%
xmm15
%
xmm6
.
byte
65
15
88
246
/
/
addps
%
xmm14
%
xmm6
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
85
198
/
/
andnps
%
xmm6
%
xmm0
.
byte
68
15
84
202
/
/
andps
%
xmm2
%
xmm9
.
byte
68
15
86
200
/
/
orps
%
xmm0
%
xmm9
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
41
76
36
184
/
/
movaps
%
xmm1
-
0x48
(
%
rsp
)
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
41
92
36
168
/
/
movaps
%
xmm3
-
0x58
(
%
rsp
)
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
194
196
1
/
/
cmpltps
%
xmm4
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
85
201
/
/
andnps
%
xmm9
%
xmm1
.
byte
69
15
92
206
/
/
subps
%
xmm14
%
xmm9
.
byte
65
15
92
246
/
/
subps
%
xmm14
%
xmm6
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
65
15
92
230
/
/
subps
%
xmm14
%
xmm4
.
byte
68
15
94
204
/
/
divps
%
xmm4
%
xmm9
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
68
15
84
200
/
/
andps
%
xmm0
%
xmm9
.
byte
68
15
86
201
/
/
orps
%
xmm1
%
xmm9
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
92
206
/
/
subps
%
xmm14
%
xmm1
.
byte
65
15
89
206
/
/
mulps
%
xmm14
%
xmm1
.
byte
65
15
94
207
/
/
divps
%
xmm15
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
85
217
/
/
andnps
%
xmm1
%
xmm3
.
byte
68
15
84
210
/
/
andps
%
xmm2
%
xmm10
.
byte
68
15
86
211
/
/
orps
%
xmm3
%
xmm10
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
85
202
/
/
andnps
%
xmm10
%
xmm1
.
byte
69
15
92
214
/
/
subps
%
xmm14
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
68
15
94
212
/
/
divps
%
xmm4
%
xmm10
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
68
15
86
209
/
/
orps
%
xmm1
%
xmm10
.
byte
65
15
40
205
/
/
movaps
%
xmm13
%
xmm1
.
byte
65
15
92
206
/
/
subps
%
xmm14
%
xmm1
.
byte
65
15
89
206
/
/
mulps
%
xmm14
%
xmm1
.
byte
65
15
94
207
/
/
divps
%
xmm15
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
68
15
84
234
/
/
andps
%
xmm2
%
xmm13
.
byte
15
85
209
/
/
andnps
%
xmm1
%
xmm2
.
byte
65
15
86
213
/
/
orps
%
xmm13
%
xmm2
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
85
202
/
/
andnps
%
xmm2
%
xmm1
.
byte
65
15
92
214
/
/
subps
%
xmm14
%
xmm2
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
94
212
/
/
divps
%
xmm4
%
xmm2
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
15
84
208
/
/
andps
%
xmm0
%
xmm2
.
byte
15
86
209
/
/
orps
%
xmm1
%
xmm2
.
byte
69
15
95
204
/
/
maxps
%
xmm12
%
xmm9
.
byte
69
15
95
212
/
/
maxps
%
xmm12
%
xmm10
.
byte
65
15
95
212
/
/
maxps
%
xmm12
%
xmm2
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
40
76
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
15
40
116
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm6
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
15
40
76
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
69
15
88
195
/
/
addps
%
xmm11
%
xmm8
.
byte
68
15
88
194
/
/
addps
%
xmm2
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
40
92
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm3
.
byte
15
40
124
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_saturation_sse2
.
globl
_sk_saturation_sse2
FUNCTION
(
_sk_saturation_sse2
)
_sk_saturation_sse2
:
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
40
198
/
/
movaps
%
xmm6
%
xmm8
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
68
15
40
216
/
/
movaps
%
xmm0
%
xmm11
.
byte
68
15
40
215
/
/
movaps
%
xmm7
%
xmm10
.
byte
68
15
89
212
/
/
mulps
%
xmm4
%
xmm10
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
41
116
36
184
/
/
movaps
%
xmm6
-
0x48
(
%
rsp
)
.
byte
68
15
40
207
/
/
movaps
%
xmm7
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
15
41
108
36
200
/
/
movaps
%
xmm5
-
0x38
(
%
rsp
)
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
69
15
40
232
/
/
movaps
%
xmm8
%
xmm13
.
byte
68
15
41
108
36
168
/
/
movaps
%
xmm13
-
0x58
(
%
rsp
)
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
41
100
36
216
/
/
movaps
%
xmm4
-
0x28
(
%
rsp
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
41
84
36
232
/
/
movaps
%
xmm2
-
0x18
(
%
rsp
)
.
byte
15
95
218
/
/
maxps
%
xmm2
%
xmm3
.
byte
65
15
40
203
/
/
movaps
%
xmm11
%
xmm1
.
byte
15
95
203
/
/
maxps
%
xmm3
%
xmm1
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
93
218
/
/
minps
%
xmm2
%
xmm3
.
byte
65
15
40
211
/
/
movaps
%
xmm11
%
xmm2
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
15
92
202
/
/
subps
%
xmm2
%
xmm1
.
byte
65
15
89
204
/
/
mulps
%
xmm12
%
xmm1
.
byte
65
15
40
228
/
/
movaps
%
xmm12
%
xmm4
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
69
15
40
194
/
/
movaps
%
xmm10
%
xmm8
.
byte
68
15
93
194
/
/
minps
%
xmm2
%
xmm8
.
byte
65
15
40
209
/
/
movaps
%
xmm9
%
xmm2
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
15
95
218
/
/
maxps
%
xmm2
%
xmm3
.
byte
65
15
92
216
/
/
subps
%
xmm8
%
xmm3
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
69
15
92
208
/
/
subps
%
xmm8
%
xmm10
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
68
15
94
211
/
/
divps
%
xmm3
%
xmm10
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
68
15
94
203
/
/
divps
%
xmm3
%
xmm9
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
94
195
/
/
divps
%
xmm3
%
xmm0
.
byte
15
194
218
4
/
/
cmpneqps
%
xmm2
%
xmm3
.
byte
68
15
84
211
/
/
andps
%
xmm3
%
xmm10
.
byte
68
15
84
203
/
/
andps
%
xmm3
%
xmm9
.
byte
15
84
195
/
/
andps
%
xmm3
%
xmm0
.
byte
68
15
40
5
141
106
1
0
/
/
movaps
0x16a8d
(
%
rip
)
%
xmm8
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
40
13
143
106
1
0
/
/
movaps
0x16a8f
(
%
rip
)
%
xmm1
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
68
15
40
37
142
106
1
0
/
/
movaps
0x16a8e
(
%
rip
)
%
xmm12
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
68
15
88
235
/
/
addps
%
xmm3
%
xmm13
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
65
15
40
217
/
/
movaps
%
xmm9
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
68
15
89
239
/
/
mulps
%
xmm7
%
xmm13
.
byte
68
15
92
234
/
/
subps
%
xmm2
%
xmm13
.
byte
69
15
88
213
/
/
addps
%
xmm13
%
xmm10
.
byte
69
15
88
205
/
/
addps
%
xmm13
%
xmm9
.
byte
68
15
88
232
/
/
addps
%
xmm0
%
xmm13
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
93
197
/
/
minps
%
xmm13
%
xmm0
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
95
197
/
/
maxps
%
xmm13
%
xmm0
.
byte
69
15
40
242
/
/
movaps
%
xmm10
%
xmm14
.
byte
68
15
95
240
/
/
maxps
%
xmm0
%
xmm14
.
byte
69
15
89
194
/
/
mulps
%
xmm10
%
xmm8
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
68
15
88
225
/
/
addps
%
xmm1
%
xmm12
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
194
211
2
/
/
cmpleps
%
xmm3
%
xmm2
.
byte
65
15
40
244
/
/
movaps
%
xmm12
%
xmm6
.
byte
15
92
243
/
/
subps
%
xmm3
%
xmm6
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
65
15
92
196
/
/
subps
%
xmm12
%
xmm0
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
15
94
198
/
/
divps
%
xmm6
%
xmm0
.
byte
65
15
88
196
/
/
addps
%
xmm12
%
xmm0
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
85
200
/
/
andnps
%
xmm0
%
xmm1
.
byte
68
15
84
210
/
/
andps
%
xmm2
%
xmm10
.
byte
68
15
86
209
/
/
orps
%
xmm1
%
xmm10
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
68
15
40
5
67
105
1
0
/
/
movaps
0x16943
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
40
224
/
/
movaps
%
xmm8
%
xmm4
.
byte
68
15
92
199
/
/
subps
%
xmm7
%
xmm8
.
byte
15
88
253
/
/
addps
%
xmm5
%
xmm7
.
byte
15
92
251
/
/
subps
%
xmm3
%
xmm7
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
65
15
194
222
1
/
/
cmpltps
%
xmm14
%
xmm3
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
65
15
85
194
/
/
andnps
%
xmm10
%
xmm0
.
byte
69
15
92
212
/
/
subps
%
xmm12
%
xmm10
.
byte
65
15
92
204
/
/
subps
%
xmm12
%
xmm1
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
69
15
92
244
/
/
subps
%
xmm12
%
xmm14
.
byte
69
15
94
214
/
/
divps
%
xmm14
%
xmm10
.
byte
69
15
88
212
/
/
addps
%
xmm12
%
xmm10
.
byte
68
15
84
211
/
/
andps
%
xmm3
%
xmm10
.
byte
68
15
86
208
/
/
orps
%
xmm0
%
xmm10
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
92
196
/
/
subps
%
xmm12
%
xmm0
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
15
94
198
/
/
divps
%
xmm6
%
xmm0
.
byte
65
15
88
196
/
/
addps
%
xmm12
%
xmm0
.
byte
68
15
40
250
/
/
movaps
%
xmm2
%
xmm15
.
byte
68
15
85
248
/
/
andnps
%
xmm0
%
xmm15
.
byte
68
15
84
202
/
/
andps
%
xmm2
%
xmm9
.
byte
69
15
86
207
/
/
orps
%
xmm15
%
xmm9
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
65
15
85
193
/
/
andnps
%
xmm9
%
xmm0
.
byte
69
15
92
204
/
/
subps
%
xmm12
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
69
15
94
206
/
/
divps
%
xmm14
%
xmm9
.
byte
69
15
88
204
/
/
addps
%
xmm12
%
xmm9
.
byte
68
15
84
203
/
/
andps
%
xmm3
%
xmm9
.
byte
68
15
86
200
/
/
orps
%
xmm0
%
xmm9
.
byte
65
15
40
197
/
/
movaps
%
xmm13
%
xmm0
.
byte
65
15
92
196
/
/
subps
%
xmm12
%
xmm0
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
15
94
198
/
/
divps
%
xmm6
%
xmm0
.
byte
65
15
88
196
/
/
addps
%
xmm12
%
xmm0
.
byte
68
15
84
234
/
/
andps
%
xmm2
%
xmm13
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
65
15
86
213
/
/
orps
%
xmm13
%
xmm2
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
85
194
/
/
andnps
%
xmm2
%
xmm0
.
byte
65
15
92
212
/
/
subps
%
xmm12
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
65
15
94
214
/
/
divps
%
xmm14
%
xmm2
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
15
84
211
/
/
andps
%
xmm3
%
xmm2
.
byte
15
86
208
/
/
orps
%
xmm0
%
xmm2
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
68
15
95
208
/
/
maxps
%
xmm0
%
xmm10
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
40
92
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
68
15
40
229
/
/
movaps
%
xmm5
%
xmm12
.
byte
65
15
92
228
/
/
subps
%
xmm12
%
xmm4
.
byte
68
15
89
220
/
/
mulps
%
xmm4
%
xmm11
.
byte
68
15
88
216
/
/
addps
%
xmm0
%
xmm11
.
byte
69
15
88
218
/
/
addps
%
xmm10
%
xmm11
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
40
108
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
40
76
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
15
89
100
36
232
/
/
mulps
-
0x18
(
%
rsp
)
%
xmm4
.
byte
15
40
68
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm0
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
68
15
88
196
/
/
addps
%
xmm4
%
xmm8
.
byte
68
15
88
194
/
/
addps
%
xmm2
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
65
15
40
252
/
/
movaps
%
xmm12
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_color_sse2
.
globl
_sk_color_sse2
FUNCTION
(
_sk_color_sse2
)
_sk_color_sse2
:
.
byte
68
15
40
199
/
/
movaps
%
xmm7
%
xmm8
.
byte
68
15
40
230
/
/
movaps
%
xmm6
%
xmm12
.
byte
68
15
41
100
36
216
/
/
movaps
%
xmm12
-
0x28
(
%
rsp
)
.
byte
68
15
40
221
/
/
movaps
%
xmm5
%
xmm11
.
byte
68
15
41
92
36
232
/
/
movaps
%
xmm11
-
0x18
(
%
rsp
)
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
41
84
36
184
/
/
movaps
%
xmm2
-
0x48
(
%
rsp
)
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
68
15
40
207
/
/
movaps
%
xmm7
%
xmm9
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
68
15
40
45
81
104
1
0
/
/
movaps
0x16851
(
%
rip
)
%
xmm13
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
68
15
40
198
/
/
movaps
%
xmm6
%
xmm8
.
byte
69
15
89
197
/
/
mulps
%
xmm13
%
xmm8
.
byte
68
15
40
53
81
104
1
0
/
/
movaps
0x16851
(
%
rip
)
%
xmm14
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
65
15
89
198
/
/
mulps
%
xmm14
%
xmm0
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
29
77
104
1
0
/
/
movaps
0x1684d
(
%
rip
)
%
xmm11
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
68
15
88
224
/
/
addps
%
xmm0
%
xmm12
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
89
197
/
/
mulps
%
xmm13
%
xmm0
.
byte
69
15
40
250
/
/
movaps
%
xmm10
%
xmm15
.
byte
69
15
89
254
/
/
mulps
%
xmm14
%
xmm15
.
byte
68
15
88
248
/
/
addps
%
xmm0
%
xmm15
.
byte
68
15
40
5
137
103
1
0
/
/
movaps
0x16789
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
40
224
/
/
movaps
%
xmm8
%
xmm4
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
40
76
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm1
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
68
15
92
224
/
/
subps
%
xmm0
%
xmm12
.
byte
69
15
88
204
/
/
addps
%
xmm12
%
xmm9
.
byte
69
15
88
212
/
/
addps
%
xmm12
%
xmm10
.
byte
68
15
88
225
/
/
addps
%
xmm1
%
xmm12
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
65
15
93
196
/
/
minps
%
xmm12
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
15
93
200
/
/
minps
%
xmm0
%
xmm1
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
65
15
95
196
/
/
maxps
%
xmm12
%
xmm0
.
byte
69
15
40
249
/
/
movaps
%
xmm9
%
xmm15
.
byte
68
15
95
248
/
/
maxps
%
xmm0
%
xmm15
.
byte
69
15
89
233
/
/
mulps
%
xmm9
%
xmm13
.
byte
69
15
89
242
/
/
mulps
%
xmm10
%
xmm14
.
byte
69
15
88
245
/
/
addps
%
xmm13
%
xmm14
.
byte
69
15
89
220
/
/
mulps
%
xmm12
%
xmm11
.
byte
69
15
88
222
/
/
addps
%
xmm14
%
xmm11
.
byte
69
15
87
237
/
/
xorps
%
xmm13
%
xmm13
.
byte
68
15
194
233
2
/
/
cmpleps
%
xmm1
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
68
15
92
241
/
/
subps
%
xmm1
%
xmm14
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
65
15
94
206
/
/
divps
%
xmm14
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
65
15
40
197
/
/
movaps
%
xmm13
%
xmm0
.
byte
15
85
193
/
/
andnps
%
xmm1
%
xmm0
.
byte
69
15
84
205
/
/
andps
%
xmm13
%
xmm9
.
byte
68
15
86
200
/
/
orps
%
xmm0
%
xmm9
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
41
84
36
200
/
/
movaps
%
xmm2
-
0x38
(
%
rsp
)
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
41
92
36
184
/
/
movaps
%
xmm3
-
0x48
(
%
rsp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
65
15
194
199
1
/
/
cmpltps
%
xmm15
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
85
201
/
/
andnps
%
xmm9
%
xmm1
.
byte
69
15
92
203
/
/
subps
%
xmm11
%
xmm9
.
byte
65
15
92
219
/
/
subps
%
xmm11
%
xmm3
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
69
15
92
251
/
/
subps
%
xmm11
%
xmm15
.
byte
69
15
94
207
/
/
divps
%
xmm15
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
68
15
84
200
/
/
andps
%
xmm0
%
xmm9
.
byte
68
15
86
201
/
/
orps
%
xmm1
%
xmm9
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
65
15
94
206
/
/
divps
%
xmm14
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
65
15
40
213
/
/
movaps
%
xmm13
%
xmm2
.
byte
15
85
209
/
/
andnps
%
xmm1
%
xmm2
.
byte
69
15
84
213
/
/
andps
%
xmm13
%
xmm10
.
byte
68
15
86
210
/
/
orps
%
xmm2
%
xmm10
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
85
202
/
/
andnps
%
xmm10
%
xmm1
.
byte
69
15
92
211
/
/
subps
%
xmm11
%
xmm10
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
69
15
94
215
/
/
divps
%
xmm15
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
68
15
86
209
/
/
orps
%
xmm1
%
xmm10
.
byte
65
15
40
204
/
/
movaps
%
xmm12
%
xmm1
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
65
15
94
206
/
/
divps
%
xmm14
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
69
15
84
229
/
/
andps
%
xmm13
%
xmm12
.
byte
68
15
85
233
/
/
andnps
%
xmm1
%
xmm13
.
byte
69
15
86
236
/
/
orps
%
xmm12
%
xmm13
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
85
205
/
/
andnps
%
xmm13
%
xmm1
.
byte
69
15
92
235
/
/
subps
%
xmm11
%
xmm13
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
69
15
94
239
/
/
divps
%
xmm15
%
xmm13
.
byte
69
15
88
235
/
/
addps
%
xmm11
%
xmm13
.
byte
68
15
84
232
/
/
andps
%
xmm0
%
xmm13
.
byte
68
15
86
233
/
/
orps
%
xmm1
%
xmm13
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
95
208
/
/
maxps
%
xmm0
%
xmm10
.
byte
68
15
95
232
/
/
maxps
%
xmm0
%
xmm13
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
65
15
88
249
/
/
addps
%
xmm9
%
xmm7
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
40
84
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm2
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
65
15
88
234
/
/
addps
%
xmm10
%
xmm5
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
68
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm0
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
68
15
88
196
/
/
addps
%
xmm4
%
xmm8
.
byte
69
15
88
197
/
/
addps
%
xmm13
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
40
92
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm3
.
byte
15
40
124
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminosity_sse2
.
globl
_sk_luminosity_sse2
FUNCTION
(
_sk_luminosity_sse2
)
_sk_luminosity_sse2
:
.
byte
68
15
40
215
/
/
movaps
%
xmm7
%
xmm10
.
byte
15
41
116
36
200
/
/
movaps
%
xmm6
-
0x38
(
%
rsp
)
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
41
116
36
232
/
/
movaps
%
xmm6
-
0x18
(
%
rsp
)
.
byte
15
41
100
36
216
/
/
movaps
%
xmm4
-
0x28
(
%
rsp
)
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
68
15
40
205
/
/
movaps
%
xmm5
%
xmm9
.
byte
68
15
89
204
/
/
mulps
%
xmm4
%
xmm9
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
68
15
40
37
16
102
1
0
/
/
movaps
0x16610
(
%
rip
)
%
xmm12
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
68
15
40
199
/
/
movaps
%
xmm7
%
xmm8
.
byte
69
15
89
196
/
/
mulps
%
xmm12
%
xmm8
.
byte
68
15
40
45
16
102
1
0
/
/
movaps
0x16610
(
%
rip
)
%
xmm13
#
3cfa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd54
>
.
byte
68
15
40
241
/
/
movaps
%
xmm1
%
xmm14
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
88
240
/
/
addps
%
xmm8
%
xmm14
.
byte
68
15
40
29
12
102
1
0
/
/
movaps
0x1660c
(
%
rip
)
%
xmm11
#
3cfb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd64
>
.
byte
68
15
40
5
100
101
1
0
/
/
movaps
0x16564
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
248
/
/
movaps
%
xmm8
%
xmm15
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
68
15
92
248
/
/
subps
%
xmm0
%
xmm15
.
byte
65
15
89
255
/
/
mulps
%
xmm15
%
xmm7
.
byte
65
15
89
207
/
/
mulps
%
xmm15
%
xmm1
.
byte
15
41
76
36
184
/
/
movaps
%
xmm1
-
0x48
(
%
rsp
)
.
byte
68
15
89
250
/
/
mulps
%
xmm2
%
xmm15
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
69
15
40
241
/
/
movaps
%
xmm9
%
xmm14
.
byte
69
15
89
244
/
/
mulps
%
xmm12
%
xmm14
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
69
15
89
213
/
/
mulps
%
xmm13
%
xmm10
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
116
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
68
15
40
244
/
/
movaps
%
xmm4
%
xmm14
.
byte
69
15
89
243
/
/
mulps
%
xmm11
%
xmm14
.
byte
69
15
88
242
/
/
addps
%
xmm10
%
xmm14
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
65
15
92
214
/
/
subps
%
xmm14
%
xmm2
.
byte
68
15
88
202
/
/
addps
%
xmm2
%
xmm9
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
93
210
/
/
minps
%
xmm2
%
xmm10
.
byte
65
15
40
225
/
/
movaps
%
xmm9
%
xmm4
.
byte
65
15
93
226
/
/
minps
%
xmm10
%
xmm4
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
95
210
/
/
maxps
%
xmm2
%
xmm10
.
byte
69
15
40
241
/
/
movaps
%
xmm9
%
xmm14
.
byte
69
15
95
242
/
/
maxps
%
xmm10
%
xmm14
.
byte
69
15
89
225
/
/
mulps
%
xmm9
%
xmm12
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
69
15
88
236
/
/
addps
%
xmm12
%
xmm13
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
221
/
/
addps
%
xmm13
%
xmm11
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
194
228
2
/
/
cmpleps
%
xmm4
%
xmm12
.
byte
69
15
40
211
/
/
movaps
%
xmm11
%
xmm10
.
byte
68
15
92
212
/
/
subps
%
xmm4
%
xmm10
.
byte
65
15
40
225
/
/
movaps
%
xmm9
%
xmm4
.
byte
65
15
92
227
/
/
subps
%
xmm11
%
xmm4
.
byte
65
15
89
227
/
/
mulps
%
xmm11
%
xmm4
.
byte
65
15
94
226
/
/
divps
%
xmm10
%
xmm4
.
byte
65
15
88
227
/
/
addps
%
xmm11
%
xmm4
.
byte
69
15
40
236
/
/
movaps
%
xmm12
%
xmm13
.
byte
68
15
85
236
/
/
andnps
%
xmm4
%
xmm13
.
byte
69
15
84
204
/
/
andps
%
xmm12
%
xmm9
.
byte
69
15
86
205
/
/
orps
%
xmm13
%
xmm9
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
41
68
36
168
/
/
movaps
%
xmm0
-
0x58
(
%
rsp
)
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
68
15
92
197
/
/
subps
%
xmm5
%
xmm8
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
68
15
40
236
/
/
movaps
%
xmm4
%
xmm13
.
byte
65
15
194
230
1
/
/
cmpltps
%
xmm14
%
xmm4
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
85
193
/
/
andnps
%
xmm9
%
xmm0
.
byte
69
15
92
203
/
/
subps
%
xmm11
%
xmm9
.
byte
69
15
92
235
/
/
subps
%
xmm11
%
xmm13
.
byte
69
15
89
205
/
/
mulps
%
xmm13
%
xmm9
.
byte
69
15
92
243
/
/
subps
%
xmm11
%
xmm14
.
byte
69
15
94
206
/
/
divps
%
xmm14
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
68
15
84
204
/
/
andps
%
xmm4
%
xmm9
.
byte
68
15
86
200
/
/
orps
%
xmm0
%
xmm9
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
65
15
94
194
/
/
divps
%
xmm10
%
xmm0
.
byte
65
15
88
195
/
/
addps
%
xmm11
%
xmm0
.
byte
65
15
40
204
/
/
movaps
%
xmm12
%
xmm1
.
byte
15
85
200
/
/
andnps
%
xmm0
%
xmm1
.
byte
65
15
84
220
/
/
andps
%
xmm12
%
xmm3
.
byte
15
86
217
/
/
orps
%
xmm1
%
xmm3
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
85
195
/
/
andnps
%
xmm3
%
xmm0
.
byte
65
15
92
219
/
/
subps
%
xmm11
%
xmm3
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
65
15
94
222
/
/
divps
%
xmm14
%
xmm3
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
15
84
220
/
/
andps
%
xmm4
%
xmm3
.
byte
15
86
216
/
/
orps
%
xmm0
%
xmm3
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
65
15
94
194
/
/
divps
%
xmm10
%
xmm0
.
byte
65
15
88
195
/
/
addps
%
xmm11
%
xmm0
.
byte
65
15
84
212
/
/
andps
%
xmm12
%
xmm2
.
byte
68
15
85
224
/
/
andnps
%
xmm0
%
xmm12
.
byte
68
15
86
226
/
/
orps
%
xmm2
%
xmm12
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
85
196
/
/
andnps
%
xmm12
%
xmm0
.
byte
69
15
92
227
/
/
subps
%
xmm11
%
xmm12
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
69
15
94
230
/
/
divps
%
xmm14
%
xmm12
.
byte
69
15
88
227
/
/
addps
%
xmm11
%
xmm12
.
byte
68
15
84
228
/
/
andps
%
xmm4
%
xmm12
.
byte
68
15
86
224
/
/
orps
%
xmm0
%
xmm12
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
15
95
216
/
/
maxps
%
xmm0
%
xmm3
.
byte
68
15
95
224
/
/
maxps
%
xmm0
%
xmm12
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
40
100
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
65
15
88
249
/
/
addps
%
xmm9
%
xmm7
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
68
15
40
84
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm10
.
byte
65
15
89
194
/
/
mulps
%
xmm10
%
xmm0
.
byte
15
40
76
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
68
15
89
198
/
/
mulps
%
xmm6
%
xmm8
.
byte
69
15
88
199
/
/
addps
%
xmm15
%
xmm8
.
byte
69
15
88
196
/
/
addps
%
xmm12
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
65
15
40
234
/
/
movaps
%
xmm10
%
xmm5
.
byte
15
40
124
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_sse2
.
globl
_sk_srcover_rgba_8888_sse2
FUNCTION
(
_sk_srcover_rgba_8888_sse2
)
_sk_srcover_rgba_8888_sse2
:
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
4
1
0
0
/
/
jne
26cb6
<
_sk_srcover_rgba_8888_sse2
+
0x123
>
.
byte
243
65
15
111
4
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
102
15
111
53
253
99
1
0
/
/
movdqa
0x163fd
(
%
rip
)
%
xmm6
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
114
213
8
/
/
psrld
0x8
%
xmm5
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
114
215
16
/
/
psrld
0x10
%
xmm7
.
byte
102
15
219
254
/
/
pand
%
xmm6
%
xmm7
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
15
91
247
/
/
cvtdq2ps
%
xmm7
%
xmm6
.
byte
102
15
114
208
24
/
/
psrld
0x18
%
xmm0
.
byte
15
91
248
/
/
cvtdq2ps
%
xmm0
%
xmm7
.
byte
68
15
40
5
14
99
1
0
/
/
movaps
0x1630e
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
68
15
40
37
226
99
1
0
/
/
movaps
0x163e2
(
%
rip
)
%
xmm12
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
65
15
89
204
/
/
mulps
%
xmm12
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
65
15
89
220
/
/
mulps
%
xmm12
%
xmm3
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
88
195
/
/
addps
%
xmm3
%
xmm8
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
95
200
/
/
maxps
%
xmm0
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
201
/
/
cvtps2dq
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
65
15
95
209
/
/
maxps
%
xmm9
%
xmm2
.
byte
65
15
93
212
/
/
minps
%
xmm12
%
xmm2
.
byte
102
15
91
210
/
/
cvtps2dq
%
xmm2
%
xmm2
.
byte
102
15
114
242
8
/
/
pslld
0x8
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
217
/
/
cvtps2dq
%
xmm1
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
69
15
95
216
/
/
maxps
%
xmm8
%
xmm11
.
byte
69
15
93
220
/
/
minps
%
xmm12
%
xmm11
.
byte
102
65
15
91
203
/
/
cvtps2dq
%
xmm11
%
xmm1
.
byte
102
15
114
241
24
/
/
pslld
0x18
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
117
69
/
/
jne
26ce5
<
_sk_srcover_rgba_8888_sse2
+
0x152
>
.
byte
243
65
15
127
12
144
/
/
movdqu
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
75
/
/
je
26d09
<
_sk_srcover_rgba_8888_sse2
+
0x176
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
20
/
/
je
26cda
<
_sk_srcover_rgba_8888_sse2
+
0x147
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
234
254
255
255
/
/
jne
26bb8
<
_sk_srcover_rgba_8888_sse2
+
0x25
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
102
65
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
233
211
254
255
255
/
/
jmpq
26bb8
<
_sk_srcover_rgba_8888_sse2
+
0x25
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
39
/
/
je
26d14
<
_sk_srcover_rgba_8888_sse2
+
0x181
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
26d01
<
_sk_srcover_rgba_8888_sse2
+
0x16e
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
177
/
/
jne
26ca6
<
_sk_srcover_rgba_8888_sse2
+
0x113
>
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
65
15
126
84
144
8
/
/
movd
%
xmm2
0x8
(
%
r8
%
rdx
4
)
.
byte
102
65
15
214
12
144
/
/
movq
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
157
/
/
jmp
26ca6
<
_sk_srcover_rgba_8888_sse2
+
0x113
>
.
byte
102
65
15
110
4
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
233
164
254
255
255
/
/
jmpq
26bb8
<
_sk_srcover_rgba_8888_sse2
+
0x25
>
.
byte
102
65
15
126
12
144
/
/
movd
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
138
/
/
jmp
26ca6
<
_sk_srcover_rgba_8888_sse2
+
0x113
>
HIDDEN
_sk_srcover_bgra_8888_sse2
.
globl
_sk_srcover_bgra_8888_sse2
FUNCTION
(
_sk_srcover_bgra_8888_sse2
)
_sk_srcover_bgra_8888_sse2
:
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
4
1
0
0
/
/
jne
26e3f
<
_sk_srcover_bgra_8888_sse2
+
0x123
>
.
byte
243
65
15
111
4
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
102
15
111
37
116
98
1
0
/
/
movdqa
0x16274
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
245
/
/
cvtdq2ps
%
xmm5
%
xmm6
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
114
213
8
/
/
psrld
0x8
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
114
215
16
/
/
psrld
0x10
%
xmm7
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
15
91
231
/
/
cvtdq2ps
%
xmm7
%
xmm4
.
byte
102
15
114
208
24
/
/
psrld
0x18
%
xmm0
.
byte
15
91
248
/
/
cvtdq2ps
%
xmm0
%
xmm7
.
byte
68
15
40
5
133
97
1
0
/
/
movaps
0x16185
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
68
15
40
37
89
98
1
0
/
/
movaps
0x16259
(
%
rip
)
%
xmm12
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
204
/
/
mulps
%
xmm12
%
xmm9
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
65
15
89
204
/
/
mulps
%
xmm12
%
xmm1
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
89
205
/
/
mulps
%
xmm5
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
65
15
89
212
/
/
mulps
%
xmm12
%
xmm2
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
65
15
89
220
/
/
mulps
%
xmm12
%
xmm3
.
byte
68
15
89
199
/
/
mulps
%
xmm7
%
xmm8
.
byte
68
15
88
195
/
/
addps
%
xmm3
%
xmm8
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
201
/
/
cvtps2dq
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
65
15
95
209
/
/
maxps
%
xmm9
%
xmm2
.
byte
65
15
93
212
/
/
minps
%
xmm12
%
xmm2
.
byte
102
15
91
210
/
/
cvtps2dq
%
xmm2
%
xmm2
.
byte
102
15
114
242
8
/
/
pslld
0x8
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
15
95
200
/
/
maxps
%
xmm0
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
102
15
91
217
/
/
cvtps2dq
%
xmm1
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
69
15
95
216
/
/
maxps
%
xmm8
%
xmm11
.
byte
69
15
93
220
/
/
minps
%
xmm12
%
xmm11
.
byte
102
65
15
91
203
/
/
cvtps2dq
%
xmm11
%
xmm1
.
byte
102
15
114
241
24
/
/
pslld
0x18
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
117
69
/
/
jne
26e6e
<
_sk_srcover_bgra_8888_sse2
+
0x152
>
.
byte
243
65
15
127
12
144
/
/
movdqu
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
75
/
/
je
26e92
<
_sk_srcover_bgra_8888_sse2
+
0x176
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
20
/
/
je
26e63
<
_sk_srcover_bgra_8888_sse2
+
0x147
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
234
254
255
255
/
/
jne
26d41
<
_sk_srcover_bgra_8888_sse2
+
0x25
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
102
65
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
233
211
254
255
255
/
/
jmpq
26d41
<
_sk_srcover_bgra_8888_sse2
+
0x25
>
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
39
/
/
je
26e9d
<
_sk_srcover_bgra_8888_sse2
+
0x181
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
26e8a
<
_sk_srcover_bgra_8888_sse2
+
0x16e
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
177
/
/
jne
26e2f
<
_sk_srcover_bgra_8888_sse2
+
0x113
>
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
65
15
126
84
144
8
/
/
movd
%
xmm2
0x8
(
%
r8
%
rdx
4
)
.
byte
102
65
15
214
12
144
/
/
movq
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
157
/
/
jmp
26e2f
<
_sk_srcover_bgra_8888_sse2
+
0x113
>
.
byte
102
65
15
110
4
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
233
164
254
255
255
/
/
jmpq
26d41
<
_sk_srcover_bgra_8888_sse2
+
0x25
>
.
byte
102
65
15
126
12
144
/
/
movd
%
xmm1
(
%
r8
%
rdx
4
)
.
byte
235
138
/
/
jmp
26e2f
<
_sk_srcover_bgra_8888_sse2
+
0x113
>
HIDDEN
_sk_clamp_0_sse2
.
globl
_sk_clamp_0_sse2
FUNCTION
(
_sk_clamp_0_sse2
)
_sk_clamp_0_sse2
:
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
95
208
/
/
maxps
%
xmm8
%
xmm2
.
byte
65
15
95
216
/
/
maxps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_1_sse2
.
globl
_sk_clamp_1_sse2
FUNCTION
(
_sk_clamp_1_sse2
)
_sk_clamp_1_sse2
:
.
byte
68
15
40
5
75
96
1
0
/
/
movaps
0x1604b
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_sse2
.
globl
_sk_clamp_a_sse2
FUNCTION
(
_sk_clamp_a_sse2
)
_sk_clamp_a_sse2
:
.
byte
15
93
29
48
96
1
0
/
/
minps
0x16030
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
93
195
/
/
minps
%
xmm3
%
xmm0
.
byte
15
93
203
/
/
minps
%
xmm3
%
xmm1
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_sse2
.
globl
_sk_clamp_a_dst_sse2
FUNCTION
(
_sk_clamp_a_dst_sse2
)
_sk_clamp_a_dst_sse2
:
.
byte
15
93
61
28
96
1
0
/
/
minps
0x1601c
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
93
231
/
/
minps
%
xmm7
%
xmm4
.
byte
15
93
239
/
/
minps
%
xmm7
%
xmm5
.
byte
15
93
247
/
/
minps
%
xmm7
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_sse2
.
globl
_sk_set_rgb_sse2
FUNCTION
(
_sk_set_rgb_sse2
)
_sk_set_rgb_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_sse2
.
globl
_sk_swap_rb_sse2
FUNCTION
(
_sk_swap_rb_sse2
)
_sk_swap_rb_sse2
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_sse2
.
globl
_sk_invert_sse2
FUNCTION
(
_sk_invert_sse2
)
_sk_invert_sse2
:
.
byte
68
15
40
5
216
95
1
0
/
/
movaps
0x15fd8
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
92
209
/
/
subps
%
xmm1
%
xmm10
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
92
218
/
/
subps
%
xmm2
%
xmm11
.
byte
68
15
92
195
/
/
subps
%
xmm3
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
40
211
/
/
movaps
%
xmm11
%
xmm2
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_sse2
.
globl
_sk_move_src_dst_sse2
FUNCTION
(
_sk_move_src_dst_sse2
)
_sk_move_src_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_sse2
.
globl
_sk_move_dst_src_sse2
FUNCTION
(
_sk_move_dst_src_sse2
)
_sk_move_dst_src_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_sse2
.
globl
_sk_premul_sse2
FUNCTION
(
_sk_premul_sse2
)
_sk_premul_sse2
:
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_sse2
.
globl
_sk_premul_dst_sse2
FUNCTION
(
_sk_premul_dst_sse2
)
_sk_premul_dst_sse2
:
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_unpremul_sse2
.
globl
_sk_unpremul_sse2
FUNCTION
(
_sk_unpremul_sse2
)
_sk_unpremul_sse2
:
.
byte
68
15
40
5
102
95
1
0
/
/
movaps
0x15f66
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
94
195
/
/
divps
%
xmm3
%
xmm8
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
68
15
194
13
69
96
1
0
1
/
/
cmpltps
0x16045
(
%
rip
)
%
xmm9
#
3d000
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdb4
>
.
byte
69
15
84
200
/
/
andps
%
xmm8
%
xmm9
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_sse2
.
globl
_sk_force_opaque_sse2
FUNCTION
(
_sk_force_opaque_sse2
)
_sk_force_opaque_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
56
95
1
0
/
/
movaps
0x15f38
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_sse2
.
globl
_sk_force_opaque_dst_sse2
FUNCTION
(
_sk_force_opaque_dst_sse2
)
_sk_force_opaque_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
45
95
1
0
/
/
movaps
0x15f2d
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_sse2
.
globl
_sk_from_srgb_sse2
FUNCTION
(
_sk_from_srgb_sse2
)
_sk_from_srgb_sse2
:
.
byte
68
15
40
5
35
96
1
0
/
/
movaps
0x16023
(
%
rip
)
%
xmm8
#
3d010
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdc4
>
.
byte
68
15
40
232
/
/
movaps
%
xmm0
%
xmm13
.
byte
69
15
89
232
/
/
mulps
%
xmm8
%
xmm13
.
byte
68
15
40
216
/
/
movaps
%
xmm0
%
xmm11
.
byte
69
15
89
219
/
/
mulps
%
xmm11
%
xmm11
.
byte
68
15
40
13
139
95
1
0
/
/
movaps
0x15f8b
(
%
rip
)
%
xmm9
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
68
15
40
240
/
/
movaps
%
xmm0
%
xmm14
.
byte
69
15
89
241
/
/
mulps
%
xmm9
%
xmm14
.
byte
68
15
40
21
11
96
1
0
/
/
movaps
0x1600b
(
%
rip
)
%
xmm10
#
3d020
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdd4
>
.
byte
69
15
88
242
/
/
addps
%
xmm10
%
xmm14
.
byte
69
15
89
243
/
/
mulps
%
xmm11
%
xmm14
.
byte
68
15
40
29
11
96
1
0
/
/
movaps
0x1600b
(
%
rip
)
%
xmm11
#
3d030
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xde4
>
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
68
15
40
37
15
96
1
0
/
/
movaps
0x1600f
(
%
rip
)
%
xmm12
#
3d040
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdf4
>
.
byte
65
15
194
196
1
/
/
cmpltps
%
xmm12
%
xmm0
.
byte
68
15
84
232
/
/
andps
%
xmm0
%
xmm13
.
byte
65
15
85
198
/
/
andnps
%
xmm14
%
xmm0
.
byte
65
15
86
197
/
/
orps
%
xmm13
%
xmm0
.
byte
68
15
40
233
/
/
movaps
%
xmm1
%
xmm13
.
byte
69
15
89
232
/
/
mulps
%
xmm8
%
xmm13
.
byte
68
15
40
241
/
/
movaps
%
xmm1
%
xmm14
.
byte
69
15
89
246
/
/
mulps
%
xmm14
%
xmm14
.
byte
68
15
40
249
/
/
movaps
%
xmm1
%
xmm15
.
byte
69
15
89
249
/
/
mulps
%
xmm9
%
xmm15
.
byte
69
15
88
250
/
/
addps
%
xmm10
%
xmm15
.
byte
69
15
89
254
/
/
mulps
%
xmm14
%
xmm15
.
byte
69
15
88
251
/
/
addps
%
xmm11
%
xmm15
.
byte
65
15
194
204
1
/
/
cmpltps
%
xmm12
%
xmm1
.
byte
68
15
84
233
/
/
andps
%
xmm1
%
xmm13
.
byte
65
15
85
207
/
/
andnps
%
xmm15
%
xmm1
.
byte
65
15
86
205
/
/
orps
%
xmm13
%
xmm1
.
byte
68
15
89
194
/
/
mulps
%
xmm2
%
xmm8
.
byte
68
15
40
234
/
/
movaps
%
xmm2
%
xmm13
.
byte
69
15
89
237
/
/
mulps
%
xmm13
%
xmm13
.
byte
68
15
89
202
/
/
mulps
%
xmm2
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
89
205
/
/
mulps
%
xmm13
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
65
15
194
212
1
/
/
cmpltps
%
xmm12
%
xmm2
.
byte
68
15
84
194
/
/
andps
%
xmm2
%
xmm8
.
byte
65
15
85
209
/
/
andnps
%
xmm9
%
xmm2
.
byte
65
15
86
208
/
/
orps
%
xmm8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_from_srgb_dst_sse2
.
globl
_sk_from_srgb_dst_sse2
FUNCTION
(
_sk_from_srgb_dst_sse2
)
_sk_from_srgb_dst_sse2
:
.
byte
68
15
40
5
96
95
1
0
/
/
movaps
0x15f60
(
%
rip
)
%
xmm8
#
3d010
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdc4
>
.
byte
68
15
40
236
/
/
movaps
%
xmm4
%
xmm13
.
byte
69
15
89
232
/
/
mulps
%
xmm8
%
xmm13
.
byte
68
15
40
220
/
/
movaps
%
xmm4
%
xmm11
.
byte
69
15
89
219
/
/
mulps
%
xmm11
%
xmm11
.
byte
68
15
40
13
200
94
1
0
/
/
movaps
0x15ec8
(
%
rip
)
%
xmm9
#
3cf90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd44
>
.
byte
68
15
40
244
/
/
movaps
%
xmm4
%
xmm14
.
byte
69
15
89
241
/
/
mulps
%
xmm9
%
xmm14
.
byte
68
15
40
21
72
95
1
0
/
/
movaps
0x15f48
(
%
rip
)
%
xmm10
#
3d020
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdd4
>
.
byte
69
15
88
242
/
/
addps
%
xmm10
%
xmm14
.
byte
69
15
89
243
/
/
mulps
%
xmm11
%
xmm14
.
byte
68
15
40
29
72
95
1
0
/
/
movaps
0x15f48
(
%
rip
)
%
xmm11
#
3d030
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xde4
>
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
68
15
40
37
76
95
1
0
/
/
movaps
0x15f4c
(
%
rip
)
%
xmm12
#
3d040
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xdf4
>
.
byte
65
15
194
228
1
/
/
cmpltps
%
xmm12
%
xmm4
.
byte
68
15
84
236
/
/
andps
%
xmm4
%
xmm13
.
byte
65
15
85
230
/
/
andnps
%
xmm14
%
xmm4
.
byte
65
15
86
229
/
/
orps
%
xmm13
%
xmm4
.
byte
68
15
40
237
/
/
movaps
%
xmm5
%
xmm13
.
byte
69
15
89
232
/
/
mulps
%
xmm8
%
xmm13
.
byte
68
15
40
245
/
/
movaps
%
xmm5
%
xmm14
.
byte
69
15
89
246
/
/
mulps
%
xmm14
%
xmm14
.
byte
68
15
40
253
/
/
movaps
%
xmm5
%
xmm15
.
byte
69
15
89
249
/
/
mulps
%
xmm9
%
xmm15
.
byte
69
15
88
250
/
/
addps
%
xmm10
%
xmm15
.
byte
69
15
89
254
/
/
mulps
%
xmm14
%
xmm15
.
byte
69
15
88
251
/
/
addps
%
xmm11
%
xmm15
.
byte
65
15
194
236
1
/
/
cmpltps
%
xmm12
%
xmm5
.
byte
68
15
84
237
/
/
andps
%
xmm5
%
xmm13
.
byte
65
15
85
239
/
/
andnps
%
xmm15
%
xmm5
.
byte
65
15
86
237
/
/
orps
%
xmm13
%
xmm5
.
byte
68
15
89
198
/
/
mulps
%
xmm6
%
xmm8
.
byte
68
15
40
238
/
/
movaps
%
xmm6
%
xmm13
.
byte
69
15
89
237
/
/
mulps
%
xmm13
%
xmm13
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
89
205
/
/
mulps
%
xmm13
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
65
15
194
244
1
/
/
cmpltps
%
xmm12
%
xmm6
.
byte
68
15
84
198
/
/
andps
%
xmm6
%
xmm8
.
byte
65
15
85
241
/
/
andnps
%
xmm9
%
xmm6
.
byte
65
15
86
240
/
/
orps
%
xmm8
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_to_srgb_sse2
.
globl
_sk_to_srgb_sse2
FUNCTION
(
_sk_to_srgb_sse2
)
_sk_to_srgb_sse2
:
.
byte
68
15
82
232
/
/
rsqrtps
%
xmm0
%
xmm13
.
byte
68
15
40
5
217
94
1
0
/
/
movaps
0x15ed9
(
%
rip
)
%
xmm8
#
3d050
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe04
>
.
byte
68
15
40
240
/
/
movaps
%
xmm0
%
xmm14
.
byte
69
15
89
240
/
/
mulps
%
xmm8
%
xmm14
.
byte
68
15
40
13
217
94
1
0
/
/
movaps
0x15ed9
(
%
rip
)
%
xmm9
#
3d060
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe14
>
.
byte
69
15
40
253
/
/
movaps
%
xmm13
%
xmm15
.
byte
69
15
89
249
/
/
mulps
%
xmm9
%
xmm15
.
byte
68
15
40
21
217
94
1
0
/
/
movaps
0x15ed9
(
%
rip
)
%
xmm10
#
3d070
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe24
>
.
byte
69
15
88
250
/
/
addps
%
xmm10
%
xmm15
.
byte
69
15
89
253
/
/
mulps
%
xmm13
%
xmm15
.
byte
68
15
40
29
217
94
1
0
/
/
movaps
0x15ed9
(
%
rip
)
%
xmm11
#
3d080
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe34
>
.
byte
69
15
88
251
/
/
addps
%
xmm11
%
xmm15
.
byte
68
15
40
37
221
94
1
0
/
/
movaps
0x15edd
(
%
rip
)
%
xmm12
#
3d090
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe44
>
.
byte
69
15
88
236
/
/
addps
%
xmm12
%
xmm13
.
byte
69
15
83
237
/
/
rcpps
%
xmm13
%
xmm13
.
byte
69
15
89
239
/
/
mulps
%
xmm15
%
xmm13
.
byte
68
15
40
61
217
94
1
0
/
/
movaps
0x15ed9
(
%
rip
)
%
xmm15
#
3d0a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe54
>
.
byte
65
15
194
199
1
/
/
cmpltps
%
xmm15
%
xmm0
.
byte
68
15
84
240
/
/
andps
%
xmm0
%
xmm14
.
byte
65
15
85
197
/
/
andnps
%
xmm13
%
xmm0
.
byte
65
15
86
198
/
/
orps
%
xmm14
%
xmm0
.
byte
68
15
82
233
/
/
rsqrtps
%
xmm1
%
xmm13
.
byte
69
15
40
245
/
/
movaps
%
xmm13
%
xmm14
.
byte
69
15
89
241
/
/
mulps
%
xmm9
%
xmm14
.
byte
69
15
88
242
/
/
addps
%
xmm10
%
xmm14
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
69
15
88
236
/
/
addps
%
xmm12
%
xmm13
.
byte
69
15
83
237
/
/
rcpps
%
xmm13
%
xmm13
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
68
15
40
241
/
/
movaps
%
xmm1
%
xmm14
.
byte
69
15
89
240
/
/
mulps
%
xmm8
%
xmm14
.
byte
65
15
194
207
1
/
/
cmpltps
%
xmm15
%
xmm1
.
byte
68
15
84
241
/
/
andps
%
xmm1
%
xmm14
.
byte
65
15
85
205
/
/
andnps
%
xmm13
%
xmm1
.
byte
65
15
86
206
/
/
orps
%
xmm14
%
xmm1
.
byte
68
15
82
234
/
/
rsqrtps
%
xmm2
%
xmm13
.
byte
69
15
89
205
/
/
mulps
%
xmm13
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
89
205
/
/
mulps
%
xmm13
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
69
15
88
236
/
/
addps
%
xmm12
%
xmm13
.
byte
69
15
83
213
/
/
rcpps
%
xmm13
%
xmm10
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
89
194
/
/
mulps
%
xmm2
%
xmm8
.
byte
65
15
194
215
1
/
/
cmpltps
%
xmm15
%
xmm2
.
byte
68
15
84
194
/
/
andps
%
xmm2
%
xmm8
.
byte
65
15
85
210
/
/
andnps
%
xmm10
%
xmm2
.
byte
65
15
86
208
/
/
orps
%
xmm8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_rgb_to_hsl_sse2
.
globl
_sk_rgb_to_hsl_sse2
FUNCTION
(
_sk_rgb_to_hsl_sse2
)
_sk_rgb_to_hsl_sse2
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
95
194
/
/
maxps
%
xmm2
%
xmm0
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
68
15
95
208
/
/
maxps
%
xmm0
%
xmm10
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
93
194
/
/
minps
%
xmm2
%
xmm0
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
93
216
/
/
minps
%
xmm0
%
xmm11
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
68
15
40
45
140
92
1
0
/
/
movaps
0x15c8c
(
%
rip
)
%
xmm13
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
94
233
/
/
divps
%
xmm1
%
xmm13
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
65
15
194
192
0
/
/
cmpeqps
%
xmm8
%
xmm0
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
68
15
92
226
/
/
subps
%
xmm2
%
xmm12
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
69
15
40
241
/
/
movaps
%
xmm9
%
xmm14
.
byte
68
15
194
242
1
/
/
cmpltps
%
xmm2
%
xmm14
.
byte
68
15
84
53
2
94
1
0
/
/
andps
0x15e02
(
%
rip
)
%
xmm14
#
3d0b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe64
>
.
byte
69
15
88
244
/
/
addps
%
xmm12
%
xmm14
.
byte
69
15
40
250
/
/
movaps
%
xmm10
%
xmm15
.
byte
69
15
194
249
0
/
/
cmpeqps
%
xmm9
%
xmm15
.
byte
65
15
92
208
/
/
subps
%
xmm8
%
xmm2
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
68
15
40
37
245
93
1
0
/
/
movaps
0x15df5
(
%
rip
)
%
xmm12
#
3d0c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe74
>
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
69
15
92
193
/
/
subps
%
xmm9
%
xmm8
.
byte
69
15
89
197
/
/
mulps
%
xmm13
%
xmm8
.
byte
68
15
88
5
241
93
1
0
/
/
addps
0x15df1
(
%
rip
)
%
xmm8
#
3d0d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe84
>
.
byte
65
15
84
215
/
/
andps
%
xmm15
%
xmm2
.
byte
69
15
85
248
/
/
andnps
%
xmm8
%
xmm15
.
byte
68
15
86
250
/
/
orps
%
xmm2
%
xmm15
.
byte
68
15
84
240
/
/
andps
%
xmm0
%
xmm14
.
byte
65
15
85
199
/
/
andnps
%
xmm15
%
xmm0
.
byte
65
15
86
198
/
/
orps
%
xmm14
%
xmm0
.
byte
15
89
5
226
93
1
0
/
/
mulps
0x15de2
(
%
rip
)
%
xmm0
#
3d0e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe94
>
.
byte
69
15
40
194
/
/
movaps
%
xmm10
%
xmm8
.
byte
69
15
194
195
4
/
/
cmpneqps
%
xmm11
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
69
15
92
226
/
/
subps
%
xmm10
%
xmm12
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
68
15
40
13
229
91
1
0
/
/
movaps
0x15be5
(
%
rip
)
%
xmm9
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
68
15
194
202
1
/
/
cmpltps
%
xmm2
%
xmm9
.
byte
69
15
92
227
/
/
subps
%
xmm11
%
xmm12
.
byte
69
15
84
225
/
/
andps
%
xmm9
%
xmm12
.
byte
69
15
85
202
/
/
andnps
%
xmm10
%
xmm9
.
byte
69
15
86
204
/
/
orps
%
xmm12
%
xmm9
.
byte
65
15
94
201
/
/
divps
%
xmm9
%
xmm1
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hsl_to_rgb_sse2
.
globl
_sk_hsl_to_rgb_sse2
FUNCTION
(
_sk_hsl_to_rgb_sse2
)
_sk_hsl_to_rgb_sse2
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
15
41
108
36
200
/
/
movaps
%
xmm5
-
0x38
(
%
rsp
)
.
byte
15
41
100
36
184
/
/
movaps
%
xmm4
-
0x48
(
%
rsp
)
.
byte
15
41
92
36
168
/
/
movaps
%
xmm3
-
0x58
(
%
rsp
)
.
byte
68
15
40
218
/
/
movaps
%
xmm2
%
xmm11
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
68
15
40
13
148
91
1
0
/
/
movaps
0x15b94
(
%
rip
)
%
xmm9
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
69
15
194
211
2
/
/
cmpleps
%
xmm11
%
xmm10
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
194
217
0
/
/
cmpeqps
%
xmm1
%
xmm3
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
15
41
124
36
136
/
/
movaps
%
xmm7
-
0x78
(
%
rsp
)
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
65
15
84
194
/
/
andps
%
xmm10
%
xmm0
.
byte
68
15
85
209
/
/
andnps
%
xmm1
%
xmm10
.
byte
68
15
86
208
/
/
orps
%
xmm0
%
xmm10
.
byte
68
15
41
92
36
152
/
/
movaps
%
xmm11
-
0x68
(
%
rsp
)
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
69
15
88
219
/
/
addps
%
xmm11
%
xmm11
.
byte
69
15
92
218
/
/
subps
%
xmm10
%
xmm11
.
byte
15
40
5
61
93
1
0
/
/
movaps
0x15d3d
(
%
rip
)
%
xmm0
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
243
15
91
200
/
/
cvttps2dq
%
xmm0
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
194
217
1
/
/
cmpltps
%
xmm1
%
xmm3
.
byte
15
84
29
69
91
1
0
/
/
andps
0x15b45
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
68
15
40
45
39
93
1
0
/
/
movaps
0x15d27
(
%
rip
)
%
xmm13
#
3d100
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xeb4
>
.
byte
69
15
40
197
/
/
movaps
%
xmm13
%
xmm8
.
byte
68
15
194
192
2
/
/
cmpleps
%
xmm0
%
xmm8
.
byte
69
15
40
242
/
/
movaps
%
xmm10
%
xmm14
.
byte
69
15
92
243
/
/
subps
%
xmm11
%
xmm14
.
byte
65
15
40
217
/
/
movaps
%
xmm9
%
xmm3
.
byte
15
194
216
2
/
/
cmpleps
%
xmm0
%
xmm3
.
byte
15
40
21
231
92
1
0
/
/
movaps
0x15ce7
(
%
rip
)
%
xmm2
#
3d0e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe94
>
.
byte
68
15
40
250
/
/
movaps
%
xmm2
%
xmm15
.
byte
68
15
194
248
2
/
/
cmpleps
%
xmm0
%
xmm15
.
byte
15
40
13
167
92
1
0
/
/
movaps
0x15ca7
(
%
rip
)
%
xmm1
#
3d0b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe64
>
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
45
189
92
1
0
/
/
movaps
0x15cbd
(
%
rip
)
%
xmm5
#
3d0d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe84
>
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
65
15
89
230
/
/
mulps
%
xmm14
%
xmm4
.
byte
65
15
88
227
/
/
addps
%
xmm11
%
xmm4
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
69
15
84
224
/
/
andps
%
xmm8
%
xmm12
.
byte
68
15
85
196
/
/
andnps
%
xmm4
%
xmm8
.
byte
69
15
86
196
/
/
orps
%
xmm12
%
xmm8
.
byte
68
15
84
195
/
/
andps
%
xmm3
%
xmm8
.
byte
65
15
85
218
/
/
andnps
%
xmm10
%
xmm3
.
byte
65
15
86
216
/
/
orps
%
xmm8
%
xmm3
.
byte
65
15
89
198
/
/
mulps
%
xmm14
%
xmm0
.
byte
65
15
88
195
/
/
addps
%
xmm11
%
xmm0
.
byte
65
15
84
223
/
/
andps
%
xmm15
%
xmm3
.
byte
68
15
85
248
/
/
andnps
%
xmm0
%
xmm15
.
byte
68
15
86
251
/
/
orps
%
xmm3
%
xmm15
.
byte
68
15
40
199
/
/
movaps
%
xmm7
%
xmm8
.
byte
69
15
85
199
/
/
andnps
%
xmm15
%
xmm8
.
byte
243
15
91
198
/
/
cvttps2dq
%
xmm6
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
194
216
1
/
/
cmpltps
%
xmm0
%
xmm3
.
byte
15
84
29
162
90
1
0
/
/
andps
0x15aa2
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
68
15
40
230
/
/
movaps
%
xmm6
%
xmm12
.
byte
68
15
92
224
/
/
subps
%
xmm0
%
xmm12
.
byte
69
15
40
253
/
/
movaps
%
xmm13
%
xmm15
.
byte
69
15
194
252
2
/
/
cmpleps
%
xmm12
%
xmm15
.
byte
65
15
40
225
/
/
movaps
%
xmm9
%
xmm4
.
byte
65
15
194
228
2
/
/
cmpleps
%
xmm12
%
xmm4
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
65
15
194
220
2
/
/
cmpleps
%
xmm12
%
xmm3
.
byte
68
15
89
225
/
/
mulps
%
xmm1
%
xmm12
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
65
15
92
196
/
/
subps
%
xmm12
%
xmm0
.
byte
65
15
89
198
/
/
mulps
%
xmm14
%
xmm0
.
byte
65
15
88
195
/
/
addps
%
xmm11
%
xmm0
.
byte
65
15
40
251
/
/
movaps
%
xmm11
%
xmm7
.
byte
65
15
84
255
/
/
andps
%
xmm15
%
xmm7
.
byte
68
15
85
248
/
/
andnps
%
xmm0
%
xmm15
.
byte
68
15
86
255
/
/
orps
%
xmm7
%
xmm15
.
byte
68
15
84
252
/
/
andps
%
xmm4
%
xmm15
.
byte
65
15
85
226
/
/
andnps
%
xmm10
%
xmm4
.
byte
65
15
86
231
/
/
orps
%
xmm15
%
xmm4
.
byte
69
15
89
230
/
/
mulps
%
xmm14
%
xmm12
.
byte
69
15
88
227
/
/
addps
%
xmm11
%
xmm12
.
byte
15
84
227
/
/
andps
%
xmm3
%
xmm4
.
byte
65
15
85
220
/
/
andnps
%
xmm12
%
xmm3
.
byte
15
86
220
/
/
orps
%
xmm4
%
xmm3
.
byte
15
40
124
36
136
/
/
movaps
-
0x78
(
%
rsp
)
%
xmm7
.
byte
15
40
231
/
/
movaps
%
xmm7
%
xmm4
.
byte
15
85
227
/
/
andnps
%
xmm3
%
xmm4
.
byte
15
88
53
42
92
1
0
/
/
addps
0x15c2a
(
%
rip
)
%
xmm6
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
243
15
91
198
/
/
cvttps2dq
%
xmm6
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
194
216
1
/
/
cmpltps
%
xmm0
%
xmm3
.
byte
15
84
29
21
90
1
0
/
/
andps
0x15a15
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
65
15
89
238
/
/
mulps
%
xmm14
%
xmm5
.
byte
65
15
89
206
/
/
mulps
%
xmm14
%
xmm1
.
byte
65
15
88
235
/
/
addps
%
xmm11
%
xmm5
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
68
15
194
238
2
/
/
cmpleps
%
xmm6
%
xmm13
.
byte
69
15
84
221
/
/
andps
%
xmm13
%
xmm11
.
byte
68
15
85
237
/
/
andnps
%
xmm5
%
xmm13
.
byte
69
15
86
235
/
/
orps
%
xmm11
%
xmm13
.
byte
68
15
194
206
2
/
/
cmpleps
%
xmm6
%
xmm9
.
byte
69
15
84
233
/
/
andps
%
xmm9
%
xmm13
.
byte
69
15
85
202
/
/
andnps
%
xmm10
%
xmm9
.
byte
69
15
86
205
/
/
orps
%
xmm13
%
xmm9
.
byte
15
194
214
2
/
/
cmpleps
%
xmm6
%
xmm2
.
byte
68
15
84
202
/
/
andps
%
xmm2
%
xmm9
.
byte
15
85
209
/
/
andnps
%
xmm1
%
xmm2
.
byte
65
15
86
209
/
/
orps
%
xmm9
%
xmm2
.
byte
15
40
68
36
152
/
/
movaps
-
0x68
(
%
rsp
)
%
xmm0
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
15
85
202
/
/
andnps
%
xmm2
%
xmm1
.
byte
68
15
86
192
/
/
orps
%
xmm0
%
xmm8
.
byte
15
86
224
/
/
orps
%
xmm0
%
xmm4
.
byte
15
86
193
/
/
orps
%
xmm1
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
40
92
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_sse2
.
globl
_sk_scale_1_float_sse2
FUNCTION
(
_sk_scale_1_float_sse2
)
_sk_scale_1_float_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_sse2
.
globl
_sk_scale_u8_sse2
FUNCTION
(
_sk_scale_u8_sse2
)
_sk_scale_u8_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
61
/
/
jne
275f6
<
_sk_scale_u8_sse2
+
0x4f
>
.
byte
102
69
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
68
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm8
.
byte
102
68
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm8
.
byte
102
68
15
219
5
238
89
1
0
/
/
pand
0x159ee
(
%
rip
)
%
xmm8
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
66
91
1
0
/
/
mulps
0x15b42
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
68
15
89
195
/
/
mulps
%
xmm3
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
216
/
/
movaps
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
57
/
/
je
27637
<
_sk_scale_u8_sse2
+
0x90
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
21
/
/
je
2761c
<
_sk_scale_u8_sse2
+
0x75
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
190
/
/
jne
275c9
<
_sk_scale_u8_sse2
+
0x22
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
69
/
/
pshufd
0x45
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
102
68
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm9
.
byte
102
68
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm9
.
byte
242
69
15
16
193
/
/
movsd
%
xmm9
%
xmm8
.
byte
235
146
/
/
jmp
275c9
<
_sk_scale_u8_sse2
+
0x22
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
134
/
/
jmp
275c9
<
_sk_scale_u8_sse2
+
0x22
>
HIDDEN
_sk_scale_565_sse2
.
globl
_sk_scale_565_sse2
FUNCTION
(
_sk_scale_565_sse2
)
_sk_scale_565_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
173
0
0
0
/
/
jne
27709
<
_sk_scale_565_sse2
+
0xc6
>
.
byte
243
69
15
126
20
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm10
.
byte
102
68
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm10
.
byte
102
68
15
111
5
192
90
1
0
/
/
movdqa
0x15ac0
(
%
rip
)
%
xmm8
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
69
15
219
194
/
/
pand
%
xmm10
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
191
90
1
0
/
/
mulps
0x15abf
(
%
rip
)
%
xmm8
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
68
15
111
13
198
90
1
0
/
/
movdqa
0x15ac6
(
%
rip
)
%
xmm9
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
69
15
219
202
/
/
pand
%
xmm10
%
xmm9
.
byte
69
15
91
201
/
/
cvtdq2ps
%
xmm9
%
xmm9
.
byte
68
15
89
13
197
90
1
0
/
/
mulps
0x15ac5
(
%
rip
)
%
xmm9
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
68
15
219
21
204
90
1
0
/
/
pand
0x15acc
(
%
rip
)
%
xmm10
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
69
15
91
210
/
/
cvtdq2ps
%
xmm10
%
xmm10
.
byte
68
15
89
21
208
90
1
0
/
/
mulps
0x15ad0
(
%
rip
)
%
xmm10
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
68
15
194
223
1
/
/
cmpltps
%
xmm7
%
xmm11
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
40
232
/
/
movaps
%
xmm8
%
xmm13
.
byte
69
15
93
236
/
/
minps
%
xmm12
%
xmm13
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
69
15
95
226
/
/
maxps
%
xmm10
%
xmm12
.
byte
69
15
40
240
/
/
movaps
%
xmm8
%
xmm14
.
byte
69
15
95
244
/
/
maxps
%
xmm12
%
xmm14
.
byte
69
15
84
235
/
/
andps
%
xmm11
%
xmm13
.
byte
69
15
85
222
/
/
andnps
%
xmm14
%
xmm11
.
byte
69
15
86
221
/
/
orps
%
xmm13
%
xmm11
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
68
15
89
219
/
/
mulps
%
xmm3
%
xmm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
219
/
/
movaps
%
xmm11
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
56
/
/
je
27749
<
_sk_scale_565_sse2
+
0x106
>
.
byte
102
69
15
239
210
/
/
pxor
%
xmm10
%
xmm10
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
25
/
/
je
27733
<
_sk_scale_565_sse2
+
0xf0
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
69
255
255
255
/
/
jne
27667
<
_sk_scale_565_sse2
+
0x24
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
208
69
/
/
pshufd
0x45
%
xmm8
%
xmm10
.
byte
102
69
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
242
69
15
112
192
212
/
/
pshuflw
0xd4
%
xmm8
%
xmm8
.
byte
242
69
15
16
208
/
/
movsd
%
xmm8
%
xmm10
.
byte
233
30
255
255
255
/
/
jmpq
27667
<
_sk_scale_565_sse2
+
0x24
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
208
/
/
movd
%
eax
%
xmm10
.
byte
233
15
255
255
255
/
/
jmpq
27667
<
_sk_scale_565_sse2
+
0x24
>
HIDDEN
_sk_lerp_1_float_sse2
.
globl
_sk_lerp_1_float_sse2
FUNCTION
(
_sk_lerp_1_float_sse2
)
_sk_lerp_1_float_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_u8_sse2
.
globl
_sk_lerp_u8_sse2
FUNCTION
(
_sk_lerp_u8_sse2
)
_sk_lerp_u8_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
81
/
/
jne
277f3
<
_sk_lerp_u8_sse2
+
0x63
>
.
byte
102
69
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
68
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm8
.
byte
102
68
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm8
.
byte
102
68
15
219
5
5
88
1
0
/
/
pand
0x15805
(
%
rip
)
%
xmm8
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
89
89
1
0
/
/
mulps
0x15959
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
60
/
/
je
27837
<
_sk_lerp_u8_sse2
+
0xa7
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
21
/
/
je
27819
<
_sk_lerp_u8_sse2
+
0x89
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
170
/
/
jne
277b2
<
_sk_lerp_u8_sse2
+
0x22
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
69
/
/
pshufd
0x45
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
102
68
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm9
.
byte
102
68
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm9
.
byte
242
69
15
16
193
/
/
movsd
%
xmm9
%
xmm8
.
byte
233
123
255
255
255
/
/
jmpq
277b2
<
_sk_lerp_u8_sse2
+
0x22
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
233
108
255
255
255
/
/
jmpq
277b2
<
_sk_lerp_u8_sse2
+
0x22
>
HIDDEN
_sk_lerp_565_sse2
.
globl
_sk_lerp_565_sse2
FUNCTION
(
_sk_lerp_565_sse2
)
_sk_lerp_565_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
173
0
0
0
/
/
jne
2790c
<
_sk_lerp_565_sse2
+
0xc6
>
.
byte
243
69
15
126
4
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
68
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm8
.
byte
102
68
15
111
13
189
88
1
0
/
/
movdqa
0x158bd
(
%
rip
)
%
xmm9
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
69
15
219
200
/
/
pand
%
xmm8
%
xmm9
.
byte
69
15
91
201
/
/
cvtdq2ps
%
xmm9
%
xmm9
.
byte
68
15
89
13
188
88
1
0
/
/
mulps
0x158bc
(
%
rip
)
%
xmm9
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
68
15
111
21
195
88
1
0
/
/
movdqa
0x158c3
(
%
rip
)
%
xmm10
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
69
15
219
208
/
/
pand
%
xmm8
%
xmm10
.
byte
69
15
91
218
/
/
cvtdq2ps
%
xmm10
%
xmm11
.
byte
68
15
89
29
194
88
1
0
/
/
mulps
0x158c2
(
%
rip
)
%
xmm11
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
68
15
219
5
201
88
1
0
/
/
pand
0x158c9
(
%
rip
)
%
xmm8
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
89
5
205
88
1
0
/
/
mulps
0x158cd
(
%
rip
)
%
xmm8
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
68
15
194
215
1
/
/
cmpltps
%
xmm7
%
xmm10
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
69
15
93
216
/
/
minps
%
xmm8
%
xmm11
.
byte
69
15
40
233
/
/
movaps
%
xmm9
%
xmm13
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
69
15
93
203
/
/
minps
%
xmm11
%
xmm9
.
byte
69
15
95
224
/
/
maxps
%
xmm8
%
xmm12
.
byte
69
15
95
236
/
/
maxps
%
xmm12
%
xmm13
.
byte
69
15
84
202
/
/
andps
%
xmm10
%
xmm9
.
byte
69
15
85
213
/
/
andnps
%
xmm13
%
xmm10
.
byte
69
15
86
209
/
/
orps
%
xmm9
%
xmm10
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
56
/
/
je
2794c
<
_sk_lerp_565_sse2
+
0x106
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
25
/
/
je
27936
<
_sk_lerp_565_sse2
+
0xf0
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
69
255
255
255
/
/
jne
2786a
<
_sk_lerp_565_sse2
+
0x24
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
69
/
/
pshufd
0x45
%
xmm8
%
xmm8
.
byte
102
69
15
110
12
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
242
69
15
112
201
212
/
/
pshuflw
0xd4
%
xmm9
%
xmm9
.
byte
242
69
15
16
193
/
/
movsd
%
xmm9
%
xmm8
.
byte
233
30
255
255
255
/
/
jmpq
2786a
<
_sk_lerp_565_sse2
+
0x24
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
233
15
255
255
255
/
/
jmpq
2786a
<
_sk_lerp_565_sse2
+
0x24
>
HIDDEN
_sk_load_tables_sse2
.
globl
_sk_load_tables_sse2
FUNCTION
(
_sk_load_tables_sse2
)
_sk_load_tables_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
34
1
0
0
/
/
jne
27a8b
<
_sk_load_tables_sse2
+
0x130
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
68
15
111
5
69
86
1
0
/
/
movdqa
0x15645
(
%
rip
)
%
xmm8
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
219
192
/
/
pand
%
xmm8
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
73
15
126
200
/
/
movq
%
xmm1
%
r8
.
byte
102
73
15
126
193
/
/
movq
%
xmm0
%
r9
.
byte
69
15
182
209
/
/
movzbl
%
r9b
%
r10d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
69
15
182
216
/
/
movzbl
%
r8b
%
r11d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
66
15
16
12
11
/
/
movss
(
%
rbx
%
r9
1
)
%
xmm1
.
byte
243
66
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm0
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
243
66
15
16
12
3
/
/
movss
(
%
rbx
%
r8
1
)
%
xmm1
.
byte
243
66
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
15
20
194
/
/
unpcklpd
%
xmm2
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
65
15
219
200
/
/
pand
%
xmm8
%
xmm1
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
68
15
182
203
/
/
movzbl
%
bl
%
r9d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
69
15
182
208
/
/
movzbl
%
r8b
%
r10d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
243
65
15
16
20
30
/
/
movss
(
%
r14
%
rbx
1
)
%
xmm2
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
243
67
15
16
20
6
/
/
movss
(
%
r14
%
r8
1
)
%
xmm2
.
byte
243
67
15
16
28
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
65
15
219
208
/
/
pand
%
xmm8
%
xmm2
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
68
15
182
200
/
/
movzbl
%
al
%
r9d
.
byte
72
193
232
30
/
/
shr
0x1e
%
rax
.
byte
68
15
182
211
/
/
movzbl
%
bl
%
r10d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
243
65
15
16
28
0
/
/
movss
(
%
r8
%
rax
1
)
%
xmm3
.
byte
243
67
15
16
20
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
243
69
15
16
4
24
/
/
movss
(
%
r8
%
rbx
1
)
%
xmm8
.
byte
243
67
15
16
28
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm3
.
byte
65
15
20
216
/
/
unpcklps
%
xmm8
%
xmm3
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
102
65
15
114
209
24
/
/
psrld
0x18
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
15
89
29
156
86
1
0
/
/
mulps
0x1569c
(
%
rip
)
%
xmm3
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
137
249
/
/
mov
%
edi
%
r9d
.
byte
65
128
225
3
/
/
and
0x3
%
r9b
.
byte
65
128
249
1
/
/
cmp
0x1
%
r9b
.
byte
116
45
/
/
je
27ac5
<
_sk_load_tables_sse2
+
0x16a
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
65
128
249
2
/
/
cmp
0x2
%
r9b
.
byte
116
23
/
/
je
27aba
<
_sk_load_tables_sse2
+
0x15f
>
.
byte
65
128
249
3
/
/
cmp
0x3
%
r9b
.
byte
15
133
194
254
255
255
/
/
jne
2796f
<
_sk_load_tables_sse2
+
0x14
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
200
69
/
/
pshufd
0x45
%
xmm0
%
xmm9
.
byte
102
69
15
18
12
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
170
254
255
255
/
/
jmpq
2796f
<
_sk_load_tables_sse2
+
0x14
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
159
254
255
255
/
/
jmpq
2796f
<
_sk_load_tables_sse2
+
0x14
>
HIDDEN
_sk_load_tables_u16_be_sse2
.
globl
_sk_load_tables_u16_be_sse2
FUNCTION
(
_sk_load_tables_u16_be_sse2
)
_sk_load_tables_u16_be_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
93
1
0
0
/
/
jne
27c43
<
_sk_load_tables_u16_be_sse2
+
0x173
>
.
byte
102
67
15
16
4
72
/
/
movupd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
243
67
15
111
76
72
16
/
/
movdqu
0x10
(
%
r8
%
r9
2
)
%
xmm1
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
68
15
40
200
/
/
movapd
%
xmm0
%
xmm9
.
byte
102
68
15
97
201
/
/
punpcklwd
%
xmm1
%
xmm9
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
68
15
105
200
/
/
punpckhwd
%
xmm0
%
xmm9
.
byte
102
15
111
21
118
86
1
0
/
/
movdqa
0x15676
(
%
rip
)
%
xmm2
#
3d190
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf44
>
.
byte
102
15
112
217
238
/
/
pshufd
0xee
%
xmm1
%
xmm3
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
65
15
97
200
/
/
punpcklwd
%
xmm8
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
102
73
15
126
201
/
/
movq
%
xmm1
%
r9
.
byte
69
15
182
209
/
/
movzbl
%
r9b
%
r10d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
69
15
182
216
/
/
movzbl
%
r8b
%
r11d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
66
15
16
12
11
/
/
movss
(
%
rbx
%
r9
1
)
%
xmm1
.
byte
243
66
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm0
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
243
70
15
16
20
3
/
/
movss
(
%
rbx
%
r8
1
)
%
xmm10
.
byte
243
66
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm1
.
byte
65
15
20
202
/
/
unpcklps
%
xmm10
%
xmm1
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
102
65
15
97
216
/
/
punpcklwd
%
xmm8
%
xmm3
.
byte
102
15
112
203
78
/
/
pshufd
0x4e
%
xmm3
%
xmm1
.
byte
102
73
15
126
200
/
/
movq
%
xmm1
%
r8
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
68
15
182
203
/
/
movzbl
%
bl
%
r9d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
69
15
182
208
/
/
movzbl
%
r8b
%
r10d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
243
65
15
16
28
30
/
/
movss
(
%
r14
%
rbx
1
)
%
xmm3
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
15
20
203
/
/
unpcklps
%
xmm3
%
xmm1
.
byte
243
71
15
16
20
6
/
/
movss
(
%
r14
%
r8
1
)
%
xmm10
.
byte
243
67
15
16
28
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm3
.
byte
65
15
20
218
/
/
unpcklps
%
xmm10
%
xmm3
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
102
65
15
97
208
/
/
punpcklwd
%
xmm8
%
xmm2
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
68
15
182
200
/
/
movzbl
%
al
%
r9d
.
byte
72
193
232
30
/
/
shr
0x1e
%
rax
.
byte
68
15
182
211
/
/
movzbl
%
bl
%
r10d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
243
65
15
16
28
0
/
/
movss
(
%
r8
%
rax
1
)
%
xmm3
.
byte
243
67
15
16
20
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
243
69
15
16
20
24
/
/
movss
(
%
r8
%
rbx
1
)
%
xmm10
.
byte
243
67
15
16
28
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm3
.
byte
65
15
20
218
/
/
unpcklps
%
xmm10
%
xmm3
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
102
65
15
112
217
78
/
/
pshufd
0x4e
%
xmm9
%
xmm3
.
byte
102
68
15
111
203
/
/
movdqa
%
xmm3
%
xmm9
.
byte
102
65
15
113
241
8
/
/
psllw
0x8
%
xmm9
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
65
15
235
217
/
/
por
%
xmm9
%
xmm3
.
byte
102
65
15
97
216
/
/
punpcklwd
%
xmm8
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
29
116
85
1
0
/
/
mulps
0x15574
(
%
rip
)
%
xmm3
#
3d1b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
67
15
16
4
72
/
/
movsd
(
%
r8
%
r9
2
)
%
xmm0
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
27c5c
<
_sk_load_tables_u16_be_sse2
+
0x18c
>
.
byte
243
15
126
192
/
/
movq
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
151
254
255
255
/
/
jmpq
27af3
<
_sk_load_tables_u16_be_sse2
+
0x23
>
.
byte
102
67
15
22
68
72
8
/
/
movhpd
0x8
(
%
r8
%
r9
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
130
254
255
255
/
/
jb
27af3
<
_sk_load_tables_u16_be_sse2
+
0x23
>
.
byte
243
67
15
126
76
72
16
/
/
movq
0x10
(
%
r8
%
r9
2
)
%
xmm1
.
byte
233
118
254
255
255
/
/
jmpq
27af3
<
_sk_load_tables_u16_be_sse2
+
0x23
>
HIDDEN
_sk_load_tables_rgb_u16_be_sse2
.
globl
_sk_load_tables_rgb_u16_be_sse2
FUNCTION
(
_sk_load_tables_rgb_u16_be_sse2
)
_sk_load_tables_rgb_u16_be_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
76
141
12
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
81
1
0
0
/
/
jne
27de0
<
_sk_load_tables_rgb_u16_be_sse2
+
0x163
>
.
byte
243
67
15
111
20
72
/
/
movdqu
(
%
r8
%
r9
2
)
%
xmm2
.
byte
243
67
15
111
76
72
8
/
/
movdqu
0x8
(
%
r8
%
r9
2
)
%
xmm1
.
byte
102
15
115
217
4
/
/
psrldq
0x4
%
xmm1
.
byte
102
68
15
111
210
/
/
movdqa
%
xmm2
%
xmm10
.
byte
102
65
15
115
218
6
/
/
psrldq
0x6
%
xmm10
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
115
216
6
/
/
psrldq
0x6
%
xmm0
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
68
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm10
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
65
15
97
194
/
/
punpcklwd
%
xmm10
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
68
15
111
5
184
84
1
0
/
/
movdqa
0x154b8
(
%
rip
)
%
xmm8
#
3d190
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf44
>
.
byte
102
65
15
219
192
/
/
pand
%
xmm8
%
xmm0
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
65
15
97
193
/
/
punpcklwd
%
xmm9
%
xmm0
.
byte
102
15
112
216
78
/
/
pshufd
0x4e
%
xmm0
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
73
15
126
193
/
/
movq
%
xmm0
%
r9
.
byte
69
15
182
209
/
/
movzbl
%
r9b
%
r10d
.
byte
73
193
233
30
/
/
shr
0x1e
%
r9
.
byte
69
15
182
216
/
/
movzbl
%
r8b
%
r11d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
66
15
16
28
11
/
/
movss
(
%
rbx
%
r9
1
)
%
xmm3
.
byte
243
66
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm0
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
243
70
15
16
28
3
/
/
movss
(
%
rbx
%
r8
1
)
%
xmm11
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
65
15
20
219
/
/
unpcklps
%
xmm11
%
xmm3
.
byte
102
15
20
195
/
/
unpcklpd
%
xmm3
%
xmm0
.
byte
102
65
15
219
200
/
/
pand
%
xmm8
%
xmm1
.
byte
102
65
15
97
201
/
/
punpcklwd
%
xmm9
%
xmm1
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
68
15
182
203
/
/
movzbl
%
bl
%
r9d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
69
15
182
208
/
/
movzbl
%
r8b
%
r10d
.
byte
73
193
232
30
/
/
shr
0x1e
%
r8
.
byte
243
65
15
16
28
30
/
/
movss
(
%
r14
%
rbx
1
)
%
xmm3
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
15
20
203
/
/
unpcklps
%
xmm3
%
xmm1
.
byte
243
71
15
16
28
6
/
/
movss
(
%
r14
%
r8
1
)
%
xmm11
.
byte
243
67
15
16
28
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm3
.
byte
65
15
20
219
/
/
unpcklps
%
xmm11
%
xmm3
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
102
65
15
105
210
/
/
punpckhwd
%
xmm10
%
xmm2
.
byte
102
65
15
219
208
/
/
pand
%
xmm8
%
xmm2
.
byte
102
65
15
97
209
/
/
punpcklwd
%
xmm9
%
xmm2
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
68
15
182
200
/
/
movzbl
%
al
%
r9d
.
byte
72
193
232
30
/
/
shr
0x1e
%
rax
.
byte
68
15
182
211
/
/
movzbl
%
bl
%
r10d
.
byte
72
193
235
30
/
/
shr
0x1e
%
rbx
.
byte
243
65
15
16
28
0
/
/
movss
(
%
r8
%
rax
1
)
%
xmm3
.
byte
243
67
15
16
20
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm2
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
243
69
15
16
4
24
/
/
movss
(
%
r8
%
rbx
1
)
%
xmm8
.
byte
243
67
15
16
28
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm3
.
byte
65
15
20
216
/
/
unpcklps
%
xmm8
%
xmm3
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
53
81
1
0
/
/
movaps
0x15135
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
67
15
110
20
72
/
/
movd
(
%
r8
%
r9
2
)
%
xmm2
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
67
15
196
84
72
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
r9
2
)
%
xmm2
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
14
/
/
jne
27e06
<
_sk_load_tables_rgb_u16_be_sse2
+
0x189
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
69
15
239
210
/
/
pxor
%
xmm10
%
xmm10
.
byte
233
175
254
255
255
/
/
jmpq
27cb5
<
_sk_load_tables_rgb_u16_be_sse2
+
0x38
>
.
byte
102
71
15
110
84
72
6
/
/
movd
0x6
(
%
r8
%
r9
2
)
%
xmm10
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
71
15
196
84
72
10
2
/
/
pinsrw
0x2
0xa
(
%
r8
%
r9
2
)
%
xmm10
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
24
/
/
jb
27e37
<
_sk_load_tables_rgb_u16_be_sse2
+
0x1ba
>
.
byte
102
67
15
110
76
72
12
/
/
movd
0xc
(
%
r8
%
r9
2
)
%
xmm1
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
67
15
196
76
72
16
2
/
/
pinsrw
0x2
0x10
(
%
r8
%
r9
2
)
%
xmm1
.
byte
233
126
254
255
255
/
/
jmpq
27cb5
<
_sk_load_tables_rgb_u16_be_sse2
+
0x38
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
117
254
255
255
/
/
jmpq
27cb5
<
_sk_load_tables_rgb_u16_be_sse2
+
0x38
>
HIDDEN
_sk_byte_tables_sse2
.
globl
_sk_byte_tables_sse2
FUNCTION
(
_sk_byte_tables_sse2
)
_sk_byte_tables_sse2
:
.
byte
85
/
/
push
%
rbp
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
68
15
95
216
/
/
maxps
%
xmm0
%
xmm11
.
byte
68
15
40
13
181
80
1
0
/
/
movaps
0x150b5
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
217
/
/
minps
%
xmm9
%
xmm11
.
byte
68
15
40
21
137
81
1
0
/
/
movaps
0x15189
(
%
rip
)
%
xmm10
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
102
65
15
91
195
/
/
cvtps2dq
%
xmm11
%
xmm0
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
77
137
194
/
/
mov
%
r8
%
r10
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
15
112
192
78
/
/
pshufd
0x4e
%
xmm0
%
xmm0
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
69
137
195
/
/
mov
%
r8d
%
r11d
.
byte
77
137
198
/
/
mov
%
r8
%
r14
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
70
15
182
28
27
/
/
movzbl
(
%
rbx
%
r11
1
)
%
r11d
.
byte
66
15
182
44
51
/
/
movzbl
(
%
rbx
%
r14
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
68
9
221
/
/
or
%
r11d
%
ebp
.
byte
70
15
182
12
11
/
/
movzbl
(
%
rbx
%
r9
1
)
%
r9d
.
byte
66
15
182
28
19
/
/
movzbl
(
%
rbx
%
r10
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
203
/
/
or
%
r9d
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
102
15
196
197
1
/
/
pinsrw
0x1
%
ebp
%
xmm0
.
byte
102
65
15
96
192
/
/
punpcklbw
%
xmm8
%
xmm0
.
byte
102
65
15
97
192
/
/
punpcklwd
%
xmm8
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
29
72
82
1
0
/
/
movaps
0x15248
(
%
rip
)
%
xmm11
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
225
/
/
minps
%
xmm9
%
xmm12
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
102
65
15
91
204
/
/
cvtps2dq
%
xmm12
%
xmm1
.
byte
102
72
15
126
205
/
/
movq
%
xmm1
%
rbp
.
byte
65
137
233
/
/
mov
%
ebp
%
r9d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
15
112
201
78
/
/
pshufd
0x4e
%
xmm1
%
xmm1
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
218
/
/
mov
%
ebx
%
r10d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
65
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
211
/
/
or
%
r10d
%
ebx
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
65
15
182
44
40
/
/
movzbl
(
%
r8
%
rbp
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
68
9
205
/
/
or
%
r9d
%
ebp
.
byte
102
15
110
205
/
/
movd
%
ebp
%
xmm1
.
byte
102
15
196
203
1
/
/
pinsrw
0x1
%
ebx
%
xmm1
.
byte
102
65
15
96
200
/
/
punpcklbw
%
xmm8
%
xmm1
.
byte
102
65
15
97
200
/
/
punpcklwd
%
xmm8
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
69
15
93
225
/
/
minps
%
xmm9
%
xmm12
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
102
65
15
91
212
/
/
cvtps2dq
%
xmm12
%
xmm2
.
byte
102
72
15
126
211
/
/
movq
%
xmm2
%
rbx
.
byte
65
137
216
/
/
mov
%
ebx
%
r8d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
15
112
210
78
/
/
pshufd
0x4e
%
xmm2
%
xmm2
.
byte
102
72
15
126
213
/
/
movq
%
xmm2
%
rbp
.
byte
65
137
234
/
/
mov
%
ebp
%
r10d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
71
15
182
20
17
/
/
movzbl
(
%
r9
%
r10
1
)
%
r10d
.
byte
65
15
182
44
41
/
/
movzbl
(
%
r9
%
rbp
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
68
9
213
/
/
or
%
r10d
%
ebp
.
byte
71
15
182
4
1
/
/
movzbl
(
%
r9
%
r8
1
)
%
r8d
.
byte
65
15
182
28
25
/
/
movzbl
(
%
r9
%
rbx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
195
/
/
or
%
r8d
%
ebx
.
byte
102
15
110
211
/
/
movd
%
ebx
%
xmm2
.
byte
102
15
196
213
1
/
/
pinsrw
0x1
%
ebp
%
xmm2
.
byte
102
65
15
96
208
/
/
punpcklbw
%
xmm8
%
xmm2
.
byte
102
65
15
97
208
/
/
punpcklwd
%
xmm8
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
72
139
64
24
/
/
mov
0x18
(
%
rax
)
%
rax
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
227
/
/
maxps
%
xmm3
%
xmm12
.
byte
69
15
93
225
/
/
minps
%
xmm9
%
xmm12
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
102
65
15
91
220
/
/
cvtps2dq
%
xmm12
%
xmm3
.
byte
102
72
15
126
221
/
/
movq
%
xmm3
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
15
112
219
78
/
/
pshufd
0x4e
%
xmm3
%
xmm3
.
byte
102
72
15
126
219
/
/
movq
%
xmm3
%
rbx
.
byte
65
137
217
/
/
mov
%
ebx
%
r9d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
70
15
182
12
8
/
/
movzbl
(
%
rax
%
r9
1
)
%
r9d
.
byte
15
182
28
24
/
/
movzbl
(
%
rax
%
rbx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
203
/
/
or
%
r9d
%
ebx
.
byte
70
15
182
4
0
/
/
movzbl
(
%
rax
%
r8
1
)
%
r8d
.
byte
15
182
4
40
/
/
movzbl
(
%
rax
%
rbp
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
68
9
192
/
/
or
%
r8d
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
102
15
196
219
1
/
/
pinsrw
0x1
%
ebx
%
xmm3
.
byte
102
65
15
96
216
/
/
punpcklbw
%
xmm8
%
xmm3
.
byte
102
65
15
97
216
/
/
punpcklwd
%
xmm8
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
219
/
/
mulps
%
xmm11
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_byte_tables_rgb_sse2
.
globl
_sk_byte_tables_rgb_sse2
FUNCTION
(
_sk_byte_tables_rgb_sse2
)
_sk_byte_tables_rgb_sse2
:
.
byte
85
/
/
push
%
rbp
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
102
69
15
110
192
/
/
movd
%
r8d
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
200
/
/
cvtdq2ps
%
xmm8
%
xmm9
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
68
15
95
216
/
/
maxps
%
xmm0
%
xmm11
.
byte
68
15
40
21
177
78
1
0
/
/
movaps
0x14eb1
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
218
/
/
minps
%
xmm10
%
xmm11
.
byte
69
15
89
217
/
/
mulps
%
xmm9
%
xmm11
.
byte
102
65
15
91
195
/
/
cvtps2dq
%
xmm11
%
xmm0
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
77
137
194
/
/
mov
%
r8
%
r10
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
15
112
192
78
/
/
pshufd
0x4e
%
xmm0
%
xmm0
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
69
137
195
/
/
mov
%
r8d
%
r11d
.
byte
77
137
198
/
/
mov
%
r8
%
r14
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
70
15
182
28
27
/
/
movzbl
(
%
rbx
%
r11
1
)
%
r11d
.
byte
66
15
182
44
51
/
/
movzbl
(
%
rbx
%
r14
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
68
9
221
/
/
or
%
r11d
%
ebp
.
byte
70
15
182
12
11
/
/
movzbl
(
%
rbx
%
r9
1
)
%
r9d
.
byte
66
15
182
28
19
/
/
movzbl
(
%
rbx
%
r10
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
203
/
/
or
%
r9d
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
102
15
196
197
1
/
/
pinsrw
0x1
%
ebp
%
xmm0
.
byte
102
65
15
96
192
/
/
punpcklbw
%
xmm8
%
xmm0
.
byte
102
65
15
97
192
/
/
punpcklwd
%
xmm8
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
29
76
80
1
0
/
/
movaps
0x1504c
(
%
rip
)
%
xmm11
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
225
/
/
mulps
%
xmm9
%
xmm12
.
byte
102
65
15
91
204
/
/
cvtps2dq
%
xmm12
%
xmm1
.
byte
102
72
15
126
205
/
/
movq
%
xmm1
%
rbp
.
byte
65
137
233
/
/
mov
%
ebp
%
r9d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
15
112
201
78
/
/
pshufd
0x4e
%
xmm1
%
xmm1
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
218
/
/
mov
%
ebx
%
r10d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
65
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
211
/
/
or
%
r10d
%
ebx
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
65
15
182
44
40
/
/
movzbl
(
%
r8
%
rbp
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
68
9
205
/
/
or
%
r9d
%
ebp
.
byte
102
15
110
205
/
/
movd
%
ebp
%
xmm1
.
byte
102
15
196
203
1
/
/
pinsrw
0x1
%
ebx
%
xmm1
.
byte
102
65
15
96
200
/
/
punpcklbw
%
xmm8
%
xmm1
.
byte
102
65
15
97
200
/
/
punpcklwd
%
xmm8
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
72
139
64
16
/
/
mov
0x10
(
%
rax
)
%
rax
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
225
/
/
mulps
%
xmm9
%
xmm12
.
byte
102
65
15
91
212
/
/
cvtps2dq
%
xmm12
%
xmm2
.
byte
102
72
15
126
213
/
/
movq
%
xmm2
%
rbp
.
byte
65
137
232
/
/
mov
%
ebp
%
r8d
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
15
112
210
78
/
/
pshufd
0x4e
%
xmm2
%
xmm2
.
byte
102
72
15
126
211
/
/
movq
%
xmm2
%
rbx
.
byte
65
137
217
/
/
mov
%
ebx
%
r9d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
70
15
182
12
8
/
/
movzbl
(
%
rax
%
r9
1
)
%
r9d
.
byte
15
182
28
24
/
/
movzbl
(
%
rax
%
rbx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
203
/
/
or
%
r9d
%
ebx
.
byte
70
15
182
4
0
/
/
movzbl
(
%
rax
%
r8
1
)
%
r8d
.
byte
15
182
4
40
/
/
movzbl
(
%
rax
%
rbp
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
68
9
192
/
/
or
%
r8d
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
102
15
196
211
1
/
/
pinsrw
0x1
%
ebx
%
xmm2
.
byte
102
65
15
96
208
/
/
punpcklbw
%
xmm8
%
xmm2
.
byte
102
65
15
97
208
/
/
punpcklwd
%
xmm8
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_r_sse2
.
globl
_sk_table_r_sse2
FUNCTION
(
_sk_table_r_sse2
)
_sk_table_r_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
13
45
77
1
0
/
/
minps
0x14d2d
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
69
15
91
193
/
/
cvtps2dq
%
xmm9
%
xmm8
.
byte
102
65
15
112
192
78
/
/
pshufd
0x4e
%
xmm8
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
77
15
126
194
/
/
movq
%
xmm8
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
71
15
16
4
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm8
.
byte
243
67
15
16
4
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm0
.
byte
65
15
20
192
/
/
unpcklps
%
xmm8
%
xmm0
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
243
71
15
16
12
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm9
.
byte
69
15
20
200
/
/
unpcklps
%
xmm8
%
xmm9
.
byte
102
65
15
20
193
/
/
unpcklpd
%
xmm9
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_g_sse2
.
globl
_sk_table_g_sse2
FUNCTION
(
_sk_table_g_sse2
)
_sk_table_g_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
201
/
/
maxps
%
xmm1
%
xmm9
.
byte
68
15
93
13
180
76
1
0
/
/
minps
0x14cb4
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
69
15
91
193
/
/
cvtps2dq
%
xmm9
%
xmm8
.
byte
102
65
15
112
200
78
/
/
pshufd
0x4e
%
xmm8
%
xmm1
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
77
15
126
194
/
/
movq
%
xmm8
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
71
15
16
4
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm8
.
byte
243
67
15
16
12
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm1
.
byte
65
15
20
200
/
/
unpcklps
%
xmm8
%
xmm1
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
243
71
15
16
12
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm9
.
byte
69
15
20
200
/
/
unpcklps
%
xmm8
%
xmm9
.
byte
102
65
15
20
201
/
/
unpcklpd
%
xmm9
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_b_sse2
.
globl
_sk_table_b_sse2
FUNCTION
(
_sk_table_b_sse2
)
_sk_table_b_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
68
15
93
13
59
76
1
0
/
/
minps
0x14c3b
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
69
15
91
193
/
/
cvtps2dq
%
xmm9
%
xmm8
.
byte
102
65
15
112
208
78
/
/
pshufd
0x4e
%
xmm8
%
xmm2
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
77
15
126
194
/
/
movq
%
xmm8
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
71
15
16
4
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm8
.
byte
243
67
15
16
20
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm2
.
byte
65
15
20
208
/
/
unpcklps
%
xmm8
%
xmm2
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
243
71
15
16
12
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm9
.
byte
69
15
20
200
/
/
unpcklps
%
xmm8
%
xmm9
.
byte
102
65
15
20
209
/
/
unpcklpd
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_table_a_sse2
.
globl
_sk_table_a_sse2
FUNCTION
(
_sk_table_a_sse2
)
_sk_table_a_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
139
64
8
/
/
mov
0x8
(
%
rax
)
%
eax
.
byte
255
200
/
/
dec
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
112
192
0
/
/
pshufd
0x0
%
xmm8
%
xmm8
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
203
/
/
maxps
%
xmm3
%
xmm9
.
byte
68
15
93
13
194
75
1
0
/
/
minps
0x14bc2
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
102
69
15
91
193
/
/
cvtps2dq
%
xmm9
%
xmm8
.
byte
102
65
15
112
216
78
/
/
pshufd
0x4e
%
xmm8
%
xmm3
.
byte
102
72
15
126
216
/
/
movq
%
xmm3
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
77
15
126
194
/
/
movq
%
xmm8
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
71
15
16
4
144
/
/
movss
(
%
r8
%
r10
4
)
%
xmm8
.
byte
243
67
15
16
28
152
/
/
movss
(
%
r8
%
r11
4
)
%
xmm3
.
byte
65
15
20
216
/
/
unpcklps
%
xmm8
%
xmm3
.
byte
243
69
15
16
4
128
/
/
movss
(
%
r8
%
rax
4
)
%
xmm8
.
byte
243
71
15
16
12
136
/
/
movss
(
%
r8
%
r9
4
)
%
xmm9
.
byte
69
15
20
200
/
/
unpcklps
%
xmm8
%
xmm9
.
byte
102
65
15
20
217
/
/
unpcklpd
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_r_sse2
.
globl
_sk_parametric_r_sse2
FUNCTION
(
_sk_parametric_r_sse2
)
_sk_parametric_r_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
65
15
194
194
2
/
/
cmpleps
%
xmm10
%
xmm0
.
byte
243
68
15
16
80
24
/
/
movss
0x18
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
194
/
/
addps
%
xmm10
%
xmm8
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
69
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm11
.
byte
68
15
89
29
186
77
1
0
/
/
mulps
0x14dba
(
%
rip
)
%
xmm11
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
194
77
1
0
/
/
movaps
0x14dc2
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
233
/
/
andps
%
xmm9
%
xmm13
.
byte
68
15
86
45
230
74
1
0
/
/
orps
0x14ae6
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
29
190
77
1
0
/
/
addps
0x14dbe
(
%
rip
)
%
xmm11
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
53
198
77
1
0
/
/
movaps
0x14dc6
(
%
rip
)
%
xmm14
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
68
15
88
45
198
77
1
0
/
/
addps
0x14dc6
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
68
15
40
53
206
77
1
0
/
/
movaps
0x14dce
(
%
rip
)
%
xmm14
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
69
15
89
220
/
/
mulps
%
xmm12
%
xmm11
.
byte
243
69
15
91
227
/
/
cvttps2dq
%
xmm11
%
xmm12
.
byte
69
15
91
236
/
/
cvtdq2ps
%
xmm12
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
194
245
1
/
/
cmpltps
%
xmm13
%
xmm14
.
byte
68
15
40
37
168
74
1
0
/
/
movaps
0x14aa8
(
%
rip
)
%
xmm12
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
244
/
/
andps
%
xmm12
%
xmm14
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
92
245
/
/
subps
%
xmm13
%
xmm14
.
byte
68
15
88
29
160
77
1
0
/
/
addps
0x14da0
(
%
rip
)
%
xmm11
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
68
15
40
45
168
77
1
0
/
/
movaps
0x14da8
(
%
rip
)
%
xmm13
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
69
15
92
221
/
/
subps
%
xmm13
%
xmm11
.
byte
68
15
40
45
168
77
1
0
/
/
movaps
0x14da8
(
%
rip
)
%
xmm13
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
68
15
40
53
172
77
1
0
/
/
movaps
0x14dac
(
%
rip
)
%
xmm14
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
68
15
89
53
172
77
1
0
/
/
mulps
0x14dac
(
%
rip
)
%
xmm14
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
69
15
91
222
/
/
cvtps2dq
%
xmm14
%
xmm11
.
byte
69
15
194
202
4
/
/
cmpneqps
%
xmm10
%
xmm9
.
byte
69
15
84
203
/
/
andps
%
xmm11
%
xmm9
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
68
15
84
192
/
/
andps
%
xmm0
%
xmm8
.
byte
65
15
85
195
/
/
andnps
%
xmm11
%
xmm0
.
byte
65
15
86
192
/
/
orps
%
xmm8
%
xmm0
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
93
196
/
/
minps
%
xmm12
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_g_sse2
.
globl
_sk_parametric_g_sse2
FUNCTION
(
_sk_parametric_g_sse2
)
_sk_parametric_g_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
193
/
/
mulps
%
xmm1
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
65
15
194
202
2
/
/
cmpleps
%
xmm10
%
xmm1
.
byte
243
68
15
16
80
24
/
/
movss
0x18
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
194
/
/
addps
%
xmm10
%
xmm8
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
69
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm11
.
byte
68
15
89
29
111
76
1
0
/
/
mulps
0x14c6f
(
%
rip
)
%
xmm11
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
119
76
1
0
/
/
movaps
0x14c77
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
233
/
/
andps
%
xmm9
%
xmm13
.
byte
68
15
86
45
155
73
1
0
/
/
orps
0x1499b
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
29
115
76
1
0
/
/
addps
0x14c73
(
%
rip
)
%
xmm11
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
53
123
76
1
0
/
/
movaps
0x14c7b
(
%
rip
)
%
xmm14
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
68
15
88
45
123
76
1
0
/
/
addps
0x14c7b
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
68
15
40
53
131
76
1
0
/
/
movaps
0x14c83
(
%
rip
)
%
xmm14
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
69
15
89
220
/
/
mulps
%
xmm12
%
xmm11
.
byte
243
69
15
91
227
/
/
cvttps2dq
%
xmm11
%
xmm12
.
byte
69
15
91
236
/
/
cvtdq2ps
%
xmm12
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
194
245
1
/
/
cmpltps
%
xmm13
%
xmm14
.
byte
68
15
40
37
93
73
1
0
/
/
movaps
0x1495d
(
%
rip
)
%
xmm12
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
244
/
/
andps
%
xmm12
%
xmm14
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
92
245
/
/
subps
%
xmm13
%
xmm14
.
byte
68
15
88
29
85
76
1
0
/
/
addps
0x14c55
(
%
rip
)
%
xmm11
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
68
15
40
45
93
76
1
0
/
/
movaps
0x14c5d
(
%
rip
)
%
xmm13
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
69
15
92
221
/
/
subps
%
xmm13
%
xmm11
.
byte
68
15
40
45
93
76
1
0
/
/
movaps
0x14c5d
(
%
rip
)
%
xmm13
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
68
15
40
53
97
76
1
0
/
/
movaps
0x14c61
(
%
rip
)
%
xmm14
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
68
15
89
53
97
76
1
0
/
/
mulps
0x14c61
(
%
rip
)
%
xmm14
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
69
15
91
222
/
/
cvtps2dq
%
xmm14
%
xmm11
.
byte
69
15
194
202
4
/
/
cmpneqps
%
xmm10
%
xmm9
.
byte
69
15
84
203
/
/
andps
%
xmm11
%
xmm9
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
68
15
84
193
/
/
andps
%
xmm1
%
xmm8
.
byte
65
15
85
203
/
/
andnps
%
xmm11
%
xmm1
.
byte
65
15
86
200
/
/
orps
%
xmm8
%
xmm1
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_b_sse2
.
globl
_sk_parametric_b_sse2
FUNCTION
(
_sk_parametric_b_sse2
)
_sk_parametric_b_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
194
/
/
mulps
%
xmm2
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
202
/
/
mulps
%
xmm2
%
xmm9
.
byte
65
15
194
210
2
/
/
cmpleps
%
xmm10
%
xmm2
.
byte
243
68
15
16
80
24
/
/
movss
0x18
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
194
/
/
addps
%
xmm10
%
xmm8
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
69
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm11
.
byte
68
15
89
29
36
75
1
0
/
/
mulps
0x14b24
(
%
rip
)
%
xmm11
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
44
75
1
0
/
/
movaps
0x14b2c
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
233
/
/
andps
%
xmm9
%
xmm13
.
byte
68
15
86
45
80
72
1
0
/
/
orps
0x14850
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
29
40
75
1
0
/
/
addps
0x14b28
(
%
rip
)
%
xmm11
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
53
48
75
1
0
/
/
movaps
0x14b30
(
%
rip
)
%
xmm14
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
68
15
88
45
48
75
1
0
/
/
addps
0x14b30
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
68
15
40
53
56
75
1
0
/
/
movaps
0x14b38
(
%
rip
)
%
xmm14
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
69
15
89
220
/
/
mulps
%
xmm12
%
xmm11
.
byte
243
69
15
91
227
/
/
cvttps2dq
%
xmm11
%
xmm12
.
byte
69
15
91
236
/
/
cvtdq2ps
%
xmm12
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
194
245
1
/
/
cmpltps
%
xmm13
%
xmm14
.
byte
68
15
40
37
18
72
1
0
/
/
movaps
0x14812
(
%
rip
)
%
xmm12
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
244
/
/
andps
%
xmm12
%
xmm14
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
92
245
/
/
subps
%
xmm13
%
xmm14
.
byte
68
15
88
29
10
75
1
0
/
/
addps
0x14b0a
(
%
rip
)
%
xmm11
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
68
15
40
45
18
75
1
0
/
/
movaps
0x14b12
(
%
rip
)
%
xmm13
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
69
15
92
221
/
/
subps
%
xmm13
%
xmm11
.
byte
68
15
40
45
18
75
1
0
/
/
movaps
0x14b12
(
%
rip
)
%
xmm13
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
68
15
40
53
22
75
1
0
/
/
movaps
0x14b16
(
%
rip
)
%
xmm14
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
68
15
89
53
22
75
1
0
/
/
mulps
0x14b16
(
%
rip
)
%
xmm14
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
69
15
91
222
/
/
cvtps2dq
%
xmm14
%
xmm11
.
byte
69
15
194
202
4
/
/
cmpneqps
%
xmm10
%
xmm9
.
byte
69
15
84
203
/
/
andps
%
xmm11
%
xmm9
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
68
15
84
194
/
/
andps
%
xmm2
%
xmm8
.
byte
65
15
85
211
/
/
andnps
%
xmm11
%
xmm2
.
byte
65
15
86
208
/
/
orps
%
xmm8
%
xmm2
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
212
/
/
minps
%
xmm12
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_parametric_a_sse2
.
globl
_sk_parametric_a_sse2
FUNCTION
(
_sk_parametric_a_sse2
)
_sk_parametric_a_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
195
/
/
mulps
%
xmm3
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
65
15
194
218
2
/
/
cmpleps
%
xmm10
%
xmm3
.
byte
243
68
15
16
80
24
/
/
movss
0x18
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
194
/
/
addps
%
xmm10
%
xmm8
.
byte
243
68
15
16
32
/
/
movss
(
%
rax
)
%
xmm12
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
69
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm11
.
byte
68
15
89
29
217
73
1
0
/
/
mulps
0x149d9
(
%
rip
)
%
xmm11
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
68
15
40
45
225
73
1
0
/
/
movaps
0x149e1
(
%
rip
)
%
xmm13
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
69
15
84
233
/
/
andps
%
xmm9
%
xmm13
.
byte
68
15
86
45
5
71
1
0
/
/
orps
0x14705
(
%
rip
)
%
xmm13
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
29
221
73
1
0
/
/
addps
0x149dd
(
%
rip
)
%
xmm11
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
53
229
73
1
0
/
/
movaps
0x149e5
(
%
rip
)
%
xmm14
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
69
15
89
245
/
/
mulps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
68
15
88
45
229
73
1
0
/
/
addps
0x149e5
(
%
rip
)
%
xmm13
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
68
15
40
53
237
73
1
0
/
/
movaps
0x149ed
(
%
rip
)
%
xmm14
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
92
222
/
/
subps
%
xmm14
%
xmm11
.
byte
69
15
89
220
/
/
mulps
%
xmm12
%
xmm11
.
byte
243
69
15
91
227
/
/
cvttps2dq
%
xmm11
%
xmm12
.
byte
69
15
91
236
/
/
cvtdq2ps
%
xmm12
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
194
245
1
/
/
cmpltps
%
xmm13
%
xmm14
.
byte
68
15
40
37
199
70
1
0
/
/
movaps
0x146c7
(
%
rip
)
%
xmm12
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
244
/
/
andps
%
xmm12
%
xmm14
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
69
15
40
243
/
/
movaps
%
xmm11
%
xmm14
.
byte
69
15
92
245
/
/
subps
%
xmm13
%
xmm14
.
byte
68
15
88
29
191
73
1
0
/
/
addps
0x149bf
(
%
rip
)
%
xmm11
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
68
15
40
45
199
73
1
0
/
/
movaps
0x149c7
(
%
rip
)
%
xmm13
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
69
15
92
221
/
/
subps
%
xmm13
%
xmm11
.
byte
68
15
40
45
199
73
1
0
/
/
movaps
0x149c7
(
%
rip
)
%
xmm13
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
69
15
92
238
/
/
subps
%
xmm14
%
xmm13
.
byte
68
15
40
53
203
73
1
0
/
/
movaps
0x149cb
(
%
rip
)
%
xmm14
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
69
15
88
243
/
/
addps
%
xmm11
%
xmm14
.
byte
68
15
89
53
203
73
1
0
/
/
mulps
0x149cb
(
%
rip
)
%
xmm14
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
102
69
15
91
222
/
/
cvtps2dq
%
xmm14
%
xmm11
.
byte
69
15
194
202
4
/
/
cmpneqps
%
xmm10
%
xmm9
.
byte
69
15
84
203
/
/
andps
%
xmm11
%
xmm9
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
68
15
84
195
/
/
andps
%
xmm3
%
xmm8
.
byte
65
15
85
219
/
/
andnps
%
xmm11
%
xmm3
.
byte
65
15
86
216
/
/
orps
%
xmm8
%
xmm3
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
93
220
/
/
minps
%
xmm12
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_sse2
.
globl
_sk_gamma_sse2
FUNCTION
(
_sk_gamma_sse2
)
_sk_gamma_sse2
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
15
41
108
36
200
/
/
movaps
%
xmm5
-
0x38
(
%
rsp
)
.
byte
15
41
100
36
184
/
/
movaps
%
xmm4
-
0x48
(
%
rsp
)
.
byte
15
41
92
36
168
/
/
movaps
%
xmm3
-
0x58
(
%
rsp
)
.
byte
68
15
40
226
/
/
movaps
%
xmm2
%
xmm12
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
91
244
/
/
cvtdq2ps
%
xmm4
%
xmm6
.
byte
15
40
5
200
72
1
0
/
/
movaps
0x148c8
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
68
15
40
5
202
72
1
0
/
/
movaps
0x148ca
(
%
rip
)
%
xmm8
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
15
86
5
236
69
1
0
/
/
orps
0x145ec
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
88
53
197
72
1
0
/
/
addps
0x148c5
(
%
rip
)
%
xmm6
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
29
205
72
1
0
/
/
movaps
0x148cd
(
%
rip
)
%
xmm11
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
68
15
40
61
203
72
1
0
/
/
movaps
0x148cb
(
%
rip
)
%
xmm15
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
68
15
40
45
207
72
1
0
/
/
movaps
0x148cf
(
%
rip
)
%
xmm13
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
65
15
40
205
/
/
movaps
%
xmm13
%
xmm1
.
byte
15
94
200
/
/
divps
%
xmm0
%
xmm1
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
24
/
/
movss
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
243
15
91
198
/
/
cvttps2dq
%
xmm6
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm1
.
byte
15
84
13
163
69
1
0
/
/
andps
0x145a3
(
%
rip
)
%
xmm1
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
88
53
163
72
1
0
/
/
addps
0x148a3
(
%
rip
)
%
xmm6
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
21
188
72
1
0
/
/
movaps
0x148bc
(
%
rip
)
%
xmm2
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
68
15
40
202
/
/
movaps
%
xmm2
%
xmm9
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
13
154
72
1
0
/
/
movaps
0x1489a
(
%
rip
)
%
xmm1
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
68
15
40
53
172
72
1
0
/
/
movaps
0x148ac
(
%
rip
)
%
xmm14
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
65
15
40
198
/
/
movaps
%
xmm14
%
xmm0
.
byte
65
15
94
193
/
/
divps
%
xmm9
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
68
15
40
13
169
72
1
0
/
/
movaps
0x148a9
(
%
rip
)
%
xmm9
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
194
230
4
/
/
cmpneqps
%
xmm6
%
xmm4
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
65
15
91
194
/
/
cvtdq2ps
%
xmm10
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
65
15
84
248
/
/
andps
%
xmm8
%
xmm7
.
byte
15
40
53
33
69
1
0
/
/
movaps
0x14521
(
%
rip
)
%
xmm6
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
86
254
/
/
orps
%
xmm6
%
xmm7
.
byte
15
88
5
247
71
1
0
/
/
addps
0x147f7
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
15
40
239
/
/
movaps
%
xmm7
%
xmm5
.
byte
65
15
89
235
/
/
mulps
%
xmm11
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
65
15
88
255
/
/
addps
%
xmm15
%
xmm7
.
byte
65
15
40
237
/
/
movaps
%
xmm13
%
xmm5
.
byte
15
94
239
/
/
divps
%
xmm7
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
243
15
91
232
/
/
cvttps2dq
%
xmm0
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
194
253
1
/
/
cmpltps
%
xmm5
%
xmm7
.
byte
68
15
40
61
246
68
1
0
/
/
movaps
0x144f6
(
%
rip
)
%
xmm15
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
84
255
/
/
andps
%
xmm15
%
xmm7
.
byte
15
92
239
/
/
subps
%
xmm7
%
xmm5
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
92
253
/
/
subps
%
xmm5
%
xmm7
.
byte
15
88
5
242
71
1
0
/
/
addps
0x147f2
(
%
rip
)
%
xmm0
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
92
239
/
/
subps
%
xmm7
%
xmm5
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
65
15
40
254
/
/
movaps
%
xmm14
%
xmm7
.
byte
15
94
253
/
/
divps
%
xmm5
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
65
15
89
249
/
/
mulps
%
xmm9
%
xmm7
.
byte
102
15
91
199
/
/
cvtps2dq
%
xmm7
%
xmm0
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
68
15
194
215
4
/
/
cmpneqps
%
xmm7
%
xmm10
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
65
15
91
196
/
/
cvtdq2ps
%
xmm12
%
xmm0
.
byte
15
89
5
93
71
1
0
/
/
mulps
0x1475d
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
68
15
86
198
/
/
orps
%
xmm6
%
xmm8
.
byte
15
88
5
110
71
1
0
/
/
addps
0x1476e
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
68
15
88
5
126
71
1
0
/
/
addps
0x1477e
(
%
rip
)
%
xmm8
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
69
15
94
232
/
/
divps
%
xmm8
%
xmm13
.
byte
65
15
92
197
/
/
subps
%
xmm13
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
243
15
91
216
/
/
cvttps2dq
%
xmm0
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
194
235
1
/
/
cmpltps
%
xmm3
%
xmm5
.
byte
65
15
84
239
/
/
andps
%
xmm15
%
xmm5
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
88
5
113
71
1
0
/
/
addps
0x14771
(
%
rip
)
%
xmm0
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
68
15
94
242
/
/
divps
%
xmm2
%
xmm14
.
byte
68
15
88
240
/
/
addps
%
xmm0
%
xmm14
.
byte
69
15
89
241
/
/
mulps
%
xmm9
%
xmm14
.
byte
68
15
194
231
4
/
/
cmpneqps
%
xmm7
%
xmm12
.
byte
102
65
15
91
198
/
/
cvtps2dq
%
xmm14
%
xmm0
.
byte
68
15
84
224
/
/
andps
%
xmm0
%
xmm12
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
40
202
/
/
movaps
%
xmm10
%
xmm1
.
byte
65
15
40
212
/
/
movaps
%
xmm12
%
xmm2
.
byte
15
40
92
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gamma_dst_sse2
.
globl
_sk_gamma_dst_sse2
FUNCTION
(
_sk_gamma_dst_sse2
)
_sk_gamma_dst_sse2
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
68
15
40
230
/
/
movaps
%
xmm6
%
xmm12
.
byte
68
15
40
213
/
/
movaps
%
xmm5
%
xmm10
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
41
92
36
216
/
/
movaps
%
xmm3
-
0x28
(
%
rsp
)
.
byte
15
41
84
36
200
/
/
movaps
%
xmm2
-
0x38
(
%
rsp
)
.
byte
15
41
76
36
184
/
/
movaps
%
xmm1
-
0x48
(
%
rsp
)
.
byte
15
41
68
36
168
/
/
movaps
%
xmm0
-
0x58
(
%
rsp
)
.
byte
15
91
221
/
/
cvtdq2ps
%
xmm5
%
xmm3
.
byte
15
40
5
152
70
1
0
/
/
movaps
0x14698
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
68
15
40
5
154
70
1
0
/
/
movaps
0x1469a
(
%
rip
)
%
xmm8
#
3d1d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf84
>
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
15
86
5
188
67
1
0
/
/
orps
0x143bc
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
88
29
149
70
1
0
/
/
addps
0x14695
(
%
rip
)
%
xmm3
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
68
15
40
29
157
70
1
0
/
/
movaps
0x1469d
(
%
rip
)
%
xmm11
#
3d1f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfa4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
68
15
40
61
155
70
1
0
/
/
movaps
0x1469b
(
%
rip
)
%
xmm15
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
68
15
40
45
159
70
1
0
/
/
movaps
0x1469f
(
%
rip
)
%
xmm13
#
3d210
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfc4
>
.
byte
65
15
40
205
/
/
movaps
%
xmm13
%
xmm1
.
byte
15
94
200
/
/
divps
%
xmm0
%
xmm1
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
32
/
/
movss
(
%
rax
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm1
.
byte
15
84
13
115
67
1
0
/
/
andps
0x14373
(
%
rip
)
%
xmm1
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
88
29
115
70
1
0
/
/
addps
0x14673
(
%
rip
)
%
xmm3
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
21
140
70
1
0
/
/
movaps
0x1468c
(
%
rip
)
%
xmm2
#
3d240
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xff4
>
.
byte
68
15
40
202
/
/
movaps
%
xmm2
%
xmm9
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
13
106
70
1
0
/
/
movaps
0x1466a
(
%
rip
)
%
xmm1
#
3d230
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfe4
>
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
68
15
40
53
124
70
1
0
/
/
movaps
0x1467c
(
%
rip
)
%
xmm14
#
3d250
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1004
>
.
byte
65
15
40
198
/
/
movaps
%
xmm14
%
xmm0
.
byte
65
15
94
193
/
/
divps
%
xmm9
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
68
15
40
13
121
70
1
0
/
/
movaps
0x14679
(
%
rip
)
%
xmm9
#
3d260
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1014
>
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
194
235
4
/
/
cmpneqps
%
xmm3
%
xmm5
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
65
15
91
194
/
/
cvtdq2ps
%
xmm10
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
65
15
40
250
/
/
movaps
%
xmm10
%
xmm7
.
byte
65
15
84
248
/
/
andps
%
xmm8
%
xmm7
.
byte
15
40
29
241
66
1
0
/
/
movaps
0x142f1
(
%
rip
)
%
xmm3
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
86
251
/
/
orps
%
xmm3
%
xmm7
.
byte
15
88
5
199
69
1
0
/
/
addps
0x145c7
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
65
15
89
243
/
/
mulps
%
xmm11
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
65
15
88
255
/
/
addps
%
xmm15
%
xmm7
.
byte
65
15
40
245
/
/
movaps
%
xmm13
%
xmm6
.
byte
15
94
247
/
/
divps
%
xmm7
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
243
15
91
240
/
/
cvttps2dq
%
xmm0
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
194
254
1
/
/
cmpltps
%
xmm6
%
xmm7
.
byte
68
15
40
61
198
66
1
0
/
/
movaps
0x142c6
(
%
rip
)
%
xmm15
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
84
255
/
/
andps
%
xmm15
%
xmm7
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
92
254
/
/
subps
%
xmm6
%
xmm7
.
byte
15
88
5
194
69
1
0
/
/
addps
0x145c2
(
%
rip
)
%
xmm0
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
65
15
40
254
/
/
movaps
%
xmm14
%
xmm7
.
byte
15
94
254
/
/
divps
%
xmm6
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
65
15
89
249
/
/
mulps
%
xmm9
%
xmm7
.
byte
102
15
91
199
/
/
cvtps2dq
%
xmm7
%
xmm0
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
68
15
194
215
4
/
/
cmpneqps
%
xmm7
%
xmm10
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
65
15
91
196
/
/
cvtdq2ps
%
xmm12
%
xmm0
.
byte
15
89
5
45
69
1
0
/
/
mulps
0x1452d
(
%
rip
)
%
xmm0
#
3d1c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf74
>
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
68
15
86
195
/
/
orps
%
xmm3
%
xmm8
.
byte
15
88
5
62
69
1
0
/
/
addps
0x1453e
(
%
rip
)
%
xmm0
#
3d1e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf94
>
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
68
15
88
5
78
69
1
0
/
/
addps
0x1454e
(
%
rip
)
%
xmm8
#
3d200
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfb4
>
.
byte
69
15
94
232
/
/
divps
%
xmm8
%
xmm13
.
byte
65
15
92
197
/
/
subps
%
xmm13
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
243
15
91
224
/
/
cvttps2dq
%
xmm0
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
194
244
1
/
/
cmpltps
%
xmm4
%
xmm6
.
byte
65
15
84
247
/
/
andps
%
xmm15
%
xmm6
.
byte
15
92
230
/
/
subps
%
xmm6
%
xmm4
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
88
5
65
69
1
0
/
/
addps
0x14541
(
%
rip
)
%
xmm0
#
3d220
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xfd4
>
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
68
15
94
242
/
/
divps
%
xmm2
%
xmm14
.
byte
68
15
88
240
/
/
addps
%
xmm0
%
xmm14
.
byte
69
15
89
241
/
/
mulps
%
xmm9
%
xmm14
.
byte
68
15
194
231
4
/
/
cmpneqps
%
xmm7
%
xmm12
.
byte
102
65
15
91
198
/
/
cvtps2dq
%
xmm14
%
xmm0
.
byte
68
15
84
224
/
/
andps
%
xmm0
%
xmm12
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
68
36
168
/
/
movaps
-
0x58
(
%
rsp
)
%
xmm0
.
byte
15
40
76
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm1
.
byte
15
40
84
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm2
.
byte
15
40
92
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm3
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
65
15
40
234
/
/
movaps
%
xmm10
%
xmm5
.
byte
65
15
40
244
/
/
movaps
%
xmm12
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lab_to_xyz_sse2
.
globl
_sk_lab_to_xyz_sse2
FUNCTION
(
_sk_lab_to_xyz_sse2
)
_sk_lab_to_xyz_sse2
:
.
byte
15
89
5
63
69
1
0
/
/
mulps
0x1453f
(
%
rip
)
%
xmm0
#
3d270
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1024
>
.
byte
68
15
40
5
183
66
1
0
/
/
movaps
0x142b7
(
%
rip
)
%
xmm8
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
68
15
40
13
59
69
1
0
/
/
movaps
0x1453b
(
%
rip
)
%
xmm9
#
3d280
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1034
>
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
15
88
5
56
69
1
0
/
/
addps
0x14538
(
%
rip
)
%
xmm0
#
3d290
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1044
>
.
byte
15
89
5
65
69
1
0
/
/
mulps
0x14541
(
%
rip
)
%
xmm0
#
3d2a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1054
>
.
byte
15
89
13
74
69
1
0
/
/
mulps
0x1454a
(
%
rip
)
%
xmm1
#
3d2b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1064
>
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
89
21
80
69
1
0
/
/
mulps
0x14550
(
%
rip
)
%
xmm2
#
3d2c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1074
>
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
92
202
/
/
subps
%
xmm2
%
xmm9
.
byte
68
15
40
225
/
/
movaps
%
xmm1
%
xmm12
.
byte
69
15
89
228
/
/
mulps
%
xmm12
%
xmm12
.
byte
68
15
89
225
/
/
mulps
%
xmm1
%
xmm12
.
byte
15
40
21
69
69
1
0
/
/
movaps
0x14545
(
%
rip
)
%
xmm2
#
3d2d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1084
>
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
69
15
194
196
1
/
/
cmpltps
%
xmm12
%
xmm8
.
byte
68
15
40
21
68
69
1
0
/
/
movaps
0x14544
(
%
rip
)
%
xmm10
#
3d2e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1094
>
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
68
15
40
29
72
69
1
0
/
/
movaps
0x14548
(
%
rip
)
%
xmm11
#
3d2f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10a4
>
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
69
15
84
224
/
/
andps
%
xmm8
%
xmm12
.
byte
68
15
85
193
/
/
andnps
%
xmm1
%
xmm8
.
byte
69
15
86
196
/
/
orps
%
xmm12
%
xmm8
.
byte
68
15
40
224
/
/
movaps
%
xmm0
%
xmm12
.
byte
69
15
89
228
/
/
mulps
%
xmm12
%
xmm12
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
65
15
194
204
1
/
/
cmpltps
%
xmm12
%
xmm1
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
68
15
84
225
/
/
andps
%
xmm1
%
xmm12
.
byte
15
85
200
/
/
andnps
%
xmm0
%
xmm1
.
byte
65
15
86
204
/
/
orps
%
xmm12
%
xmm1
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
15
194
208
1
/
/
cmpltps
%
xmm0
%
xmm2
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
15
84
194
/
/
andps
%
xmm2
%
xmm0
.
byte
65
15
85
209
/
/
andnps
%
xmm9
%
xmm2
.
byte
15
86
208
/
/
orps
%
xmm0
%
xmm2
.
byte
68
15
89
5
248
68
1
0
/
/
mulps
0x144f8
(
%
rip
)
%
xmm8
#
3d300
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10b4
>
.
byte
15
89
21
1
69
1
0
/
/
mulps
0x14501
(
%
rip
)
%
xmm2
#
3d310
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10c4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_sse2
.
globl
_sk_load_a8_sse2
FUNCTION
(
_sk_load_a8_sse2
)
_sk_load_a8_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
46
/
/
jne
28e57
<
_sk_load_a8_sse2
+
0x40
>
.
byte
102
65
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
102
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm0
.
byte
102
15
219
5
129
65
1
0
/
/
pand
0x14181
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
216
/
/
cvtdq2ps
%
xmm0
%
xmm3
.
byte
15
89
29
215
66
1
0
/
/
mulps
0x142d7
(
%
rip
)
%
xmm3
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
50
/
/
je
28e91
<
_sk_load_a8_sse2
+
0x7a
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
28e7a
<
_sk_load_a8_sse2
+
0x63
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
204
/
/
jne
28e37
<
_sk_load_a8_sse2
+
0x20
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
200
/
/
movd
%
eax
%
xmm1
.
byte
102
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
242
15
16
193
/
/
movsd
%
xmm1
%
xmm0
.
byte
235
166
/
/
jmp
28e37
<
_sk_load_a8_sse2
+
0x20
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
155
/
/
jmp
28e37
<
_sk_load_a8_sse2
+
0x20
>
HIDDEN
_sk_load_a8_dst_sse2
.
globl
_sk_load_a8_dst_sse2
FUNCTION
(
_sk_load_a8_dst_sse2
)
_sk_load_a8_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
46
/
/
jne
28edc
<
_sk_load_a8_dst_sse2
+
0x40
>
.
byte
102
65
15
110
36
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
219
37
252
64
1
0
/
/
pand
0x140fc
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
252
/
/
cvtdq2ps
%
xmm4
%
xmm7
.
byte
15
89
61
82
66
1
0
/
/
mulps
0x14252
(
%
rip
)
%
xmm7
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
102
15
87
237
/
/
xorpd
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
50
/
/
je
28f16
<
_sk_load_a8_dst_sse2
+
0x7a
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
28eff
<
_sk_load_a8_dst_sse2
+
0x63
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
204
/
/
jne
28ebc
<
_sk_load_a8_dst_sse2
+
0x20
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
102
15
97
232
/
/
punpcklwd
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
235
166
/
/
jmp
28ebc
<
_sk_load_a8_dst_sse2
+
0x20
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
155
/
/
jmp
28ebc
<
_sk_load_a8_dst_sse2
+
0x20
>
HIDDEN
_sk_gather_a8_sse2
.
globl
_sk_gather_a8_sse2
FUNCTION
(
_sk_gather_a8_sse2
)
_sk_gather_a8_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
65
15
254
217
/
/
paddd
%
xmm9
%
xmm3
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
193
/
/
paddd
%
xmm9
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
216
245
/
/
pshufd
0xf5
%
xmm0
%
xmm3
.
byte
102
15
244
217
/
/
pmuludq
%
xmm1
%
xmm3
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
202
/
/
cvttps2dq
%
xmm2
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
73
15
126
194
/
/
movq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
65
193
226
8
/
/
shl
0x8
%
r10d
.
byte
69
9
218
/
/
or
%
r11d
%
r10d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
68
9
200
/
/
or
%
r9d
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
65
15
196
194
1
/
/
pinsrw
0x1
%
r10d
%
xmm0
.
byte
102
65
15
96
192
/
/
punpcklbw
%
xmm8
%
xmm0
.
byte
102
65
15
97
192
/
/
punpcklwd
%
xmm8
%
xmm0
.
byte
15
91
216
/
/
cvtdq2ps
%
xmm0
%
xmm3
.
byte
15
89
29
53
65
1
0
/
/
mulps
0x14135
(
%
rip
)
%
xmm3
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_a8_sse2
.
globl
_sk_store_a8_sse2
FUNCTION
(
_sk_store_a8_sse2
)
_sk_store_a8_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
68
15
93
5
250
62
1
0
/
/
minps
0x13efa
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
89
5
210
63
1
0
/
/
mulps
0x13fd2
(
%
rip
)
%
xmm8
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
68
15
107
192
/
/
packssdw
%
xmm0
%
xmm8
.
byte
102
68
15
103
192
/
/
packuswb
%
xmm0
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
13
/
/
jne
2904b
<
_sk_store_a8_sse2
+
0x52
>
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
65
137
4
16
/
/
mov
%
eax
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
68
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm8
.
byte
102
68
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
50
/
/
je
2908f
<
_sk_store_a8_sse2
+
0x96
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
15
/
/
je
29070
<
_sk_store_a8_sse2
+
0x77
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
226
/
/
jne
29047
<
_sk_store_a8_sse2
+
0x4e
>
.
byte
102
65
15
197
192
4
/
/
pextrw
0x4
%
xmm8
%
eax
.
byte
65
136
68
16
2
/
/
mov
%
al
0x2
(
%
r8
%
rdx
1
)
.
byte
102
68
15
219
5
71
63
1
0
/
/
pand
0x13f47
(
%
rip
)
%
xmm8
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
69
15
103
192
/
/
packuswb
%
xmm8
%
xmm8
.
byte
102
69
15
103
192
/
/
packuswb
%
xmm8
%
xmm8
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
102
65
137
4
16
/
/
mov
%
ax
(
%
r8
%
rdx
1
)
.
byte
235
184
/
/
jmp
29047
<
_sk_store_a8_sse2
+
0x4e
>
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
65
136
4
16
/
/
mov
%
al
(
%
r8
%
rdx
1
)
.
byte
235
173
/
/
jmp
29047
<
_sk_store_a8_sse2
+
0x4e
>
HIDDEN
_sk_load_g8_sse2
.
globl
_sk_load_g8_sse2
FUNCTION
(
_sk_load_g8_sse2
)
_sk_load_g8_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
49
/
/
jne
290dd
<
_sk_load_g8_sse2
+
0x43
>
.
byte
102
65
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
102
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm0
.
byte
102
15
219
5
254
62
1
0
/
/
pand
0x13efe
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
84
64
1
0
/
/
mulps
0x14054
(
%
rip
)
%
xmm0
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
59
62
1
0
/
/
movaps
0x13e3b
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
50
/
/
je
29117
<
_sk_load_g8_sse2
+
0x7d
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
29100
<
_sk_load_g8_sse2
+
0x66
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
201
/
/
jne
290ba
<
_sk_load_g8_sse2
+
0x20
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
200
/
/
movd
%
eax
%
xmm1
.
byte
102
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
242
15
16
193
/
/
movsd
%
xmm1
%
xmm0
.
byte
235
163
/
/
jmp
290ba
<
_sk_load_g8_sse2
+
0x20
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
152
/
/
jmp
290ba
<
_sk_load_g8_sse2
+
0x20
>
HIDDEN
_sk_load_g8_dst_sse2
.
globl
_sk_load_g8_dst_sse2
FUNCTION
(
_sk_load_g8_dst_sse2
)
_sk_load_g8_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
49
/
/
jne
29165
<
_sk_load_g8_dst_sse2
+
0x43
>
.
byte
102
65
15
110
36
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
219
37
118
62
1
0
/
/
pand
0x13e76
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
37
204
63
1
0
/
/
mulps
0x13fcc
(
%
rip
)
%
xmm4
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
179
61
1
0
/
/
movaps
0x13db3
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
50
/
/
je
2919f
<
_sk_load_g8_dst_sse2
+
0x7d
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
29188
<
_sk_load_g8_dst_sse2
+
0x66
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
201
/
/
jne
29142
<
_sk_load_g8_dst_sse2
+
0x20
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
102
15
97
232
/
/
punpcklwd
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
235
163
/
/
jmp
29142
<
_sk_load_g8_dst_sse2
+
0x20
>
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
152
/
/
jmp
29142
<
_sk_load_g8_dst_sse2
+
0x20
>
HIDDEN
_sk_gather_g8_sse2
.
globl
_sk_gather_g8_sse2
FUNCTION
(
_sk_gather_g8_sse2
)
_sk_gather_g8_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
65
15
254
217
/
/
paddd
%
xmm9
%
xmm3
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
193
/
/
paddd
%
xmm9
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
216
245
/
/
pshufd
0xf5
%
xmm0
%
xmm3
.
byte
102
15
244
217
/
/
pmuludq
%
xmm1
%
xmm3
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
202
/
/
cvttps2dq
%
xmm2
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
73
15
126
194
/
/
movq
%
xmm0
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
182
28
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
r11d
.
byte
71
15
182
20
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
r10d
.
byte
65
193
226
8
/
/
shl
0x8
%
r10d
.
byte
69
9
218
/
/
or
%
r11d
%
r10d
.
byte
71
15
182
12
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
r9d
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
68
9
200
/
/
or
%
r9d
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
65
15
196
194
1
/
/
pinsrw
0x1
%
r10d
%
xmm0
.
byte
102
65
15
96
192
/
/
punpcklbw
%
xmm8
%
xmm0
.
byte
102
65
15
97
192
/
/
punpcklwd
%
xmm8
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
172
62
1
0
/
/
mulps
0x13eac
(
%
rip
)
%
xmm0
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
147
60
1
0
/
/
movaps
0x13c93
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_sse2
.
globl
_sk_load_565_sse2
FUNCTION
(
_sk_load_565_sse2
)
_sk_load_565_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
83
/
/
jne
292ed
<
_sk_load_565_sse2
+
0x68
>
.
byte
243
65
15
126
20
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
15
111
5
132
62
1
0
/
/
movdqa
0x13e84
(
%
rip
)
%
xmm0
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
134
62
1
0
/
/
mulps
0x13e86
(
%
rip
)
%
xmm0
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
13
142
62
1
0
/
/
movdqa
0x13e8e
(
%
rip
)
%
xmm1
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
144
62
1
0
/
/
mulps
0x13e90
(
%
rip
)
%
xmm1
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
15
219
21
152
62
1
0
/
/
pand
0x13e98
(
%
rip
)
%
xmm2
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
158
62
1
0
/
/
mulps
0x13e9e
(
%
rip
)
%
xmm2
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
37
60
1
0
/
/
movaps
0x13c25
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
44
/
/
je
29321
<
_sk_load_565_sse2
+
0x9c
>
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
29310
<
_sk_load_565_sse2
+
0x8b
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
163
/
/
jne
292a4
<
_sk_load_565_sse2
+
0x1f
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
208
69
/
/
pshufd
0x45
%
xmm0
%
xmm2
.
byte
102
65
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
242
15
112
192
212
/
/
pshuflw
0xd4
%
xmm0
%
xmm0
.
byte
242
15
16
208
/
/
movsd
%
xmm0
%
xmm2
.
byte
235
131
/
/
jmp
292a4
<
_sk_load_565_sse2
+
0x1f
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
233
117
255
255
255
/
/
jmpq
292a4
<
_sk_load_565_sse2
+
0x1f
>
HIDDEN
_sk_load_565_dst_sse2
.
globl
_sk_load_565_dst_sse2
FUNCTION
(
_sk_load_565_dst_sse2
)
_sk_load_565_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
83
/
/
jne
29397
<
_sk_load_565_dst_sse2
+
0x68
>
.
byte
243
65
15
126
52
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
102
15
97
240
/
/
punpcklwd
%
xmm0
%
xmm6
.
byte
102
15
111
37
218
61
1
0
/
/
movdqa
0x13dda
(
%
rip
)
%
xmm4
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
37
220
61
1
0
/
/
mulps
0x13ddc
(
%
rip
)
%
xmm4
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
45
228
61
1
0
/
/
movdqa
0x13de4
(
%
rip
)
%
xmm5
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
45
230
61
1
0
/
/
mulps
0x13de6
(
%
rip
)
%
xmm5
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
15
219
53
238
61
1
0
/
/
pand
0x13dee
(
%
rip
)
%
xmm6
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
53
244
61
1
0
/
/
mulps
0x13df4
(
%
rip
)
%
xmm6
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
123
59
1
0
/
/
movaps
0x13b7b
(
%
rip
)
%
xmm7
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
44
/
/
je
293cb
<
_sk_load_565_dst_sse2
+
0x9c
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
293ba
<
_sk_load_565_dst_sse2
+
0x8b
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
163
/
/
jne
2934e
<
_sk_load_565_dst_sse2
+
0x1f
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
244
69
/
/
pshufd
0x45
%
xmm4
%
xmm6
.
byte
102
65
15
110
36
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
242
15
112
228
212
/
/
pshuflw
0xd4
%
xmm4
%
xmm4
.
byte
242
15
16
244
/
/
movsd
%
xmm4
%
xmm6
.
byte
235
131
/
/
jmp
2934e
<
_sk_load_565_dst_sse2
+
0x1f
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
240
/
/
movd
%
eax
%
xmm6
.
byte
233
117
255
255
255
/
/
jmpq
2934e
<
_sk_load_565_dst_sse2
+
0x1f
>
HIDDEN
_sk_gather_565_sse2
.
globl
_sk_gather_565_sse2
FUNCTION
(
_sk_gather_565_sse2
)
_sk_gather_565_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
65
15
254
217
/
/
paddd
%
xmm9
%
xmm3
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
193
/
/
paddd
%
xmm9
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
216
245
/
/
pshufd
0xf5
%
xmm0
%
xmm3
.
byte
102
15
244
217
/
/
pmuludq
%
xmm1
%
xmm3
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
202
/
/
cvttps2dq
%
xmm2
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
102
65
15
110
211
/
/
movd
%
r11d
%
xmm2
.
byte
102
65
15
196
210
1
/
/
pinsrw
0x1
%
r10d
%
xmm2
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
102
65
15
196
209
2
/
/
pinsrw
0x2
%
r9d
%
xmm2
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
102
15
196
208
3
/
/
pinsrw
0x3
%
eax
%
xmm2
.
byte
102
65
15
97
208
/
/
punpcklwd
%
xmm8
%
xmm2
.
byte
102
15
111
5
149
60
1
0
/
/
movdqa
0x13c95
(
%
rip
)
%
xmm0
#
3d130
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xee4
>
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
151
60
1
0
/
/
mulps
0x13c97
(
%
rip
)
%
xmm0
#
3d140
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xef4
>
.
byte
102
15
111
13
159
60
1
0
/
/
movdqa
0x13c9f
(
%
rip
)
%
xmm1
#
3d150
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf04
>
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
161
60
1
0
/
/
mulps
0x13ca1
(
%
rip
)
%
xmm1
#
3d160
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf14
>
.
byte
102
15
219
21
169
60
1
0
/
/
pand
0x13ca9
(
%
rip
)
%
xmm2
#
3d170
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf24
>
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
175
60
1
0
/
/
mulps
0x13caf
(
%
rip
)
%
xmm2
#
3d180
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf34
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
54
58
1
0
/
/
movaps
0x13a36
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_565_sse2
.
globl
_sk_store_565_sse2
FUNCTION
(
_sk_store_565_sse2
)
_sk_store_565_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
16
58
1
0
/
/
movaps
0x13a10
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
36
62
1
0
/
/
movaps
0x13e24
(
%
rip
)
%
xmm11
#
3d330
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10e4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
11
/
/
pslld
0xb
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
68
15
89
37
17
62
1
0
/
/
mulps
0x13e11
(
%
rip
)
%
xmm12
#
3d340
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x10f4
>
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
5
/
/
pslld
0x5
%
xmm12
.
byte
68
15
95
194
/
/
maxps
%
xmm2
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
69
15
86
193
/
/
orpd
%
xmm9
%
xmm8
.
byte
102
69
15
86
196
/
/
orpd
%
xmm12
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
68
15
107
192
/
/
packssdw
%
xmm0
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
29575
<
_sk_store_565_sse2
+
0x99
>
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
68
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
34
/
/
je
295a4
<
_sk_store_565_sse2
+
0xc8
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
29596
<
_sk_store_565_sse2
+
0xba
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
231
/
/
jne
29571
<
_sk_store_565_sse2
+
0x95
>
.
byte
102
65
15
197
192
4
/
/
pextrw
0x4
%
xmm8
%
eax
.
byte
102
65
137
68
80
4
/
/
mov
%
ax
0x4
(
%
r8
%
rdx
2
)
.
byte
242
69
15
112
192
232
/
/
pshuflw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
205
/
/
jmp
29571
<
_sk_store_565_sse2
+
0x95
>
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
102
65
137
4
80
/
/
mov
%
ax
(
%
r8
%
rdx
2
)
.
byte
235
193
/
/
jmp
29571
<
_sk_store_565_sse2
+
0x95
>
HIDDEN
_sk_load_4444_sse2
.
globl
_sk_load_4444_sse2
FUNCTION
(
_sk_load_4444_sse2
)
_sk_load_4444_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
98
/
/
jne
29627
<
_sk_load_4444_sse2
+
0x77
>
.
byte
243
65
15
126
28
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm3
.
byte
102
15
97
216
/
/
punpcklwd
%
xmm0
%
xmm3
.
byte
102
15
111
5
121
61
1
0
/
/
movdqa
0x13d79
(
%
rip
)
%
xmm0
#
3d350
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1104
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
123
61
1
0
/
/
mulps
0x13d7b
(
%
rip
)
%
xmm0
#
3d360
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1114
>
.
byte
102
15
111
13
131
61
1
0
/
/
movdqa
0x13d83
(
%
rip
)
%
xmm1
#
3d370
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1124
>
.
byte
102
15
219
203
/
/
pand
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
133
61
1
0
/
/
mulps
0x13d85
(
%
rip
)
%
xmm1
#
3d380
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1134
>
.
byte
102
15
111
21
141
61
1
0
/
/
movdqa
0x13d8d
(
%
rip
)
%
xmm2
#
3d390
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1144
>
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
143
61
1
0
/
/
mulps
0x13d8f
(
%
rip
)
%
xmm2
#
3d3a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1154
>
.
byte
102
15
219
29
151
61
1
0
/
/
pand
0x13d97
(
%
rip
)
%
xmm3
#
3d3b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1164
>
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
29
157
61
1
0
/
/
mulps
0x13d9d
(
%
rip
)
%
xmm3
#
3d3c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1174
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
47
/
/
je
2965e
<
_sk_load_4444_sse2
+
0xae
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
2964a
<
_sk_load_4444_sse2
+
0x9a
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
148
/
/
jne
295cf
<
_sk_load_4444_sse2
+
0x1f
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
65
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
242
15
112
192
212
/
/
pshuflw
0xd4
%
xmm0
%
xmm0
.
byte
242
15
16
216
/
/
movsd
%
xmm0
%
xmm3
.
byte
233
113
255
255
255
/
/
jmpq
295cf
<
_sk_load_4444_sse2
+
0x1f
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
233
99
255
255
255
/
/
jmpq
295cf
<
_sk_load_4444_sse2
+
0x1f
>
HIDDEN
_sk_load_4444_dst_sse2
.
globl
_sk_load_4444_dst_sse2
FUNCTION
(
_sk_load_4444_dst_sse2
)
_sk_load_4444_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
98
/
/
jne
296e3
<
_sk_load_4444_dst_sse2
+
0x77
>
.
byte
243
65
15
126
60
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm7
.
byte
102
15
97
248
/
/
punpcklwd
%
xmm0
%
xmm7
.
byte
102
15
111
37
189
60
1
0
/
/
movdqa
0x13cbd
(
%
rip
)
%
xmm4
#
3d350
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1104
>
.
byte
102
15
219
231
/
/
pand
%
xmm7
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
37
191
60
1
0
/
/
mulps
0x13cbf
(
%
rip
)
%
xmm4
#
3d360
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1114
>
.
byte
102
15
111
45
199
60
1
0
/
/
movdqa
0x13cc7
(
%
rip
)
%
xmm5
#
3d370
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1124
>
.
byte
102
15
219
239
/
/
pand
%
xmm7
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
45
201
60
1
0
/
/
mulps
0x13cc9
(
%
rip
)
%
xmm5
#
3d380
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1134
>
.
byte
102
15
111
53
209
60
1
0
/
/
movdqa
0x13cd1
(
%
rip
)
%
xmm6
#
3d390
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1144
>
.
byte
102
15
219
247
/
/
pand
%
xmm7
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
53
211
60
1
0
/
/
mulps
0x13cd3
(
%
rip
)
%
xmm6
#
3d3a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1154
>
.
byte
102
15
219
61
219
60
1
0
/
/
pand
0x13cdb
(
%
rip
)
%
xmm7
#
3d3b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1164
>
.
byte
15
91
255
/
/
cvtdq2ps
%
xmm7
%
xmm7
.
byte
15
89
61
225
60
1
0
/
/
mulps
0x13ce1
(
%
rip
)
%
xmm7
#
3d3c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1174
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
47
/
/
je
2971a
<
_sk_load_4444_dst_sse2
+
0xae
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
29706
<
_sk_load_4444_dst_sse2
+
0x9a
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
148
/
/
jne
2968b
<
_sk_load_4444_dst_sse2
+
0x1f
>
.
byte
65
15
183
68
80
4
/
/
movzwl
0x4
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
112
252
69
/
/
pshufd
0x45
%
xmm4
%
xmm7
.
byte
102
65
15
110
36
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
242
15
112
228
212
/
/
pshuflw
0xd4
%
xmm4
%
xmm4
.
byte
242
15
16
252
/
/
movsd
%
xmm4
%
xmm7
.
byte
233
113
255
255
255
/
/
jmpq
2968b
<
_sk_load_4444_dst_sse2
+
0x1f
>
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
248
/
/
movd
%
eax
%
xmm7
.
byte
233
99
255
255
255
/
/
jmpq
2968b
<
_sk_load_4444_dst_sse2
+
0x1f
>
HIDDEN
_sk_gather_4444_sse2
.
globl
_sk_gather_4444_sse2
FUNCTION
(
_sk_gather_4444_sse2
)
_sk_gather_4444_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
65
15
254
217
/
/
paddd
%
xmm9
%
xmm3
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
208
/
/
maxps
%
xmm0
%
xmm2
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
193
/
/
paddd
%
xmm9
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
216
245
/
/
pshufd
0xf5
%
xmm0
%
xmm3
.
byte
102
15
244
217
/
/
pmuludq
%
xmm1
%
xmm3
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
202
/
/
cvttps2dq
%
xmm2
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
71
15
183
20
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
r10d
.
byte
71
15
183
28
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
r11d
.
byte
102
65
15
110
219
/
/
movd
%
r11d
%
xmm3
.
byte
102
65
15
196
218
1
/
/
pinsrw
0x1
%
r10d
%
xmm3
.
byte
71
15
183
12
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
r9d
.
byte
102
65
15
196
217
2
/
/
pinsrw
0x2
%
r9d
%
xmm3
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
102
15
196
216
3
/
/
pinsrw
0x3
%
eax
%
xmm3
.
byte
102
65
15
97
216
/
/
punpcklwd
%
xmm8
%
xmm3
.
byte
102
15
111
5
102
59
1
0
/
/
movdqa
0x13b66
(
%
rip
)
%
xmm0
#
3d350
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1104
>
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
5
104
59
1
0
/
/
mulps
0x13b68
(
%
rip
)
%
xmm0
#
3d360
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1114
>
.
byte
102
15
111
13
112
59
1
0
/
/
movdqa
0x13b70
(
%
rip
)
%
xmm1
#
3d370
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1124
>
.
byte
102
15
219
203
/
/
pand
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
13
114
59
1
0
/
/
mulps
0x13b72
(
%
rip
)
%
xmm1
#
3d380
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1134
>
.
byte
102
15
111
21
122
59
1
0
/
/
movdqa
0x13b7a
(
%
rip
)
%
xmm2
#
3d390
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1144
>
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
21
124
59
1
0
/
/
mulps
0x13b7c
(
%
rip
)
%
xmm2
#
3d3a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1154
>
.
byte
102
15
219
29
132
59
1
0
/
/
pand
0x13b84
(
%
rip
)
%
xmm3
#
3d3b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1164
>
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
29
138
59
1
0
/
/
mulps
0x13b8a
(
%
rip
)
%
xmm3
#
3d3c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1174
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_4444_sse2
.
globl
_sk_store_4444_sse2
FUNCTION
(
_sk_store_4444_sse2
)
_sk_store_4444_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
178
54
1
0
/
/
movaps
0x136b2
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
102
59
1
0
/
/
movaps
0x13b66
(
%
rip
)
%
xmm11
#
3d3d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1184
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
12
/
/
pslld
0xc
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
8
/
/
pslld
0x8
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
4
/
/
pslld
0x4
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
69
15
86
193
/
/
orpd
%
xmm9
%
xmm8
.
byte
102
69
15
86
196
/
/
orpd
%
xmm12
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
68
15
107
192
/
/
packssdw
%
xmm0
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
298f0
<
_sk_store_4444_sse2
+
0xb6
>
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
68
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
34
/
/
je
2991f
<
_sk_store_4444_sse2
+
0xe5
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
16
/
/
je
29911
<
_sk_store_4444_sse2
+
0xd7
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
231
/
/
jne
298ec
<
_sk_store_4444_sse2
+
0xb2
>
.
byte
102
65
15
197
192
4
/
/
pextrw
0x4
%
xmm8
%
eax
.
byte
102
65
137
68
80
4
/
/
mov
%
ax
0x4
(
%
r8
%
rdx
2
)
.
byte
242
69
15
112
192
232
/
/
pshuflw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
205
/
/
jmp
298ec
<
_sk_store_4444_sse2
+
0xb2
>
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
102
65
137
4
80
/
/
mov
%
ax
(
%
r8
%
rdx
2
)
.
byte
235
193
/
/
jmp
298ec
<
_sk_store_4444_sse2
+
0xb2
>
HIDDEN
_sk_load_8888_sse2
.
globl
_sk_load_8888_sse2
FUNCTION
(
_sk_load_8888_sse2
)
_sk_load_8888_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
98
/
/
jne
299a3
<
_sk_load_8888_sse2
+
0x78
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
21
113
54
1
0
/
/
movdqa
0x13671
(
%
rip
)
%
xmm2
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
189
55
1
0
/
/
movaps
0x137bd
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
16
/
/
psrld
0x10
%
xmm3
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
15
91
211
/
/
cvtdq2ps
%
xmm3
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
114
209
24
/
/
psrld
0x18
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
299d0
<
_sk_load_8888_sse2
+
0xa5
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
299c5
<
_sk_load_8888_sse2
+
0x9a
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
143
/
/
jne
29947
<
_sk_load_8888_sse2
+
0x1c
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
200
69
/
/
pshufd
0x45
%
xmm0
%
xmm9
.
byte
102
69
15
18
12
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
119
255
255
255
/
/
jmpq
29947
<
_sk_load_8888_sse2
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
108
255
255
255
/
/
jmpq
29947
<
_sk_load_8888_sse2
+
0x1c
>
HIDDEN
_sk_load_8888_dst_sse2
.
globl
_sk_load_8888_dst_sse2
FUNCTION
(
_sk_load_8888_dst_sse2
)
_sk_load_8888_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
98
/
/
jne
29a53
<
_sk_load_8888_dst_sse2
+
0x78
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
53
193
53
1
0
/
/
movdqa
0x135c1
(
%
rip
)
%
xmm6
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
111
225
/
/
movdqa
%
xmm9
%
xmm4
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
68
15
40
5
13
55
1
0
/
/
movaps
0x1370d
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
224
/
/
mulps
%
xmm8
%
xmm4
.
byte
102
65
15
111
233
/
/
movdqa
%
xmm9
%
xmm5
.
byte
102
15
114
213
8
/
/
psrld
0x8
%
xmm5
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
102
65
15
111
249
/
/
movdqa
%
xmm9
%
xmm7
.
byte
102
15
114
215
16
/
/
psrld
0x10
%
xmm7
.
byte
102
15
219
254
/
/
pand
%
xmm6
%
xmm7
.
byte
15
91
247
/
/
cvtdq2ps
%
xmm7
%
xmm6
.
byte
65
15
89
240
/
/
mulps
%
xmm8
%
xmm6
.
byte
102
65
15
114
209
24
/
/
psrld
0x18
%
xmm9
.
byte
65
15
91
249
/
/
cvtdq2ps
%
xmm9
%
xmm7
.
byte
65
15
89
248
/
/
mulps
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
29a80
<
_sk_load_8888_dst_sse2
+
0xa5
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
29a75
<
_sk_load_8888_dst_sse2
+
0x9a
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
143
/
/
jne
299f7
<
_sk_load_8888_dst_sse2
+
0x1c
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
204
69
/
/
pshufd
0x45
%
xmm4
%
xmm9
.
byte
102
69
15
18
12
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
119
255
255
255
/
/
jmpq
299f7
<
_sk_load_8888_dst_sse2
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
108
255
255
255
/
/
jmpq
299f7
<
_sk_load_8888_dst_sse2
+
0x1c
>
HIDDEN
_sk_gather_8888_sse2
.
globl
_sk_gather_8888_sse2
FUNCTION
(
_sk_gather_8888_sse2
)
_sk_gather_8888_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
65
15
91
201
/
/
cvttps2dq
%
xmm9
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
67
15
110
4
144
/
/
movd
(
%
r8
%
r10
4
)
%
xmm0
.
byte
102
71
15
110
12
152
/
/
movd
(
%
r8
%
r11
4
)
%
xmm9
.
byte
102
68
15
98
200
/
/
punpckldq
%
xmm0
%
xmm9
.
byte
102
65
15
110
4
128
/
/
movd
(
%
r8
%
rax
4
)
%
xmm0
.
byte
102
67
15
110
12
136
/
/
movd
(
%
r8
%
r9
4
)
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
68
15
108
201
/
/
punpcklqdq
%
xmm1
%
xmm9
.
byte
102
15
111
21
125
52
1
0
/
/
movdqa
0x1347d
(
%
rip
)
%
xmm2
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
201
53
1
0
/
/
movaps
0x135c9
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
16
/
/
psrld
0x10
%
xmm3
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
15
91
211
/
/
cvtdq2ps
%
xmm3
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
114
209
24
/
/
psrld
0x18
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_8888_sse2
.
globl
_sk_store_8888_sse2
FUNCTION
(
_sk_store_8888_sse2
)
_sk_store_8888_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
84
51
1
0
/
/
movaps
0x13354
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
40
52
1
0
/
/
movaps
0x13428
(
%
rip
)
%
xmm11
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
8
/
/
pslld
0x8
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
87
201
/
/
xorpd
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
24
/
/
pslld
0x18
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
29c3d
<
_sk_store_8888_sse2
+
0xa6
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
29
/
/
je
29c62
<
_sk_store_8888_sse2
+
0xcb
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
29c5a
<
_sk_store_8888_sse2
+
0xc3
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
236
/
/
jne
29c39
<
_sk_store_8888_sse2
+
0xa2
>
.
byte
102
69
15
112
200
78
/
/
pshufd
0x4e
%
xmm8
%
xmm9
.
byte
102
69
15
126
76
144
8
/
/
movd
%
xmm9
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
29c39
<
_sk_store_8888_sse2
+
0xa2
>
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
207
/
/
jmp
29c39
<
_sk_store_8888_sse2
+
0xa2
>
HIDDEN
_sk_load_bgra_sse2
.
globl
_sk_load_bgra_sse2
FUNCTION
(
_sk_load_bgra_sse2
)
_sk_load_bgra_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
98
/
/
jne
29ce2
<
_sk_load_bgra_sse2
+
0x78
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
5
50
51
1
0
/
/
movdqa
0x13332
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
209
/
/
cvtdq2ps
%
xmm1
%
xmm2
.
byte
68
15
40
5
126
52
1
0
/
/
movaps
0x1347e
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
16
/
/
psrld
0x10
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
15
91
195
/
/
cvtdq2ps
%
xmm3
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
114
209
24
/
/
psrld
0x18
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
29d0f
<
_sk_load_bgra_sse2
+
0xa5
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
29d04
<
_sk_load_bgra_sse2
+
0x9a
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
143
/
/
jne
29c86
<
_sk_load_bgra_sse2
+
0x1c
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
200
69
/
/
pshufd
0x45
%
xmm0
%
xmm9
.
byte
102
69
15
18
12
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
119
255
255
255
/
/
jmpq
29c86
<
_sk_load_bgra_sse2
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
108
255
255
255
/
/
jmpq
29c86
<
_sk_load_bgra_sse2
+
0x1c
>
HIDDEN
_sk_load_bgra_dst_sse2
.
globl
_sk_load_bgra_dst_sse2
FUNCTION
(
_sk_load_bgra_dst_sse2
)
_sk_load_bgra_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
98
/
/
jne
29d92
<
_sk_load_bgra_dst_sse2
+
0x78
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
37
130
50
1
0
/
/
movdqa
0x13282
(
%
rip
)
%
xmm4
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
111
233
/
/
movdqa
%
xmm9
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
245
/
/
cvtdq2ps
%
xmm5
%
xmm6
.
byte
68
15
40
5
206
51
1
0
/
/
movaps
0x133ce
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
240
/
/
mulps
%
xmm8
%
xmm6
.
byte
102
65
15
111
233
/
/
movdqa
%
xmm9
%
xmm5
.
byte
102
15
114
213
8
/
/
psrld
0x8
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
102
65
15
111
249
/
/
movdqa
%
xmm9
%
xmm7
.
byte
102
15
114
215
16
/
/
psrld
0x10
%
xmm7
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
15
91
231
/
/
cvtdq2ps
%
xmm7
%
xmm4
.
byte
65
15
89
224
/
/
mulps
%
xmm8
%
xmm4
.
byte
102
65
15
114
209
24
/
/
psrld
0x18
%
xmm9
.
byte
65
15
91
249
/
/
cvtdq2ps
%
xmm9
%
xmm7
.
byte
65
15
89
248
/
/
mulps
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
29dbf
<
_sk_load_bgra_dst_sse2
+
0xa5
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
29db4
<
_sk_load_bgra_dst_sse2
+
0x9a
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
143
/
/
jne
29d36
<
_sk_load_bgra_dst_sse2
+
0x1c
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
204
69
/
/
pshufd
0x45
%
xmm4
%
xmm9
.
byte
102
69
15
18
12
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
119
255
255
255
/
/
jmpq
29d36
<
_sk_load_bgra_dst_sse2
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
108
255
255
255
/
/
jmpq
29d36
<
_sk_load_bgra_dst_sse2
+
0x1c
>
HIDDEN
_sk_gather_bgra_sse2
.
globl
_sk_gather_bgra_sse2
FUNCTION
(
_sk_gather_bgra_sse2
)
_sk_gather_bgra_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
65
15
91
201
/
/
cvttps2dq
%
xmm9
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
67
15
110
4
144
/
/
movd
(
%
r8
%
r10
4
)
%
xmm0
.
byte
102
71
15
110
12
152
/
/
movd
(
%
r8
%
r11
4
)
%
xmm9
.
byte
102
68
15
98
200
/
/
punpckldq
%
xmm0
%
xmm9
.
byte
102
65
15
110
4
128
/
/
movd
(
%
r8
%
rax
4
)
%
xmm0
.
byte
102
67
15
110
12
136
/
/
movd
(
%
r8
%
r9
4
)
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
68
15
108
201
/
/
punpcklqdq
%
xmm1
%
xmm9
.
byte
102
15
111
5
62
49
1
0
/
/
movdqa
0x1313e
(
%
rip
)
%
xmm0
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
209
/
/
cvtdq2ps
%
xmm1
%
xmm2
.
byte
68
15
40
5
138
50
1
0
/
/
movaps
0x1328a
(
%
rip
)
%
xmm8
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
16
/
/
psrld
0x10
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
15
91
195
/
/
cvtdq2ps
%
xmm3
%
xmm0
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
114
209
24
/
/
psrld
0x18
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_bgra_sse2
.
globl
_sk_store_bgra_sse2
FUNCTION
(
_sk_store_bgra_sse2
)
_sk_store_bgra_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
68
15
40
21
21
48
1
0
/
/
movaps
0x13015
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
233
48
1
0
/
/
movaps
0x130e9
(
%
rip
)
%
xmm11
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
8
/
/
pslld
0x8
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
87
201
/
/
xorpd
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
24
/
/
pslld
0x18
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
29f7c
<
_sk_store_bgra_sse2
+
0xa6
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
29
/
/
je
29fa1
<
_sk_store_bgra_sse2
+
0xcb
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
29f99
<
_sk_store_bgra_sse2
+
0xc3
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
236
/
/
jne
29f78
<
_sk_store_bgra_sse2
+
0xa2
>
.
byte
102
69
15
112
200
78
/
/
pshufd
0x4e
%
xmm8
%
xmm9
.
byte
102
69
15
126
76
144
8
/
/
movd
%
xmm9
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
29f78
<
_sk_store_bgra_sse2
+
0xa2
>
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
207
/
/
jmp
29f78
<
_sk_store_bgra_sse2
+
0xa2
>
HIDDEN
_sk_load_1010102_sse2
.
globl
_sk_load_1010102_sse2
FUNCTION
(
_sk_load_1010102_sse2
)
_sk_load_1010102_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
2a024
<
_sk_load_1010102_sse2
+
0x7b
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
21
19
52
1
0
/
/
movdqa
0x13413
(
%
rip
)
%
xmm2
#
3d3e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1194
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
15
52
1
0
/
/
movaps
0x1340f
(
%
rip
)
%
xmm8
#
3d3f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11a4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
10
/
/
psrld
0xa
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
20
/
/
psrld
0x14
%
xmm3
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
15
91
211
/
/
cvtdq2ps
%
xmm3
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
114
209
30
/
/
psrld
0x1e
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
15
89
29
208
48
1
0
/
/
mulps
0x130d0
(
%
rip
)
%
xmm3
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
2a051
<
_sk_load_1010102_sse2
+
0xa8
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
2a046
<
_sk_load_1010102_sse2
+
0x9d
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
140
/
/
jne
29fc5
<
_sk_load_1010102_sse2
+
0x1c
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
200
69
/
/
pshufd
0x45
%
xmm0
%
xmm9
.
byte
102
69
15
18
12
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
116
255
255
255
/
/
jmpq
29fc5
<
_sk_load_1010102_sse2
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
105
255
255
255
/
/
jmpq
29fc5
<
_sk_load_1010102_sse2
+
0x1c
>
HIDDEN
_sk_load_1010102_dst_sse2
.
globl
_sk_load_1010102_dst_sse2
FUNCTION
(
_sk_load_1010102_dst_sse2
)
_sk_load_1010102_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
101
/
/
jne
2a0d7
<
_sk_load_1010102_dst_sse2
+
0x7b
>
.
byte
243
69
15
111
12
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
102
15
111
53
96
51
1
0
/
/
movdqa
0x13360
(
%
rip
)
%
xmm6
#
3d3e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1194
>
.
byte
102
65
15
111
225
/
/
movdqa
%
xmm9
%
xmm4
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
68
15
40
5
92
51
1
0
/
/
movaps
0x1335c
(
%
rip
)
%
xmm8
#
3d3f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11a4
>
.
byte
65
15
89
224
/
/
mulps
%
xmm8
%
xmm4
.
byte
102
65
15
111
233
/
/
movdqa
%
xmm9
%
xmm5
.
byte
102
15
114
213
10
/
/
psrld
0xa
%
xmm5
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
102
65
15
111
249
/
/
movdqa
%
xmm9
%
xmm7
.
byte
102
15
114
215
20
/
/
psrld
0x14
%
xmm7
.
byte
102
15
219
254
/
/
pand
%
xmm6
%
xmm7
.
byte
15
91
247
/
/
cvtdq2ps
%
xmm7
%
xmm6
.
byte
65
15
89
240
/
/
mulps
%
xmm8
%
xmm6
.
byte
102
65
15
114
209
30
/
/
psrld
0x1e
%
xmm9
.
byte
65
15
91
249
/
/
cvtdq2ps
%
xmm9
%
xmm7
.
byte
15
89
61
29
48
1
0
/
/
mulps
0x1301d
(
%
rip
)
%
xmm7
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
2a104
<
_sk_load_1010102_dst_sse2
+
0xa8
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
2a0f9
<
_sk_load_1010102_dst_sse2
+
0x9d
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
140
/
/
jne
2a078
<
_sk_load_1010102_dst_sse2
+
0x1c
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
204
69
/
/
pshufd
0x45
%
xmm4
%
xmm9
.
byte
102
69
15
18
12
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
116
255
255
255
/
/
jmpq
2a078
<
_sk_load_1010102_dst_sse2
+
0x1c
>
.
byte
102
69
15
110
12
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm9
.
byte
233
105
255
255
255
/
/
jmpq
2a078
<
_sk_load_1010102_dst_sse2
+
0x1c
>
HIDDEN
_sk_gather_1010102_sse2
.
globl
_sk_gather_1010102_sse2
FUNCTION
(
_sk_gather_1010102_sse2
)
_sk_gather_1010102_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
192
/
/
pcmpeqd
%
xmm8
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
93
202
/
/
minps
%
xmm2
%
xmm9
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
65
15
91
201
/
/
cvttps2dq
%
xmm9
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
67
15
110
4
144
/
/
movd
(
%
r8
%
r10
4
)
%
xmm0
.
byte
102
71
15
110
12
152
/
/
movd
(
%
r8
%
r11
4
)
%
xmm9
.
byte
102
68
15
98
200
/
/
punpckldq
%
xmm0
%
xmm9
.
byte
102
65
15
110
4
128
/
/
movd
(
%
r8
%
rax
4
)
%
xmm0
.
byte
102
67
15
110
12
136
/
/
movd
(
%
r8
%
r9
4
)
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
68
15
108
201
/
/
punpcklqdq
%
xmm1
%
xmm9
.
byte
102
15
111
21
25
50
1
0
/
/
movdqa
0x13219
(
%
rip
)
%
xmm2
#
3d3e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1194
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
68
15
40
5
21
50
1
0
/
/
movaps
0x13215
(
%
rip
)
%
xmm8
#
3d3f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11a4
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
114
209
10
/
/
psrld
0xa
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
15
114
211
20
/
/
psrld
0x14
%
xmm3
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
15
91
211
/
/
cvtdq2ps
%
xmm3
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
65
15
114
209
30
/
/
psrld
0x1e
%
xmm9
.
byte
65
15
91
217
/
/
cvtdq2ps
%
xmm9
%
xmm3
.
byte
15
89
29
214
46
1
0
/
/
mulps
0x12ed6
(
%
rip
)
%
xmm3
#
3d0f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xea4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_1010102_sse2
.
globl
_sk_store_1010102_sse2
FUNCTION
(
_sk_store_1010102_sse2
)
_sk_store_1010102_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
95
200
/
/
maxps
%
xmm0
%
xmm9
.
byte
68
15
40
21
205
44
1
0
/
/
movaps
0x12ccd
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
68
15
40
29
177
49
1
0
/
/
movaps
0x131b1
(
%
rip
)
%
xmm11
#
3d400
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11b4
>
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
10
/
/
pslld
0xa
%
xmm12
.
byte
102
69
15
235
225
/
/
por
%
xmm9
%
xmm12
.
byte
102
69
15
87
201
/
/
xorpd
%
xmm9
%
xmm9
.
byte
68
15
95
202
/
/
maxps
%
xmm2
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
20
/
/
pslld
0x14
%
xmm9
.
byte
102
69
15
235
204
/
/
por
%
xmm12
%
xmm9
.
byte
68
15
95
195
/
/
maxps
%
xmm3
%
xmm8
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
68
15
89
5
103
49
1
0
/
/
mulps
0x13167
(
%
rip
)
%
xmm8
#
3d410
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11c4
>
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
30
/
/
pslld
0x1e
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
10
/
/
jne
2a2c8
<
_sk_store_1010102_sse2
+
0xaa
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
29
/
/
je
2a2ed
<
_sk_store_1010102_sse2
+
0xcf
>
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
17
/
/
je
2a2e5
<
_sk_store_1010102_sse2
+
0xc7
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
117
236
/
/
jne
2a2c4
<
_sk_store_1010102_sse2
+
0xa6
>
.
byte
102
69
15
112
200
78
/
/
pshufd
0x4e
%
xmm8
%
xmm9
.
byte
102
69
15
126
76
144
8
/
/
movd
%
xmm9
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
215
/
/
jmp
2a2c4
<
_sk_store_1010102_sse2
+
0xa6
>
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
207
/
/
jmp
2a2c4
<
_sk_store_1010102_sse2
+
0xa6
>
HIDDEN
_sk_load_f16_sse2
.
globl
_sk_load_f16_sse2
FUNCTION
(
_sk_load_f16_sse2
)
_sk_load_f16_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
47
1
0
0
/
/
jne
2a43e
<
_sk_load_f16_sse2
+
0x149
>
.
byte
102
65
15
16
4
208
/
/
movupd
(
%
r8
%
rdx
8
)
%
xmm0
.
byte
243
65
15
111
76
208
16
/
/
movdqu
0x10
(
%
r8
%
rdx
8
)
%
xmm1
.
byte
102
68
15
40
192
/
/
movapd
%
xmm0
%
xmm8
.
byte
102
68
15
97
193
/
/
punpcklwd
%
xmm1
%
xmm8
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
68
15
105
192
/
/
punpckhwd
%
xmm0
%
xmm8
.
byte
102
69
15
239
210
/
/
pxor
%
xmm10
%
xmm10
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
65
15
97
202
/
/
punpcklwd
%
xmm10
%
xmm1
.
byte
102
68
15
111
13
209
48
1
0
/
/
movdqa
0x130d1
(
%
rip
)
%
xmm9
#
3d420
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11d4
>
.
byte
102
68
15
111
225
/
/
movdqa
%
xmm1
%
xmm12
.
byte
102
69
15
219
225
/
/
pand
%
xmm9
%
xmm12
.
byte
102
68
15
111
29
206
48
1
0
/
/
movdqa
0x130ce
(
%
rip
)
%
xmm11
#
3d430
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11e4
>
.
byte
102
65
15
219
203
/
/
pand
%
xmm11
%
xmm1
.
byte
102
15
111
29
209
48
1
0
/
/
movdqa
0x130d1
(
%
rip
)
%
xmm3
#
3d440
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11f4
>
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
102
193
/
/
pcmpgtd
%
xmm1
%
xmm0
.
byte
102
15
114
241
13
/
/
pslld
0xd
%
xmm1
.
byte
102
65
15
235
204
/
/
por
%
xmm12
%
xmm1
.
byte
102
68
15
111
37
192
48
1
0
/
/
movdqa
0x130c0
(
%
rip
)
%
xmm12
#
3d450
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1204
>
.
byte
102
65
15
254
204
/
/
paddd
%
xmm12
%
xmm1
.
byte
102
15
223
193
/
/
pandn
%
xmm1
%
xmm0
.
byte
102
65
15
105
210
/
/
punpckhwd
%
xmm10
%
xmm2
.
byte
102
68
15
111
234
/
/
movdqa
%
xmm2
%
xmm13
.
byte
102
69
15
219
233
/
/
pand
%
xmm9
%
xmm13
.
byte
102
65
15
219
211
/
/
pand
%
xmm11
%
xmm2
.
byte
102
65
15
114
245
16
/
/
pslld
0x10
%
xmm13
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
102
202
/
/
pcmpgtd
%
xmm2
%
xmm1
.
byte
102
15
114
242
13
/
/
pslld
0xd
%
xmm2
.
byte
102
65
15
235
213
/
/
por
%
xmm13
%
xmm2
.
byte
102
65
15
254
212
/
/
paddd
%
xmm12
%
xmm2
.
byte
102
15
223
202
/
/
pandn
%
xmm2
%
xmm1
.
byte
102
69
15
111
232
/
/
movdqa
%
xmm8
%
xmm13
.
byte
102
69
15
97
234
/
/
punpcklwd
%
xmm10
%
xmm13
.
byte
102
69
15
111
245
/
/
movdqa
%
xmm13
%
xmm14
.
byte
102
69
15
219
241
/
/
pand
%
xmm9
%
xmm14
.
byte
102
69
15
219
235
/
/
pand
%
xmm11
%
xmm13
.
byte
102
65
15
114
246
16
/
/
pslld
0x10
%
xmm14
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
65
15
102
213
/
/
pcmpgtd
%
xmm13
%
xmm2
.
byte
102
65
15
114
245
13
/
/
pslld
0xd
%
xmm13
.
byte
102
69
15
235
238
/
/
por
%
xmm14
%
xmm13
.
byte
102
69
15
254
236
/
/
paddd
%
xmm12
%
xmm13
.
byte
102
65
15
223
213
/
/
pandn
%
xmm13
%
xmm2
.
byte
102
69
15
105
194
/
/
punpckhwd
%
xmm10
%
xmm8
.
byte
102
69
15
219
200
/
/
pand
%
xmm8
%
xmm9
.
byte
102
69
15
219
195
/
/
pand
%
xmm11
%
xmm8
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
102
65
15
102
216
/
/
pcmpgtd
%
xmm8
%
xmm3
.
byte
102
65
15
114
240
13
/
/
pslld
0xd
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
102
69
15
254
196
/
/
paddd
%
xmm12
%
xmm8
.
byte
102
65
15
223
216
/
/
pandn
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
65
15
16
4
208
/
/
movsd
(
%
r8
%
rdx
8
)
%
xmm0
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
2a457
<
_sk_load_f16_sse2
+
0x162
>
.
byte
243
15
126
192
/
/
movq
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
197
254
255
255
/
/
jmpq
2a31c
<
_sk_load_f16_sse2
+
0x27
>
.
byte
102
65
15
22
68
208
8
/
/
movhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
176
254
255
255
/
/
jb
2a31c
<
_sk_load_f16_sse2
+
0x27
>
.
byte
243
65
15
126
76
208
16
/
/
movq
0x10
(
%
r8
%
rdx
8
)
%
xmm1
.
byte
233
164
254
255
255
/
/
jmpq
2a31c
<
_sk_load_f16_sse2
+
0x27
>
HIDDEN
_sk_load_f16_dst_sse2
.
globl
_sk_load_f16_dst_sse2
FUNCTION
(
_sk_load_f16_dst_sse2
)
_sk_load_f16_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
47
1
0
0
/
/
jne
2a5c1
<
_sk_load_f16_dst_sse2
+
0x149
>
.
byte
102
65
15
16
36
208
/
/
movupd
(
%
r8
%
rdx
8
)
%
xmm4
.
byte
243
65
15
111
108
208
16
/
/
movdqu
0x10
(
%
r8
%
rdx
8
)
%
xmm5
.
byte
102
68
15
40
196
/
/
movapd
%
xmm4
%
xmm8
.
byte
102
68
15
97
197
/
/
punpcklwd
%
xmm5
%
xmm8
.
byte
102
15
105
229
/
/
punpckhwd
%
xmm5
%
xmm4
.
byte
102
65
15
111
240
/
/
movdqa
%
xmm8
%
xmm6
.
byte
102
15
97
244
/
/
punpcklwd
%
xmm4
%
xmm6
.
byte
102
68
15
105
196
/
/
punpckhwd
%
xmm4
%
xmm8
.
byte
102
69
15
239
210
/
/
pxor
%
xmm10
%
xmm10
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
65
15
97
234
/
/
punpcklwd
%
xmm10
%
xmm5
.
byte
102
68
15
111
13
78
47
1
0
/
/
movdqa
0x12f4e
(
%
rip
)
%
xmm9
#
3d420
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11d4
>
.
byte
102
68
15
111
229
/
/
movdqa
%
xmm5
%
xmm12
.
byte
102
69
15
219
225
/
/
pand
%
xmm9
%
xmm12
.
byte
102
68
15
111
29
75
47
1
0
/
/
movdqa
0x12f4b
(
%
rip
)
%
xmm11
#
3d430
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11e4
>
.
byte
102
65
15
219
235
/
/
pand
%
xmm11
%
xmm5
.
byte
102
15
111
61
78
47
1
0
/
/
movdqa
0x12f4e
(
%
rip
)
%
xmm7
#
3d440
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11f4
>
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
102
229
/
/
pcmpgtd
%
xmm5
%
xmm4
.
byte
102
15
114
245
13
/
/
pslld
0xd
%
xmm5
.
byte
102
65
15
235
236
/
/
por
%
xmm12
%
xmm5
.
byte
102
68
15
111
37
61
47
1
0
/
/
movdqa
0x12f3d
(
%
rip
)
%
xmm12
#
3d450
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1204
>
.
byte
102
65
15
254
236
/
/
paddd
%
xmm12
%
xmm5
.
byte
102
15
223
229
/
/
pandn
%
xmm5
%
xmm4
.
byte
102
65
15
105
242
/
/
punpckhwd
%
xmm10
%
xmm6
.
byte
102
68
15
111
238
/
/
movdqa
%
xmm6
%
xmm13
.
byte
102
69
15
219
233
/
/
pand
%
xmm9
%
xmm13
.
byte
102
65
15
219
243
/
/
pand
%
xmm11
%
xmm6
.
byte
102
65
15
114
245
16
/
/
pslld
0x10
%
xmm13
.
byte
102
15
111
239
/
/
movdqa
%
xmm7
%
xmm5
.
byte
102
15
102
238
/
/
pcmpgtd
%
xmm6
%
xmm5
.
byte
102
15
114
246
13
/
/
pslld
0xd
%
xmm6
.
byte
102
65
15
235
245
/
/
por
%
xmm13
%
xmm6
.
byte
102
65
15
254
244
/
/
paddd
%
xmm12
%
xmm6
.
byte
102
15
223
238
/
/
pandn
%
xmm6
%
xmm5
.
byte
102
69
15
111
232
/
/
movdqa
%
xmm8
%
xmm13
.
byte
102
69
15
97
234
/
/
punpcklwd
%
xmm10
%
xmm13
.
byte
102
69
15
111
245
/
/
movdqa
%
xmm13
%
xmm14
.
byte
102
69
15
219
241
/
/
pand
%
xmm9
%
xmm14
.
byte
102
69
15
219
235
/
/
pand
%
xmm11
%
xmm13
.
byte
102
65
15
114
246
16
/
/
pslld
0x10
%
xmm14
.
byte
102
15
111
247
/
/
movdqa
%
xmm7
%
xmm6
.
byte
102
65
15
102
245
/
/
pcmpgtd
%
xmm13
%
xmm6
.
byte
102
65
15
114
245
13
/
/
pslld
0xd
%
xmm13
.
byte
102
69
15
235
238
/
/
por
%
xmm14
%
xmm13
.
byte
102
69
15
254
236
/
/
paddd
%
xmm12
%
xmm13
.
byte
102
65
15
223
245
/
/
pandn
%
xmm13
%
xmm6
.
byte
102
69
15
105
194
/
/
punpckhwd
%
xmm10
%
xmm8
.
byte
102
69
15
219
200
/
/
pand
%
xmm8
%
xmm9
.
byte
102
69
15
219
195
/
/
pand
%
xmm11
%
xmm8
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
102
65
15
102
248
/
/
pcmpgtd
%
xmm8
%
xmm7
.
byte
102
65
15
114
240
13
/
/
pslld
0xd
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
102
69
15
254
196
/
/
paddd
%
xmm12
%
xmm8
.
byte
102
65
15
223
248
/
/
pandn
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
65
15
16
36
208
/
/
movsd
(
%
r8
%
rdx
8
)
%
xmm4
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
2a5da
<
_sk_load_f16_dst_sse2
+
0x162
>
.
byte
243
15
126
228
/
/
movq
%
xmm4
%
xmm4
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
233
197
254
255
255
/
/
jmpq
2a49f
<
_sk_load_f16_dst_sse2
+
0x27
>
.
byte
102
65
15
22
100
208
8
/
/
movhpd
0x8
(
%
r8
%
rdx
8
)
%
xmm4
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
176
254
255
255
/
/
jb
2a49f
<
_sk_load_f16_dst_sse2
+
0x27
>
.
byte
243
65
15
126
108
208
16
/
/
movq
0x10
(
%
r8
%
rdx
8
)
%
xmm5
.
byte
233
164
254
255
255
/
/
jmpq
2a49f
<
_sk_load_f16_dst_sse2
+
0x27
>
HIDDEN
_sk_gather_f16_sse2
.
globl
_sk_gather_f16_sse2
FUNCTION
(
_sk_gather_f16_sse2
)
_sk_gather_f16_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
65
15
254
209
/
/
paddd
%
xmm9
%
xmm2
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
216
/
/
maxps
%
xmm0
%
xmm3
.
byte
15
93
218
/
/
minps
%
xmm2
%
xmm3
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
65
15
254
193
/
/
paddd
%
xmm9
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
243
67
15
126
4
208
/
/
movq
(
%
r8
%
r10
8
)
%
xmm0
.
byte
243
67
15
126
12
216
/
/
movq
(
%
r8
%
r11
8
)
%
xmm1
.
byte
102
15
108
200
/
/
punpcklqdq
%
xmm0
%
xmm1
.
byte
243
65
15
126
4
192
/
/
movq
(
%
r8
%
rax
8
)
%
xmm0
.
byte
243
67
15
126
20
200
/
/
movq
(
%
r8
%
r9
8
)
%
xmm2
.
byte
102
15
108
208
/
/
punpcklqdq
%
xmm0
%
xmm2
.
byte
102
68
15
111
201
/
/
movdqa
%
xmm1
%
xmm9
.
byte
102
68
15
97
202
/
/
punpcklwd
%
xmm2
%
xmm9
.
byte
102
15
105
202
/
/
punpckhwd
%
xmm2
%
xmm1
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
68
15
105
201
/
/
punpckhwd
%
xmm1
%
xmm9
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
65
15
97
200
/
/
punpcklwd
%
xmm8
%
xmm1
.
byte
102
68
15
111
21
76
45
1
0
/
/
movdqa
0x12d4c
(
%
rip
)
%
xmm10
#
3d420
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11d4
>
.
byte
102
68
15
111
225
/
/
movdqa
%
xmm1
%
xmm12
.
byte
102
69
15
219
226
/
/
pand
%
xmm10
%
xmm12
.
byte
102
68
15
111
29
73
45
1
0
/
/
movdqa
0x12d49
(
%
rip
)
%
xmm11
#
3d430
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11e4
>
.
byte
102
65
15
219
203
/
/
pand
%
xmm11
%
xmm1
.
byte
102
15
111
29
76
45
1
0
/
/
movdqa
0x12d4c
(
%
rip
)
%
xmm3
#
3d440
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x11f4
>
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
102
193
/
/
pcmpgtd
%
xmm1
%
xmm0
.
byte
102
15
114
241
13
/
/
pslld
0xd
%
xmm1
.
byte
102
65
15
235
204
/
/
por
%
xmm12
%
xmm1
.
byte
102
68
15
111
37
59
45
1
0
/
/
movdqa
0x12d3b
(
%
rip
)
%
xmm12
#
3d450
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1204
>
.
byte
102
65
15
254
204
/
/
paddd
%
xmm12
%
xmm1
.
byte
102
15
223
193
/
/
pandn
%
xmm1
%
xmm0
.
byte
102
65
15
105
208
/
/
punpckhwd
%
xmm8
%
xmm2
.
byte
102
68
15
111
234
/
/
movdqa
%
xmm2
%
xmm13
.
byte
102
69
15
219
234
/
/
pand
%
xmm10
%
xmm13
.
byte
102
65
15
219
211
/
/
pand
%
xmm11
%
xmm2
.
byte
102
65
15
114
245
16
/
/
pslld
0x10
%
xmm13
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
102
202
/
/
pcmpgtd
%
xmm2
%
xmm1
.
byte
102
15
114
242
13
/
/
pslld
0xd
%
xmm2
.
byte
102
65
15
235
213
/
/
por
%
xmm13
%
xmm2
.
byte
102
65
15
254
212
/
/
paddd
%
xmm12
%
xmm2
.
byte
102
15
223
202
/
/
pandn
%
xmm2
%
xmm1
.
byte
102
69
15
111
233
/
/
movdqa
%
xmm9
%
xmm13
.
byte
102
69
15
97
232
/
/
punpcklwd
%
xmm8
%
xmm13
.
byte
102
69
15
111
245
/
/
movdqa
%
xmm13
%
xmm14
.
byte
102
69
15
219
242
/
/
pand
%
xmm10
%
xmm14
.
byte
102
69
15
219
235
/
/
pand
%
xmm11
%
xmm13
.
byte
102
65
15
114
246
16
/
/
pslld
0x10
%
xmm14
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
65
15
102
213
/
/
pcmpgtd
%
xmm13
%
xmm2
.
byte
102
65
15
114
245
13
/
/
pslld
0xd
%
xmm13
.
byte
102
69
15
235
238
/
/
por
%
xmm14
%
xmm13
.
byte
102
69
15
254
236
/
/
paddd
%
xmm12
%
xmm13
.
byte
102
65
15
223
213
/
/
pandn
%
xmm13
%
xmm2
.
byte
102
69
15
105
200
/
/
punpckhwd
%
xmm8
%
xmm9
.
byte
102
69
15
219
209
/
/
pand
%
xmm9
%
xmm10
.
byte
102
69
15
219
203
/
/
pand
%
xmm11
%
xmm9
.
byte
102
65
15
114
242
16
/
/
pslld
0x10
%
xmm10
.
byte
102
65
15
102
217
/
/
pcmpgtd
%
xmm9
%
xmm3
.
byte
102
65
15
114
241
13
/
/
pslld
0xd
%
xmm9
.
byte
102
69
15
235
202
/
/
por
%
xmm10
%
xmm9
.
byte
102
69
15
254
204
/
/
paddd
%
xmm12
%
xmm9
.
byte
102
65
15
223
217
/
/
pandn
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_store_f16_sse2
.
globl
_sk_store_f16_sse2
FUNCTION
(
_sk_store_f16_sse2
)
_sk_store_f16_sse2
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
102
68
15
111
29
143
44
1
0
/
/
movdqa
0x12c8f
(
%
rip
)
%
xmm11
#
3d460
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1214
>
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
102
69
15
219
195
/
/
pand
%
xmm11
%
xmm8
.
byte
102
68
15
111
21
140
44
1
0
/
/
movdqa
0x12c8c
(
%
rip
)
%
xmm10
#
3d470
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1224
>
.
byte
102
68
15
111
240
/
/
movdqa
%
xmm0
%
xmm14
.
byte
102
69
15
219
242
/
/
pand
%
xmm10
%
xmm14
.
byte
102
15
111
61
138
44
1
0
/
/
movdqa
0x12c8a
(
%
rip
)
%
xmm7
#
3d480
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1234
>
.
byte
102
68
15
111
37
17
46
1
0
/
/
movdqa
0x12e11
(
%
rip
)
%
xmm12
#
3d610
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13c4
>
.
byte
102
68
15
111
248
/
/
movdqa
%
xmm0
%
xmm15
.
byte
102
65
15
114
247
3
/
/
pslld
0x3
%
xmm15
.
byte
102
69
15
219
252
/
/
pand
%
xmm12
%
xmm15
.
byte
102
69
15
254
248
/
/
paddd
%
xmm8
%
xmm15
.
byte
102
68
15
111
45
163
40
1
0
/
/
movdqa
0x128a3
(
%
rip
)
%
xmm13
#
3d0c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe74
>
.
byte
102
69
15
254
253
/
/
paddd
%
xmm13
%
xmm15
.
byte
102
65
15
114
231
16
/
/
psrad
0x10
%
xmm15
.
byte
102
68
15
111
199
/
/
movdqa
%
xmm7
%
xmm8
.
byte
102
69
15
102
198
/
/
pcmpgtd
%
xmm14
%
xmm8
.
byte
102
69
15
223
199
/
/
pandn
%
xmm15
%
xmm8
.
byte
102
68
15
111
241
/
/
movdqa
%
xmm1
%
xmm14
.
byte
102
69
15
219
243
/
/
pand
%
xmm11
%
xmm14
.
byte
102
68
15
111
249
/
/
movdqa
%
xmm1
%
xmm15
.
byte
102
65
15
114
247
3
/
/
pslld
0x3
%
xmm15
.
byte
102
69
15
219
252
/
/
pand
%
xmm12
%
xmm15
.
byte
102
69
15
254
254
/
/
paddd
%
xmm14
%
xmm15
.
byte
102
68
15
111
241
/
/
movdqa
%
xmm1
%
xmm14
.
byte
102
69
15
219
242
/
/
pand
%
xmm10
%
xmm14
.
byte
102
68
15
111
207
/
/
movdqa
%
xmm7
%
xmm9
.
byte
102
69
15
102
206
/
/
pcmpgtd
%
xmm14
%
xmm9
.
byte
102
68
15
107
192
/
/
packssdw
%
xmm0
%
xmm8
.
byte
102
69
15
254
253
/
/
paddd
%
xmm13
%
xmm15
.
byte
102
65
15
114
231
16
/
/
psrad
0x10
%
xmm15
.
byte
102
69
15
223
207
/
/
pandn
%
xmm15
%
xmm9
.
byte
102
68
15
107
200
/
/
packssdw
%
xmm0
%
xmm9
.
byte
102
69
15
97
193
/
/
punpcklwd
%
xmm9
%
xmm8
.
byte
102
68
15
111
202
/
/
movdqa
%
xmm2
%
xmm9
.
byte
102
69
15
219
203
/
/
pand
%
xmm11
%
xmm9
.
byte
102
68
15
111
250
/
/
movdqa
%
xmm2
%
xmm15
.
byte
102
65
15
114
247
3
/
/
pslld
0x3
%
xmm15
.
byte
102
69
15
219
252
/
/
pand
%
xmm12
%
xmm15
.
byte
102
69
15
254
249
/
/
paddd
%
xmm9
%
xmm15
.
byte
102
68
15
111
202
/
/
movdqa
%
xmm2
%
xmm9
.
byte
102
69
15
219
202
/
/
pand
%
xmm10
%
xmm9
.
byte
102
68
15
111
247
/
/
movdqa
%
xmm7
%
xmm14
.
byte
102
69
15
102
241
/
/
pcmpgtd
%
xmm9
%
xmm14
.
byte
102
69
15
254
253
/
/
paddd
%
xmm13
%
xmm15
.
byte
102
65
15
114
231
16
/
/
psrad
0x10
%
xmm15
.
byte
102
69
15
223
247
/
/
pandn
%
xmm15
%
xmm14
.
byte
102
68
15
111
203
/
/
movdqa
%
xmm3
%
xmm9
.
byte
102
65
15
114
241
3
/
/
pslld
0x3
%
xmm9
.
byte
102
69
15
219
204
/
/
pand
%
xmm12
%
xmm9
.
byte
102
68
15
219
219
/
/
pand
%
xmm3
%
xmm11
.
byte
102
69
15
254
203
/
/
paddd
%
xmm11
%
xmm9
.
byte
102
69
15
254
205
/
/
paddd
%
xmm13
%
xmm9
.
byte
102
68
15
219
211
/
/
pand
%
xmm3
%
xmm10
.
byte
102
65
15
102
250
/
/
pcmpgtd
%
xmm10
%
xmm7
.
byte
102
65
15
114
225
16
/
/
psrad
0x10
%
xmm9
.
byte
102
65
15
223
249
/
/
pandn
%
xmm9
%
xmm7
.
byte
102
68
15
107
240
/
/
packssdw
%
xmm0
%
xmm14
.
byte
102
15
107
248
/
/
packssdw
%
xmm0
%
xmm7
.
byte
102
68
15
97
247
/
/
punpcklwd
%
xmm7
%
xmm14
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
3
/
/
shl
0x3
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
37
/
/
jne
2a949
<
_sk_store_f16_sse2
+
0x186
>
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
65
15
98
254
/
/
punpckldq
%
xmm14
%
xmm7
.
byte
243
65
15
127
60
208
/
/
movdqu
%
xmm7
(
%
r8
%
rdx
8
)
.
byte
102
69
15
106
198
/
/
punpckhdq
%
xmm14
%
xmm8
.
byte
243
69
15
127
68
208
16
/
/
movdqu
%
xmm8
0x10
(
%
r8
%
rdx
8
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
69
15
98
206
/
/
punpckldq
%
xmm14
%
xmm9
.
byte
102
69
15
214
12
208
/
/
movq
%
xmm9
(
%
r8
%
rdx
8
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
225
/
/
je
2a940
<
_sk_store_f16_sse2
+
0x17d
>
.
byte
102
69
15
23
76
208
8
/
/
movhpd
%
xmm9
0x8
(
%
r8
%
rdx
8
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
212
/
/
jb
2a940
<
_sk_store_f16_sse2
+
0x17d
>
.
byte
102
69
15
106
198
/
/
punpckhdq
%
xmm14
%
xmm8
.
byte
102
69
15
214
68
208
16
/
/
movq
%
xmm8
0x10
(
%
r8
%
rdx
8
)
.
byte
235
198
/
/
jmp
2a940
<
_sk_store_f16_sse2
+
0x17d
>
HIDDEN
_sk_load_u16_be_sse2
.
globl
_sk_load_u16_be_sse2
FUNCTION
(
_sk_load_u16_be_sse2
)
_sk_load_u16_be_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
190
0
0
0
/
/
jne
2aa59
<
_sk_load_u16_be_sse2
+
0xdf
>
.
byte
102
67
15
16
4
65
/
/
movupd
(
%
r9
%
r8
2
)
%
xmm0
.
byte
243
67
15
111
76
65
16
/
/
movdqu
0x10
(
%
r9
%
r8
2
)
%
xmm1
.
byte
102
15
40
208
/
/
movapd
%
xmm0
%
xmm2
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
65
15
97
201
/
/
punpcklwd
%
xmm9
%
xmm1
.
byte
15
91
193
/
/
cvtdq2ps
%
xmm1
%
xmm0
.
byte
68
15
40
5
196
39
1
0
/
/
movaps
0x127c4
(
%
rip
)
%
xmm8
#
3d1b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf64
>
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
235
217
/
/
por
%
xmm1
%
xmm3
.
byte
102
65
15
97
217
/
/
punpcklwd
%
xmm9
%
xmm3
.
byte
15
91
203
/
/
cvtdq2ps
%
xmm3
%
xmm1
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
102
68
15
111
210
/
/
movdqa
%
xmm2
%
xmm10
.
byte
102
65
15
113
242
8
/
/
psllw
0x8
%
xmm10
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
65
15
235
210
/
/
por
%
xmm10
%
xmm2
.
byte
102
65
15
97
209
/
/
punpcklwd
%
xmm9
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
102
68
15
111
211
/
/
movdqa
%
xmm3
%
xmm10
.
byte
102
65
15
113
242
8
/
/
psllw
0x8
%
xmm10
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
65
15
235
218
/
/
por
%
xmm10
%
xmm3
.
byte
102
65
15
97
217
/
/
punpcklwd
%
xmm9
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
242
67
15
16
4
65
/
/
movsd
(
%
r9
%
r8
2
)
%
xmm0
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
2aa72
<
_sk_load_u16_be_sse2
+
0xf8
>
.
byte
243
15
126
192
/
/
movq
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
54
255
255
255
/
/
jmpq
2a9a8
<
_sk_load_u16_be_sse2
+
0x2e
>
.
byte
102
67
15
22
68
65
8
/
/
movhpd
0x8
(
%
r9
%
r8
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
15
130
33
255
255
255
/
/
jb
2a9a8
<
_sk_load_u16_be_sse2
+
0x2e
>
.
byte
243
67
15
126
76
65
16
/
/
movq
0x10
(
%
r9
%
r8
2
)
%
xmm1
.
byte
233
21
255
255
255
/
/
jmpq
2a9a8
<
_sk_load_u16_be_sse2
+
0x2e
>
HIDDEN
_sk_load_rgb_u16_be_sse2
.
globl
_sk_load_rgb_u16_be_sse2
FUNCTION
(
_sk_load_rgb_u16_be_sse2
)
_sk_load_rgb_u16_be_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
82
/
/
lea
(
%
rdx
%
rdx
2
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
15
133
175
0
0
0
/
/
jne
2ab5f
<
_sk_load_rgb_u16_be_sse2
+
0xcc
>
.
byte
243
67
15
111
20
65
/
/
movdqu
(
%
r9
%
r8
2
)
%
xmm2
.
byte
243
67
15
111
92
65
8
/
/
movdqu
0x8
(
%
r9
%
r8
2
)
%
xmm3
.
byte
102
15
115
219
4
/
/
psrldq
0x4
%
xmm3
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
115
216
6
/
/
psrldq
0x6
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
115
217
6
/
/
psrldq
0x6
%
xmm1
.
byte
102
15
97
193
/
/
punpcklwd
%
xmm1
%
xmm0
.
byte
102
15
97
211
/
/
punpcklwd
%
xmm3
%
xmm2
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
65
15
97
200
/
/
punpcklwd
%
xmm8
%
xmm1
.
byte
15
91
193
/
/
cvtdq2ps
%
xmm1
%
xmm0
.
byte
68
15
40
13
156
38
1
0
/
/
movaps
0x1269c
(
%
rip
)
%
xmm9
#
3d1b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xf64
>
.
byte
65
15
89
193
/
/
mulps
%
xmm9
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
235
217
/
/
por
%
xmm1
%
xmm3
.
byte
102
65
15
97
216
/
/
punpcklwd
%
xmm8
%
xmm3
.
byte
15
91
203
/
/
cvtdq2ps
%
xmm3
%
xmm1
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
65
15
97
208
/
/
punpcklwd
%
xmm8
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
179
35
1
0
/
/
movaps
0x123b3
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
67
15
110
20
65
/
/
movd
(
%
r9
%
r8
2
)
%
xmm2
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
67
15
196
84
65
4
2
/
/
pinsrw
0x2
0x4
(
%
r9
%
r8
2
)
%
xmm2
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
13
/
/
jne
2ab84
<
_sk_load_rgb_u16_be_sse2
+
0xf1
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
233
80
255
255
255
/
/
jmpq
2aad4
<
_sk_load_rgb_u16_be_sse2
+
0x41
>
.
byte
102
67
15
110
68
65
6
/
/
movd
0x6
(
%
r9
%
r8
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
67
15
196
68
65
10
2
/
/
pinsrw
0x2
0xa
(
%
r9
%
r8
2
)
%
xmm0
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
24
/
/
jb
2abb5
<
_sk_load_rgb_u16_be_sse2
+
0x122
>
.
byte
102
67
15
110
92
65
12
/
/
movd
0xc
(
%
r9
%
r8
2
)
%
xmm3
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
67
15
196
92
65
16
2
/
/
pinsrw
0x2
0x10
(
%
r9
%
r8
2
)
%
xmm3
.
byte
233
31
255
255
255
/
/
jmpq
2aad4
<
_sk_load_rgb_u16_be_sse2
+
0x41
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
233
22
255
255
255
/
/
jmpq
2aad4
<
_sk_load_rgb_u16_be_sse2
+
0x41
>
HIDDEN
_sk_store_u16_be_sse2
.
globl
_sk_store_u16_be_sse2
FUNCTION
(
_sk_store_u16_be_sse2
)
_sk_store_u16_be_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
77
1
201
/
/
add
%
r9
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
68
15
40
21
38
35
1
0
/
/
movaps
0x12326
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
93
194
/
/
minps
%
xmm10
%
xmm8
.
byte
68
15
40
29
170
40
1
0
/
/
movaps
0x128aa
(
%
rip
)
%
xmm11
#
3d4a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1254
>
.
byte
69
15
89
195
/
/
mulps
%
xmm11
%
xmm8
.
byte
102
69
15
91
192
/
/
cvtps2dq
%
xmm8
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
68
15
107
192
/
/
packssdw
%
xmm0
%
xmm8
.
byte
102
69
15
111
224
/
/
movdqa
%
xmm8
%
xmm12
.
byte
102
65
15
113
244
8
/
/
psllw
0x8
%
xmm12
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
68
15
95
225
/
/
maxps
%
xmm1
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
65
15
114
228
16
/
/
psrad
0x10
%
xmm12
.
byte
102
68
15
107
224
/
/
packssdw
%
xmm0
%
xmm12
.
byte
102
69
15
111
236
/
/
movdqa
%
xmm12
%
xmm13
.
byte
102
65
15
113
245
8
/
/
psllw
0x8
%
xmm13
.
byte
102
65
15
113
212
8
/
/
psrlw
0x8
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
102
69
15
97
196
/
/
punpcklwd
%
xmm12
%
xmm8
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
68
15
95
226
/
/
maxps
%
xmm2
%
xmm12
.
byte
69
15
93
226
/
/
minps
%
xmm10
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
102
69
15
91
228
/
/
cvtps2dq
%
xmm12
%
xmm12
.
byte
102
65
15
114
244
16
/
/
pslld
0x10
%
xmm12
.
byte
102
65
15
114
228
16
/
/
psrad
0x10
%
xmm12
.
byte
102
68
15
107
224
/
/
packssdw
%
xmm0
%
xmm12
.
byte
102
69
15
111
236
/
/
movdqa
%
xmm12
%
xmm13
.
byte
102
65
15
113
245
8
/
/
psllw
0x8
%
xmm13
.
byte
102
65
15
113
212
8
/
/
psrlw
0x8
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
68
15
95
203
/
/
maxps
%
xmm3
%
xmm9
.
byte
69
15
93
202
/
/
minps
%
xmm10
%
xmm9
.
byte
69
15
89
203
/
/
mulps
%
xmm11
%
xmm9
.
byte
102
69
15
91
201
/
/
cvtps2dq
%
xmm9
%
xmm9
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
102
65
15
114
225
16
/
/
psrad
0x10
%
xmm9
.
byte
102
68
15
107
200
/
/
packssdw
%
xmm0
%
xmm9
.
byte
102
69
15
111
209
/
/
movdqa
%
xmm9
%
xmm10
.
byte
102
65
15
113
242
8
/
/
psllw
0x8
%
xmm10
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
69
15
235
202
/
/
por
%
xmm10
%
xmm9
.
byte
102
69
15
97
225
/
/
punpcklwd
%
xmm9
%
xmm12
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
69
15
98
204
/
/
punpckldq
%
xmm12
%
xmm9
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
21
/
/
jne
2ad06
<
_sk_store_u16_be_sse2
+
0x148
>
.
byte
71
15
17
12
65
/
/
movups
%
xmm9
(
%
r9
%
r8
2
)
.
byte
102
69
15
106
196
/
/
punpckhdq
%
xmm12
%
xmm8
.
byte
243
71
15
127
68
65
16
/
/
movdqu
%
xmm8
0x10
(
%
r9
%
r8
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
71
15
214
12
65
/
/
movq
%
xmm9
(
%
r9
%
r8
2
)
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
240
/
/
je
2ad02
<
_sk_store_u16_be_sse2
+
0x144
>
.
byte
102
71
15
23
76
65
8
/
/
movhpd
%
xmm9
0x8
(
%
r9
%
r8
2
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
227
/
/
jb
2ad02
<
_sk_store_u16_be_sse2
+
0x144
>
.
byte
102
69
15
106
196
/
/
punpckhdq
%
xmm12
%
xmm8
.
byte
102
71
15
214
68
65
16
/
/
movq
%
xmm8
0x10
(
%
r9
%
r8
2
)
.
byte
235
213
/
/
jmp
2ad02
<
_sk_store_u16_be_sse2
+
0x144
>
HIDDEN
_sk_load_f32_sse2
.
globl
_sk_load_f32_sse2
FUNCTION
(
_sk_load_f32_sse2
)
_sk_load_f32_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
137
208
/
/
mov
%
rdx
%
rax
.
byte
72
193
224
4
/
/
shl
0x4
%
rax
.
byte
70
15
16
4
8
/
/
movups
(
%
rax
%
r9
1
)
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
66
/
/
jne
2ad99
<
_sk_load_f32_sse2
+
0x6c
>
.
byte
67
15
16
68
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm0
.
byte
67
15
16
92
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm3
.
byte
71
15
16
76
129
48
/
/
movups
0x30
(
%
r9
%
r8
4
)
%
xmm9
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
15
20
208
/
/
unpcklps
%
xmm0
%
xmm2
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
65
15
20
201
/
/
unpcklps
%
xmm9
%
xmm1
.
byte
68
15
21
192
/
/
unpckhps
%
xmm0
%
xmm8
.
byte
65
15
21
217
/
/
unpckhps
%
xmm9
%
xmm3
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
15
18
202
/
/
movhlps
%
xmm2
%
xmm1
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
65
15
18
216
/
/
movhlps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
8
/
/
jne
2adab
<
_sk_load_f32_sse2
+
0x7e
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
235
190
/
/
jmp
2ad69
<
_sk_load_f32_sse2
+
0x3c
>
.
byte
67
15
16
68
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm0
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
8
/
/
jb
2adbf
<
_sk_load_f32_sse2
+
0x92
>
.
byte
67
15
16
92
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm3
.
byte
235
170
/
/
jmp
2ad69
<
_sk_load_f32_sse2
+
0x3c
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
235
165
/
/
jmp
2ad69
<
_sk_load_f32_sse2
+
0x3c
>
HIDDEN
_sk_load_f32_dst_sse2
.
globl
_sk_load_f32_dst_sse2
FUNCTION
(
_sk_load_f32_dst_sse2
)
_sk_load_f32_dst_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
137
208
/
/
mov
%
rdx
%
rax
.
byte
72
193
224
4
/
/
shl
0x4
%
rax
.
byte
70
15
16
4
8
/
/
movups
(
%
rax
%
r9
1
)
%
xmm8
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
66
/
/
jne
2ae30
<
_sk_load_f32_dst_sse2
+
0x6c
>
.
byte
67
15
16
100
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm4
.
byte
67
15
16
124
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm7
.
byte
71
15
16
76
129
48
/
/
movups
0x30
(
%
r9
%
r8
4
)
%
xmm9
.
byte
65
15
40
240
/
/
movaps
%
xmm8
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
15
40
239
/
/
movaps
%
xmm7
%
xmm5
.
byte
65
15
20
233
/
/
unpcklps
%
xmm9
%
xmm5
.
byte
68
15
21
196
/
/
unpckhps
%
xmm4
%
xmm8
.
byte
65
15
21
249
/
/
unpckhps
%
xmm9
%
xmm7
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
102
15
20
229
/
/
unpcklpd
%
xmm5
%
xmm4
.
byte
15
18
238
/
/
movhlps
%
xmm6
%
xmm5
.
byte
65
15
40
240
/
/
movaps
%
xmm8
%
xmm6
.
byte
102
15
20
247
/
/
unpcklpd
%
xmm7
%
xmm6
.
byte
65
15
18
248
/
/
movhlps
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
117
8
/
/
jne
2ae42
<
_sk_load_f32_dst_sse2
+
0x7e
>
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
235
190
/
/
jmp
2ae00
<
_sk_load_f32_dst_sse2
+
0x3c
>
.
byte
67
15
16
100
129
16
/
/
movups
0x10
(
%
r9
%
r8
4
)
%
xmm4
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
8
/
/
jb
2ae56
<
_sk_load_f32_dst_sse2
+
0x92
>
.
byte
67
15
16
124
129
32
/
/
movups
0x20
(
%
r9
%
r8
4
)
%
xmm7
.
byte
235
170
/
/
jmp
2ae00
<
_sk_load_f32_dst_sse2
+
0x3c
>
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
235
165
/
/
jmp
2ae00
<
_sk_load_f32_dst_sse2
+
0x3c
>
HIDDEN
_sk_store_f32_sse2
.
globl
_sk_store_f32_sse2
FUNCTION
(
_sk_store_f32_sse2
)
_sk_store_f32_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
141
4
149
0
0
0
0
/
/
lea
0x0
(
%
rdx
4
)
%
r8
.
byte
76
99
72
8
/
/
movslq
0x8
(
%
rax
)
%
r9
.
byte
76
15
175
201
/
/
imul
%
rcx
%
r9
.
byte
73
193
225
2
/
/
shl
0x2
%
r9
.
byte
76
3
8
/
/
add
(
%
rax
)
%
r9
.
byte
72
137
208
/
/
mov
%
rdx
%
rax
.
byte
72
193
224
4
/
/
shl
0x4
%
rax
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
20
201
/
/
unpcklps
%
xmm1
%
xmm9
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
68
15
20
195
/
/
unpcklps
%
xmm3
%
xmm8
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
68
15
21
209
/
/
unpckhps
%
xmm1
%
xmm10
.
byte
68
15
40
218
/
/
movaps
%
xmm2
%
xmm11
.
byte
68
15
21
219
/
/
unpckhps
%
xmm3
%
xmm11
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
102
69
15
20
224
/
/
unpcklpd
%
xmm8
%
xmm12
.
byte
69
15
18
193
/
/
movhlps
%
xmm9
%
xmm8
.
byte
69
15
40
202
/
/
movaps
%
xmm10
%
xmm9
.
byte
102
69
15
20
203
/
/
unpcklpd
%
xmm11
%
xmm9
.
byte
102
70
15
17
36
8
/
/
movupd
%
xmm12
(
%
rax
%
r9
1
)
.
byte
72
133
255
/
/
test
%
rdi
%
rdi
.
byte
117
29
/
/
jne
2aed9
<
_sk_store_f32_sse2
+
0x7e
>
.
byte
102
69
15
21
211
/
/
unpckhpd
%
xmm11
%
xmm10
.
byte
71
15
17
68
129
16
/
/
movups
%
xmm8
0x10
(
%
r9
%
r8
4
)
.
byte
102
71
15
17
76
129
32
/
/
movupd
%
xmm9
0x20
(
%
r9
%
r8
4
)
.
byte
102
71
15
17
84
129
48
/
/
movupd
%
xmm10
0x30
(
%
r9
%
r8
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
72
131
255
1
/
/
cmp
0x1
%
rdi
.
byte
116
246
/
/
je
2aed5
<
_sk_store_f32_sse2
+
0x7a
>
.
byte
71
15
17
68
129
16
/
/
movups
%
xmm8
0x10
(
%
r9
%
r8
4
)
.
byte
72
131
255
3
/
/
cmp
0x3
%
rdi
.
byte
114
234
/
/
jb
2aed5
<
_sk_store_f32_sse2
+
0x7a
>
.
byte
102
71
15
17
76
129
32
/
/
movupd
%
xmm9
0x20
(
%
r9
%
r8
4
)
.
byte
235
225
/
/
jmp
2aed5
<
_sk_store_f32_sse2
+
0x7a
>
HIDDEN
_sk_repeat_x_sse2
.
globl
_sk_repeat_x_sse2
FUNCTION
(
_sk_repeat_x_sse2
)
_sk_repeat_x_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
243
69
15
91
209
/
/
cvttps2dq
%
xmm9
%
xmm10
.
byte
69
15
91
210
/
/
cvtdq2ps
%
xmm10
%
xmm10
.
byte
69
15
194
202
1
/
/
cmpltps
%
xmm10
%
xmm9
.
byte
68
15
84
13
240
31
1
0
/
/
andps
0x11ff0
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
209
/
/
subps
%
xmm9
%
xmm10
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
69
15
89
194
/
/
mulps
%
xmm10
%
xmm8
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_y_sse2
.
globl
_sk_repeat_y_sse2
FUNCTION
(
_sk_repeat_y_sse2
)
_sk_repeat_y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
243
69
15
91
209
/
/
cvttps2dq
%
xmm9
%
xmm10
.
byte
69
15
91
210
/
/
cvtdq2ps
%
xmm10
%
xmm10
.
byte
69
15
194
202
1
/
/
cmpltps
%
xmm10
%
xmm9
.
byte
68
15
84
13
175
31
1
0
/
/
andps
0x11faf
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
209
/
/
subps
%
xmm9
%
xmm10
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
69
15
89
194
/
/
mulps
%
xmm10
%
xmm8
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_sse2
.
globl
_sk_mirror_x_sse2
FUNCTION
(
_sk_mirror_x_sse2
)
_sk_mirror_x_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
92
194
/
/
subps
%
xmm10
%
xmm0
.
byte
243
69
15
88
192
/
/
addss
%
xmm8
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
89
13
85
21
1
0
/
/
mulss
0x11555
(
%
rip
)
%
xmm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
243
69
15
91
217
/
/
cvttps2dq
%
xmm9
%
xmm11
.
byte
69
15
91
219
/
/
cvtdq2ps
%
xmm11
%
xmm11
.
byte
69
15
194
203
1
/
/
cmpltps
%
xmm11
%
xmm9
.
byte
68
15
84
13
78
31
1
0
/
/
andps
0x11f4e
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
69
15
92
217
/
/
subps
%
xmm9
%
xmm11
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
65
15
92
194
/
/
subps
%
xmm10
%
xmm0
.
byte
68
15
92
224
/
/
subps
%
xmm0
%
xmm12
.
byte
65
15
84
196
/
/
andps
%
xmm12
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_y_sse2
.
globl
_sk_mirror_y_sse2
FUNCTION
(
_sk_mirror_y_sse2
)
_sk_mirror_y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
40
208
/
/
movaps
%
xmm8
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
92
202
/
/
subps
%
xmm10
%
xmm1
.
byte
243
69
15
88
192
/
/
addss
%
xmm8
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
89
13
233
20
1
0
/
/
mulss
0x114e9
(
%
rip
)
%
xmm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
243
69
15
91
217
/
/
cvttps2dq
%
xmm9
%
xmm11
.
byte
69
15
91
219
/
/
cvtdq2ps
%
xmm11
%
xmm11
.
byte
69
15
194
203
1
/
/
cmpltps
%
xmm11
%
xmm9
.
byte
68
15
84
13
226
30
1
0
/
/
andps
0x11ee2
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
69
15
92
217
/
/
subps
%
xmm9
%
xmm11
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
65
15
92
202
/
/
subps
%
xmm10
%
xmm1
.
byte
68
15
92
225
/
/
subps
%
xmm1
%
xmm12
.
byte
65
15
84
204
/
/
andps
%
xmm12
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_x_1_sse2
.
globl
_sk_clamp_x_1_sse2
FUNCTION
(
_sk_clamp_x_1_sse2
)
_sk_clamp_x_1_sse2
:
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
68
15
93
5
178
30
1
0
/
/
minps
0x11eb2
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_sse2
.
globl
_sk_repeat_x_1_sse2
FUNCTION
(
_sk_repeat_x_1_sse2
)
_sk_repeat_x_1_sse2
:
.
byte
243
68
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm8
.
byte
69
15
91
200
/
/
cvtdq2ps
%
xmm8
%
xmm9
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
69
15
194
209
1
/
/
cmpltps
%
xmm9
%
xmm10
.
byte
68
15
40
29
144
30
1
0
/
/
movaps
0x11e90
(
%
rip
)
%
xmm11
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
211
/
/
andps
%
xmm11
%
xmm10
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
92
202
/
/
subps
%
xmm10
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
68
15
95
192
/
/
maxps
%
xmm0
%
xmm8
.
byte
69
15
93
195
/
/
minps
%
xmm11
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_sse2
.
globl
_sk_mirror_x_1_sse2
FUNCTION
(
_sk_mirror_x_1_sse2
)
_sk_mirror_x_1_sse2
:
.
byte
68
15
40
13
200
30
1
0
/
/
movaps
0x11ec8
(
%
rip
)
%
xmm9
#
3cf70
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd24
>
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
68
15
40
21
76
30
1
0
/
/
movaps
0x11e4c
(
%
rip
)
%
xmm10
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
243
69
15
91
194
/
/
cvttps2dq
%
xmm10
%
xmm8
.
byte
69
15
91
216
/
/
cvtdq2ps
%
xmm8
%
xmm11
.
byte
69
15
194
211
1
/
/
cmpltps
%
xmm11
%
xmm10
.
byte
68
15
40
37
66
30
1
0
/
/
movaps
0x11e42
(
%
rip
)
%
xmm12
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
212
/
/
andps
%
xmm12
%
xmm10
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
92
218
/
/
subps
%
xmm10
%
xmm11
.
byte
69
15
88
219
/
/
addps
%
xmm11
%
xmm11
.
byte
65
15
92
195
/
/
subps
%
xmm11
%
xmm0
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
68
15
84
200
/
/
andps
%
xmm0
%
xmm9
.
byte
69
15
95
193
/
/
maxps
%
xmm9
%
xmm8
.
byte
69
15
93
196
/
/
minps
%
xmm12
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_sse2
.
globl
_sk_decal_x_sse2
FUNCTION
(
_sk_decal_x_sse2
)
_sk_decal_x_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
194
192
2
/
/
cmpleps
%
xmm0
%
xmm8
.
byte
243
68
15
16
72
64
/
/
movss
0x40
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
208
/
/
movaps
%
xmm0
%
xmm10
.
byte
69
15
194
209
1
/
/
cmpltps
%
xmm9
%
xmm10
.
byte
69
15
84
208
/
/
andps
%
xmm8
%
xmm10
.
byte
68
15
17
16
/
/
movups
%
xmm10
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_sse2
.
globl
_sk_decal_y_sse2
FUNCTION
(
_sk_decal_y_sse2
)
_sk_decal_y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
194
193
2
/
/
cmpleps
%
xmm1
%
xmm8
.
byte
243
68
15
16
72
68
/
/
movss
0x44
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
69
15
194
209
1
/
/
cmpltps
%
xmm9
%
xmm10
.
byte
69
15
84
208
/
/
andps
%
xmm8
%
xmm10
.
byte
68
15
17
16
/
/
movups
%
xmm10
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_sse2
.
globl
_sk_decal_x_and_y_sse2
FUNCTION
(
_sk_decal_x_and_y_sse2
)
_sk_decal_x_and_y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
200
2
/
/
cmpleps
%
xmm0
%
xmm9
.
byte
243
68
15
16
80
64
/
/
movss
0x40
(
%
rax
)
%
xmm10
.
byte
243
68
15
16
88
68
/
/
movss
0x44
(
%
rax
)
%
xmm11
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
40
224
/
/
movaps
%
xmm0
%
xmm12
.
byte
69
15
194
226
1
/
/
cmpltps
%
xmm10
%
xmm12
.
byte
68
15
194
193
2
/
/
cmpleps
%
xmm1
%
xmm8
.
byte
69
15
84
193
/
/
andps
%
xmm9
%
xmm8
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
194
203
1
/
/
cmpltps
%
xmm11
%
xmm9
.
byte
69
15
84
200
/
/
andps
%
xmm8
%
xmm9
.
byte
68
15
17
8
/
/
movups
%
xmm9
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_sse2
.
globl
_sk_check_decal_mask_sse2
FUNCTION
(
_sk_check_decal_mask_sse2
)
_sk_check_decal_mask_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
0
/
/
movups
(
%
rax
)
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
65
15
84
208
/
/
andps
%
xmm8
%
xmm2
.
byte
65
15
84
216
/
/
andps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_luminance_to_alpha_sse2
.
globl
_sk_luminance_to_alpha_sse2
FUNCTION
(
_sk_luminance_to_alpha_sse2
)
_sk_luminance_to_alpha_sse2
:
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
89
5
228
34
1
0
/
/
mulps
0x122e4
(
%
rip
)
%
xmm0
#
3d4b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1264
>
.
byte
15
89
13
237
34
1
0
/
/
mulps
0x122ed
(
%
rip
)
%
xmm1
#
3d4c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1274
>
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
89
29
243
34
1
0
/
/
mulps
0x122f3
(
%
rip
)
%
xmm3
#
3d4d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1284
>
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_sse2
.
globl
_sk_matrix_translate_sse2
FUNCTION
(
_sk_matrix_translate_sse2
)
_sk_matrix_translate_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
65
15
88
201
/
/
addps
%
xmm9
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_sse2
.
globl
_sk_matrix_scale_translate_sse2
FUNCTION
(
_sk_matrix_scale_translate_sse2
)
_sk_matrix_scale_translate_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
89
201
/
/
mulps
%
xmm9
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_sse2
.
globl
_sk_matrix_2x3_sse2
FUNCTION
(
_sk_matrix_2x3_sse2
)
_sk_matrix_2x3_sse2
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
16
/
/
movss
0x10
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
68
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_3x4_sse2
.
globl
_sk_matrix_3x4_sse2
FUNCTION
(
_sk_matrix_3x4_sse2
)
_sk_matrix_3x4_sse2
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
36
/
/
movss
0x24
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
28
/
/
movss
0x1c
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
40
/
/
movss
0x28
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
32
/
/
movss
0x20
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
44
/
/
movss
0x2c
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
89
226
/
/
mulps
%
xmm2
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
217
/
/
mulps
%
xmm9
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x5_sse2
.
globl
_sk_matrix_4x5_sse2
FUNCTION
(
_sk_matrix_4x5_sse2
)
_sk_matrix_4x5_sse2
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
32
/
/
movss
0x20
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
48
/
/
movss
0x30
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
64
/
/
movss
0x40
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
68
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
36
/
/
movss
0x24
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
52
/
/
movss
0x34
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
68
/
/
movss
0x44
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
40
/
/
movss
0x28
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
56
/
/
movss
0x38
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
243
68
15
16
112
72
/
/
movss
0x48
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
68
15
89
226
/
/
mulps
%
xmm2
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
217
/
/
mulps
%
xmm9
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
243
68
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
28
/
/
movss
0x1c
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
44
/
/
movss
0x2c
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
243
68
15
16
112
60
/
/
movss
0x3c
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
243
68
15
16
120
76
/
/
movss
0x4c
(
%
rax
)
%
xmm15
.
byte
69
15
198
255
0
/
/
shufps
0x0
%
xmm15
%
xmm15
.
byte
68
15
89
243
/
/
mulps
%
xmm3
%
xmm14
.
byte
69
15
88
247
/
/
addps
%
xmm15
%
xmm14
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
69
15
89
225
/
/
mulps
%
xmm9
%
xmm12
.
byte
69
15
88
229
/
/
addps
%
xmm13
%
xmm12
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
219
/
/
movaps
%
xmm11
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_4x3_sse2
.
globl
_sk_matrix_4x3_sse2
FUNCTION
(
_sk_matrix_4x3_sse2
)
_sk_matrix_4x3_sse2
:
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
32
/
/
movss
0x20
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
36
/
/
movss
0x24
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
80
40
/
/
movss
0x28
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
89
217
/
/
mulps
%
xmm9
%
xmm3
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
80
28
/
/
movss
0x1c
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
44
/
/
movss
0x2c
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
65
15
88
218
/
/
addps
%
xmm10
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_sse2
.
globl
_sk_matrix_perspective_sse2
FUNCTION
(
_sk_matrix_perspective_sse2
)
_sk_matrix_perspective_sse2
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
193
/
/
addps
%
xmm9
%
xmm0
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
20
/
/
movss
0x14
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
243
68
15
16
80
24
/
/
movss
0x18
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
88
28
/
/
movss
0x1c
(
%
rax
)
%
xmm11
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
96
32
/
/
movss
0x20
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
69
15
88
211
/
/
addps
%
xmm11
%
xmm10
.
byte
65
15
83
202
/
/
rcpps
%
xmm10
%
xmm1
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_sse2
.
globl
_sk_evenly_spaced_gradient_sse2
FUNCTION
(
_sk_evenly_spaced_gradient_sse2
)
_sk_evenly_spaced_gradient_sse2
:
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
76
139
112
8
/
/
mov
0x8
(
%
rax
)
%
r14
.
byte
72
255
203
/
/
dec
%
rbx
.
byte
120
7
/
/
js
2b67d
<
_sk_evenly_spaced_gradient_sse2
+
0x18
>
.
byte
243
72
15
42
203
/
/
cvtsi2ss
%
rbx
%
xmm1
.
byte
235
21
/
/
jmp
2b692
<
_sk_evenly_spaced_gradient_sse2
+
0x2d
>
.
byte
73
137
216
/
/
mov
%
rbx
%
r8
.
byte
73
209
232
/
/
shr
%
r8
.
byte
131
227
1
/
/
and
0x1
%
ebx
.
byte
76
9
195
/
/
or
%
r8
%
rbx
.
byte
243
72
15
42
203
/
/
cvtsi2ss
%
rbx
%
xmm1
.
byte
243
15
88
201
/
/
addss
%
xmm1
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
209
/
/
movq
%
xmm2
%
r9
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
102
73
15
126
203
/
/
movq
%
xmm1
%
r11
.
byte
69
137
218
/
/
mov
%
r11d
%
r10d
.
byte
73
193
235
32
/
/
shr
0x20
%
r11
.
byte
243
67
15
16
12
158
/
/
movss
(
%
r14
%
r11
4
)
%
xmm1
.
byte
243
71
15
16
4
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm8
.
byte
68
15
20
193
/
/
unpcklps
%
xmm1
%
xmm8
.
byte
243
67
15
16
12
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm1
.
byte
243
67
15
16
20
134
/
/
movss
(
%
r14
%
r8
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
68
15
20
194
/
/
unpcklpd
%
xmm2
%
xmm8
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
243
66
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm1
.
byte
243
70
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm9
.
byte
68
15
20
201
/
/
unpcklps
%
xmm1
%
xmm9
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
68
15
20
202
/
/
unpcklpd
%
xmm2
%
xmm9
.
byte
72
139
88
16
/
/
mov
0x10
(
%
rax
)
%
rbx
.
byte
243
66
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
243
66
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
243
70
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm10
.
byte
68
15
20
210
/
/
unpcklps
%
xmm2
%
xmm10
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
68
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm10
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
243
66
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
243
70
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm11
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
65
15
20
219
/
/
unpcklps
%
xmm11
%
xmm3
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
243
70
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm11
.
byte
68
15
20
219
/
/
unpcklps
%
xmm3
%
xmm11
.
byte
243
70
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm12
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
65
15
20
220
/
/
unpcklps
%
xmm12
%
xmm3
.
byte
102
68
15
20
219
/
/
unpcklpd
%
xmm3
%
xmm11
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
243
70
15
16
36
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm12
.
byte
243
66
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
65
15
20
220
/
/
unpcklps
%
xmm12
%
xmm3
.
byte
243
70
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm12
.
byte
243
70
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm13
.
byte
69
15
20
236
/
/
unpcklps
%
xmm12
%
xmm13
.
byte
102
65
15
20
221
/
/
unpcklpd
%
xmm13
%
xmm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
70
15
16
36
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm12
.
byte
243
70
15
16
44
144
/
/
movss
(
%
rax
%
r10
4
)
%
xmm13
.
byte
69
15
20
236
/
/
unpcklps
%
xmm12
%
xmm13
.
byte
243
70
15
16
36
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm12
.
byte
243
70
15
16
52
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm14
.
byte
69
15
20
244
/
/
unpcklps
%
xmm12
%
xmm14
.
byte
102
69
15
20
238
/
/
unpcklpd
%
xmm14
%
xmm13
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
221
/
/
addps
%
xmm13
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_sse2
.
globl
_sk_gradient_sse2
FUNCTION
(
_sk_gradient_sse2
)
_sk_gradient_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
73
131
248
2
/
/
cmp
0x2
%
r8
.
byte
114
41
/
/
jb
2b856
<
_sk_gradient_sse2
+
0x38
>
.
byte
76
139
72
72
/
/
mov
0x48
(
%
rax
)
%
r9
.
byte
73
255
200
/
/
dec
%
r8
.
byte
73
131
193
4
/
/
add
0x4
%
r9
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
243
65
15
16
17
/
/
movss
(
%
r9
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
194
208
2
/
/
cmpleps
%
xmm0
%
xmm2
.
byte
102
15
250
202
/
/
psubd
%
xmm2
%
xmm1
.
byte
73
131
193
4
/
/
add
0x4
%
r9
.
byte
73
255
200
/
/
dec
%
r8
.
byte
117
230
/
/
jne
2b83c
<
_sk_gradient_sse2
+
0x1e
>
.
byte
65
86
/
/
push
%
r14
.
byte
83
/
/
push
%
rbx
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
209
/
/
movq
%
xmm2
%
r9
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
102
73
15
126
203
/
/
movq
%
xmm1
%
r11
.
byte
69
137
218
/
/
mov
%
r11d
%
r10d
.
byte
73
193
235
32
/
/
shr
0x20
%
r11
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
76
139
112
16
/
/
mov
0x10
(
%
rax
)
%
r14
.
byte
243
66
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm1
.
byte
243
70
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm8
.
byte
68
15
20
193
/
/
unpcklps
%
xmm1
%
xmm8
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
68
15
20
194
/
/
unpcklpd
%
xmm2
%
xmm8
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
243
66
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm1
.
byte
243
70
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm9
.
byte
68
15
20
201
/
/
unpcklps
%
xmm1
%
xmm9
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
68
15
20
202
/
/
unpcklpd
%
xmm2
%
xmm9
.
byte
243
67
15
16
20
158
/
/
movss
(
%
r14
%
r11
4
)
%
xmm2
.
byte
243
67
15
16
12
150
/
/
movss
(
%
r14
%
r10
4
)
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
243
67
15
16
20
142
/
/
movss
(
%
r14
%
r9
4
)
%
xmm2
.
byte
243
67
15
16
28
134
/
/
movss
(
%
r14
%
r8
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
243
66
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm2
.
byte
243
70
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm10
.
byte
68
15
20
210
/
/
unpcklps
%
xmm2
%
xmm10
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
68
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm10
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
243
66
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
243
70
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm11
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
65
15
20
219
/
/
unpcklps
%
xmm11
%
xmm3
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
243
70
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm11
.
byte
68
15
20
219
/
/
unpcklps
%
xmm3
%
xmm11
.
byte
243
70
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm12
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
65
15
20
220
/
/
unpcklps
%
xmm12
%
xmm3
.
byte
102
68
15
20
219
/
/
unpcklpd
%
xmm3
%
xmm11
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
243
70
15
16
36
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm12
.
byte
243
66
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
65
15
20
220
/
/
unpcklps
%
xmm12
%
xmm3
.
byte
243
70
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm12
.
byte
243
70
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm13
.
byte
69
15
20
236
/
/
unpcklps
%
xmm12
%
xmm13
.
byte
102
65
15
20
221
/
/
unpcklpd
%
xmm13
%
xmm3
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
70
15
16
36
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm12
.
byte
243
70
15
16
44
144
/
/
movss
(
%
rax
%
r10
4
)
%
xmm13
.
byte
69
15
20
236
/
/
unpcklps
%
xmm12
%
xmm13
.
byte
243
70
15
16
36
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm12
.
byte
243
70
15
16
52
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm14
.
byte
69
15
20
244
/
/
unpcklps
%
xmm12
%
xmm14
.
byte
102
69
15
20
238
/
/
unpcklpd
%
xmm14
%
xmm13
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
69
15
88
193
/
/
addps
%
xmm9
%
xmm8
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
221
/
/
addps
%
xmm13
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
91
/
/
pop
%
rbx
.
byte
65
94
/
/
pop
%
r14
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_sse2
.
globl
_sk_evenly_spaced_2_stop_gradient_sse2
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_sse2
)
_sk_evenly_spaced_2_stop_gradient_sse2
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
88
24
/
/
movss
0x18
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
65
15
89
208
/
/
mulps
%
xmm8
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
72
28
/
/
movss
0x1c
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_sse2
.
globl
_sk_xy_to_unit_angle_sse2
FUNCTION
(
_sk_xy_to_unit_angle_sse2
)
_sk_xy_to_unit_angle_sse2
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
69
15
84
200
/
/
andps
%
xmm8
%
xmm9
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
68
15
92
209
/
/
subps
%
xmm1
%
xmm10
.
byte
68
15
84
209
/
/
andps
%
xmm1
%
xmm10
.
byte
69
15
40
217
/
/
movaps
%
xmm9
%
xmm11
.
byte
69
15
93
218
/
/
minps
%
xmm10
%
xmm11
.
byte
69
15
40
225
/
/
movaps
%
xmm9
%
xmm12
.
byte
69
15
95
226
/
/
maxps
%
xmm10
%
xmm12
.
byte
69
15
94
220
/
/
divps
%
xmm12
%
xmm11
.
byte
69
15
40
227
/
/
movaps
%
xmm11
%
xmm12
.
byte
69
15
89
228
/
/
mulps
%
xmm12
%
xmm12
.
byte
68
15
40
45
79
26
1
0
/
/
movaps
0x11a4f
(
%
rip
)
%
xmm13
#
3d4e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1294
>
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
68
15
88
45
83
26
1
0
/
/
addps
0x11a53
(
%
rip
)
%
xmm13
#
3d4f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12a4
>
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
68
15
88
45
87
26
1
0
/
/
addps
0x11a57
(
%
rip
)
%
xmm13
#
3d500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12b4
>
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
68
15
88
45
91
26
1
0
/
/
addps
0x11a5b
(
%
rip
)
%
xmm13
#
3d510
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12c4
>
.
byte
69
15
89
235
/
/
mulps
%
xmm11
%
xmm13
.
byte
69
15
194
202
1
/
/
cmpltps
%
xmm10
%
xmm9
.
byte
68
15
40
21
90
26
1
0
/
/
movaps
0x11a5a
(
%
rip
)
%
xmm10
#
3d520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12d4
>
.
byte
69
15
92
213
/
/
subps
%
xmm13
%
xmm10
.
byte
69
15
84
209
/
/
andps
%
xmm9
%
xmm10
.
byte
69
15
85
205
/
/
andnps
%
xmm13
%
xmm9
.
byte
69
15
86
202
/
/
orps
%
xmm10
%
xmm9
.
byte
68
15
194
192
1
/
/
cmpltps
%
xmm0
%
xmm8
.
byte
68
15
40
21
29
20
1
0
/
/
movaps
0x1141d
(
%
rip
)
%
xmm10
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
92
209
/
/
subps
%
xmm9
%
xmm10
.
byte
69
15
84
208
/
/
andps
%
xmm8
%
xmm10
.
byte
69
15
85
193
/
/
andnps
%
xmm9
%
xmm8
.
byte
69
15
86
194
/
/
orps
%
xmm10
%
xmm8
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm9
.
byte
68
15
40
21
12
20
1
0
/
/
movaps
0x1140c
(
%
rip
)
%
xmm10
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
208
/
/
subps
%
xmm8
%
xmm10
.
byte
69
15
84
209
/
/
andps
%
xmm9
%
xmm10
.
byte
69
15
85
200
/
/
andnps
%
xmm8
%
xmm9
.
byte
69
15
86
202
/
/
orps
%
xmm10
%
xmm9
.
byte
65
15
194
193
7
/
/
cmpordps
%
xmm9
%
xmm0
.
byte
65
15
84
193
/
/
andps
%
xmm9
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_sse2
.
globl
_sk_xy_to_radius_sse2
FUNCTION
(
_sk_xy_to_radius_sse2
)
_sk_xy_to_radius_sse2
:
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
65
15
81
192
/
/
sqrtps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_negate_x_sse2
.
globl
_sk_negate_x_sse2
FUNCTION
(
_sk_negate_x_sse2
)
_sk_negate_x_sse2
:
.
byte
15
87
5
33
25
1
0
/
/
xorps
0x11921
(
%
rip
)
%
xmm0
#
3d460
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1214
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_strip_sse2
.
globl
_sk_xy_to_2pt_conical_strip_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_strip_sse2
)
_sk_xy_to_2pt_conical_strip_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
69
15
92
193
/
/
subps
%
xmm9
%
xmm8
.
byte
69
15
81
192
/
/
sqrtps
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_focal_on_circle_sse2
.
globl
_sk_xy_to_2pt_conical_focal_on_circle_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_focal_on_circle_sse2
)
_sk_xy_to_2pt_conical_focal_on_circle_sse2
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
94
192
/
/
divps
%
xmm0
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_well_behaved_sse2
.
globl
_sk_xy_to_2pt_conical_well_behaved_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_well_behaved_sse2
)
_sk_xy_to_2pt_conical_well_behaved_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
65
15
81
193
/
/
sqrtps
%
xmm9
%
xmm0
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_greater_sse2
.
globl
_sk_xy_to_2pt_conical_greater_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_greater_sse2
)
_sk_xy_to_2pt_conical_greater_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
15
81
192
/
/
sqrtps
%
xmm0
%
xmm0
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_2pt_conical_smaller_sse2
.
globl
_sk_xy_to_2pt_conical_smaller_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_smaller_sse2
)
_sk_xy_to_2pt_conical_smaller_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
64
/
/
movss
0x40
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
15
81
192
/
/
sqrtps
%
xmm0
%
xmm0
.
byte
15
87
5
99
24
1
0
/
/
xorps
0x11863
(
%
rip
)
%
xmm0
#
3d460
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1214
>
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_compensate_focal_sse2
.
globl
_sk_alter_2pt_conical_compensate_focal_sse2
FUNCTION
(
_sk_alter_2pt_conical_compensate_focal_sse2
)
_sk_alter_2pt_conical_compensate_focal_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
68
/
/
movss
0x44
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_alter_2pt_conical_unswap_sse2
.
globl
_sk_alter_2pt_conical_unswap_sse2
FUNCTION
(
_sk_alter_2pt_conical_unswap_sse2
)
_sk_alter_2pt_conical_unswap_sse2
:
.
byte
68
15
40
5
238
18
1
0
/
/
movaps
0x112ee
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
68
15
92
192
/
/
subps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_nan_sse2
.
globl
_sk_mask_2pt_conical_nan_sse2
FUNCTION
(
_sk_mask_2pt_conical_nan_sse2
)
_sk_mask_2pt_conical_nan_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
194
192
7
/
/
cmpordps
%
xmm0
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
68
15
17
0
/
/
movups
%
xmm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mask_2pt_conical_degenerates_sse2
.
globl
_sk_mask_2pt_conical_degenerates_sse2
FUNCTION
(
_sk_mask_2pt_conical_degenerates_sse2
)
_sk_mask_2pt_conical_degenerates_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
69
15
95
193
/
/
maxps
%
xmm9
%
xmm8
.
byte
68
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm9
.
byte
68
15
17
8
/
/
movups
%
xmm9
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_apply_vector_mask_sse2
.
globl
_sk_apply_vector_mask_sse2
FUNCTION
(
_sk_apply_vector_mask_sse2
)
_sk_apply_vector_mask_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
0
/
/
movups
(
%
rax
)
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
65
15
84
208
/
/
andps
%
xmm8
%
xmm2
.
byte
65
15
84
216
/
/
andps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_save_xy_sse2
.
globl
_sk_save_xy_sse2
FUNCTION
(
_sk_save_xy_sse2
)
_sk_save_xy_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
40
5
120
18
1
0
/
/
movaps
0x11278
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
17
0
/
/
movups
%
xmm0
(
%
rax
)
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
69
15
88
200
/
/
addps
%
xmm8
%
xmm9
.
byte
243
69
15
91
209
/
/
cvttps2dq
%
xmm9
%
xmm10
.
byte
69
15
91
210
/
/
cvtdq2ps
%
xmm10
%
xmm10
.
byte
69
15
40
217
/
/
movaps
%
xmm9
%
xmm11
.
byte
69
15
194
218
1
/
/
cmpltps
%
xmm10
%
xmm11
.
byte
68
15
40
37
99
18
1
0
/
/
movaps
0x11263
(
%
rip
)
%
xmm12
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
220
/
/
andps
%
xmm12
%
xmm11
.
byte
69
15
92
211
/
/
subps
%
xmm11
%
xmm10
.
byte
69
15
92
202
/
/
subps
%
xmm10
%
xmm9
.
byte
68
15
88
193
/
/
addps
%
xmm1
%
xmm8
.
byte
243
69
15
91
208
/
/
cvttps2dq
%
xmm8
%
xmm10
.
byte
69
15
91
210
/
/
cvtdq2ps
%
xmm10
%
xmm10
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
69
15
194
218
1
/
/
cmpltps
%
xmm10
%
xmm11
.
byte
69
15
84
220
/
/
andps
%
xmm12
%
xmm11
.
byte
69
15
92
211
/
/
subps
%
xmm11
%
xmm10
.
byte
69
15
92
194
/
/
subps
%
xmm10
%
xmm8
.
byte
15
17
72
64
/
/
movups
%
xmm1
0x40
(
%
rax
)
.
byte
68
15
17
136
128
0
0
0
/
/
movups
%
xmm9
0x80
(
%
rax
)
.
byte
68
15
17
128
192
0
0
0
/
/
movups
%
xmm8
0xc0
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_accumulate_sse2
.
globl
_sk_accumulate_sse2
FUNCTION
(
_sk_accumulate_sse2
)
_sk_accumulate_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
128
0
1
0
0
/
/
movups
0x100
(
%
rax
)
%
xmm8
.
byte
68
15
16
136
64
1
0
0
/
/
movups
0x140
(
%
rax
)
%
xmm9
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
65
15
88
224
/
/
addps
%
xmm8
%
xmm4
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
89
193
/
/
mulps
%
xmm1
%
xmm8
.
byte
65
15
88
232
/
/
addps
%
xmm8
%
xmm5
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
68
15
89
194
/
/
mulps
%
xmm2
%
xmm8
.
byte
65
15
88
240
/
/
addps
%
xmm8
%
xmm6
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
65
15
88
249
/
/
addps
%
xmm9
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_nx_sse2
.
globl
_sk_bilinear_nx_sse2
FUNCTION
(
_sk_bilinear_nx_sse2
)
_sk_bilinear_nx_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
227
23
1
0
/
/
addps
0x117e3
(
%
rip
)
%
xmm0
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
187
17
1
0
/
/
movaps
0x111bb
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
17
136
0
1
0
0
/
/
movups
%
xmm9
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_px_sse2
.
globl
_sk_bilinear_px_sse2
FUNCTION
(
_sk_bilinear_px_sse2
)
_sk_bilinear_px_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
135
17
1
0
/
/
addps
0x11187
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
17
128
0
1
0
0
/
/
movups
%
xmm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_ny_sse2
.
globl
_sk_bilinear_ny_sse2
FUNCTION
(
_sk_bilinear_ny_sse2
)
_sk_bilinear_ny_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
150
23
1
0
/
/
addps
0x11796
(
%
rip
)
%
xmm1
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
110
17
1
0
/
/
movaps
0x1116e
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
17
136
64
1
0
0
/
/
movups
%
xmm9
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilinear_py_sse2
.
globl
_sk_bilinear_py_sse2
FUNCTION
(
_sk_bilinear_py_sse2
)
_sk_bilinear_py_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
57
17
1
0
/
/
addps
0x11139
(
%
rip
)
%
xmm1
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
17
128
64
1
0
0
/
/
movups
%
xmm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3x_sse2
.
globl
_sk_bicubic_n3x_sse2
FUNCTION
(
_sk_bicubic_n3x_sse2
)
_sk_bicubic_n3x_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
89
23
1
0
/
/
addps
0x11759
(
%
rip
)
%
xmm0
#
3d540
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12f4
>
.
byte
68
15
40
13
33
17
1
0
/
/
movaps
0x11121
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
89
13
77
23
1
0
/
/
mulps
0x1174d
(
%
rip
)
%
xmm9
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
13
5
19
1
0
/
/
addps
0x11305
(
%
rip
)
%
xmm9
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
68
15
17
136
0
1
0
0
/
/
movups
%
xmm9
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1x_sse2
.
globl
_sk_bicubic_n1x_sse2
FUNCTION
(
_sk_bicubic_n1x_sse2
)
_sk_bicubic_n1x_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
1
23
1
0
/
/
addps
0x11701
(
%
rip
)
%
xmm0
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
217
16
1
0
/
/
movaps
0x110d9
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
40
5
29
23
1
0
/
/
movaps
0x1171d
(
%
rip
)
%
xmm8
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
33
23
1
0
/
/
addps
0x11721
(
%
rip
)
%
xmm8
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
165
16
1
0
/
/
addps
0x110a5
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
25
23
1
0
/
/
addps
0x11719
(
%
rip
)
%
xmm8
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
128
0
1
0
0
/
/
movups
%
xmm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1x_sse2
.
globl
_sk_bicubic_p1x_sse2
FUNCTION
(
_sk_bicubic_p1x_sse2
)
_sk_bicubic_p1x_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
40
5
131
16
1
0
/
/
movaps
0x11083
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
136
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm9
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
21
204
22
1
0
/
/
movaps
0x116cc
(
%
rip
)
%
xmm10
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
208
22
1
0
/
/
addps
0x116d0
(
%
rip
)
%
xmm10
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
208
/
/
addps
%
xmm8
%
xmm10
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
204
22
1
0
/
/
addps
0x116cc
(
%
rip
)
%
xmm10
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
144
0
1
0
0
/
/
movups
%
xmm10
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3x_sse2
.
globl
_sk_bicubic_p3x_sse2
FUNCTION
(
_sk_bicubic_p3x_sse2
)
_sk_bicubic_p3x_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
68
15
16
128
128
0
0
0
/
/
movups
0x80
(
%
rax
)
%
xmm8
.
byte
15
88
5
156
22
1
0
/
/
addps
0x1169c
(
%
rip
)
%
xmm0
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
89
5
108
22
1
0
/
/
mulps
0x1166c
(
%
rip
)
%
xmm8
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
5
36
18
1
0
/
/
addps
0x11224
(
%
rip
)
%
xmm8
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
17
128
0
1
0
0
/
/
movups
%
xmm8
0x100
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n3y_sse2
.
globl
_sk_bicubic_n3y_sse2
FUNCTION
(
_sk_bicubic_n3y_sse2
)
_sk_bicubic_n3y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
47
22
1
0
/
/
addps
0x1162f
(
%
rip
)
%
xmm1
#
3d540
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12f4
>
.
byte
68
15
40
13
247
15
1
0
/
/
movaps
0x10ff7
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
69
15
40
193
/
/
movaps
%
xmm9
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
89
13
35
22
1
0
/
/
mulps
0x11623
(
%
rip
)
%
xmm9
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
13
219
17
1
0
/
/
addps
0x111db
(
%
rip
)
%
xmm9
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
68
15
17
136
64
1
0
0
/
/
movups
%
xmm9
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_n1y_sse2
.
globl
_sk_bicubic_n1y_sse2
FUNCTION
(
_sk_bicubic_n1y_sse2
)
_sk_bicubic_n1y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
214
21
1
0
/
/
addps
0x115d6
(
%
rip
)
%
xmm1
#
3d530
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12e4
>
.
byte
68
15
40
13
174
15
1
0
/
/
movaps
0x10fae
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
92
200
/
/
subps
%
xmm8
%
xmm9
.
byte
68
15
40
5
242
21
1
0
/
/
movaps
0x115f2
(
%
rip
)
%
xmm8
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
246
21
1
0
/
/
addps
0x115f6
(
%
rip
)
%
xmm8
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
122
15
1
0
/
/
addps
0x10f7a
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
88
5
238
21
1
0
/
/
addps
0x115ee
(
%
rip
)
%
xmm8
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
128
64
1
0
0
/
/
movups
%
xmm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p1y_sse2
.
globl
_sk_bicubic_p1y_sse2
FUNCTION
(
_sk_bicubic_p1y_sse2
)
_sk_bicubic_p1y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
40
5
88
15
1
0
/
/
movaps
0x10f58
(
%
rip
)
%
xmm8
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
136
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm9
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
68
15
40
21
160
21
1
0
/
/
movaps
0x115a0
(
%
rip
)
%
xmm10
#
3d560
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1314
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
164
21
1
0
/
/
addps
0x115a4
(
%
rip
)
%
xmm10
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
69
15
88
208
/
/
addps
%
xmm8
%
xmm10
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
88
21
160
21
1
0
/
/
addps
0x115a0
(
%
rip
)
%
xmm10
#
3d580
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1334
>
.
byte
68
15
17
144
64
1
0
0
/
/
movups
%
xmm10
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bicubic_p3y_sse2
.
globl
_sk_bicubic_p3y_sse2
FUNCTION
(
_sk_bicubic_p3y_sse2
)
_sk_bicubic_p3y_sse2
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
16
72
64
/
/
movups
0x40
(
%
rax
)
%
xmm1
.
byte
68
15
16
128
192
0
0
0
/
/
movups
0xc0
(
%
rax
)
%
xmm8
.
byte
15
88
13
111
21
1
0
/
/
addps
0x1156f
(
%
rip
)
%
xmm1
#
3d570
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1324
>
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
89
5
63
21
1
0
/
/
mulps
0x1153f
(
%
rip
)
%
xmm8
#
3d550
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1304
>
.
byte
68
15
88
5
247
16
1
0
/
/
addps
0x110f7
(
%
rip
)
%
xmm8
#
3d110
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xec4
>
.
byte
69
15
89
193
/
/
mulps
%
xmm9
%
xmm8
.
byte
68
15
17
128
64
1
0
0
/
/
movups
%
xmm8
0x140
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_callback_sse2
.
globl
_sk_callback_sse2
FUNCTION
(
_sk_callback_sse2
)
_sk_callback_sse2
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
72
/
/
sub
0x48
%
rsp
.
byte
15
41
125
144
/
/
movaps
%
xmm7
-
0x70
(
%
rbp
)
.
byte
15
41
117
160
/
/
movaps
%
xmm6
-
0x60
(
%
rbp
)
.
byte
15
41
109
176
/
/
movaps
%
xmm5
-
0x50
(
%
rbp
)
.
byte
15
41
101
192
/
/
movaps
%
xmm4
-
0x40
(
%
rbp
)
.
byte
73
137
206
/
/
mov
%
rcx
%
r14
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
73
137
253
/
/
mov
%
rdi
%
r13
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
137
195
/
/
mov
%
rax
%
rbx
.
byte
73
137
244
/
/
mov
%
rsi
%
r12
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
15
21
193
/
/
unpckhps
%
xmm1
%
xmm0
.
byte
15
21
211
/
/
unpckhps
%
xmm3
%
xmm2
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
15
18
236
/
/
movhlps
%
xmm4
%
xmm5
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
102
15
20
218
/
/
unpcklpd
%
xmm2
%
xmm3
.
byte
102
15
17
75
8
/
/
movupd
%
xmm1
0x8
(
%
rbx
)
.
byte
15
18
208
/
/
movhlps
%
xmm0
%
xmm2
.
byte
15
17
107
24
/
/
movups
%
xmm5
0x18
(
%
rbx
)
.
byte
102
15
17
91
40
/
/
movupd
%
xmm3
0x28
(
%
rbx
)
.
byte
15
17
83
56
/
/
movups
%
xmm2
0x38
(
%
rbx
)
.
byte
77
133
237
/
/
test
%
r13
%
r13
.
byte
190
4
0
0
0
/
/
mov
0x4
%
esi
.
byte
65
15
69
245
/
/
cmovne
%
r13d
%
esi
.
byte
72
137
223
/
/
mov
%
rbx
%
rdi
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
72
139
131
8
1
0
0
/
/
mov
0x108
(
%
rbx
)
%
rax
.
byte
15
16
32
/
/
movups
(
%
rax
)
%
xmm4
.
byte
15
16
64
16
/
/
movups
0x10
(
%
rax
)
%
xmm0
.
byte
15
16
88
32
/
/
movups
0x20
(
%
rax
)
%
xmm3
.
byte
15
16
80
48
/
/
movups
0x30
(
%
rax
)
%
xmm2
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
15
21
224
/
/
unpckhps
%
xmm0
%
xmm4
.
byte
15
21
218
/
/
unpckhps
%
xmm2
%
xmm3
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
15
18
205
/
/
movhlps
%
xmm5
%
xmm1
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
15
18
220
/
/
movhlps
%
xmm4
%
xmm3
.
byte
76
137
230
/
/
mov
%
r12
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
137
239
/
/
mov
%
r13
%
rdi
.
byte
76
137
250
/
/
mov
%
r15
%
rdx
.
byte
76
137
241
/
/
mov
%
r14
%
rcx
.
byte
15
40
101
192
/
/
movaps
-
0x40
(
%
rbp
)
%
xmm4
.
byte
15
40
109
176
/
/
movaps
-
0x50
(
%
rbp
)
%
xmm5
.
byte
15
40
117
160
/
/
movaps
-
0x60
(
%
rbp
)
%
xmm6
.
byte
15
40
125
144
/
/
movaps
-
0x70
(
%
rbp
)
%
xmm7
.
byte
72
131
196
72
/
/
add
0x48
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_3D_sse2
.
globl
_sk_clut_3D_sse2
FUNCTION
(
_sk_clut_3D_sse2
)
_sk_clut_3D_sse2
:
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
64
/
/
sub
0x40
%
rsp
.
byte
15
41
124
36
48
/
/
movaps
%
xmm7
0x30
(
%
rsp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
rsp
)
.
byte
15
41
108
36
16
/
/
movaps
%
xmm5
0x10
(
%
rsp
)
.
byte
15
41
36
36
/
/
movaps
%
xmm4
(
%
rsp
)
.
byte
15
41
92
36
240
/
/
movaps
%
xmm3
-
0x10
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
217
/
/
movd
%
r9d
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
243
68
15
91
235
/
/
cvttps2dq
%
xmm3
%
xmm13
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
68
15
41
92
36
224
/
/
movaps
%
xmm11
-
0x20
(
%
rsp
)
.
byte
102
65
15
110
208
/
/
movd
%
r8d
%
xmm2
.
byte
102
68
15
112
250
0
/
/
pshufd
0x0
%
xmm2
%
xmm15
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
217
/
/
movd
%
r9d
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
15
91
251
/
/
cvtdq2ps
%
xmm3
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
243
15
91
239
/
/
cvttps2dq
%
xmm7
%
xmm5
.
byte
102
15
127
108
36
128
/
/
movdqa
%
xmm5
-
0x80
(
%
rsp
)
.
byte
15
41
124
36
144
/
/
movaps
%
xmm7
-
0x70
(
%
rsp
)
.
byte
102
15
112
205
245
/
/
pshufd
0xf5
%
xmm5
%
xmm1
.
byte
102
65
15
244
207
/
/
pmuludq
%
xmm15
%
xmm1
.
byte
102
65
15
111
223
/
/
movdqa
%
xmm15
%
xmm3
.
byte
102
15
244
221
/
/
pmuludq
%
xmm5
%
xmm3
.
byte
102
68
15
112
211
232
/
/
pshufd
0xe8
%
xmm3
%
xmm10
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
68
15
98
209
/
/
punpckldq
%
xmm1
%
xmm10
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
112
209
0
/
/
pshufd
0x0
%
xmm1
%
xmm2
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
68
15
91
241
/
/
cvtdq2ps
%
xmm1
%
xmm14
.
byte
68
15
89
240
/
/
mulps
%
xmm0
%
xmm14
.
byte
243
69
15
91
206
/
/
cvttps2dq
%
xmm14
%
xmm9
.
byte
102
65
15
112
193
245
/
/
pshufd
0xf5
%
xmm9
%
xmm0
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
65
15
244
201
/
/
pmuludq
%
xmm9
%
xmm1
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
98
216
/
/
punpckldq
%
xmm0
%
xmm3
.
byte
102
15
127
92
36
176
/
/
movdqa
%
xmm3
-
0x50
(
%
rsp
)
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
254
205
/
/
paddd
%
xmm13
%
xmm1
.
byte
102
68
15
127
108
36
208
/
/
movdqa
%
xmm13
-
0x30
(
%
rsp
)
.
byte
102
15
254
217
/
/
paddd
%
xmm1
%
xmm3
.
byte
102
68
15
111
5
134
19
1
0
/
/
movdqa
0x11386
(
%
rip
)
%
xmm8
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
112
195
245
/
/
pshufd
0xf5
%
xmm3
%
xmm0
.
byte
102
65
15
244
216
/
/
pmuludq
%
xmm8
%
xmm3
.
byte
102
65
15
244
192
/
/
pmuludq
%
xmm8
%
xmm0
.
byte
102
15
112
224
232
/
/
pshufd
0xe8
%
xmm0
%
xmm4
.
byte
102
15
112
195
232
/
/
pshufd
0xe8
%
xmm3
%
xmm0
.
byte
102
15
98
196
/
/
punpckldq
%
xmm4
%
xmm0
.
byte
102
65
15
126
216
/
/
movd
%
xmm3
%
r8d
.
byte
102
15
112
216
229
/
/
pshufd
0xe5
%
xmm0
%
xmm3
.
byte
102
65
15
126
217
/
/
movd
%
xmm3
%
r9d
.
byte
102
15
112
216
78
/
/
pshufd
0x4e
%
xmm0
%
xmm3
.
byte
102
65
15
126
218
/
/
movd
%
xmm3
%
r10d
.
byte
102
15
112
216
231
/
/
pshufd
0xe7
%
xmm0
%
xmm3
.
byte
102
65
15
126
219
/
/
movd
%
xmm3
%
r11d
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
227
/
/
unpcklps
%
xmm3
%
xmm4
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
102
15
20
244
/
/
unpcklpd
%
xmm4
%
xmm6
.
byte
102
15
118
228
/
/
pcmpeqd
%
xmm4
%
xmm4
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
250
220
/
/
psubd
%
xmm4
%
xmm3
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
73
15
126
217
/
/
movq
%
xmm3
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
243
66
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
15
20
220
/
/
unpcklps
%
xmm4
%
xmm3
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
221
/
/
unpcklpd
%
xmm5
%
xmm3
.
byte
102
68
15
111
37
109
12
1
0
/
/
movdqa
0x10c6d
(
%
rip
)
%
xmm12
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
65
15
254
196
/
/
paddd
%
xmm12
%
xmm0
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
73
15
126
193
/
/
movq
%
xmm0
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
102
15
20
236
/
/
unpcklpd
%
xmm4
%
xmm5
.
byte
15
40
37
114
18
1
0
/
/
movaps
0x11272
(
%
rip
)
%
xmm4
#
3d590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1344
>
.
byte
68
15
88
220
/
/
addps
%
xmm4
%
xmm11
.
byte
68
15
41
92
36
192
/
/
movaps
%
xmm11
-
0x40
(
%
rsp
)
.
byte
68
15
40
223
/
/
movaps
%
xmm7
%
xmm11
.
byte
68
15
88
220
/
/
addps
%
xmm4
%
xmm11
.
byte
65
15
88
230
/
/
addps
%
xmm14
%
xmm4
.
byte
243
15
91
228
/
/
cvttps2dq
%
xmm4
%
xmm4
.
byte
102
15
112
252
245
/
/
pshufd
0xf5
%
xmm4
%
xmm7
.
byte
102
15
244
250
/
/
pmuludq
%
xmm2
%
xmm7
.
byte
102
15
244
212
/
/
pmuludq
%
xmm4
%
xmm2
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
112
215
232
/
/
pshufd
0xe8
%
xmm7
%
xmm2
.
byte
102
15
98
194
/
/
punpckldq
%
xmm2
%
xmm0
.
byte
102
15
127
68
36
160
/
/
movdqa
%
xmm0
-
0x60
(
%
rsp
)
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
209
245
/
/
pshufd
0xf5
%
xmm1
%
xmm2
.
byte
102
65
15
244
200
/
/
pmuludq
%
xmm8
%
xmm1
.
byte
102
65
15
244
208
/
/
pmuludq
%
xmm8
%
xmm2
.
byte
102
15
112
226
232
/
/
pshufd
0xe8
%
xmm2
%
xmm4
.
byte
102
15
112
209
232
/
/
pshufd
0xe8
%
xmm1
%
xmm2
.
byte
102
15
98
212
/
/
punpckldq
%
xmm4
%
xmm2
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
15
112
202
229
/
/
pshufd
0xe5
%
xmm2
%
xmm1
.
byte
102
65
15
126
201
/
/
movd
%
xmm1
%
r9d
.
byte
102
15
112
202
78
/
/
pshufd
0x4e
%
xmm2
%
xmm1
.
byte
102
65
15
126
202
/
/
movd
%
xmm1
%
r10d
.
byte
102
15
112
202
231
/
/
pshufd
0xe7
%
xmm2
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
66
15
16
60
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm7
.
byte
15
20
249
/
/
unpcklps
%
xmm1
%
xmm7
.
byte
102
15
20
252
/
/
unpcklpd
%
xmm4
%
xmm7
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
250
13
231
17
1
0
/
/
psubd
0x111e7
(
%
rip
)
%
xmm1
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
102
15
20
224
/
/
unpcklpd
%
xmm0
%
xmm4
.
byte
102
65
15
254
212
/
/
paddd
%
xmm12
%
xmm2
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
208
/
/
unpcklps
%
xmm0
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
15
20
209
/
/
unpcklpd
%
xmm1
%
xmm2
.
byte
65
15
91
193
/
/
cvtdq2ps
%
xmm9
%
xmm0
.
byte
68
15
92
240
/
/
subps
%
xmm0
%
xmm14
.
byte
15
92
254
/
/
subps
%
xmm6
%
xmm7
.
byte
65
15
89
254
/
/
mulps
%
xmm14
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
65
15
89
230
/
/
mulps
%
xmm14
%
xmm4
.
byte
15
88
227
/
/
addps
%
xmm3
%
xmm4
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
65
15
89
214
/
/
mulps
%
xmm14
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
243
65
15
91
195
/
/
cvttps2dq
%
xmm11
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
65
15
244
207
/
/
pmuludq
%
xmm15
%
xmm1
.
byte
102
68
15
244
248
/
/
pmuludq
%
xmm0
%
xmm15
.
byte
102
69
15
112
231
232
/
/
pshufd
0xe8
%
xmm15
%
xmm12
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
68
15
98
224
/
/
punpckldq
%
xmm0
%
xmm12
.
byte
102
65
15
111
244
/
/
movdqa
%
xmm12
%
xmm6
.
byte
102
65
15
254
245
/
/
paddd
%
xmm13
%
xmm6
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
254
76
36
176
/
/
paddd
-
0x50
(
%
rsp
)
%
xmm1
.
byte
102
15
112
193
245
/
/
pshufd
0xf5
%
xmm1
%
xmm0
.
byte
102
65
15
244
200
/
/
pmuludq
%
xmm8
%
xmm1
.
byte
102
65
15
244
192
/
/
pmuludq
%
xmm8
%
xmm0
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
15
112
216
232
/
/
pshufd
0xe8
%
xmm0
%
xmm3
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
98
195
/
/
punpckldq
%
xmm3
%
xmm0
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
65
15
126
201
/
/
movd
%
xmm1
%
r9d
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
65
15
126
202
/
/
movd
%
xmm1
%
r10d
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
70
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm8
.
byte
68
15
20
193
/
/
unpcklps
%
xmm1
%
xmm8
.
byte
102
68
15
20
195
/
/
unpcklpd
%
xmm3
%
xmm8
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
65
15
250
201
/
/
psubd
%
xmm9
%
xmm1
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
233
/
/
unpcklps
%
xmm1
%
xmm5
.
byte
102
15
20
221
/
/
unpcklpd
%
xmm5
%
xmm3
.
byte
102
68
15
111
45
223
9
1
0
/
/
movdqa
0x109df
(
%
rip
)
%
xmm13
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
65
15
254
197
/
/
paddd
%
xmm13
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
73
15
126
200
/
/
movq
%
xmm1
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
102
15
254
116
36
160
/
/
paddd
-
0x60
(
%
rsp
)
%
xmm6
.
byte
102
15
112
198
245
/
/
pshufd
0xf5
%
xmm6
%
xmm0
.
byte
102
65
15
244
243
/
/
pmuludq
%
xmm11
%
xmm6
.
byte
102
65
15
244
195
/
/
pmuludq
%
xmm11
%
xmm0
.
byte
102
15
112
232
232
/
/
pshufd
0xe8
%
xmm0
%
xmm5
.
byte
102
15
112
198
232
/
/
pshufd
0xe8
%
xmm6
%
xmm0
.
byte
102
15
98
197
/
/
punpckldq
%
xmm5
%
xmm0
.
byte
102
65
15
126
240
/
/
movd
%
xmm6
%
r8d
.
byte
102
15
112
232
229
/
/
pshufd
0xe5
%
xmm0
%
xmm5
.
byte
102
65
15
126
233
/
/
movd
%
xmm5
%
r9d
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
65
15
126
234
/
/
movd
%
xmm5
%
r10d
.
byte
102
15
112
232
231
/
/
pshufd
0xe7
%
xmm0
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
52
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
243
70
15
16
60
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm15
.
byte
68
15
20
253
/
/
unpcklps
%
xmm5
%
xmm15
.
byte
102
68
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm15
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
65
15
250
233
/
/
psubd
%
xmm9
%
xmm5
.
byte
102
15
112
245
78
/
/
pshufd
0x4e
%
xmm5
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
232
/
/
movq
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
70
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm11
.
byte
68
15
20
221
/
/
unpcklps
%
xmm5
%
xmm11
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
68
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm11
.
byte
102
65
15
254
197
/
/
paddd
%
xmm13
%
xmm0
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
73
15
126
232
/
/
movq
%
xmm5
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
70
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm9
.
byte
68
15
20
200
/
/
unpcklps
%
xmm0
%
xmm9
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
102
68
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm9
.
byte
69
15
92
248
/
/
subps
%
xmm8
%
xmm15
.
byte
69
15
89
254
/
/
mulps
%
xmm14
%
xmm15
.
byte
69
15
88
248
/
/
addps
%
xmm8
%
xmm15
.
byte
68
15
92
219
/
/
subps
%
xmm3
%
xmm11
.
byte
69
15
89
222
/
/
mulps
%
xmm14
%
xmm11
.
byte
68
15
88
219
/
/
addps
%
xmm3
%
xmm11
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
69
15
89
206
/
/
mulps
%
xmm14
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
15
91
68
36
128
/
/
cvtdq2ps
-
0x80
(
%
rsp
)
%
xmm0
.
byte
15
40
76
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
41
76
36
144
/
/
movaps
%
xmm1
-
0x70
(
%
rsp
)
.
byte
68
15
92
255
/
/
subps
%
xmm7
%
xmm15
.
byte
68
15
89
249
/
/
mulps
%
xmm1
%
xmm15
.
byte
68
15
88
255
/
/
addps
%
xmm7
%
xmm15
.
byte
68
15
92
220
/
/
subps
%
xmm4
%
xmm11
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
68
15
88
220
/
/
addps
%
xmm4
%
xmm11
.
byte
68
15
92
202
/
/
subps
%
xmm2
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
68
15
88
202
/
/
addps
%
xmm2
%
xmm9
.
byte
243
15
91
68
36
192
/
/
cvttps2dq
-
0x40
(
%
rsp
)
%
xmm0
.
byte
102
15
127
68
36
128
/
/
movdqa
%
xmm0
-
0x80
(
%
rsp
)
.
byte
102
68
15
254
208
/
/
paddd
%
xmm0
%
xmm10
.
byte
102
15
111
116
36
176
/
/
movdqa
-
0x50
(
%
rsp
)
%
xmm6
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
65
15
254
202
/
/
paddd
%
xmm10
%
xmm1
.
byte
102
15
112
209
245
/
/
pshufd
0xf5
%
xmm1
%
xmm2
.
byte
102
15
111
29
130
14
1
0
/
/
movdqa
0x10e82
(
%
rip
)
%
xmm3
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
244
203
/
/
pmuludq
%
xmm3
%
xmm1
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
15
112
203
229
/
/
pshufd
0xe5
%
xmm3
%
xmm1
.
byte
102
65
15
126
201
/
/
movd
%
xmm1
%
r9d
.
byte
102
15
112
203
78
/
/
pshufd
0x4e
%
xmm3
%
xmm1
.
byte
102
65
15
126
202
/
/
movd
%
xmm1
%
r10d
.
byte
102
15
112
203
231
/
/
pshufd
0xe7
%
xmm3
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
15
20
204
/
/
unpcklps
%
xmm4
%
xmm1
.
byte
102
15
20
202
/
/
unpcklpd
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
69
15
118
237
/
/
pcmpeqd
%
xmm13
%
xmm13
.
byte
102
65
15
250
213
/
/
psubd
%
xmm13
%
xmm2
.
byte
102
15
112
226
78
/
/
pshufd
0x4e
%
xmm2
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
212
/
/
unpcklps
%
xmm4
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
213
/
/
unpcklpd
%
xmm5
%
xmm2
.
byte
102
15
111
5
114
7
1
0
/
/
movdqa
0x10772
(
%
rip
)
%
xmm0
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
254
216
/
/
paddd
%
xmm0
%
xmm3
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
216
/
/
movq
%
xmm3
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
15
20
220
/
/
unpcklps
%
xmm4
%
xmm3
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
221
/
/
unpcklpd
%
xmm5
%
xmm3
.
byte
102
68
15
254
84
36
160
/
/
paddd
-
0x60
(
%
rsp
)
%
xmm10
.
byte
102
65
15
112
226
245
/
/
pshufd
0xf5
%
xmm10
%
xmm4
.
byte
102
68
15
244
215
/
/
pmuludq
%
xmm7
%
xmm10
.
byte
102
15
244
231
/
/
pmuludq
%
xmm7
%
xmm4
.
byte
102
15
112
236
232
/
/
pshufd
0xe8
%
xmm4
%
xmm5
.
byte
102
65
15
112
226
232
/
/
pshufd
0xe8
%
xmm10
%
xmm4
.
byte
102
15
98
229
/
/
punpckldq
%
xmm5
%
xmm4
.
byte
102
69
15
126
208
/
/
movd
%
xmm10
%
r8d
.
byte
102
15
112
236
229
/
/
pshufd
0xe5
%
xmm4
%
xmm5
.
byte
102
65
15
126
233
/
/
movd
%
xmm5
%
r9d
.
byte
102
15
112
236
78
/
/
pshufd
0x4e
%
xmm4
%
xmm5
.
byte
102
65
15
126
234
/
/
movd
%
xmm5
%
r10d
.
byte
102
15
112
236
231
/
/
pshufd
0xe7
%
xmm4
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
60
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
243
70
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm8
.
byte
68
15
20
197
/
/
unpcklps
%
xmm5
%
xmm8
.
byte
102
68
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm8
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
65
15
250
237
/
/
psubd
%
xmm13
%
xmm5
.
byte
102
15
112
253
78
/
/
pshufd
0x4e
%
xmm5
%
xmm7
.
byte
102
73
15
126
248
/
/
movq
%
xmm7
%
r8
.
byte
102
72
15
126
232
/
/
movq
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
70
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm13
.
byte
68
15
20
237
/
/
unpcklps
%
xmm5
%
xmm13
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
102
68
15
20
239
/
/
unpcklpd
%
xmm7
%
xmm13
.
byte
102
15
254
224
/
/
paddd
%
xmm0
%
xmm4
.
byte
102
15
112
236
78
/
/
pshufd
0x4e
%
xmm4
%
xmm5
.
byte
102
73
15
126
232
/
/
movq
%
xmm5
%
r8
.
byte
102
72
15
126
224
/
/
movq
%
xmm4
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
252
/
/
unpcklps
%
xmm4
%
xmm7
.
byte
102
15
20
239
/
/
unpcklpd
%
xmm7
%
xmm5
.
byte
68
15
92
193
/
/
subps
%
xmm1
%
xmm8
.
byte
69
15
89
198
/
/
mulps
%
xmm14
%
xmm8
.
byte
68
15
88
193
/
/
addps
%
xmm1
%
xmm8
.
byte
68
15
92
234
/
/
subps
%
xmm2
%
xmm13
.
byte
69
15
89
238
/
/
mulps
%
xmm14
%
xmm13
.
byte
68
15
88
234
/
/
addps
%
xmm2
%
xmm13
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
65
15
89
238
/
/
mulps
%
xmm14
%
xmm5
.
byte
15
88
235
/
/
addps
%
xmm3
%
xmm5
.
byte
102
68
15
254
100
36
128
/
/
paddd
-
0x80
(
%
rsp
)
%
xmm12
.
byte
102
15
111
214
/
/
movdqa
%
xmm6
%
xmm2
.
byte
102
65
15
254
212
/
/
paddd
%
xmm12
%
xmm2
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
13
95
12
1
0
/
/
movdqa
0x10c5f
(
%
rip
)
%
xmm1
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
112
200
232
/
/
pshufd
0xe8
%
xmm0
%
xmm1
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
65
15
126
208
/
/
movd
%
xmm2
%
r8d
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
65
15
126
201
/
/
movd
%
xmm1
%
r9d
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
65
15
126
202
/
/
movd
%
xmm1
%
r10d
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
70
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm10
.
byte
68
15
20
209
/
/
unpcklps
%
xmm1
%
xmm10
.
byte
102
68
15
20
210
/
/
unpcklpd
%
xmm2
%
xmm10
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
250
13
4
12
1
0
/
/
psubd
0x10c04
(
%
rip
)
%
xmm1
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
243
15
16
20
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
15
20
226
/
/
unpcklpd
%
xmm2
%
xmm4
.
byte
102
15
254
5
79
5
1
0
/
/
paddd
0x1054f
(
%
rip
)
%
xmm0
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
73
15
126
200
/
/
movq
%
xmm1
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
15
20
217
/
/
unpcklpd
%
xmm1
%
xmm3
.
byte
102
68
15
254
100
36
160
/
/
paddd
-
0x60
(
%
rsp
)
%
xmm12
.
byte
102
65
15
112
196
245
/
/
pshufd
0xf5
%
xmm12
%
xmm0
.
byte
102
68
15
244
230
/
/
pmuludq
%
xmm6
%
xmm12
.
byte
102
15
244
198
/
/
pmuludq
%
xmm6
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
65
15
112
212
232
/
/
pshufd
0xe8
%
xmm12
%
xmm2
.
byte
102
15
98
208
/
/
punpckldq
%
xmm0
%
xmm2
.
byte
102
69
15
126
224
/
/
movd
%
xmm12
%
r8d
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
65
15
126
194
/
/
movd
%
xmm0
%
r10d
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
15
20
199
/
/
unpcklps
%
xmm7
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
250
13
14
11
1
0
/
/
psubd
0x10b0e
(
%
rip
)
%
xmm1
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
249
78
/
/
pshufd
0x4e
%
xmm1
%
xmm7
.
byte
102
73
15
126
248
/
/
movq
%
xmm7
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
60
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm7
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
247
/
/
unpcklps
%
xmm7
%
xmm6
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
254
21
89
4
1
0
/
/
paddd
0x10459
(
%
rip
)
%
xmm2
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
112
242
78
/
/
pshufd
0x4e
%
xmm2
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
214
/
/
unpcklps
%
xmm6
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
102
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm2
.
byte
65
15
92
194
/
/
subps
%
xmm10
%
xmm0
.
byte
65
15
89
198
/
/
mulps
%
xmm14
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
65
15
89
206
/
/
mulps
%
xmm14
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
65
15
89
214
/
/
mulps
%
xmm14
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
15
40
92
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
65
15
92
205
/
/
subps
%
xmm13
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
65
15
88
205
/
/
addps
%
xmm13
%
xmm1
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
91
92
36
208
/
/
cvtdq2ps
-
0x30
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
224
/
/
movaps
-
0x20
(
%
rsp
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
65
15
92
199
/
/
subps
%
xmm15
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
65
15
88
199
/
/
addps
%
xmm15
%
xmm0
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
65
15
92
209
/
/
subps
%
xmm9
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
92
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm3
.
byte
15
40
36
36
/
/
movaps
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
16
/
/
movaps
0x10
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
32
/
/
movaps
0x20
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
48
/
/
movaps
0x30
(
%
rsp
)
%
xmm7
.
byte
72
131
196
64
/
/
add
0x40
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clut_4D_sse2
.
globl
_sk_clut_4D_sse2
FUNCTION
(
_sk_clut_4D_sse2
)
_sk_clut_4D_sse2
:
.
byte
83
/
/
push
%
rbx
.
byte
72
129
236
160
0
0
0
/
/
sub
0xa0
%
rsp
.
byte
15
41
188
36
144
0
0
0
/
/
movaps
%
xmm7
0x90
(
%
rsp
)
.
byte
15
41
180
36
128
0
0
0
/
/
movaps
%
xmm6
0x80
(
%
rsp
)
.
byte
15
41
108
36
112
/
/
movaps
%
xmm5
0x70
(
%
rsp
)
.
byte
15
41
100
36
96
/
/
movaps
%
xmm4
0x60
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
139
64
20
/
/
mov
0x14
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
225
/
/
movd
%
r9d
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
102
65
15
110
216
/
/
movd
%
r8d
%
xmm3
.
byte
102
15
112
251
0
/
/
pshufd
0x0
%
xmm3
%
xmm7
.
byte
102
15
127
124
36
128
/
/
movdqa
%
xmm7
-
0x80
(
%
rsp
)
.
byte
68
139
64
16
/
/
mov
0x10
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
225
/
/
movd
%
r9d
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
226
/
/
mulps
%
xmm2
%
xmm4
.
byte
243
15
91
244
/
/
cvttps2dq
%
xmm4
%
xmm6
.
byte
102
15
127
116
36
208
/
/
movdqa
%
xmm6
-
0x30
(
%
rsp
)
.
byte
68
15
40
252
/
/
movaps
%
xmm4
%
xmm15
.
byte
68
15
41
124
36
240
/
/
movaps
%
xmm15
-
0x10
(
%
rsp
)
.
byte
102
15
112
214
245
/
/
pshufd
0xf5
%
xmm6
%
xmm2
.
byte
102
15
244
215
/
/
pmuludq
%
xmm7
%
xmm2
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
244
230
/
/
pmuludq
%
xmm6
%
xmm4
.
byte
102
68
15
112
244
232
/
/
pshufd
0xe8
%
xmm4
%
xmm14
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
68
15
98
242
/
/
punpckldq
%
xmm2
%
xmm14
.
byte
102
65
15
110
208
/
/
movd
%
r8d
%
xmm2
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
244
216
/
/
pmuludq
%
xmm0
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
98
211
/
/
punpckldq
%
xmm3
%
xmm2
.
byte
102
68
15
112
226
0
/
/
pshufd
0x0
%
xmm2
%
xmm12
.
byte
68
139
64
12
/
/
mov
0xc
(
%
rax
)
%
r8d
.
byte
69
141
72
255
/
/
lea
-
0x1
(
%
r8
)
%
r9d
.
byte
102
65
15
110
217
/
/
movd
%
r9d
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
68
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm11
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
243
65
15
91
227
/
/
cvttps2dq
%
xmm11
%
xmm4
.
byte
102
15
127
100
36
192
/
/
movdqa
%
xmm4
-
0x40
(
%
rsp
)
.
byte
68
15
41
92
36
144
/
/
movaps
%
xmm11
-
0x70
(
%
rsp
)
.
byte
102
15
112
204
245
/
/
pshufd
0xf5
%
xmm4
%
xmm1
.
byte
102
65
15
244
204
/
/
pmuludq
%
xmm12
%
xmm1
.
byte
102
65
15
111
220
/
/
movdqa
%
xmm12
%
xmm3
.
byte
102
15
244
220
/
/
pmuludq
%
xmm4
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
98
217
/
/
punpckldq
%
xmm1
%
xmm3
.
byte
102
15
127
28
36
/
/
movdqa
%
xmm3
(
%
rsp
)
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
68
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8d
.
byte
65
255
200
/
/
dec
%
r8d
.
byte
102
65
15
110
208
/
/
movd
%
r8d
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
68
15
91
234
/
/
cvtdq2ps
%
xmm2
%
xmm13
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
243
69
15
91
205
/
/
cvttps2dq
%
xmm13
%
xmm9
.
byte
102
65
15
112
193
245
/
/
pshufd
0xf5
%
xmm9
%
xmm0
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
65
15
244
209
/
/
pmuludq
%
xmm9
%
xmm2
.
byte
102
15
112
226
232
/
/
pshufd
0xe8
%
xmm2
%
xmm4
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
98
224
/
/
punpckldq
%
xmm0
%
xmm4
.
byte
102
15
127
100
36
160
/
/
movdqa
%
xmm4
-
0x60
(
%
rsp
)
.
byte
243
15
91
197
/
/
cvttps2dq
%
xmm5
%
xmm0
.
byte
102
15
127
68
36
48
/
/
movdqa
%
xmm0
0x30
(
%
rsp
)
.
byte
68
15
40
197
/
/
movaps
%
xmm5
%
xmm8
.
byte
68
15
41
68
36
80
/
/
movaps
%
xmm8
0x50
(
%
rsp
)
.
byte
102
69
15
111
214
/
/
movdqa
%
xmm14
%
xmm10
.
byte
102
68
15
254
208
/
/
paddd
%
xmm0
%
xmm10
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
65
15
254
210
/
/
paddd
%
xmm10
%
xmm2
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
102
15
254
218
/
/
paddd
%
xmm2
%
xmm3
.
byte
102
15
111
37
81
8
1
0
/
/
movdqa
0x10851
(
%
rip
)
%
xmm4
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
112
195
245
/
/
pshufd
0xf5
%
xmm3
%
xmm0
.
byte
102
15
244
220
/
/
pmuludq
%
xmm4
%
xmm3
.
byte
102
15
244
196
/
/
pmuludq
%
xmm4
%
xmm0
.
byte
102
15
112
224
232
/
/
pshufd
0xe8
%
xmm0
%
xmm4
.
byte
102
15
112
195
232
/
/
pshufd
0xe8
%
xmm3
%
xmm0
.
byte
102
15
98
196
/
/
punpckldq
%
xmm4
%
xmm0
.
byte
102
65
15
126
216
/
/
movd
%
xmm3
%
r8d
.
byte
102
15
112
216
229
/
/
pshufd
0xe5
%
xmm0
%
xmm3
.
byte
102
65
15
126
217
/
/
movd
%
xmm3
%
r9d
.
byte
102
15
112
216
78
/
/
pshufd
0x4e
%
xmm0
%
xmm3
.
byte
102
65
15
126
218
/
/
movd
%
xmm3
%
r10d
.
byte
102
15
112
216
231
/
/
pshufd
0xe7
%
xmm0
%
xmm3
.
byte
102
65
15
126
219
/
/
movd
%
xmm3
%
r11d
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
243
66
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm3
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
227
/
/
unpcklps
%
xmm3
%
xmm4
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
243
66
15
16
60
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
102
15
20
252
/
/
unpcklpd
%
xmm4
%
xmm7
.
byte
102
15
118
228
/
/
pcmpeqd
%
xmm4
%
xmm4
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
250
220
/
/
psubd
%
xmm4
%
xmm3
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
73
15
126
217
/
/
movq
%
xmm3
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
102
15
254
5
59
1
1
0
/
/
paddd
0x1013b
(
%
rip
)
%
xmm0
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
73
15
126
193
/
/
movq
%
xmm0
%
r9
.
byte
69
137
202
/
/
mov
%
r9d
%
r10d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
243
66
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
15
20
220
/
/
unpcklps
%
xmm4
%
xmm3
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
15
40
53
69
7
1
0
/
/
movaps
0x10745
(
%
rip
)
%
xmm6
#
3d590
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1344
>
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
68
15
41
68
36
176
/
/
movaps
%
xmm8
-
0x50
(
%
rsp
)
.
byte
68
15
88
254
/
/
addps
%
xmm6
%
xmm15
.
byte
68
15
41
124
36
64
/
/
movaps
%
xmm15
0x40
(
%
rsp
)
.
byte
69
15
40
195
/
/
movaps
%
xmm11
%
xmm8
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
65
15
88
245
/
/
addps
%
xmm13
%
xmm6
.
byte
243
15
91
246
/
/
cvttps2dq
%
xmm6
%
xmm6
.
byte
102
15
112
198
245
/
/
pshufd
0xf5
%
xmm6
%
xmm0
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
244
206
/
/
pmuludq
%
xmm6
%
xmm1
.
byte
102
68
15
112
249
232
/
/
pshufd
0xe8
%
xmm1
%
xmm15
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
68
15
98
248
/
/
punpckldq
%
xmm0
%
xmm15
.
byte
102
65
15
254
215
/
/
paddd
%
xmm15
%
xmm2
.
byte
102
68
15
127
124
36
224
/
/
movdqa
%
xmm15
-
0x20
(
%
rsp
)
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
13
251
6
1
0
/
/
movdqa
0x106fb
(
%
rip
)
%
xmm1
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
65
15
126
208
/
/
movd
%
xmm2
%
r8d
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
65
15
126
194
/
/
movd
%
xmm0
%
r10d
.
byte
102
15
112
193
231
/
/
pshufd
0xe7
%
xmm1
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
15
20
208
/
/
unpcklps
%
xmm0
%
xmm2
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
102
15
20
242
/
/
unpcklpd
%
xmm2
%
xmm6
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
250
5
166
6
1
0
/
/
psubd
0x106a6
(
%
rip
)
%
xmm0
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
208
/
/
unpcklps
%
xmm0
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
68
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm11
.
byte
68
15
20
216
/
/
unpcklps
%
xmm0
%
xmm11
.
byte
102
65
15
20
211
/
/
unpcklpd
%
xmm11
%
xmm2
.
byte
102
15
254
13
238
255
0
0
/
/
paddd
0xffee
(
%
rip
)
%
xmm1
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
73
15
126
192
/
/
movq
%
xmm0
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
102
15
20
204
/
/
unpcklpd
%
xmm4
%
xmm1
.
byte
65
15
91
193
/
/
cvtdq2ps
%
xmm9
%
xmm0
.
byte
68
15
92
232
/
/
subps
%
xmm0
%
xmm13
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
65
15
89
245
/
/
mulps
%
xmm13
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
65
15
89
213
/
/
mulps
%
xmm13
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
65
15
89
205
/
/
mulps
%
xmm13
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
243
65
15
91
192
/
/
cvttps2dq
%
xmm8
%
xmm0
.
byte
102
15
112
216
245
/
/
pshufd
0xf5
%
xmm0
%
xmm3
.
byte
102
65
15
244
220
/
/
pmuludq
%
xmm12
%
xmm3
.
byte
102
68
15
244
224
/
/
pmuludq
%
xmm0
%
xmm12
.
byte
102
65
15
112
228
232
/
/
pshufd
0xe8
%
xmm12
%
xmm4
.
byte
102
15
112
195
232
/
/
pshufd
0xe8
%
xmm3
%
xmm0
.
byte
102
15
98
224
/
/
punpckldq
%
xmm0
%
xmm4
.
byte
102
15
127
100
36
16
/
/
movdqa
%
xmm4
0x10
(
%
rsp
)
.
byte
102
68
15
254
212
/
/
paddd
%
xmm4
%
xmm10
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
15
254
68
36
160
/
/
paddd
-
0x60
(
%
rsp
)
%
xmm0
.
byte
102
15
112
216
245
/
/
pshufd
0xf5
%
xmm0
%
xmm3
.
byte
102
15
111
37
165
5
1
0
/
/
movdqa
0x105a5
(
%
rip
)
%
xmm4
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
244
196
/
/
pmuludq
%
xmm4
%
xmm0
.
byte
102
15
244
220
/
/
pmuludq
%
xmm4
%
xmm3
.
byte
102
68
15
111
220
/
/
movdqa
%
xmm4
%
xmm11
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
224
232
/
/
pshufd
0xe8
%
xmm0
%
xmm4
.
byte
102
15
98
227
/
/
punpckldq
%
xmm3
%
xmm4
.
byte
102
65
15
126
192
/
/
movd
%
xmm0
%
r8d
.
byte
102
15
112
196
229
/
/
pshufd
0xe5
%
xmm4
%
xmm0
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
15
112
196
78
/
/
pshufd
0x4e
%
xmm4
%
xmm0
.
byte
102
65
15
126
194
/
/
movd
%
xmm0
%
r10d
.
byte
102
15
112
196
231
/
/
pshufd
0xe7
%
xmm4
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
70
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm9
.
byte
68
15
20
200
/
/
unpcklps
%
xmm0
%
xmm9
.
byte
102
68
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm9
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
69
15
118
228
/
/
pcmpeqd
%
xmm12
%
xmm12
.
byte
102
65
15
250
196
/
/
psubd
%
xmm12
%
xmm0
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
73
15
126
232
/
/
movq
%
xmm5
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
15
20
197
/
/
unpcklps
%
xmm5
%
xmm0
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
102
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm0
.
byte
102
68
15
111
5
145
254
0
0
/
/
movdqa
0xfe91
(
%
rip
)
%
xmm8
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
65
15
254
224
/
/
paddd
%
xmm8
%
xmm4
.
byte
102
15
112
236
78
/
/
pshufd
0x4e
%
xmm4
%
xmm5
.
byte
102
73
15
126
232
/
/
movq
%
xmm5
%
r8
.
byte
102
72
15
126
224
/
/
movq
%
xmm4
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
102
15
20
231
/
/
unpcklpd
%
xmm7
%
xmm4
.
byte
102
69
15
254
215
/
/
paddd
%
xmm15
%
xmm10
.
byte
102
65
15
112
234
245
/
/
pshufd
0xf5
%
xmm10
%
xmm5
.
byte
102
69
15
244
211
/
/
pmuludq
%
xmm11
%
xmm10
.
byte
102
65
15
244
235
/
/
pmuludq
%
xmm11
%
xmm5
.
byte
102
69
15
111
251
/
/
movdqa
%
xmm11
%
xmm15
.
byte
102
15
112
253
232
/
/
pshufd
0xe8
%
xmm5
%
xmm7
.
byte
102
65
15
112
234
232
/
/
pshufd
0xe8
%
xmm10
%
xmm5
.
byte
102
15
98
239
/
/
punpckldq
%
xmm7
%
xmm5
.
byte
102
69
15
126
208
/
/
movd
%
xmm10
%
r8d
.
byte
102
15
112
253
229
/
/
pshufd
0xe5
%
xmm5
%
xmm7
.
byte
102
65
15
126
249
/
/
movd
%
xmm7
%
r9d
.
byte
102
15
112
253
78
/
/
pshufd
0x4e
%
xmm5
%
xmm7
.
byte
102
65
15
126
250
/
/
movd
%
xmm7
%
r10d
.
byte
102
15
112
253
231
/
/
pshufd
0xe7
%
xmm5
%
xmm7
.
byte
102
15
126
248
/
/
movd
%
xmm7
%
eax
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
243
66
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm3
.
byte
15
20
223
/
/
unpcklps
%
xmm7
%
xmm3
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
243
70
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm11
.
byte
68
15
20
223
/
/
unpcklps
%
xmm7
%
xmm11
.
byte
102
68
15
20
219
/
/
unpcklpd
%
xmm3
%
xmm11
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
102
65
15
250
220
/
/
psubd
%
xmm12
%
xmm3
.
byte
102
15
112
251
78
/
/
pshufd
0x4e
%
xmm3
%
xmm7
.
byte
102
73
15
126
248
/
/
movq
%
xmm7
%
r8
.
byte
102
72
15
126
216
/
/
movq
%
xmm3
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
70
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm10
.
byte
68
15
20
211
/
/
unpcklps
%
xmm3
%
xmm10
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
102
68
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm10
.
byte
102
65
15
254
232
/
/
paddd
%
xmm8
%
xmm5
.
byte
102
15
112
221
78
/
/
pshufd
0x4e
%
xmm5
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
72
15
126
232
/
/
movq
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
70
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm12
.
byte
68
15
20
227
/
/
unpcklps
%
xmm3
%
xmm12
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
102
68
15
20
231
/
/
unpcklpd
%
xmm7
%
xmm12
.
byte
69
15
92
217
/
/
subps
%
xmm9
%
xmm11
.
byte
68
15
41
108
36
32
/
/
movaps
%
xmm13
0x20
(
%
rsp
)
.
byte
69
15
89
221
/
/
mulps
%
xmm13
%
xmm11
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
68
15
92
208
/
/
subps
%
xmm0
%
xmm10
.
byte
69
15
89
213
/
/
mulps
%
xmm13
%
xmm10
.
byte
68
15
88
208
/
/
addps
%
xmm0
%
xmm10
.
byte
68
15
92
228
/
/
subps
%
xmm4
%
xmm12
.
byte
69
15
89
229
/
/
mulps
%
xmm13
%
xmm12
.
byte
68
15
88
228
/
/
addps
%
xmm4
%
xmm12
.
byte
15
91
68
36
192
/
/
cvtdq2ps
-
0x40
(
%
rsp
)
%
xmm0
.
byte
15
40
92
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm3
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
41
92
36
144
/
/
movaps
%
xmm3
-
0x70
(
%
rsp
)
.
byte
68
15
92
222
/
/
subps
%
xmm6
%
xmm11
.
byte
68
15
89
219
/
/
mulps
%
xmm3
%
xmm11
.
byte
68
15
88
222
/
/
addps
%
xmm6
%
xmm11
.
byte
68
15
92
210
/
/
subps
%
xmm2
%
xmm10
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
68
15
88
210
/
/
addps
%
xmm2
%
xmm10
.
byte
68
15
92
225
/
/
subps
%
xmm1
%
xmm12
.
byte
68
15
89
227
/
/
mulps
%
xmm3
%
xmm12
.
byte
68
15
88
225
/
/
addps
%
xmm1
%
xmm12
.
byte
243
15
91
68
36
64
/
/
cvttps2dq
0x40
(
%
rsp
)
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
111
84
36
128
/
/
movdqa
-
0x80
(
%
rsp
)
%
xmm2
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
68
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm9
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
68
15
98
200
/
/
punpckldq
%
xmm0
%
xmm9
.
byte
102
68
15
127
76
36
128
/
/
movdqa
%
xmm9
-
0x80
(
%
rsp
)
.
byte
102
68
15
254
76
36
48
/
/
paddd
0x30
(
%
rsp
)
%
xmm9
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
254
4
36
/
/
paddd
(
%
rsp
)
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
68
15
111
68
36
160
/
/
movdqa
-
0x60
(
%
rsp
)
%
xmm8
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
102
15
112
202
245
/
/
pshufd
0xf5
%
xmm2
%
xmm1
.
byte
102
65
15
111
223
/
/
movdqa
%
xmm15
%
xmm3
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
244
203
/
/
pmuludq
%
xmm3
%
xmm1
.
byte
102
15
111
243
/
/
movdqa
%
xmm3
%
xmm6
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
203
/
/
punpckldq
%
xmm3
%
xmm1
.
byte
102
65
15
126
208
/
/
movd
%
xmm2
%
r8d
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
65
15
126
209
/
/
movd
%
xmm2
%
r9d
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
65
15
126
210
/
/
movd
%
xmm2
%
r10d
.
byte
102
15
112
209
231
/
/
pshufd
0xe7
%
xmm1
%
xmm2
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
243
15
16
20
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm2
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
226
/
/
unpcklps
%
xmm2
%
xmm4
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
15
20
220
/
/
unpcklpd
%
xmm4
%
xmm3
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
118
237
/
/
pcmpeqd
%
xmm5
%
xmm5
.
byte
102
15
250
213
/
/
psubd
%
xmm5
%
xmm2
.
byte
102
15
112
226
78
/
/
pshufd
0x4e
%
xmm2
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
20
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm2
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
15
20
250
/
/
unpcklps
%
xmm2
%
xmm7
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
15
20
226
/
/
unpcklps
%
xmm2
%
xmm4
.
byte
102
15
20
252
/
/
unpcklpd
%
xmm4
%
xmm7
.
byte
102
68
15
111
61
235
251
0
0
/
/
movdqa
0xfbeb
(
%
rip
)
%
xmm15
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
65
15
254
207
/
/
paddd
%
xmm15
%
xmm1
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
102
15
20
212
/
/
unpcklpd
%
xmm4
%
xmm2
.
byte
102
68
15
111
108
36
224
/
/
movdqa
-
0x20
(
%
rsp
)
%
xmm13
.
byte
102
65
15
254
197
/
/
paddd
%
xmm13
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
244
198
/
/
pmuludq
%
xmm6
%
xmm0
.
byte
102
15
244
206
/
/
pmuludq
%
xmm6
%
xmm1
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
224
232
/
/
pshufd
0xe8
%
xmm0
%
xmm4
.
byte
102
15
98
225
/
/
punpckldq
%
xmm1
%
xmm4
.
byte
102
65
15
126
192
/
/
movd
%
xmm0
%
r8d
.
byte
102
15
112
196
229
/
/
pshufd
0xe5
%
xmm4
%
xmm0
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
15
112
196
78
/
/
pshufd
0x4e
%
xmm4
%
xmm0
.
byte
102
65
15
126
194
/
/
movd
%
xmm0
%
r10d
.
byte
102
15
112
196
231
/
/
pshufd
0xe7
%
xmm4
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
52
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm6
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
250
197
/
/
psubd
%
xmm5
%
xmm0
.
byte
102
15
112
240
78
/
/
pshufd
0x4e
%
xmm0
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
15
20
198
/
/
unpcklps
%
xmm6
%
xmm0
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
238
/
/
unpcklps
%
xmm6
%
xmm5
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
102
65
15
254
231
/
/
paddd
%
xmm15
%
xmm4
.
byte
102
15
112
236
78
/
/
pshufd
0x4e
%
xmm4
%
xmm5
.
byte
102
73
15
126
232
/
/
movq
%
xmm5
%
r8
.
byte
102
72
15
126
224
/
/
movq
%
xmm4
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
15
20
230
/
/
unpcklpd
%
xmm6
%
xmm4
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
40
108
36
32
/
/
movaps
0x20
(
%
rsp
)
%
xmm5
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
226
/
/
addps
%
xmm2
%
xmm4
.
byte
102
68
15
254
76
36
16
/
/
paddd
0x10
(
%
rsp
)
%
xmm9
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
65
15
254
216
/
/
paddd
%
xmm8
%
xmm3
.
byte
102
15
112
211
245
/
/
pshufd
0xf5
%
xmm3
%
xmm2
.
byte
102
15
111
45
218
0
1
0
/
/
movdqa
0x100da
(
%
rip
)
%
xmm5
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
244
221
/
/
pmuludq
%
xmm5
%
xmm3
.
byte
102
15
244
213
/
/
pmuludq
%
xmm5
%
xmm2
.
byte
102
15
112
234
232
/
/
pshufd
0xe8
%
xmm2
%
xmm5
.
byte
102
15
112
211
232
/
/
pshufd
0xe8
%
xmm3
%
xmm2
.
byte
102
15
98
213
/
/
punpckldq
%
xmm5
%
xmm2
.
byte
102
65
15
126
216
/
/
movd
%
xmm3
%
r8d
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
65
15
126
217
/
/
movd
%
xmm3
%
r9d
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
65
15
126
218
/
/
movd
%
xmm3
%
r10d
.
byte
102
15
112
218
231
/
/
pshufd
0xe7
%
xmm2
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
243
70
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm8
.
byte
68
15
20
198
/
/
unpcklps
%
xmm6
%
xmm8
.
byte
102
68
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm8
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
250
45
131
0
1
0
/
/
psubd
0x10083
(
%
rip
)
%
xmm5
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
245
78
/
/
pshufd
0x4e
%
xmm5
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
232
/
/
movq
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm7
.
byte
102
65
15
254
215
/
/
paddd
%
xmm15
%
xmm2
.
byte
102
15
112
234
78
/
/
pshufd
0x4e
%
xmm2
%
xmm5
.
byte
102
73
15
126
232
/
/
movq
%
xmm5
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
15
20
214
/
/
unpcklpd
%
xmm6
%
xmm2
.
byte
102
69
15
254
205
/
/
paddd
%
xmm13
%
xmm9
.
byte
102
65
15
112
233
245
/
/
pshufd
0xf5
%
xmm9
%
xmm5
.
byte
102
15
111
29
225
255
0
0
/
/
movdqa
0xffe1
(
%
rip
)
%
xmm3
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
68
15
244
203
/
/
pmuludq
%
xmm3
%
xmm9
.
byte
102
15
244
235
/
/
pmuludq
%
xmm3
%
xmm5
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
102
69
15
112
249
232
/
/
pshufd
0xe8
%
xmm9
%
xmm15
.
byte
102
68
15
98
253
/
/
punpckldq
%
xmm5
%
xmm15
.
byte
102
69
15
126
200
/
/
movd
%
xmm9
%
r8d
.
byte
102
65
15
112
239
229
/
/
pshufd
0xe5
%
xmm15
%
xmm5
.
byte
102
65
15
126
233
/
/
movd
%
xmm5
%
r9d
.
byte
102
65
15
112
239
78
/
/
pshufd
0x4e
%
xmm15
%
xmm5
.
byte
102
65
15
126
234
/
/
movd
%
xmm5
%
r10d
.
byte
102
65
15
112
239
231
/
/
pshufd
0xe7
%
xmm15
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
243
68
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm9
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
65
15
20
233
/
/
unpcklps
%
xmm9
%
xmm5
.
byte
243
70
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm9
.
byte
243
70
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm13
.
byte
69
15
20
233
/
/
unpcklps
%
xmm9
%
xmm13
.
byte
102
68
15
20
237
/
/
unpcklpd
%
xmm5
%
xmm13
.
byte
102
65
15
111
239
/
/
movdqa
%
xmm15
%
xmm5
.
byte
102
15
250
45
129
255
0
0
/
/
psubd
0xff81
(
%
rip
)
%
xmm5
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
221
78
/
/
pshufd
0x4e
%
xmm5
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
72
15
126
232
/
/
movq
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
102
15
20
245
/
/
unpcklpd
%
xmm5
%
xmm6
.
byte
102
68
15
111
13
203
248
0
0
/
/
movdqa
0xf8cb
(
%
rip
)
%
xmm9
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
69
15
254
249
/
/
paddd
%
xmm9
%
xmm15
.
byte
102
65
15
112
223
78
/
/
pshufd
0x4e
%
xmm15
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
76
15
126
248
/
/
movq
%
xmm15
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
70
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm15
.
byte
68
15
20
251
/
/
unpcklps
%
xmm3
%
xmm15
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
102
68
15
20
253
/
/
unpcklpd
%
xmm5
%
xmm15
.
byte
69
15
92
232
/
/
subps
%
xmm8
%
xmm13
.
byte
15
40
92
36
32
/
/
movaps
0x20
(
%
rsp
)
%
xmm3
.
byte
68
15
89
235
/
/
mulps
%
xmm3
%
xmm13
.
byte
69
15
88
232
/
/
addps
%
xmm8
%
xmm13
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
68
15
92
250
/
/
subps
%
xmm2
%
xmm15
.
byte
68
15
89
251
/
/
mulps
%
xmm3
%
xmm15
.
byte
68
15
40
195
/
/
movaps
%
xmm3
%
xmm8
.
byte
68
15
88
250
/
/
addps
%
xmm2
%
xmm15
.
byte
68
15
92
233
/
/
subps
%
xmm1
%
xmm13
.
byte
15
40
84
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm2
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
68
15
88
233
/
/
addps
%
xmm1
%
xmm13
.
byte
65
15
40
205
/
/
movaps
%
xmm13
%
xmm1
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
15
88
240
/
/
addps
%
xmm0
%
xmm6
.
byte
68
15
92
252
/
/
subps
%
xmm4
%
xmm15
.
byte
68
15
89
250
/
/
mulps
%
xmm2
%
xmm15
.
byte
68
15
88
252
/
/
addps
%
xmm4
%
xmm15
.
byte
15
91
68
36
208
/
/
cvtdq2ps
-
0x30
(
%
rsp
)
%
xmm0
.
byte
15
40
84
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm2
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
15
41
84
36
240
/
/
movaps
%
xmm2
-
0x10
(
%
rsp
)
.
byte
65
15
92
203
/
/
subps
%
xmm11
%
xmm1
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
15
41
76
36
208
/
/
movaps
%
xmm1
-
0x30
(
%
rsp
)
.
byte
65
15
92
242
/
/
subps
%
xmm10
%
xmm6
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
65
15
88
242
/
/
addps
%
xmm10
%
xmm6
.
byte
15
41
116
36
192
/
/
movaps
%
xmm6
-
0x40
(
%
rsp
)
.
byte
69
15
92
252
/
/
subps
%
xmm12
%
xmm15
.
byte
68
15
89
250
/
/
mulps
%
xmm2
%
xmm15
.
byte
69
15
88
252
/
/
addps
%
xmm12
%
xmm15
.
byte
243
15
91
68
36
176
/
/
cvttps2dq
-
0x50
(
%
rsp
)
%
xmm0
.
byte
102
15
127
68
36
176
/
/
movdqa
%
xmm0
-
0x50
(
%
rsp
)
.
byte
102
68
15
254
240
/
/
paddd
%
xmm0
%
xmm14
.
byte
102
15
111
28
36
/
/
movdqa
(
%
rsp
)
%
xmm3
.
byte
102
65
15
254
222
/
/
paddd
%
xmm14
%
xmm3
.
byte
102
68
15
111
92
36
160
/
/
movdqa
-
0x60
(
%
rsp
)
%
xmm11
.
byte
102
65
15
111
195
/
/
movdqa
%
xmm11
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
111
21
28
254
0
0
/
/
movdqa
0xfe1c
(
%
rip
)
%
xmm2
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
68
15
111
234
/
/
movdqa
%
xmm2
%
xmm13
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
208
232
/
/
pshufd
0xe8
%
xmm0
%
xmm2
.
byte
102
15
98
209
/
/
punpckldq
%
xmm1
%
xmm2
.
byte
102
65
15
126
192
/
/
movd
%
xmm0
%
r8d
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
65
15
126
194
/
/
movd
%
xmm0
%
r10d
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
15
20
204
/
/
unpcklpd
%
xmm4
%
xmm1
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
118
255
/
/
pcmpeqd
%
xmm7
%
xmm7
.
byte
102
15
250
199
/
/
psubd
%
xmm7
%
xmm0
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
15
20
196
/
/
unpcklps
%
xmm4
%
xmm0
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
102
65
15
254
209
/
/
paddd
%
xmm9
%
xmm2
.
byte
102
15
112
226
78
/
/
pshufd
0x4e
%
xmm2
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
212
/
/
unpcklps
%
xmm4
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
213
/
/
unpcklpd
%
xmm5
%
xmm2
.
byte
102
68
15
111
84
36
224
/
/
movdqa
-
0x20
(
%
rsp
)
%
xmm10
.
byte
102
65
15
254
218
/
/
paddd
%
xmm10
%
xmm3
.
byte
102
15
112
227
245
/
/
pshufd
0xf5
%
xmm3
%
xmm4
.
byte
102
65
15
244
221
/
/
pmuludq
%
xmm13
%
xmm3
.
byte
102
65
15
244
229
/
/
pmuludq
%
xmm13
%
xmm4
.
byte
102
15
112
236
232
/
/
pshufd
0xe8
%
xmm4
%
xmm5
.
byte
102
15
112
227
232
/
/
pshufd
0xe8
%
xmm3
%
xmm4
.
byte
102
15
98
229
/
/
punpckldq
%
xmm5
%
xmm4
.
byte
102
65
15
126
216
/
/
movd
%
xmm3
%
r8d
.
byte
102
15
112
220
229
/
/
pshufd
0xe5
%
xmm4
%
xmm3
.
byte
102
65
15
126
217
/
/
movd
%
xmm3
%
r9d
.
byte
102
15
112
220
78
/
/
pshufd
0x4e
%
xmm4
%
xmm3
.
byte
102
65
15
126
218
/
/
movd
%
xmm3
%
r10d
.
byte
102
15
112
220
231
/
/
pshufd
0xe7
%
xmm4
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
15
20
222
/
/
unpcklps
%
xmm6
%
xmm3
.
byte
102
15
20
221
/
/
unpcklpd
%
xmm5
%
xmm3
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
250
239
/
/
psubd
%
xmm7
%
xmm5
.
byte
102
69
15
118
228
/
/
pcmpeqd
%
xmm12
%
xmm12
.
byte
102
15
112
245
78
/
/
pshufd
0x4e
%
xmm5
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
232
/
/
movq
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
15
20
238
/
/
unpcklps
%
xmm6
%
xmm5
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
243
15
16
60
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
102
15
20
239
/
/
unpcklpd
%
xmm7
%
xmm5
.
byte
102
65
15
254
225
/
/
paddd
%
xmm9
%
xmm4
.
byte
102
15
112
244
78
/
/
pshufd
0x4e
%
xmm4
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
224
/
/
movq
%
xmm4
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
15
20
252
/
/
unpcklps
%
xmm4
%
xmm7
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
102
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm7
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
65
15
89
216
/
/
mulps
%
xmm8
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
92
232
/
/
subps
%
xmm0
%
xmm5
.
byte
65
15
89
232
/
/
mulps
%
xmm8
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
65
15
89
248
/
/
mulps
%
xmm8
%
xmm7
.
byte
15
88
250
/
/
addps
%
xmm2
%
xmm7
.
byte
102
68
15
254
116
36
16
/
/
paddd
0x10
(
%
rsp
)
%
xmm14
.
byte
102
65
15
111
206
/
/
movdqa
%
xmm14
%
xmm1
.
byte
102
65
15
254
203
/
/
paddd
%
xmm11
%
xmm1
.
byte
102
15
112
193
245
/
/
pshufd
0xf5
%
xmm1
%
xmm0
.
byte
102
65
15
244
205
/
/
pmuludq
%
xmm13
%
xmm1
.
byte
102
65
15
244
197
/
/
pmuludq
%
xmm13
%
xmm0
.
byte
102
15
112
208
232
/
/
pshufd
0xe8
%
xmm0
%
xmm2
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
98
194
/
/
punpckldq
%
xmm2
%
xmm0
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
65
15
126
201
/
/
movd
%
xmm1
%
r9d
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
65
15
126
202
/
/
movd
%
xmm1
%
r10d
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
70
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm11
.
byte
68
15
20
217
/
/
unpcklps
%
xmm1
%
xmm11
.
byte
102
68
15
20
220
/
/
unpcklpd
%
xmm4
%
xmm11
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
65
15
250
204
/
/
psubd
%
xmm12
%
xmm1
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
15
20
204
/
/
unpcklps
%
xmm4
%
xmm1
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
65
15
254
193
/
/
paddd
%
xmm9
%
xmm0
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
15
20
196
/
/
unpcklps
%
xmm4
%
xmm0
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
69
15
254
242
/
/
paddd
%
xmm10
%
xmm14
.
byte
102
69
15
111
226
/
/
movdqa
%
xmm10
%
xmm12
.
byte
102
65
15
112
230
245
/
/
pshufd
0xf5
%
xmm14
%
xmm4
.
byte
102
69
15
244
245
/
/
pmuludq
%
xmm13
%
xmm14
.
byte
102
65
15
244
229
/
/
pmuludq
%
xmm13
%
xmm4
.
byte
102
15
112
244
232
/
/
pshufd
0xe8
%
xmm4
%
xmm6
.
byte
102
65
15
112
230
232
/
/
pshufd
0xe8
%
xmm14
%
xmm4
.
byte
102
15
98
230
/
/
punpckldq
%
xmm6
%
xmm4
.
byte
102
69
15
126
240
/
/
movd
%
xmm14
%
r8d
.
byte
102
15
112
244
229
/
/
pshufd
0xe5
%
xmm4
%
xmm6
.
byte
102
65
15
126
241
/
/
movd
%
xmm6
%
r9d
.
byte
102
15
112
244
78
/
/
pshufd
0x4e
%
xmm4
%
xmm6
.
byte
102
65
15
126
242
/
/
movd
%
xmm6
%
r10d
.
byte
102
15
112
244
231
/
/
pshufd
0xe7
%
xmm4
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
243
66
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
15
20
214
/
/
unpcklps
%
xmm6
%
xmm2
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
243
70
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm13
.
byte
68
15
20
238
/
/
unpcklps
%
xmm6
%
xmm13
.
byte
102
68
15
20
234
/
/
unpcklpd
%
xmm2
%
xmm13
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
250
21
184
250
0
0
/
/
psubd
0xfab8
(
%
rip
)
%
xmm2
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
242
78
/
/
pshufd
0x4e
%
xmm2
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
20
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm2
.
byte
243
70
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm10
.
byte
68
15
20
210
/
/
unpcklps
%
xmm2
%
xmm10
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
102
68
15
20
214
/
/
unpcklpd
%
xmm6
%
xmm10
.
byte
102
65
15
254
225
/
/
paddd
%
xmm9
%
xmm4
.
byte
102
15
112
212
78
/
/
pshufd
0x4e
%
xmm4
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
102
72
15
126
224
/
/
movq
%
xmm4
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
20
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm2
.
byte
243
70
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm9
.
byte
68
15
20
202
/
/
unpcklps
%
xmm2
%
xmm9
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
20
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm2
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
102
68
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm9
.
byte
69
15
92
235
/
/
subps
%
xmm11
%
xmm13
.
byte
69
15
89
232
/
/
mulps
%
xmm8
%
xmm13
.
byte
69
15
88
235
/
/
addps
%
xmm11
%
xmm13
.
byte
68
15
92
209
/
/
subps
%
xmm1
%
xmm10
.
byte
69
15
89
208
/
/
mulps
%
xmm8
%
xmm10
.
byte
68
15
88
209
/
/
addps
%
xmm1
%
xmm10
.
byte
68
15
92
200
/
/
subps
%
xmm0
%
xmm9
.
byte
69
15
89
200
/
/
mulps
%
xmm8
%
xmm9
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
88
200
/
/
addps
%
xmm0
%
xmm9
.
byte
68
15
92
235
/
/
subps
%
xmm3
%
xmm13
.
byte
15
40
68
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm0
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
68
15
88
235
/
/
addps
%
xmm3
%
xmm13
.
byte
68
15
92
213
/
/
subps
%
xmm5
%
xmm10
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
68
15
88
213
/
/
addps
%
xmm5
%
xmm10
.
byte
68
15
92
207
/
/
subps
%
xmm7
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
68
15
88
207
/
/
addps
%
xmm7
%
xmm9
.
byte
102
15
111
68
36
128
/
/
movdqa
-
0x80
(
%
rsp
)
%
xmm0
.
byte
102
15
254
68
36
176
/
/
paddd
-
0x50
(
%
rsp
)
%
xmm0
.
byte
102
15
127
68
36
128
/
/
movdqa
%
xmm0
-
0x80
(
%
rsp
)
.
byte
102
15
111
12
36
/
/
movdqa
(
%
rsp
)
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
68
15
111
116
36
160
/
/
movdqa
-
0x60
(
%
rsp
)
%
xmm14
.
byte
102
65
15
254
198
/
/
paddd
%
xmm14
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
68
15
111
5
151
249
0
0
/
/
movdqa
0xf997
(
%
rip
)
%
xmm8
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
65
15
244
192
/
/
pmuludq
%
xmm8
%
xmm0
.
byte
102
65
15
244
200
/
/
pmuludq
%
xmm8
%
xmm1
.
byte
102
65
15
111
240
/
/
movdqa
%
xmm8
%
xmm6
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
208
232
/
/
pshufd
0xe8
%
xmm0
%
xmm2
.
byte
102
15
98
209
/
/
punpckldq
%
xmm1
%
xmm2
.
byte
102
65
15
126
192
/
/
movd
%
xmm0
%
r8d
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
65
15
126
194
/
/
movd
%
xmm0
%
r10d
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
118
255
/
/
pcmpeqd
%
xmm7
%
xmm7
.
byte
102
15
250
207
/
/
psubd
%
xmm7
%
xmm1
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
15
20
203
/
/
unpcklps
%
xmm3
%
xmm1
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
102
68
15
111
5
133
242
0
0
/
/
movdqa
0xf285
(
%
rip
)
%
xmm8
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
65
15
254
208
/
/
paddd
%
xmm8
%
xmm2
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
73
15
126
216
/
/
movq
%
xmm3
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
102
15
20
213
/
/
unpcklpd
%
xmm5
%
xmm2
.
byte
102
65
15
254
228
/
/
paddd
%
xmm12
%
xmm4
.
byte
102
15
112
220
245
/
/
pshufd
0xf5
%
xmm4
%
xmm3
.
byte
102
15
244
230
/
/
pmuludq
%
xmm6
%
xmm4
.
byte
102
15
244
222
/
/
pmuludq
%
xmm6
%
xmm3
.
byte
102
15
112
235
232
/
/
pshufd
0xe8
%
xmm3
%
xmm5
.
byte
102
15
112
220
232
/
/
pshufd
0xe8
%
xmm4
%
xmm3
.
byte
102
15
98
221
/
/
punpckldq
%
xmm5
%
xmm3
.
byte
102
65
15
126
224
/
/
movd
%
xmm4
%
r8d
.
byte
102
15
112
235
229
/
/
pshufd
0xe5
%
xmm3
%
xmm5
.
byte
102
65
15
126
233
/
/
movd
%
xmm5
%
r9d
.
byte
102
15
112
235
78
/
/
pshufd
0x4e
%
xmm3
%
xmm5
.
byte
102
65
15
126
234
/
/
movd
%
xmm5
%
r10d
.
byte
102
15
112
235
231
/
/
pshufd
0xe7
%
xmm3
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
52
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
243
70
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm12
.
byte
68
15
20
229
/
/
unpcklps
%
xmm5
%
xmm12
.
byte
102
68
15
20
230
/
/
unpcklpd
%
xmm6
%
xmm12
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
250
239
/
/
psubd
%
xmm7
%
xmm5
.
byte
102
15
112
245
78
/
/
pshufd
0x4e
%
xmm5
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
232
/
/
movq
%
xmm5
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
44
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm5
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm7
.
byte
102
65
15
254
216
/
/
paddd
%
xmm8
%
xmm3
.
byte
102
15
112
235
78
/
/
pshufd
0x4e
%
xmm3
%
xmm5
.
byte
102
73
15
126
232
/
/
movq
%
xmm5
%
r8
.
byte
102
72
15
126
216
/
/
movq
%
xmm3
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
28
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm3
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
28
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm3
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
68
15
92
224
/
/
subps
%
xmm0
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
68
15
88
224
/
/
addps
%
xmm0
%
xmm12
.
byte
15
92
249
/
/
subps
%
xmm1
%
xmm7
.
byte
65
15
89
251
/
/
mulps
%
xmm11
%
xmm7
.
byte
15
88
249
/
/
addps
%
xmm1
%
xmm7
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
65
15
89
235
/
/
mulps
%
xmm11
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
102
15
111
68
36
128
/
/
movdqa
-
0x80
(
%
rsp
)
%
xmm0
.
byte
102
15
254
68
36
16
/
/
paddd
0x10
(
%
rsp
)
%
xmm0
.
byte
102
65
15
111
214
/
/
movdqa
%
xmm14
%
xmm2
.
byte
102
15
254
208
/
/
paddd
%
xmm0
%
xmm2
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
37
113
247
0
0
/
/
movdqa
0xf771
(
%
rip
)
%
xmm4
#
3d5a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1354
>
.
byte
102
15
244
212
/
/
pmuludq
%
xmm4
%
xmm2
.
byte
102
15
244
196
/
/
pmuludq
%
xmm4
%
xmm0
.
byte
102
15
112
200
232
/
/
pshufd
0xe8
%
xmm0
%
xmm1
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
65
15
126
208
/
/
movd
%
xmm2
%
r8d
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
65
15
126
201
/
/
movd
%
xmm1
%
r9d
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
65
15
126
202
/
/
movd
%
xmm1
%
r10d
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
66
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
70
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm8
.
byte
68
15
20
193
/
/
unpcklps
%
xmm1
%
xmm8
.
byte
102
68
15
20
194
/
/
unpcklpd
%
xmm2
%
xmm8
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
250
13
26
247
0
0
/
/
psubd
0xf71a
(
%
rip
)
%
xmm1
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
243
70
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm14
.
byte
68
15
20
241
/
/
unpcklps
%
xmm1
%
xmm14
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
243
15
16
20
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
68
15
20
242
/
/
unpcklpd
%
xmm2
%
xmm14
.
byte
102
15
254
5
99
240
0
0
/
/
paddd
0xf063
(
%
rip
)
%
xmm0
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
73
15
126
200
/
/
movq
%
xmm1
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
28
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm3
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
243
15
16
12
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
15
20
217
/
/
unpcklpd
%
xmm1
%
xmm3
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
254
76
36
224
/
/
paddd
-
0x20
(
%
rsp
)
%
xmm1
.
byte
102
15
112
193
245
/
/
pshufd
0xf5
%
xmm1
%
xmm0
.
byte
102
15
244
204
/
/
pmuludq
%
xmm4
%
xmm1
.
byte
102
15
244
196
/
/
pmuludq
%
xmm4
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
209
232
/
/
pshufd
0xe8
%
xmm1
%
xmm2
.
byte
102
15
98
208
/
/
punpckldq
%
xmm0
%
xmm2
.
byte
102
65
15
126
200
/
/
movd
%
xmm1
%
r8d
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
65
15
126
193
/
/
movd
%
xmm0
%
r9d
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
65
15
126
194
/
/
movd
%
xmm0
%
r10d
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
243
15
16
4
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm0
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
15
20
198
/
/
unpcklps
%
xmm6
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
250
13
34
246
0
0
/
/
psubd
0xf622
(
%
rip
)
%
xmm1
#
3d5b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1364
>
.
byte
102
15
112
241
78
/
/
pshufd
0x4e
%
xmm1
%
xmm6
.
byte
102
73
15
126
240
/
/
movq
%
xmm6
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
15
20
206
/
/
unpcklps
%
xmm6
%
xmm1
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
15
20
230
/
/
unpcklps
%
xmm6
%
xmm4
.
byte
102
15
20
204
/
/
unpcklpd
%
xmm4
%
xmm1
.
byte
102
15
254
21
109
239
0
0
/
/
paddd
0xef6d
(
%
rip
)
%
xmm2
#
3cf40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcf4
>
.
byte
102
15
112
226
78
/
/
pshufd
0x4e
%
xmm2
%
xmm4
.
byte
102
73
15
126
224
/
/
movq
%
xmm4
%
r8
.
byte
102
72
15
126
208
/
/
movq
%
xmm2
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
243
15
16
36
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm4
.
byte
243
66
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm2
.
byte
15
20
212
/
/
unpcklps
%
xmm4
%
xmm2
.
byte
68
137
192
/
/
mov
%
r8d
%
eax
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
15
16
52
131
/
/
movss
(
%
rbx
%
rax
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
102
15
20
214
/
/
unpcklpd
%
xmm6
%
xmm2
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
65
15
89
195
/
/
mulps
%
xmm11
%
xmm0
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
65
15
92
206
/
/
subps
%
xmm14
%
xmm1
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
65
15
88
206
/
/
addps
%
xmm14
%
xmm1
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
65
15
89
211
/
/
mulps
%
xmm11
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
65
15
92
196
/
/
subps
%
xmm12
%
xmm0
.
byte
15
40
92
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
65
15
88
196
/
/
addps
%
xmm12
%
xmm0
.
byte
15
92
207
/
/
subps
%
xmm7
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
65
15
92
197
/
/
subps
%
xmm13
%
xmm0
.
byte
15
40
92
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
65
15
88
197
/
/
addps
%
xmm13
%
xmm0
.
byte
65
15
92
202
/
/
subps
%
xmm10
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
65
15
92
209
/
/
subps
%
xmm9
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
15
91
92
36
48
/
/
cvtdq2ps
0x30
(
%
rsp
)
%
xmm3
.
byte
15
40
100
36
80
/
/
movaps
0x50
(
%
rsp
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
40
92
36
208
/
/
movaps
-
0x30
(
%
rsp
)
%
xmm3
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
40
92
36
192
/
/
movaps
-
0x40
(
%
rsp
)
%
xmm3
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
65
15
92
215
/
/
subps
%
xmm15
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
65
15
88
215
/
/
addps
%
xmm15
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
89
238
0
0
/
/
movaps
0xee59
(
%
rip
)
%
xmm3
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
100
36
96
/
/
movaps
0x60
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
112
/
/
movaps
0x70
(
%
rsp
)
%
xmm5
.
byte
15
40
180
36
128
0
0
0
/
/
movaps
0x80
(
%
rsp
)
%
xmm6
.
byte
15
40
188
36
144
0
0
0
/
/
movaps
0x90
(
%
rsp
)
%
xmm7
.
byte
72
129
196
160
0
0
0
/
/
add
0xa0
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gauss_a_to_rgba_sse2
.
globl
_sk_gauss_a_to_rgba_sse2
FUNCTION
(
_sk_gauss_a_to_rgba_sse2
)
_sk_gauss_a_to_rgba_sse2
:
.
byte
15
40
5
222
244
0
0
/
/
movaps
0xf4de
(
%
rip
)
%
xmm0
#
3d5c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1374
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
228
244
0
0
/
/
addps
0xf4e4
(
%
rip
)
%
xmm0
#
3d5d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1384
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
234
244
0
0
/
/
addps
0xf4ea
(
%
rip
)
%
xmm0
#
3d5e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1394
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
240
244
0
0
/
/
addps
0xf4f0
(
%
rip
)
%
xmm0
#
3d5f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13a4
>
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
5
246
244
0
0
/
/
addps
0xf4f6
(
%
rip
)
%
xmm0
#
3d600
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13b4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_bilerp_clamp_8888_sse2
.
globl
_sk_bilerp_clamp_8888_sse2
FUNCTION
(
_sk_bilerp_clamp_8888_sse2
)
_sk_bilerp_clamp_8888_sse2
:
.
byte
72
131
236
88
/
/
sub
0x58
%
rsp
.
byte
15
41
124
36
176
/
/
movaps
%
xmm7
-
0x50
(
%
rsp
)
.
byte
15
41
116
36
160
/
/
movaps
%
xmm6
-
0x60
(
%
rsp
)
.
byte
15
41
108
36
144
/
/
movaps
%
xmm5
-
0x70
(
%
rsp
)
.
byte
15
41
100
36
128
/
/
movaps
%
xmm4
-
0x80
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
37
200
237
0
0
/
/
movaps
0xedc8
(
%
rip
)
%
xmm4
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
41
68
36
64
/
/
movaps
%
xmm0
0x40
(
%
rsp
)
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
88
236
/
/
addps
%
xmm4
%
xmm5
.
byte
243
15
91
197
/
/
cvttps2dq
%
xmm5
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
15
194
208
1
/
/
cmpltps
%
xmm0
%
xmm2
.
byte
15
40
53
184
237
0
0
/
/
movaps
0xedb8
(
%
rip
)
%
xmm6
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
84
214
/
/
andps
%
xmm6
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
92
232
/
/
subps
%
xmm0
%
xmm5
.
byte
15
41
12
36
/
/
movaps
%
xmm1
(
%
rsp
)
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
243
15
91
204
/
/
cvttps2dq
%
xmm4
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
15
194
209
1
/
/
cmpltps
%
xmm1
%
xmm2
.
byte
15
84
214
/
/
andps
%
xmm6
%
xmm2
.
byte
15
92
202
/
/
subps
%
xmm2
%
xmm1
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
41
108
36
48
/
/
movaps
%
xmm5
0x30
(
%
rsp
)
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
rsp
)
.
byte
15
41
100
36
240
/
/
movaps
%
xmm4
-
0x10
(
%
rsp
)
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
41
116
36
192
/
/
movaps
%
xmm6
-
0x40
(
%
rsp
)
.
byte
243
68
15
16
112
12
/
/
movss
0xc
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
102
15
118
201
/
/
pcmpeqd
%
xmm1
%
xmm1
.
byte
102
68
15
254
241
/
/
paddd
%
xmm1
%
xmm14
.
byte
243
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
15
127
68
36
224
/
/
movdqa
%
xmm0
-
0x20
(
%
rsp
)
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
102
15
110
72
8
/
/
movd
0x8
(
%
rax
)
%
xmm1
.
byte
102
15
112
193
0
/
/
pshufd
0x0
%
xmm1
%
xmm0
.
byte
102
15
127
68
36
208
/
/
movdqa
%
xmm0
-
0x30
(
%
rsp
)
.
byte
243
15
16
5
111
228
0
0
/
/
movss
0xe46f
(
%
rip
)
%
xmm0
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
102
15
111
45
215
237
0
0
/
/
movdqa
0xedd7
(
%
rip
)
%
xmm5
#
3cfc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd74
>
.
byte
15
40
37
48
239
0
0
/
/
movaps
0xef30
(
%
rip
)
%
xmm4
#
3d120
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xed4
>
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
52
36
/
/
addps
(
%
rsp
)
%
xmm6
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
95
206
/
/
maxps
%
xmm6
%
xmm1
.
byte
15
41
124
36
16
/
/
movaps
%
xmm7
0x10
(
%
rsp
)
.
byte
15
46
61
112
228
0
0
/
/
ucomiss
0xe470
(
%
rip
)
%
xmm7
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
68
15
40
124
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm15
.
byte
119
6
/
/
ja
2e22a
<
_sk_bilerp_clamp_8888_sse2
+
0x113
>
.
byte
68
15
40
124
36
192
/
/
movaps
-
0x40
(
%
rsp
)
%
xmm15
.
byte
15
93
76
36
224
/
/
minps
-
0x20
(
%
rsp
)
%
xmm1
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
111
68
36
208
/
/
movdqa
-
0x30
(
%
rsp
)
%
xmm0
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
244
241
/
/
pmuludq
%
xmm1
%
xmm6
.
byte
102
68
15
112
230
232
/
/
pshufd
0xe8
%
xmm6
%
xmm12
.
byte
102
15
112
201
245
/
/
pshufd
0xf5
%
xmm1
%
xmm1
.
byte
102
15
112
240
245
/
/
pshufd
0xf5
%
xmm0
%
xmm6
.
byte
102
15
244
241
/
/
pmuludq
%
xmm1
%
xmm6
.
byte
102
15
112
206
232
/
/
pshufd
0xe8
%
xmm6
%
xmm1
.
byte
102
68
15
98
225
/
/
punpckldq
%
xmm1
%
xmm12
.
byte
243
15
16
13
233
227
0
0
/
/
movss
0xe3e9
(
%
rip
)
%
xmm1
#
3c650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x404
>
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
116
36
64
/
/
addps
0x40
(
%
rsp
)
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
95
254
/
/
maxps
%
xmm6
%
xmm7
.
byte
65
15
93
254
/
/
minps
%
xmm14
%
xmm7
.
byte
243
15
91
247
/
/
cvttps2dq
%
xmm7
%
xmm6
.
byte
102
65
15
254
244
/
/
paddd
%
xmm12
%
xmm6
.
byte
102
15
112
254
78
/
/
pshufd
0x4e
%
xmm6
%
xmm7
.
byte
102
72
15
126
248
/
/
movq
%
xmm7
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
242
/
/
movq
%
xmm6
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
67
15
110
52
144
/
/
movd
(
%
r8
%
r10
4
)
%
xmm6
.
byte
102
67
15
110
4
152
/
/
movd
(
%
r8
%
r11
4
)
%
xmm0
.
byte
102
15
98
198
/
/
punpckldq
%
xmm6
%
xmm0
.
byte
102
65
15
110
52
128
/
/
movd
(
%
r8
%
rax
4
)
%
xmm6
.
byte
102
67
15
110
60
136
/
/
movd
(
%
r8
%
r9
4
)
%
xmm7
.
byte
102
15
98
254
/
/
punpckldq
%
xmm6
%
xmm7
.
byte
102
15
108
199
/
/
punpcklqdq
%
xmm7
%
xmm0
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
68
15
91
238
/
/
cvtdq2ps
%
xmm6
%
xmm13
.
byte
68
15
89
236
/
/
mulps
%
xmm4
%
xmm13
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
114
214
8
/
/
psrld
0x8
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
114
215
16
/
/
psrld
0x10
%
xmm7
.
byte
102
15
219
253
/
/
pand
%
xmm5
%
xmm7
.
byte
15
91
255
/
/
cvtdq2ps
%
xmm7
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
102
15
114
208
24
/
/
psrld
0x18
%
xmm0
.
byte
68
15
91
208
/
/
cvtdq2ps
%
xmm0
%
xmm10
.
byte
68
15
89
212
/
/
mulps
%
xmm4
%
xmm10
.
byte
15
46
13
123
227
0
0
/
/
ucomiss
0xe37b
(
%
rip
)
%
xmm1
#
3c68c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x440
>
.
byte
68
15
40
92
36
48
/
/
movaps
0x30
(
%
rsp
)
%
xmm11
.
byte
119
6
/
/
ja
2e31f
<
_sk_bilerp_clamp_8888_sse2
+
0x208
>
.
byte
68
15
40
92
36
32
/
/
movaps
0x20
(
%
rsp
)
%
xmm11
.
byte
69
15
89
223
/
/
mulps
%
xmm15
%
xmm11
.
byte
69
15
89
235
/
/
mulps
%
xmm11
%
xmm13
.
byte
69
15
88
205
/
/
addps
%
xmm13
%
xmm9
.
byte
65
15
89
243
/
/
mulps
%
xmm11
%
xmm6
.
byte
68
15
88
198
/
/
addps
%
xmm6
%
xmm8
.
byte
65
15
89
251
/
/
mulps
%
xmm11
%
xmm7
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
243
15
88
13
178
225
0
0
/
/
addss
0xe1b2
(
%
rip
)
%
xmm1
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
243
15
16
5
166
225
0
0
/
/
movss
0xe1a6
(
%
rip
)
%
xmm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
15
46
193
/
/
ucomiss
%
xmm1
%
xmm0
.
byte
15
131
12
255
255
255
/
/
jae
2e267
<
_sk_bilerp_clamp_8888_sse2
+
0x150
>
.
byte
15
40
124
36
16
/
/
movaps
0x10
(
%
rsp
)
%
xmm7
.
byte
243
15
88
61
148
225
0
0
/
/
addss
0xe194
(
%
rip
)
%
xmm7
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
243
15
16
5
136
225
0
0
/
/
movss
0xe188
(
%
rip
)
%
xmm0
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
15
46
199
/
/
ucomiss
%
xmm7
%
xmm0
.
byte
15
131
133
254
255
255
/
/
jae
2e1fe
<
_sk_bilerp_clamp_8888_sse2
+
0xe7
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
40
200
/
/
movaps
%
xmm8
%
xmm1
.
byte
15
40
100
36
128
/
/
movaps
-
0x80
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
144
/
/
movaps
-
0x70
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
160
/
/
movaps
-
0x60
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
176
/
/
movaps
-
0x50
(
%
rsp
)
%
xmm7
.
byte
72
131
196
88
/
/
add
0x58
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
144
/
/
nop
.
byte
144
/
/
nop
HIDDEN
_sk_start_pipeline_hsw_lowp
.
globl
_sk_start_pipeline_hsw_lowp
FUNCTION
(
_sk_start_pipeline_hsw_lowp
)
_sk_start_pipeline_hsw_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
151
0
0
0
/
/
jae
2e46a
<
_sk_start_pipeline_hsw_lowp
+
0xca
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
16
/
/
lea
0x10
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
119
67
/
/
ja
2e42c
<
_sk_start_pipeline_hsw_lowp
+
0x8c
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
16
/
/
lea
0x10
(
%
r12
)
%
rdx
.
byte
73
131
196
32
/
/
add
0x20
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
193
/
/
jbe
2e3ed
<
_sk_start_pipeline_hsw_lowp
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
41
/
/
je
2e45d
<
_sk_start_pipeline_hsw_lowp
+
0xbd
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
255
195
/
/
inc
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
15
133
117
255
255
255
/
/
jne
2e3df
<
_sk_start_pipeline_hsw_lowp
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
197
248
119
/
/
vzeroupper
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_hsw_lowp
.
globl
_sk_just_return_hsw_lowp
FUNCTION
(
_sk_just_return_hsw_lowp
)
_sk_just_return_hsw_lowp
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_hsw_lowp
.
globl
_sk_seed_shader_hsw_lowp
FUNCTION
(
_sk_seed_shader_hsw_lowp
)
_sk_seed_shader_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
249
110
194
/
/
vmovd
%
edx
%
xmm0
.
byte
196
226
125
88
192
/
/
vpbroadcastd
%
xmm0
%
ymm0
.
byte
197
252
91
192
/
/
vcvtdq2ps
%
ymm0
%
ymm0
.
byte
197
252
88
72
32
/
/
vaddps
0x20
(
%
rax
)
%
ymm0
%
ymm1
.
byte
197
252
88
0
/
/
vaddps
(
%
rax
)
%
ymm0
%
ymm0
.
byte
197
249
110
209
/
/
vmovd
%
ecx
%
xmm2
.
byte
196
226
125
88
210
/
/
vpbroadcastd
%
xmm2
%
ymm2
.
byte
197
252
91
210
/
/
vcvtdq2ps
%
ymm2
%
ymm2
.
byte
196
226
125
24
29
77
224
0
0
/
/
vbroadcastss
0xe04d
(
%
rip
)
%
ymm3
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
236
88
211
/
/
vaddps
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
218
/
/
vmovaps
%
ymm2
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_hsw_lowp
.
globl
_sk_matrix_translate_hsw_lowp
FUNCTION
(
_sk_matrix_translate_hsw_lowp
)
_sk_matrix_translate_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
196
193
108
88
208
/
/
vaddps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
88
216
/
/
vaddps
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_hsw_lowp
.
globl
_sk_matrix_scale_translate_hsw_lowp
FUNCTION
(
_sk_matrix_scale_translate_hsw_lowp
)
_sk_matrix_scale_translate_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
0
/
/
vbroadcastss
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm9
.
byte
196
194
61
168
193
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm0
.
byte
196
194
61
168
201
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm1
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
194
61
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm2
.
byte
196
194
61
168
217
/
/
vfmadd213ps
%
ymm9
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_hsw_lowp
.
globl
_sk_matrix_2x3_hsw_lowp
FUNCTION
(
_sk_matrix_2x3_hsw_lowp
)
_sk_matrix_2x3_hsw_lowp
:
.
byte
197
124
40
192
/
/
vmovaps
%
ymm0
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm10
.
byte
196
226
125
24
64
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm0
.
byte
196
98
125
24
88
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm11
.
byte
197
124
40
200
/
/
vmovaps
%
ymm0
%
ymm9
.
byte
196
66
101
168
203
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm9
.
byte
196
194
109
168
195
/
/
vfmadd213ps
%
ymm11
%
ymm2
%
ymm0
.
byte
196
194
61
184
194
/
/
vfmadd231ps
%
ymm10
%
ymm8
%
ymm0
.
byte
196
66
117
184
202
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm9
.
byte
196
98
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
88
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm11
.
byte
196
98
125
24
96
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm12
.
byte
196
194
37
168
220
/
/
vfmadd213ps
%
ymm12
%
ymm11
%
ymm3
.
byte
196
66
109
168
220
/
/
vfmadd213ps
%
ymm12
%
ymm2
%
ymm11
.
byte
196
66
45
168
195
/
/
vfmadd213ps
%
ymm11
%
ymm10
%
ymm8
.
byte
196
194
117
184
218
/
/
vfmadd231ps
%
ymm10
%
ymm1
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
41
201
/
/
vmovaps
%
ymm9
%
ymm1
.
byte
197
124
41
194
/
/
vmovaps
%
ymm8
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_hsw_lowp
.
globl
_sk_matrix_perspective_hsw_lowp
FUNCTION
(
_sk_matrix_perspective_hsw_lowp
)
_sk_matrix_perspective_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
24
8
/
/
vbroadcastss
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
64
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm8
.
byte
196
98
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm10
.
byte
196
65
124
40
216
/
/
vmovaps
%
ymm8
%
ymm11
.
byte
196
66
109
168
218
/
/
vfmadd213ps
%
ymm10
%
ymm2
%
ymm11
.
byte
196
66
101
168
194
/
/
vfmadd213ps
%
ymm10
%
ymm3
%
ymm8
.
byte
196
66
117
184
193
/
/
vfmadd231ps
%
ymm9
%
ymm1
%
ymm8
.
byte
196
66
125
184
217
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm11
.
byte
196
98
125
24
72
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
80
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm10
.
byte
196
98
125
24
96
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm12
.
byte
196
65
124
40
234
/
/
vmovaps
%
ymm10
%
ymm13
.
byte
196
66
109
168
236
/
/
vfmadd213ps
%
ymm12
%
ymm2
%
ymm13
.
byte
196
66
101
168
212
/
/
vfmadd213ps
%
ymm12
%
ymm3
%
ymm10
.
byte
196
66
117
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm1
%
ymm10
.
byte
196
66
125
184
233
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm13
.
byte
196
98
125
24
72
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm9
.
byte
196
98
125
24
96
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
112
32
/
/
vbroadcastss
0x20
(
%
rax
)
%
ymm14
.
byte
196
194
29
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm12
%
ymm2
.
byte
196
66
101
168
230
/
/
vfmadd213ps
%
ymm14
%
ymm3
%
ymm12
.
byte
196
194
53
168
204
/
/
vfmadd213ps
%
ymm12
%
ymm9
%
ymm1
.
byte
196
194
125
184
209
/
/
vfmadd231ps
%
ymm9
%
ymm0
%
ymm2
.
byte
197
252
83
210
/
/
vrcpps
%
ymm2
%
ymm2
.
byte
197
252
83
217
/
/
vrcpps
%
ymm1
%
ymm3
.
byte
197
164
89
194
/
/
vmulps
%
ymm2
%
ymm11
%
ymm0
.
byte
197
188
89
203
/
/
vmulps
%
ymm3
%
ymm8
%
ymm1
.
byte
197
148
89
210
/
/
vmulps
%
ymm2
%
ymm13
%
ymm2
.
byte
197
172
89
219
/
/
vmulps
%
ymm3
%
ymm10
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_hsw_lowp
.
globl
_sk_uniform_color_hsw_lowp
FUNCTION
(
_sk_uniform_color_hsw_lowp
)
_sk_uniform_color_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
64
16
/
/
vpbroadcastw
0x10
(
%
rax
)
%
ymm0
.
byte
196
226
125
121
72
18
/
/
vpbroadcastw
0x12
(
%
rax
)
%
ymm1
.
byte
196
226
125
121
80
20
/
/
vpbroadcastw
0x14
(
%
rax
)
%
ymm2
.
byte
196
226
125
121
88
22
/
/
vpbroadcastw
0x16
(
%
rax
)
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_hsw_lowp
.
globl
_sk_black_color_hsw_lowp
FUNCTION
(
_sk_black_color_hsw_lowp
)
_sk_black_color_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
29
16
230
0
0
/
/
vpbroadcastw
0xe610
(
%
rip
)
%
ymm3
#
3cc40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x9f4
>
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_hsw_lowp
.
globl
_sk_white_color_hsw_lowp
FUNCTION
(
_sk_white_color_hsw_lowp
)
_sk_white_color_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
5
249
229
0
0
/
/
vpbroadcastw
0xe5f9
(
%
rip
)
%
ymm0
#
3cc42
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x9f6
>
.
byte
197
253
111
200
/
/
vmovdqa
%
ymm0
%
ymm1
.
byte
197
253
111
208
/
/
vmovdqa
%
ymm0
%
ymm2
.
byte
197
253
111
216
/
/
vmovdqa
%
ymm0
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_hsw_lowp
.
globl
_sk_set_rgb_hsw_lowp
FUNCTION
(
_sk_set_rgb_hsw_lowp
)
_sk_set_rgb_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
5
151
222
0
0
/
/
vmovss
0xde97
(
%
rip
)
%
xmm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
250
16
21
191
222
0
0
/
/
vmovss
0xdebf
(
%
rip
)
%
xmm2
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
197
250
16
0
/
/
vmovss
(
%
rax
)
%
xmm0
.
byte
196
226
57
153
194
/
/
vfmadd132ss
%
xmm2
%
xmm8
%
xmm0
.
byte
197
122
44
192
/
/
vcvttss2si
%
xmm0
%
r8d
.
byte
196
193
121
110
192
/
/
vmovd
%
r8d
%
xmm0
.
byte
196
226
125
121
192
/
/
vpbroadcastw
%
xmm0
%
ymm0
.
byte
197
250
16
72
4
/
/
vmovss
0x4
(
%
rax
)
%
xmm1
.
byte
196
226
57
153
202
/
/
vfmadd132ss
%
xmm2
%
xmm8
%
xmm1
.
byte
197
122
44
193
/
/
vcvttss2si
%
xmm1
%
r8d
.
byte
196
193
121
110
200
/
/
vmovd
%
r8d
%
xmm1
.
byte
196
226
125
121
201
/
/
vpbroadcastw
%
xmm1
%
ymm1
.
byte
196
226
57
153
80
8
/
/
vfmadd132ss
0x8
(
%
rax
)
%
xmm8
%
xmm2
.
byte
197
250
44
194
/
/
vcvttss2si
%
xmm2
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
226
125
121
210
/
/
vpbroadcastw
%
xmm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_hsw_lowp
.
globl
_sk_clamp_a_hsw_lowp
FUNCTION
(
_sk_clamp_a_hsw_lowp
)
_sk_clamp_a_hsw_lowp
:
.
byte
196
226
125
58
195
/
/
vpminuw
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
117
58
203
/
/
vpminuw
%
ymm3
%
ymm1
%
ymm1
.
byte
196
226
109
58
211
/
/
vpminuw
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_hsw_lowp
.
globl
_sk_clamp_a_dst_hsw_lowp
FUNCTION
(
_sk_clamp_a_dst_hsw_lowp
)
_sk_clamp_a_dst_hsw_lowp
:
.
byte
196
226
93
58
231
/
/
vpminuw
%
ymm7
%
ymm4
%
ymm4
.
byte
196
226
85
58
239
/
/
vpminuw
%
ymm7
%
ymm5
%
ymm5
.
byte
196
226
77
58
247
/
/
vpminuw
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_hsw_lowp
.
globl
_sk_premul_hsw_lowp
FUNCTION
(
_sk_premul_hsw_lowp
)
_sk_premul_hsw_lowp
:
.
byte
197
229
213
192
/
/
vpmullw
%
ymm0
%
ymm3
%
ymm0
.
byte
196
98
125
121
5
98
229
0
0
/
/
vpbroadcastw
0xe562
(
%
rip
)
%
ymm8
#
3cc44
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x9f8
>
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
229
213
201
/
/
vpmullw
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
229
213
210
/
/
vpmullw
%
ymm2
%
ymm3
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_hsw_lowp
.
globl
_sk_premul_dst_hsw_lowp
FUNCTION
(
_sk_premul_dst_hsw_lowp
)
_sk_premul_dst_hsw_lowp
:
.
byte
197
197
213
228
/
/
vpmullw
%
ymm4
%
ymm7
%
ymm4
.
byte
196
98
125
121
5
45
229
0
0
/
/
vpbroadcastw
0xe52d
(
%
rip
)
%
ymm8
#
3cc46
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x9fa
>
.
byte
196
193
93
253
224
/
/
vpaddw
%
ymm8
%
ymm4
%
ymm4
.
byte
197
221
113
212
8
/
/
vpsrlw
0x8
%
ymm4
%
ymm4
.
byte
197
197
213
237
/
/
vpmullw
%
ymm5
%
ymm7
%
ymm5
.
byte
196
193
85
253
232
/
/
vpaddw
%
ymm8
%
ymm5
%
ymm5
.
byte
197
213
113
213
8
/
/
vpsrlw
0x8
%
ymm5
%
ymm5
.
byte
197
197
213
246
/
/
vpmullw
%
ymm6
%
ymm7
%
ymm6
.
byte
196
193
77
253
240
/
/
vpaddw
%
ymm8
%
ymm6
%
ymm6
.
byte
197
205
113
214
8
/
/
vpsrlw
0x8
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_hsw_lowp
.
globl
_sk_force_opaque_hsw_lowp
FUNCTION
(
_sk_force_opaque_hsw_lowp
)
_sk_force_opaque_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
29
250
228
0
0
/
/
vpbroadcastw
0xe4fa
(
%
rip
)
%
ymm3
#
3cc48
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x9fc
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_hsw_lowp
.
globl
_sk_force_opaque_dst_hsw_lowp
FUNCTION
(
_sk_force_opaque_dst_hsw_lowp
)
_sk_force_opaque_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
61
239
228
0
0
/
/
vpbroadcastw
0xe4ef
(
%
rip
)
%
ymm7
#
3cc4a
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x9fe
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_hsw_lowp
.
globl
_sk_swap_rb_hsw_lowp
FUNCTION
(
_sk_swap_rb_hsw_lowp
)
_sk_swap_rb_hsw_lowp
:
.
byte
197
124
40
192
/
/
vmovaps
%
ymm0
%
ymm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
194
/
/
vmovaps
%
ymm2
%
ymm0
.
byte
197
124
41
194
/
/
vmovaps
%
ymm8
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_hsw_lowp
.
globl
_sk_move_src_dst_hsw_lowp
FUNCTION
(
_sk_move_src_dst_hsw_lowp
)
_sk_move_src_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
224
/
/
vmovaps
%
ymm0
%
ymm4
.
byte
197
252
40
233
/
/
vmovaps
%
ymm1
%
ymm5
.
byte
197
252
40
242
/
/
vmovaps
%
ymm2
%
ymm6
.
byte
197
252
40
251
/
/
vmovaps
%
ymm3
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_hsw_lowp
.
globl
_sk_move_dst_src_hsw_lowp
FUNCTION
(
_sk_move_dst_src_hsw_lowp
)
_sk_move_dst_src_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
196
/
/
vmovaps
%
ymm4
%
ymm0
.
byte
197
252
40
205
/
/
vmovaps
%
ymm5
%
ymm1
.
byte
197
252
40
214
/
/
vmovaps
%
ymm6
%
ymm2
.
byte
197
252
40
223
/
/
vmovaps
%
ymm7
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_hsw_lowp
.
globl
_sk_invert_hsw_lowp
FUNCTION
(
_sk_invert_hsw_lowp
)
_sk_invert_hsw_lowp
:
.
byte
196
98
125
121
5
174
228
0
0
/
/
vpbroadcastw
0xe4ae
(
%
rip
)
%
ymm8
#
3cc4c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa00
>
.
byte
197
189
249
192
/
/
vpsubw
%
ymm0
%
ymm8
%
ymm0
.
byte
197
189
249
201
/
/
vpsubw
%
ymm1
%
ymm8
%
ymm1
.
byte
197
189
249
210
/
/
vpsubw
%
ymm2
%
ymm8
%
ymm2
.
byte
197
189
249
219
/
/
vpsubw
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_hsw_lowp
.
globl
_sk_clear_hsw_lowp
FUNCTION
(
_sk_clear_hsw_lowp
)
_sk_clear_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
87
192
/
/
vxorps
%
ymm0
%
ymm0
%
ymm0
.
byte
197
244
87
201
/
/
vxorps
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_hsw_lowp
.
globl
_sk_srcatop_hsw_lowp
FUNCTION
(
_sk_srcatop_hsw_lowp
)
_sk_srcatop_hsw_lowp
:
.
byte
197
197
213
192
/
/
vpmullw
%
ymm0
%
ymm7
%
ymm0
.
byte
196
98
125
121
5
123
228
0
0
/
/
vpbroadcastw
0xe47b
(
%
rip
)
%
ymm8
#
3cc4e
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa02
>
.
byte
197
189
249
219
/
/
vpsubw
%
ymm3
%
ymm8
%
ymm3
.
byte
197
101
213
204
/
/
vpmullw
%
ymm4
%
ymm3
%
ymm9
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
181
253
192
/
/
vpaddw
%
ymm0
%
ymm9
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
197
213
201
/
/
vpmullw
%
ymm1
%
ymm7
%
ymm1
.
byte
197
101
213
205
/
/
vpmullw
%
ymm5
%
ymm3
%
ymm9
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
181
253
201
/
/
vpaddw
%
ymm1
%
ymm9
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
197
213
210
/
/
vpmullw
%
ymm2
%
ymm7
%
ymm2
.
byte
197
229
213
222
/
/
vpmullw
%
ymm6
%
ymm3
%
ymm3
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
229
253
210
/
/
vpaddw
%
ymm2
%
ymm3
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
196
193
69
213
216
/
/
vpmullw
%
ymm8
%
ymm7
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_hsw_lowp
.
globl
_sk_dstatop_hsw_lowp
FUNCTION
(
_sk_dstatop_hsw_lowp
)
_sk_dstatop_hsw_lowp
:
.
byte
197
93
213
195
/
/
vpmullw
%
ymm3
%
ymm4
%
ymm8
.
byte
196
98
125
121
13
27
228
0
0
/
/
vpbroadcastw
0xe41b
(
%
rip
)
%
ymm9
#
3cc50
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa04
>
.
byte
197
53
249
215
/
/
vpsubw
%
ymm7
%
ymm9
%
ymm10
.
byte
197
173
213
192
/
/
vpmullw
%
ymm0
%
ymm10
%
ymm0
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
85
213
195
/
/
vpmullw
%
ymm3
%
ymm5
%
ymm8
.
byte
197
173
213
201
/
/
vpmullw
%
ymm1
%
ymm10
%
ymm1
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
77
213
195
/
/
vpmullw
%
ymm3
%
ymm6
%
ymm8
.
byte
197
173
213
210
/
/
vpmullw
%
ymm2
%
ymm10
%
ymm2
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
196
193
101
213
217
/
/
vpmullw
%
ymm9
%
ymm3
%
ymm3
.
byte
196
193
101
253
217
/
/
vpaddw
%
ymm9
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_hsw_lowp
.
globl
_sk_srcin_hsw_lowp
FUNCTION
(
_sk_srcin_hsw_lowp
)
_sk_srcin_hsw_lowp
:
.
byte
197
197
213
192
/
/
vpmullw
%
ymm0
%
ymm7
%
ymm0
.
byte
196
98
125
121
5
184
227
0
0
/
/
vpbroadcastw
0xe3b8
(
%
rip
)
%
ymm8
#
3cc52
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa06
>
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
197
213
201
/
/
vpmullw
%
ymm1
%
ymm7
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
197
213
210
/
/
vpmullw
%
ymm2
%
ymm7
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
197
213
219
/
/
vpmullw
%
ymm3
%
ymm7
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_hsw_lowp
.
globl
_sk_dstin_hsw_lowp
FUNCTION
(
_sk_dstin_hsw_lowp
)
_sk_dstin_hsw_lowp
:
.
byte
197
221
213
195
/
/
vpmullw
%
ymm3
%
ymm4
%
ymm0
.
byte
196
98
125
121
5
117
227
0
0
/
/
vpbroadcastw
0xe375
(
%
rip
)
%
ymm8
#
3cc54
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa08
>
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
213
213
203
/
/
vpmullw
%
ymm3
%
ymm5
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
205
213
211
/
/
vpmullw
%
ymm3
%
ymm6
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
197
213
219
/
/
vpmullw
%
ymm3
%
ymm7
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_hsw_lowp
.
globl
_sk_srcout_hsw_lowp
FUNCTION
(
_sk_srcout_hsw_lowp
)
_sk_srcout_hsw_lowp
:
.
byte
196
98
125
121
5
54
227
0
0
/
/
vpbroadcastw
0xe336
(
%
rip
)
%
ymm8
#
3cc56
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa0a
>
.
byte
197
61
249
207
/
/
vpsubw
%
ymm7
%
ymm8
%
ymm9
.
byte
197
181
213
192
/
/
vpmullw
%
ymm0
%
ymm9
%
ymm0
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
181
213
201
/
/
vpmullw
%
ymm1
%
ymm9
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
181
213
210
/
/
vpmullw
%
ymm2
%
ymm9
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
181
213
219
/
/
vpmullw
%
ymm3
%
ymm9
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_hsw_lowp
.
globl
_sk_dstout_hsw_lowp
FUNCTION
(
_sk_dstout_hsw_lowp
)
_sk_dstout_hsw_lowp
:
.
byte
196
98
125
121
5
239
226
0
0
/
/
vpbroadcastw
0xe2ef
(
%
rip
)
%
ymm8
#
3cc58
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa0c
>
.
byte
197
189
249
219
/
/
vpsubw
%
ymm3
%
ymm8
%
ymm3
.
byte
197
229
213
196
/
/
vpmullw
%
ymm4
%
ymm3
%
ymm0
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
229
213
205
/
/
vpmullw
%
ymm5
%
ymm3
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
229
213
214
/
/
vpmullw
%
ymm6
%
ymm3
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
229
213
223
/
/
vpmullw
%
ymm7
%
ymm3
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_hsw_lowp
.
globl
_sk_srcover_hsw_lowp
FUNCTION
(
_sk_srcover_hsw_lowp
)
_sk_srcover_hsw_lowp
:
.
byte
196
98
125
121
5
168
226
0
0
/
/
vpbroadcastw
0xe2a8
(
%
rip
)
%
ymm8
#
3cc5a
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa0e
>
.
byte
197
61
249
203
/
/
vpsubw
%
ymm3
%
ymm8
%
ymm9
.
byte
197
53
213
212
/
/
vpmullw
%
ymm4
%
ymm9
%
ymm10
.
byte
196
65
45
253
208
/
/
vpaddw
%
ymm8
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
192
/
/
vpaddw
%
ymm0
%
ymm10
%
ymm0
.
byte
197
53
213
213
/
/
vpmullw
%
ymm5
%
ymm9
%
ymm10
.
byte
196
65
45
253
208
/
/
vpaddw
%
ymm8
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
201
/
/
vpaddw
%
ymm1
%
ymm10
%
ymm1
.
byte
197
53
213
214
/
/
vpmullw
%
ymm6
%
ymm9
%
ymm10
.
byte
196
65
45
253
208
/
/
vpaddw
%
ymm8
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
210
/
/
vpaddw
%
ymm2
%
ymm10
%
ymm2
.
byte
197
53
213
207
/
/
vpmullw
%
ymm7
%
ymm9
%
ymm9
.
byte
196
65
53
253
192
/
/
vpaddw
%
ymm8
%
ymm9
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_hsw_lowp
.
globl
_sk_dstover_hsw_lowp
FUNCTION
(
_sk_dstover_hsw_lowp
)
_sk_dstover_hsw_lowp
:
.
byte
196
98
125
121
5
77
226
0
0
/
/
vpbroadcastw
0xe24d
(
%
rip
)
%
ymm8
#
3cc5c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa10
>
.
byte
197
61
249
207
/
/
vpsubw
%
ymm7
%
ymm8
%
ymm9
.
byte
197
181
213
192
/
/
vpmullw
%
ymm0
%
ymm9
%
ymm0
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
253
253
196
/
/
vpaddw
%
ymm4
%
ymm0
%
ymm0
.
byte
197
181
213
201
/
/
vpmullw
%
ymm1
%
ymm9
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
245
253
205
/
/
vpaddw
%
ymm5
%
ymm1
%
ymm1
.
byte
197
181
213
210
/
/
vpmullw
%
ymm2
%
ymm9
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
237
253
214
/
/
vpaddw
%
ymm6
%
ymm2
%
ymm2
.
byte
197
181
213
219
/
/
vpmullw
%
ymm3
%
ymm9
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
197
229
253
223
/
/
vpaddw
%
ymm7
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_hsw_lowp
.
globl
_sk_modulate_hsw_lowp
FUNCTION
(
_sk_modulate_hsw_lowp
)
_sk_modulate_hsw_lowp
:
.
byte
197
221
213
192
/
/
vpmullw
%
ymm0
%
ymm4
%
ymm0
.
byte
196
98
125
121
5
242
225
0
0
/
/
vpbroadcastw
0xe1f2
(
%
rip
)
%
ymm8
#
3cc5e
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa12
>
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
213
213
201
/
/
vpmullw
%
ymm1
%
ymm5
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
205
213
210
/
/
vpmullw
%
ymm2
%
ymm6
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
197
213
219
/
/
vpmullw
%
ymm3
%
ymm7
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_hsw_lowp
.
globl
_sk_multiply_hsw_lowp
FUNCTION
(
_sk_multiply_hsw_lowp
)
_sk_multiply_hsw_lowp
:
.
byte
196
98
125
121
5
179
225
0
0
/
/
vpbroadcastw
0xe1b3
(
%
rip
)
%
ymm8
#
3cc60
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa14
>
.
byte
197
61
249
203
/
/
vpsubw
%
ymm3
%
ymm8
%
ymm9
.
byte
197
53
213
212
/
/
vpmullw
%
ymm4
%
ymm9
%
ymm10
.
byte
196
65
93
253
216
/
/
vpaddw
%
ymm8
%
ymm4
%
ymm11
.
byte
197
37
249
223
/
/
vpsubw
%
ymm7
%
ymm11
%
ymm11
.
byte
197
165
213
192
/
/
vpmullw
%
ymm0
%
ymm11
%
ymm0
.
byte
196
65
45
253
208
/
/
vpaddw
%
ymm8
%
ymm10
%
ymm10
.
byte
196
193
125
253
194
/
/
vpaddw
%
ymm10
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
53
213
213
/
/
vpmullw
%
ymm5
%
ymm9
%
ymm10
.
byte
196
65
85
253
216
/
/
vpaddw
%
ymm8
%
ymm5
%
ymm11
.
byte
197
37
249
223
/
/
vpsubw
%
ymm7
%
ymm11
%
ymm11
.
byte
197
165
213
201
/
/
vpmullw
%
ymm1
%
ymm11
%
ymm1
.
byte
196
65
45
253
208
/
/
vpaddw
%
ymm8
%
ymm10
%
ymm10
.
byte
196
193
117
253
202
/
/
vpaddw
%
ymm10
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
53
213
214
/
/
vpmullw
%
ymm6
%
ymm9
%
ymm10
.
byte
196
65
77
253
216
/
/
vpaddw
%
ymm8
%
ymm6
%
ymm11
.
byte
197
37
249
223
/
/
vpsubw
%
ymm7
%
ymm11
%
ymm11
.
byte
197
165
213
210
/
/
vpmullw
%
ymm2
%
ymm11
%
ymm2
.
byte
196
65
45
253
208
/
/
vpaddw
%
ymm8
%
ymm10
%
ymm10
.
byte
196
193
109
253
210
/
/
vpaddw
%
ymm10
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
53
213
207
/
/
vpmullw
%
ymm7
%
ymm9
%
ymm9
.
byte
196
193
101
213
216
/
/
vpmullw
%
ymm8
%
ymm3
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
181
253
219
/
/
vpaddw
%
ymm3
%
ymm9
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__hsw_lowp
.
globl
_sk_plus__hsw_lowp
FUNCTION
(
_sk_plus__hsw_lowp
)
_sk_plus__hsw_lowp
:
.
byte
197
221
253
192
/
/
vpaddw
%
ymm0
%
ymm4
%
ymm0
.
byte
196
98
125
121
5
41
225
0
0
/
/
vpbroadcastw
0xe129
(
%
rip
)
%
ymm8
#
3cc62
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa16
>
.
byte
196
194
125
58
192
/
/
vpminuw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
213
253
201
/
/
vpaddw
%
ymm1
%
ymm5
%
ymm1
.
byte
196
194
117
58
200
/
/
vpminuw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
205
253
210
/
/
vpaddw
%
ymm2
%
ymm6
%
ymm2
.
byte
196
194
109
58
208
/
/
vpminuw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
197
253
219
/
/
vpaddw
%
ymm3
%
ymm7
%
ymm3
.
byte
196
194
101
58
216
/
/
vpminuw
%
ymm8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_hsw_lowp
.
globl
_sk_screen_hsw_lowp
FUNCTION
(
_sk_screen_hsw_lowp
)
_sk_screen_hsw_lowp
:
.
byte
197
93
253
192
/
/
vpaddw
%
ymm0
%
ymm4
%
ymm8
.
byte
197
221
213
192
/
/
vpmullw
%
ymm0
%
ymm4
%
ymm0
.
byte
196
98
125
121
13
246
224
0
0
/
/
vpbroadcastw
0xe0f6
(
%
rip
)
%
ymm9
#
3cc64
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa18
>
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
189
249
192
/
/
vpsubw
%
ymm0
%
ymm8
%
ymm0
.
byte
197
85
253
193
/
/
vpaddw
%
ymm1
%
ymm5
%
ymm8
.
byte
197
213
213
201
/
/
vpmullw
%
ymm1
%
ymm5
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
189
249
201
/
/
vpsubw
%
ymm1
%
ymm8
%
ymm1
.
byte
197
77
253
194
/
/
vpaddw
%
ymm2
%
ymm6
%
ymm8
.
byte
197
205
213
210
/
/
vpmullw
%
ymm2
%
ymm6
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
189
249
210
/
/
vpsubw
%
ymm2
%
ymm8
%
ymm2
.
byte
197
69
253
195
/
/
vpaddw
%
ymm3
%
ymm7
%
ymm8
.
byte
197
197
213
219
/
/
vpmullw
%
ymm3
%
ymm7
%
ymm3
.
byte
196
193
101
253
217
/
/
vpaddw
%
ymm9
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
197
189
249
219
/
/
vpsubw
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__hsw_lowp
.
globl
_sk_xor__hsw_lowp
FUNCTION
(
_sk_xor__hsw_lowp
)
_sk_xor__hsw_lowp
:
.
byte
196
98
125
121
5
155
224
0
0
/
/
vpbroadcastw
0xe09b
(
%
rip
)
%
ymm8
#
3cc66
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa1a
>
.
byte
197
61
249
207
/
/
vpsubw
%
ymm7
%
ymm8
%
ymm9
.
byte
197
181
213
192
/
/
vpmullw
%
ymm0
%
ymm9
%
ymm0
.
byte
197
61
249
211
/
/
vpsubw
%
ymm3
%
ymm8
%
ymm10
.
byte
197
45
213
220
/
/
vpmullw
%
ymm4
%
ymm10
%
ymm11
.
byte
197
165
253
192
/
/
vpaddw
%
ymm0
%
ymm11
%
ymm0
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
181
213
201
/
/
vpmullw
%
ymm1
%
ymm9
%
ymm1
.
byte
197
45
213
221
/
/
vpmullw
%
ymm5
%
ymm10
%
ymm11
.
byte
197
165
253
201
/
/
vpaddw
%
ymm1
%
ymm11
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
181
213
210
/
/
vpmullw
%
ymm2
%
ymm9
%
ymm2
.
byte
197
45
213
222
/
/
vpmullw
%
ymm6
%
ymm10
%
ymm11
.
byte
197
165
253
210
/
/
vpaddw
%
ymm2
%
ymm11
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
181
213
219
/
/
vpmullw
%
ymm3
%
ymm9
%
ymm3
.
byte
197
45
213
207
/
/
vpmullw
%
ymm7
%
ymm10
%
ymm9
.
byte
197
181
253
219
/
/
vpaddw
%
ymm3
%
ymm9
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_hsw_lowp
.
globl
_sk_darken_hsw_lowp
FUNCTION
(
_sk_darken_hsw_lowp
)
_sk_darken_hsw_lowp
:
.
byte
197
93
253
192
/
/
vpaddw
%
ymm0
%
ymm4
%
ymm8
.
byte
197
197
213
192
/
/
vpmullw
%
ymm0
%
ymm7
%
ymm0
.
byte
197
93
213
203
/
/
vpmullw
%
ymm3
%
ymm4
%
ymm9
.
byte
196
194
125
62
193
/
/
vpmaxuw
%
ymm9
%
ymm0
%
ymm0
.
byte
196
98
125
121
13
31
224
0
0
/
/
vpbroadcastw
0xe01f
(
%
rip
)
%
ymm9
#
3cc68
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa1c
>
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
189
249
192
/
/
vpsubw
%
ymm0
%
ymm8
%
ymm0
.
byte
197
85
253
193
/
/
vpaddw
%
ymm1
%
ymm5
%
ymm8
.
byte
197
197
213
201
/
/
vpmullw
%
ymm1
%
ymm7
%
ymm1
.
byte
197
85
213
211
/
/
vpmullw
%
ymm3
%
ymm5
%
ymm10
.
byte
196
194
117
62
202
/
/
vpmaxuw
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
189
249
201
/
/
vpsubw
%
ymm1
%
ymm8
%
ymm1
.
byte
197
77
253
194
/
/
vpaddw
%
ymm2
%
ymm6
%
ymm8
.
byte
197
197
213
210
/
/
vpmullw
%
ymm2
%
ymm7
%
ymm2
.
byte
197
77
213
211
/
/
vpmullw
%
ymm3
%
ymm6
%
ymm10
.
byte
196
194
109
62
210
/
/
vpmaxuw
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
189
249
210
/
/
vpsubw
%
ymm2
%
ymm8
%
ymm2
.
byte
197
53
249
195
/
/
vpsubw
%
ymm3
%
ymm9
%
ymm8
.
byte
197
61
213
199
/
/
vpmullw
%
ymm7
%
ymm8
%
ymm8
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_hsw_lowp
.
globl
_sk_lighten_hsw_lowp
FUNCTION
(
_sk_lighten_hsw_lowp
)
_sk_lighten_hsw_lowp
:
.
byte
197
93
253
192
/
/
vpaddw
%
ymm0
%
ymm4
%
ymm8
.
byte
197
197
213
192
/
/
vpmullw
%
ymm0
%
ymm7
%
ymm0
.
byte
197
93
213
203
/
/
vpmullw
%
ymm3
%
ymm4
%
ymm9
.
byte
196
194
125
58
193
/
/
vpminuw
%
ymm9
%
ymm0
%
ymm0
.
byte
196
98
125
121
13
160
223
0
0
/
/
vpbroadcastw
0xdfa0
(
%
rip
)
%
ymm9
#
3cc6a
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa1e
>
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
189
249
192
/
/
vpsubw
%
ymm0
%
ymm8
%
ymm0
.
byte
197
85
253
193
/
/
vpaddw
%
ymm1
%
ymm5
%
ymm8
.
byte
197
197
213
201
/
/
vpmullw
%
ymm1
%
ymm7
%
ymm1
.
byte
197
85
213
211
/
/
vpmullw
%
ymm3
%
ymm5
%
ymm10
.
byte
196
194
117
58
202
/
/
vpminuw
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
189
249
201
/
/
vpsubw
%
ymm1
%
ymm8
%
ymm1
.
byte
197
77
253
194
/
/
vpaddw
%
ymm2
%
ymm6
%
ymm8
.
byte
197
197
213
210
/
/
vpmullw
%
ymm2
%
ymm7
%
ymm2
.
byte
197
77
213
211
/
/
vpmullw
%
ymm3
%
ymm6
%
ymm10
.
byte
196
194
109
58
210
/
/
vpminuw
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
189
249
210
/
/
vpsubw
%
ymm2
%
ymm8
%
ymm2
.
byte
197
53
249
195
/
/
vpsubw
%
ymm3
%
ymm9
%
ymm8
.
byte
197
61
213
199
/
/
vpmullw
%
ymm7
%
ymm8
%
ymm8
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_hsw_lowp
.
globl
_sk_difference_hsw_lowp
FUNCTION
(
_sk_difference_hsw_lowp
)
_sk_difference_hsw_lowp
:
.
byte
197
93
253
192
/
/
vpaddw
%
ymm0
%
ymm4
%
ymm8
.
byte
197
197
213
192
/
/
vpmullw
%
ymm0
%
ymm7
%
ymm0
.
byte
197
93
213
203
/
/
vpmullw
%
ymm3
%
ymm4
%
ymm9
.
byte
196
194
125
58
193
/
/
vpminuw
%
ymm9
%
ymm0
%
ymm0
.
byte
196
98
125
121
13
33
223
0
0
/
/
vpbroadcastw
0xdf21
(
%
rip
)
%
ymm9
#
3cc6c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa20
>
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
113
208
7
/
/
vpsrlw
0x7
%
ymm0
%
ymm0
.
byte
196
98
125
121
21
16
223
0
0
/
/
vpbroadcastw
0xdf10
(
%
rip
)
%
ymm10
#
3cc6e
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa22
>
.
byte
196
193
125
219
194
/
/
vpand
%
ymm10
%
ymm0
%
ymm0
.
byte
197
189
249
192
/
/
vpsubw
%
ymm0
%
ymm8
%
ymm0
.
byte
197
85
253
193
/
/
vpaddw
%
ymm1
%
ymm5
%
ymm8
.
byte
197
197
213
201
/
/
vpmullw
%
ymm1
%
ymm7
%
ymm1
.
byte
197
85
213
219
/
/
vpmullw
%
ymm3
%
ymm5
%
ymm11
.
byte
196
194
117
58
203
/
/
vpminuw
%
ymm11
%
ymm1
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
245
113
209
7
/
/
vpsrlw
0x7
%
ymm1
%
ymm1
.
byte
196
193
117
219
202
/
/
vpand
%
ymm10
%
ymm1
%
ymm1
.
byte
197
189
249
201
/
/
vpsubw
%
ymm1
%
ymm8
%
ymm1
.
byte
197
77
253
194
/
/
vpaddw
%
ymm2
%
ymm6
%
ymm8
.
byte
197
197
213
210
/
/
vpmullw
%
ymm2
%
ymm7
%
ymm2
.
byte
197
77
213
219
/
/
vpmullw
%
ymm3
%
ymm6
%
ymm11
.
byte
196
194
109
58
211
/
/
vpminuw
%
ymm11
%
ymm2
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
237
113
210
7
/
/
vpsrlw
0x7
%
ymm2
%
ymm2
.
byte
196
193
109
219
210
/
/
vpand
%
ymm10
%
ymm2
%
ymm2
.
byte
197
189
249
210
/
/
vpsubw
%
ymm2
%
ymm8
%
ymm2
.
byte
197
53
249
195
/
/
vpsubw
%
ymm3
%
ymm9
%
ymm8
.
byte
197
61
213
199
/
/
vpmullw
%
ymm7
%
ymm8
%
ymm8
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_hsw_lowp
.
globl
_sk_exclusion_hsw_lowp
FUNCTION
(
_sk_exclusion_hsw_lowp
)
_sk_exclusion_hsw_lowp
:
.
byte
197
93
253
192
/
/
vpaddw
%
ymm0
%
ymm4
%
ymm8
.
byte
197
221
213
192
/
/
vpmullw
%
ymm0
%
ymm4
%
ymm0
.
byte
196
98
125
121
13
149
222
0
0
/
/
vpbroadcastw
0xde95
(
%
rip
)
%
ymm9
#
3cc70
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa24
>
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
113
208
7
/
/
vpsrlw
0x7
%
ymm0
%
ymm0
.
byte
196
98
125
121
21
132
222
0
0
/
/
vpbroadcastw
0xde84
(
%
rip
)
%
ymm10
#
3cc72
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa26
>
.
byte
196
193
125
219
194
/
/
vpand
%
ymm10
%
ymm0
%
ymm0
.
byte
197
189
249
192
/
/
vpsubw
%
ymm0
%
ymm8
%
ymm0
.
byte
197
85
253
193
/
/
vpaddw
%
ymm1
%
ymm5
%
ymm8
.
byte
197
213
213
201
/
/
vpmullw
%
ymm1
%
ymm5
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
245
113
209
7
/
/
vpsrlw
0x7
%
ymm1
%
ymm1
.
byte
196
193
117
219
202
/
/
vpand
%
ymm10
%
ymm1
%
ymm1
.
byte
197
189
249
201
/
/
vpsubw
%
ymm1
%
ymm8
%
ymm1
.
byte
197
77
253
194
/
/
vpaddw
%
ymm2
%
ymm6
%
ymm8
.
byte
197
205
213
210
/
/
vpmullw
%
ymm2
%
ymm6
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
237
113
210
7
/
/
vpsrlw
0x7
%
ymm2
%
ymm2
.
byte
196
193
109
219
210
/
/
vpand
%
ymm10
%
ymm2
%
ymm2
.
byte
197
189
249
210
/
/
vpsubw
%
ymm2
%
ymm8
%
ymm2
.
byte
197
53
249
195
/
/
vpsubw
%
ymm3
%
ymm9
%
ymm8
.
byte
197
61
213
199
/
/
vpmullw
%
ymm7
%
ymm8
%
ymm8
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_hsw_lowp
.
globl
_sk_hardlight_hsw_lowp
FUNCTION
(
_sk_hardlight_hsw_lowp
)
_sk_hardlight_hsw_lowp
:
.
byte
197
125
253
192
/
/
vpaddw
%
ymm0
%
ymm0
%
ymm8
.
byte
196
98
125
121
13
33
222
0
0
/
/
vpbroadcastw
0xde21
(
%
rip
)
%
ymm9
#
3cc76
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa2a
>
.
byte
196
65
101
239
209
/
/
vpxor
%
ymm9
%
ymm3
%
ymm10
.
byte
196
65
61
239
217
/
/
vpxor
%
ymm9
%
ymm8
%
ymm11
.
byte
196
65
37
101
218
/
/
vpcmpgtw
%
ymm10
%
ymm11
%
ymm11
.
byte
197
61
213
228
/
/
vpmullw
%
ymm4
%
ymm8
%
ymm12
.
byte
197
69
213
195
/
/
vpmullw
%
ymm3
%
ymm7
%
ymm8
.
byte
197
101
249
232
/
/
vpsubw
%
ymm0
%
ymm3
%
ymm13
.
byte
197
69
249
244
/
/
vpsubw
%
ymm4
%
ymm7
%
ymm14
.
byte
196
65
21
213
238
/
/
vpmullw
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
21
253
237
/
/
vpaddw
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
61
249
237
/
/
vpsubw
%
ymm13
%
ymm8
%
ymm13
.
byte
196
67
29
76
221
176
/
/
vpblendvb
%
ymm11
%
ymm13
%
ymm12
%
ymm11
.
byte
197
117
253
225
/
/
vpaddw
%
ymm1
%
ymm1
%
ymm12
.
byte
197
101
249
233
/
/
vpsubw
%
ymm1
%
ymm3
%
ymm13
.
byte
197
69
249
245
/
/
vpsubw
%
ymm5
%
ymm7
%
ymm14
.
byte
196
65
21
213
238
/
/
vpmullw
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
29
239
241
/
/
vpxor
%
ymm9
%
ymm12
%
ymm14
.
byte
196
65
13
101
242
/
/
vpcmpgtw
%
ymm10
%
ymm14
%
ymm14
.
byte
197
29
213
229
/
/
vpmullw
%
ymm5
%
ymm12
%
ymm12
.
byte
196
65
21
253
237
/
/
vpaddw
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
61
249
237
/
/
vpsubw
%
ymm13
%
ymm8
%
ymm13
.
byte
196
67
29
76
229
224
/
/
vpblendvb
%
ymm14
%
ymm13
%
ymm12
%
ymm12
.
byte
197
109
253
234
/
/
vpaddw
%
ymm2
%
ymm2
%
ymm13
.
byte
196
65
21
239
201
/
/
vpxor
%
ymm9
%
ymm13
%
ymm9
.
byte
196
65
53
101
202
/
/
vpcmpgtw
%
ymm10
%
ymm9
%
ymm9
.
byte
197
101
249
210
/
/
vpsubw
%
ymm2
%
ymm3
%
ymm10
.
byte
197
69
249
246
/
/
vpsubw
%
ymm6
%
ymm7
%
ymm14
.
byte
196
65
45
213
214
/
/
vpmullw
%
ymm14
%
ymm10
%
ymm10
.
byte
196
98
125
121
53
152
221
0
0
/
/
vpbroadcastw
0xdd98
(
%
rip
)
%
ymm14
#
3cc74
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa28
>
.
byte
196
65
45
253
210
/
/
vpaddw
%
ymm10
%
ymm10
%
ymm10
.
byte
196
65
61
249
194
/
/
vpsubw
%
ymm10
%
ymm8
%
ymm8
.
byte
197
13
249
215
/
/
vpsubw
%
ymm7
%
ymm14
%
ymm10
.
byte
197
173
213
192
/
/
vpmullw
%
ymm0
%
ymm10
%
ymm0
.
byte
197
21
213
238
/
/
vpmullw
%
ymm6
%
ymm13
%
ymm13
.
byte
196
67
21
76
192
144
/
/
vpblendvb
%
ymm9
%
ymm8
%
ymm13
%
ymm8
.
byte
197
13
249
203
/
/
vpsubw
%
ymm3
%
ymm14
%
ymm9
.
byte
197
53
213
236
/
/
vpmullw
%
ymm4
%
ymm9
%
ymm13
.
byte
197
149
253
192
/
/
vpaddw
%
ymm0
%
ymm13
%
ymm0
.
byte
196
65
37
253
222
/
/
vpaddw
%
ymm14
%
ymm11
%
ymm11
.
byte
196
193
125
253
195
/
/
vpaddw
%
ymm11
%
ymm0
%
ymm0
.
byte
197
173
213
201
/
/
vpmullw
%
ymm1
%
ymm10
%
ymm1
.
byte
197
53
213
221
/
/
vpmullw
%
ymm5
%
ymm9
%
ymm11
.
byte
197
165
253
201
/
/
vpaddw
%
ymm1
%
ymm11
%
ymm1
.
byte
196
65
29
253
222
/
/
vpaddw
%
ymm14
%
ymm12
%
ymm11
.
byte
196
193
117
253
203
/
/
vpaddw
%
ymm11
%
ymm1
%
ymm1
.
byte
197
173
213
210
/
/
vpmullw
%
ymm2
%
ymm10
%
ymm2
.
byte
197
53
213
214
/
/
vpmullw
%
ymm6
%
ymm9
%
ymm10
.
byte
197
173
253
210
/
/
vpaddw
%
ymm2
%
ymm10
%
ymm2
.
byte
196
65
61
253
198
/
/
vpaddw
%
ymm14
%
ymm8
%
ymm8
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
53
213
199
/
/
vpmullw
%
ymm7
%
ymm9
%
ymm8
.
byte
196
65
61
253
198
/
/
vpaddw
%
ymm14
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_hsw_lowp
.
globl
_sk_overlay_hsw_lowp
FUNCTION
(
_sk_overlay_hsw_lowp
)
_sk_overlay_hsw_lowp
:
.
byte
197
93
253
196
/
/
vpaddw
%
ymm4
%
ymm4
%
ymm8
.
byte
196
98
125
121
13
13
221
0
0
/
/
vpbroadcastw
0xdd0d
(
%
rip
)
%
ymm9
#
3cc7a
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa2e
>
.
byte
196
65
69
239
209
/
/
vpxor
%
ymm9
%
ymm7
%
ymm10
.
byte
196
65
61
239
193
/
/
vpxor
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
61
101
218
/
/
vpcmpgtw
%
ymm10
%
ymm8
%
ymm11
.
byte
197
125
213
196
/
/
vpmullw
%
ymm4
%
ymm0
%
ymm8
.
byte
196
65
61
253
224
/
/
vpaddw
%
ymm8
%
ymm8
%
ymm12
.
byte
197
69
213
195
/
/
vpmullw
%
ymm3
%
ymm7
%
ymm8
.
byte
197
101
249
232
/
/
vpsubw
%
ymm0
%
ymm3
%
ymm13
.
byte
197
69
249
244
/
/
vpsubw
%
ymm4
%
ymm7
%
ymm14
.
byte
196
65
21
213
238
/
/
vpmullw
%
ymm14
%
ymm13
%
ymm13
.
byte
196
65
21
253
237
/
/
vpaddw
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
61
249
237
/
/
vpsubw
%
ymm13
%
ymm8
%
ymm13
.
byte
196
67
29
76
221
176
/
/
vpblendvb
%
ymm11
%
ymm13
%
ymm12
%
ymm11
.
byte
197
85
253
229
/
/
vpaddw
%
ymm5
%
ymm5
%
ymm12
.
byte
196
65
29
239
225
/
/
vpxor
%
ymm9
%
ymm12
%
ymm12
.
byte
196
65
29
101
226
/
/
vpcmpgtw
%
ymm10
%
ymm12
%
ymm12
.
byte
197
101
249
233
/
/
vpsubw
%
ymm1
%
ymm3
%
ymm13
.
byte
197
69
249
245
/
/
vpsubw
%
ymm5
%
ymm7
%
ymm14
.
byte
196
65
21
213
238
/
/
vpmullw
%
ymm14
%
ymm13
%
ymm13
.
byte
197
117
213
245
/
/
vpmullw
%
ymm5
%
ymm1
%
ymm14
.
byte
196
65
13
253
246
/
/
vpaddw
%
ymm14
%
ymm14
%
ymm14
.
byte
196
65
21
253
237
/
/
vpaddw
%
ymm13
%
ymm13
%
ymm13
.
byte
196
65
61
249
237
/
/
vpsubw
%
ymm13
%
ymm8
%
ymm13
.
byte
196
67
13
76
229
192
/
/
vpblendvb
%
ymm12
%
ymm13
%
ymm14
%
ymm12
.
byte
196
98
125
121
45
149
220
0
0
/
/
vpbroadcastw
0xdc95
(
%
rip
)
%
ymm13
#
3cc78
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa2c
>
.
byte
197
77
253
246
/
/
vpaddw
%
ymm6
%
ymm6
%
ymm14
.
byte
196
65
13
239
201
/
/
vpxor
%
ymm9
%
ymm14
%
ymm9
.
byte
196
65
53
101
202
/
/
vpcmpgtw
%
ymm10
%
ymm9
%
ymm9
.
byte
197
101
249
210
/
/
vpsubw
%
ymm2
%
ymm3
%
ymm10
.
byte
197
69
249
246
/
/
vpsubw
%
ymm6
%
ymm7
%
ymm14
.
byte
196
65
45
213
214
/
/
vpmullw
%
ymm14
%
ymm10
%
ymm10
.
byte
197
21
249
247
/
/
vpsubw
%
ymm7
%
ymm13
%
ymm14
.
byte
197
141
213
192
/
/
vpmullw
%
ymm0
%
ymm14
%
ymm0
.
byte
196
65
45
253
210
/
/
vpaddw
%
ymm10
%
ymm10
%
ymm10
.
byte
196
65
61
249
194
/
/
vpsubw
%
ymm10
%
ymm8
%
ymm8
.
byte
197
109
213
214
/
/
vpmullw
%
ymm6
%
ymm2
%
ymm10
.
byte
196
65
45
253
210
/
/
vpaddw
%
ymm10
%
ymm10
%
ymm10
.
byte
196
67
45
76
192
144
/
/
vpblendvb
%
ymm9
%
ymm8
%
ymm10
%
ymm8
.
byte
197
21
249
203
/
/
vpsubw
%
ymm3
%
ymm13
%
ymm9
.
byte
197
53
213
212
/
/
vpmullw
%
ymm4
%
ymm9
%
ymm10
.
byte
197
173
253
192
/
/
vpaddw
%
ymm0
%
ymm10
%
ymm0
.
byte
196
65
37
253
213
/
/
vpaddw
%
ymm13
%
ymm11
%
ymm10
.
byte
196
193
125
253
194
/
/
vpaddw
%
ymm10
%
ymm0
%
ymm0
.
byte
197
141
213
201
/
/
vpmullw
%
ymm1
%
ymm14
%
ymm1
.
byte
197
53
213
213
/
/
vpmullw
%
ymm5
%
ymm9
%
ymm10
.
byte
197
173
253
201
/
/
vpaddw
%
ymm1
%
ymm10
%
ymm1
.
byte
196
65
29
253
213
/
/
vpaddw
%
ymm13
%
ymm12
%
ymm10
.
byte
196
193
117
253
202
/
/
vpaddw
%
ymm10
%
ymm1
%
ymm1
.
byte
197
141
213
210
/
/
vpmullw
%
ymm2
%
ymm14
%
ymm2
.
byte
197
53
213
214
/
/
vpmullw
%
ymm6
%
ymm9
%
ymm10
.
byte
197
173
253
210
/
/
vpaddw
%
ymm2
%
ymm10
%
ymm2
.
byte
196
65
61
253
197
/
/
vpaddw
%
ymm13
%
ymm8
%
ymm8
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
53
213
199
/
/
vpmullw
%
ymm7
%
ymm9
%
ymm8
.
byte
196
65
61
253
197
/
/
vpaddw
%
ymm13
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_8888_hsw_lowp
.
globl
_sk_load_8888_hsw_lowp
FUNCTION
(
_sk_load_8888_hsw_lowp
)
_sk_load_8888_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
55
/
/
ja
2f0d9
<
_sk_load_8888_hsw_lowp
+
0x52
>
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
160
1
0
0
/
/
lea
0x1a0
(
%
rip
)
%
r9
#
2f250
<
_sk_load_8888_hsw_lowp
+
0x1c9
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
12
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
233
39
1
0
0
/
/
jmpq
2f200
<
_sk_load_8888_hsw_lowp
+
0x179
>
.
byte
196
193
126
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm1
.
byte
196
193
126
111
68
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
ymm0
.
byte
233
21
1
0
0
/
/
jmpq
2f200
<
_sk_load_8888_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
196
227
101
2
192
4
/
/
vpblendd
0x4
%
ymm0
%
ymm3
%
ymm0
.
byte
196
194
121
53
12
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
196
227
125
2
201
3
/
/
vpblendd
0x3
%
ymm1
%
ymm0
%
ymm1
.
byte
197
253
111
195
/
/
vmovdqa
%
ymm3
%
ymm0
.
byte
233
229
0
0
0
/
/
jmpq
2f200
<
_sk_load_8888_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
192
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
125
57
194
1
/
/
vextracti128
0x1
%
ymm0
%
xmm2
.
byte
196
195
105
34
84
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm2
%
xmm2
.
byte
196
227
125
56
194
1
/
/
vinserti128
0x1
%
xmm2
%
ymm0
%
ymm0
.
byte
197
253
111
209
/
/
vmovdqa
%
ymm1
%
ymm2
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
195
113
34
76
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
197
125
111
194
/
/
vmovdqa
%
ymm2
%
ymm8
.
byte
196
193
122
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
196
227
117
2
200
240
/
/
vpblendd
0xf0
%
ymm0
%
ymm1
%
ymm1
.
byte
197
125
127
192
/
/
vmovdqa
%
ymm8
%
ymm0
.
byte
233
138
0
0
0
/
/
jmpq
2f200
<
_sk_load_8888_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
40
/
/
vmovd
0x28
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
192
4
/
/
vpblendd
0x4
%
ymm0
%
ymm1
%
ymm0
.
byte
196
195
121
34
76
144
36
1
/
/
vpinsrd
0x1
0x24
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm1
.
byte
196
227
125
2
193
15
/
/
vpblendd
0xf
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
121
110
76
144
32
/
/
vmovd
0x20
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
196
227
125
2
193
1
/
/
vpblendd
0x1
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
126
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm1
.
byte
235
81
/
/
jmp
2f200
<
_sk_load_8888_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
56
/
/
vmovd
0x38
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
192
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
195
113
34
76
144
52
1
/
/
vpinsrd
0x1
0x34
(
%
r8
%
rdx
4
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
195
113
34
76
144
48
0
/
/
vpinsrd
0x0
0x30
(
%
r8
%
rdx
4
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
126
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm1
.
byte
196
193
122
111
84
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
xmm2
.
byte
196
227
109
2
192
240
/
/
vpblendd
0xf0
%
ymm0
%
ymm2
%
ymm0
.
byte
196
227
117
56
208
1
/
/
vinserti128
0x1
%
xmm0
%
ymm1
%
ymm2
.
byte
196
227
117
70
216
49
/
/
vperm2i128
0x31
%
ymm0
%
ymm1
%
ymm3
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
227
101
14
200
170
/
/
vpblendw
0xaa
%
ymm0
%
ymm3
%
ymm1
.
byte
196
227
109
14
192
170
/
/
vpblendw
0xaa
%
ymm0
%
ymm2
%
ymm0
.
byte
196
226
125
43
201
/
/
vpackusdw
%
ymm1
%
ymm0
%
ymm1
.
byte
197
125
111
5
87
218
0
0
/
/
vmovdqa
0xda57
(
%
rip
)
%
ymm8
#
3cc80
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa34
>
.
byte
196
193
117
219
192
/
/
vpand
%
ymm8
%
ymm1
%
ymm0
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
229
114
211
16
/
/
vpsrld
0x10
%
ymm3
%
ymm3
.
byte
197
237
114
210
16
/
/
vpsrld
0x10
%
ymm2
%
ymm2
.
byte
196
226
109
43
219
/
/
vpackusdw
%
ymm3
%
ymm2
%
ymm3
.
byte
196
193
101
219
208
/
/
vpand
%
ymm8
%
ymm3
%
ymm2
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
122
254
/
/
jp
2f250
<
_sk_load_8888_hsw_lowp
+
0x1c9
>
.
byte
255
/
/
(
bad
)
.
byte
255
177
254
255
255
155
/
/
pushq
-
0x64000002
(
%
rcx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
17
/
/
callq
*
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
249
/
/
stc
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
225
/
/
jmpq
*
%
rcx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
203
/
/
dec
%
ebx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
87
255
/
/
callq
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
74
255
/
/
decl
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
60
255
/
/
cmp
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
38
/
/
jmpq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
157
255
255
255
137
/
/
lcall
*
-
0x76000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
117
255
/
/
pushq
-
0x1
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
95
255
/
/
lcall
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_dst_hsw_lowp
.
globl
_sk_load_8888_dst_hsw_lowp
FUNCTION
(
_sk_load_8888_dst_hsw_lowp
)
_sk_load_8888_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
55
/
/
ja
2f2de
<
_sk_load_8888_dst_hsw_lowp
+
0x52
>
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
163
1
0
0
/
/
lea
0x1a3
(
%
rip
)
%
r9
#
2f458
<
_sk_load_8888_dst_hsw_lowp
+
0x1cc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
205
239
246
/
/
vpxor
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
44
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
233
39
1
0
0
/
/
jmpq
2f405
<
_sk_load_8888_dst_hsw_lowp
+
0x179
>
.
byte
196
193
126
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm5
.
byte
196
193
126
111
100
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
ymm4
.
byte
233
21
1
0
0
/
/
jmpq
2f405
<
_sk_load_8888_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
196
227
69
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm7
%
ymm4
.
byte
196
194
121
53
44
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
196
227
93
2
237
3
/
/
vpblendd
0x3
%
ymm5
%
ymm4
%
ymm5
.
byte
197
253
111
231
/
/
vmovdqa
%
ymm7
%
ymm4
.
byte
233
229
0
0
0
/
/
jmpq
2f405
<
_sk_load_8888_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
230
1
/
/
vextracti128
0x1
%
ymm4
%
xmm6
.
byte
196
195
73
34
116
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm6
%
xmm6
.
byte
196
227
93
56
230
1
/
/
vinserti128
0x1
%
xmm6
%
ymm4
%
ymm4
.
byte
197
253
111
245
/
/
vmovdqa
%
ymm5
%
ymm6
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
108
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
197
125
111
198
/
/
vmovdqa
%
ymm6
%
ymm8
.
byte
196
193
122
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
196
227
85
2
236
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm5
%
ymm5
.
byte
197
125
127
196
/
/
vmovdqa
%
ymm8
%
ymm4
.
byte
233
138
0
0
0
/
/
jmpq
2f405
<
_sk_load_8888_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
40
/
/
vmovd
0x28
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm4
.
byte
196
195
89
34
108
144
36
1
/
/
vpinsrd
0x1
0x24
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
121
110
108
144
32
/
/
vmovd
0x20
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
196
227
93
2
229
1
/
/
vpblendd
0x1
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm5
.
byte
235
81
/
/
jmp
2f405
<
_sk_load_8888_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
56
/
/
vmovd
0x38
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
108
144
52
1
/
/
vpinsrd
0x1
0x34
(
%
r8
%
rdx
4
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
108
144
48
0
/
/
vpinsrd
0x0
0x30
(
%
r8
%
rdx
4
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm5
.
byte
196
193
122
111
116
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
196
227
77
2
228
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm6
%
ymm4
.
byte
196
227
85
56
244
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm6
.
byte
196
227
85
70
252
49
/
/
vperm2i128
0x31
%
ymm4
%
ymm5
%
ymm7
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
196
227
69
14
236
170
/
/
vpblendw
0xaa
%
ymm4
%
ymm7
%
ymm5
.
byte
196
227
77
14
228
170
/
/
vpblendw
0xaa
%
ymm4
%
ymm6
%
ymm4
.
byte
196
226
93
43
237
/
/
vpackusdw
%
ymm5
%
ymm4
%
ymm5
.
byte
197
125
111
5
114
216
0
0
/
/
vmovdqa
0xd872
(
%
rip
)
%
ymm8
#
3cca0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa54
>
.
byte
196
193
85
219
224
/
/
vpand
%
ymm8
%
ymm5
%
ymm4
.
byte
197
213
113
213
8
/
/
vpsrlw
0x8
%
ymm5
%
ymm5
.
byte
197
197
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm7
.
byte
197
205
114
214
16
/
/
vpsrld
0x10
%
ymm6
%
ymm6
.
byte
196
226
77
43
255
/
/
vpackusdw
%
ymm7
%
ymm6
%
ymm7
.
byte
196
193
69
219
240
/
/
vpand
%
ymm8
%
ymm7
%
ymm6
.
byte
197
197
113
215
8
/
/
vpsrlw
0x8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
119
254
/
/
ja
2f458
<
_sk_load_8888_dst_hsw_lowp
+
0x1cc
>
.
byte
255
/
/
(
bad
)
.
byte
255
174
254
255
255
152
/
/
ljmp
*
-
0x67000002
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
246
/
/
push
%
rsi
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
254
/
/
fdivrp
%
st
%
st
(
6
)
.
byte
255
/
/
(
bad
)
.
byte
255
200
/
/
dec
%
eax
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
84
255
255
/
/
callq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
71
255
/
/
incl
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
57
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
35
/
/
jmpq
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
154
255
255
255
134
/
/
lcall
*
-
0x79000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_8888_hsw_lowp
.
globl
_sk_store_8888_hsw_lowp
FUNCTION
(
_sk_store_8888_hsw_lowp
)
_sk_store_8888_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
197
189
113
241
8
/
/
vpsllw
0x8
%
ymm1
%
ymm8
.
byte
197
61
235
192
/
/
vpor
%
ymm0
%
ymm8
%
ymm8
.
byte
196
66
125
51
200
/
/
vpmovzxwd
%
xmm8
%
ymm9
.
byte
196
67
125
57
192
1
/
/
vextracti128
0x1
%
ymm8
%
xmm8
.
byte
196
66
125
51
208
/
/
vpmovzxwd
%
xmm8
%
ymm10
.
byte
197
189
113
243
8
/
/
vpsllw
0x8
%
ymm3
%
ymm8
.
byte
197
61
235
194
/
/
vpor
%
ymm2
%
ymm8
%
ymm8
.
byte
196
67
125
57
195
1
/
/
vextracti128
0x1
%
ymm8
%
xmm11
.
byte
196
66
125
51
219
/
/
vpmovzxwd
%
xmm11
%
ymm11
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
193
61
114
240
16
/
/
vpslld
0x10
%
ymm8
%
ymm8
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
53
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm9
.
byte
196
65
53
235
202
/
/
vpor
%
ymm10
%
ymm9
%
ymm9
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
30
/
/
ja
2f515
<
_sk_store_8888_hsw_lowp
+
0x81
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
195
0
0
0
/
/
lea
0xc3
(
%
rip
)
%
r9
#
2f5c4
<
_sk_store_8888_hsw_lowp
+
0x130
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
171
0
0
0
/
/
jmpq
2f5c0
<
_sk_store_8888_hsw_lowp
+
0x12c
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
196
65
126
127
76
144
32
/
/
vmovdqu
%
ymm9
0x20
(
%
r8
%
rdx
4
)
.
byte
233
153
0
0
0
/
/
jmpq
2f5c0
<
_sk_store_8888_hsw_lowp
+
0x12c
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
134
0
0
0
/
/
jmpq
2f5c0
<
_sk_store_8888_hsw_lowp
+
0x12c
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
85
/
/
jmp
2f5c0
<
_sk_store_8888_hsw_lowp
+
0x12c
>
.
byte
196
67
121
22
76
144
40
2
/
/
vpextrd
0x2
%
xmm9
0x28
(
%
r8
%
rdx
4
)
.
byte
196
67
121
22
76
144
36
1
/
/
vpextrd
0x1
%
xmm9
0x24
(
%
r8
%
rdx
4
)
.
byte
196
65
121
126
76
144
32
/
/
vmovd
%
xmm9
0x20
(
%
r8
%
rdx
4
)
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
235
54
/
/
jmp
2f5c0
<
_sk_store_8888_hsw_lowp
+
0x12c
>
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
84
144
56
2
/
/
vpextrd
0x2
%
xmm10
0x38
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
84
144
52
1
/
/
vpextrd
0x1
%
xmm10
0x34
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
65
121
126
84
144
48
/
/
vmovd
%
xmm10
0x30
(
%
r8
%
rdx
4
)
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
76
144
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
70
255
/
/
rex
.
RX
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
107
255
/
/
ljmp
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
99
255
/
/
jmpq
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
159
255
255
255
146
/
/
lcall
*
-
0x6d000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
132
255
255
255
118
255
/
/
incl
-
0x890001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
183
/
/
mov
0xb7ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
175
255
255
255
167
/
/
ljmp
*
-
0x58000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
226
/
/
jmpq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_hsw_lowp
.
globl
_sk_load_bgra_hsw_lowp
FUNCTION
(
_sk_load_bgra_hsw_lowp
)
_sk_load_bgra_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
55
/
/
ja
2f652
<
_sk_load_bgra_hsw_lowp
+
0x52
>
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
163
1
0
0
/
/
lea
0x1a3
(
%
rip
)
%
r9
#
2f7cc
<
_sk_load_bgra_hsw_lowp
+
0x1cc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
12
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
233
39
1
0
0
/
/
jmpq
2f779
<
_sk_load_bgra_hsw_lowp
+
0x179
>
.
byte
196
193
126
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm1
.
byte
196
193
126
111
68
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
ymm0
.
byte
233
21
1
0
0
/
/
jmpq
2f779
<
_sk_load_bgra_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
196
227
101
2
192
4
/
/
vpblendd
0x4
%
ymm0
%
ymm3
%
ymm0
.
byte
196
194
121
53
12
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
197
249
112
201
232
/
/
vpshufd
0xe8
%
xmm1
%
xmm1
.
byte
196
227
125
2
201
3
/
/
vpblendd
0x3
%
ymm1
%
ymm0
%
ymm1
.
byte
197
253
111
195
/
/
vmovdqa
%
ymm3
%
ymm0
.
byte
233
229
0
0
0
/
/
jmpq
2f779
<
_sk_load_bgra_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
192
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
125
57
194
1
/
/
vextracti128
0x1
%
ymm0
%
xmm2
.
byte
196
195
105
34
84
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm2
%
xmm2
.
byte
196
227
125
56
194
1
/
/
vinserti128
0x1
%
xmm2
%
ymm0
%
ymm0
.
byte
197
253
111
209
/
/
vmovdqa
%
ymm1
%
ymm2
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
195
113
34
76
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
197
125
111
194
/
/
vmovdqa
%
ymm2
%
ymm8
.
byte
196
193
122
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
196
227
117
2
200
240
/
/
vpblendd
0xf0
%
ymm0
%
ymm1
%
ymm1
.
byte
197
125
127
192
/
/
vmovdqa
%
ymm8
%
ymm0
.
byte
233
138
0
0
0
/
/
jmpq
2f779
<
_sk_load_bgra_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
40
/
/
vmovd
0x28
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
121
89
192
/
/
vpbroadcastq
%
xmm0
%
xmm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
192
4
/
/
vpblendd
0x4
%
ymm0
%
ymm1
%
ymm0
.
byte
196
195
121
34
76
144
36
1
/
/
vpinsrd
0x1
0x24
(
%
r8
%
rdx
4
)
%
xmm0
%
xmm1
.
byte
196
227
125
2
193
15
/
/
vpblendd
0xf
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
121
110
76
144
32
/
/
vmovd
0x20
(
%
r8
%
rdx
4
)
%
xmm1
.
byte
196
227
125
2
193
1
/
/
vpblendd
0x1
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
126
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm1
.
byte
235
81
/
/
jmp
2f779
<
_sk_load_bgra_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
68
144
56
/
/
vmovd
0x38
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
196
226
125
89
192
/
/
vpbroadcastq
%
xmm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
117
2
192
64
/
/
vpblendd
0x40
%
ymm0
%
ymm1
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
195
113
34
76
144
52
1
/
/
vpinsrd
0x1
0x34
(
%
r8
%
rdx
4
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
195
113
34
76
144
48
0
/
/
vpinsrd
0x0
0x30
(
%
r8
%
rdx
4
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
126
111
12
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm1
.
byte
196
193
122
111
84
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
xmm2
.
byte
196
227
109
2
192
240
/
/
vpblendd
0xf0
%
ymm0
%
ymm2
%
ymm0
.
byte
196
227
117
56
216
1
/
/
vinserti128
0x1
%
xmm0
%
ymm1
%
ymm3
.
byte
196
227
117
70
192
49
/
/
vperm2i128
0x31
%
ymm0
%
ymm1
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
196
227
125
14
209
170
/
/
vpblendw
0xaa
%
ymm1
%
ymm0
%
ymm2
.
byte
196
227
101
14
201
170
/
/
vpblendw
0xaa
%
ymm1
%
ymm3
%
ymm1
.
byte
196
226
117
43
202
/
/
vpackusdw
%
ymm2
%
ymm1
%
ymm1
.
byte
197
125
111
5
30
213
0
0
/
/
vmovdqa
0xd51e
(
%
rip
)
%
ymm8
#
3ccc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa74
>
.
byte
196
193
117
219
208
/
/
vpand
%
ymm8
%
ymm1
%
ymm2
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
253
114
208
16
/
/
vpsrld
0x10
%
ymm0
%
ymm0
.
byte
197
229
114
211
16
/
/
vpsrld
0x10
%
ymm3
%
ymm3
.
byte
196
226
101
43
216
/
/
vpackusdw
%
ymm0
%
ymm3
%
ymm3
.
byte
196
193
101
219
192
/
/
vpand
%
ymm8
%
ymm3
%
ymm0
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
119
254
/
/
ja
2f7cc
<
_sk_load_bgra_hsw_lowp
+
0x1cc
>
.
byte
255
/
/
(
bad
)
.
byte
255
174
254
255
255
152
/
/
ljmp
*
-
0x67000002
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
246
/
/
push
%
rsi
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
254
/
/
fdivrp
%
st
%
st
(
6
)
.
byte
255
/
/
(
bad
)
.
byte
255
200
/
/
dec
%
eax
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
84
255
255
/
/
callq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
71
255
/
/
incl
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
57
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
35
/
/
jmpq
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
154
255
255
255
134
/
/
lcall
*
-
0x79000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_dst_hsw_lowp
.
globl
_sk_load_bgra_dst_hsw_lowp
FUNCTION
(
_sk_load_bgra_dst_hsw_lowp
)
_sk_load_bgra_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
55
/
/
ja
2f85a
<
_sk_load_bgra_dst_hsw_lowp
+
0x52
>
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
163
1
0
0
/
/
lea
0x1a3
(
%
rip
)
%
r9
#
2f9d4
<
_sk_load_bgra_dst_hsw_lowp
+
0x1cc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
205
239
246
/
/
vpxor
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
44
144
/
/
vmovd
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
233
39
1
0
0
/
/
jmpq
2f981
<
_sk_load_bgra_dst_hsw_lowp
+
0x179
>
.
byte
196
193
126
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm5
.
byte
196
193
126
111
100
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
ymm4
.
byte
233
21
1
0
0
/
/
jmpq
2f981
<
_sk_load_bgra_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
8
/
/
vmovd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
196
227
69
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm7
%
ymm4
.
byte
196
194
121
53
44
144
/
/
vpmovzxdq
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
196
227
93
2
237
3
/
/
vpblendd
0x3
%
ymm5
%
ymm4
%
ymm5
.
byte
197
253
111
231
/
/
vmovdqa
%
ymm7
%
ymm4
.
byte
233
229
0
0
0
/
/
jmpq
2f981
<
_sk_load_bgra_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
24
/
/
vmovd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
230
1
/
/
vextracti128
0x1
%
ymm4
%
xmm6
.
byte
196
195
73
34
116
144
20
1
/
/
vpinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm6
%
xmm6
.
byte
196
227
93
56
230
1
/
/
vinserti128
0x1
%
xmm6
%
ymm4
%
ymm4
.
byte
197
253
111
245
/
/
vmovdqa
%
ymm5
%
ymm6
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
108
144
16
0
/
/
vpinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
197
125
111
198
/
/
vmovdqa
%
ymm6
%
ymm8
.
byte
196
193
122
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
196
227
85
2
236
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm5
%
ymm5
.
byte
197
125
127
196
/
/
vmovdqa
%
ymm8
%
ymm4
.
byte
233
138
0
0
0
/
/
jmpq
2f981
<
_sk_load_bgra_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
40
/
/
vmovd
0x28
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm4
.
byte
196
195
89
34
108
144
36
1
/
/
vpinsrd
0x1
0x24
(
%
r8
%
rdx
4
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
121
110
108
144
32
/
/
vmovd
0x20
(
%
r8
%
rdx
4
)
%
xmm5
.
byte
196
227
93
2
229
1
/
/
vpblendd
0x1
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm5
.
byte
235
81
/
/
jmp
2f981
<
_sk_load_bgra_dst_hsw_lowp
+
0x179
>
.
byte
196
193
121
110
100
144
56
/
/
vmovd
0x38
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
108
144
52
1
/
/
vpinsrd
0x1
0x34
(
%
r8
%
rdx
4
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
108
144
48
0
/
/
vpinsrd
0x0
0x30
(
%
r8
%
rdx
4
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
44
144
/
/
vmovdqu
(
%
r8
%
rdx
4
)
%
ymm5
.
byte
196
193
122
111
116
144
32
/
/
vmovdqu
0x20
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
196
227
77
2
228
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm6
%
ymm4
.
byte
196
227
85
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm7
.
byte
196
227
85
70
228
49
/
/
vperm2i128
0x31
%
ymm4
%
ymm5
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
93
14
245
170
/
/
vpblendw
0xaa
%
ymm5
%
ymm4
%
ymm6
.
byte
196
227
69
14
237
170
/
/
vpblendw
0xaa
%
ymm5
%
ymm7
%
ymm5
.
byte
196
226
85
43
238
/
/
vpackusdw
%
ymm6
%
ymm5
%
ymm5
.
byte
197
125
111
5
54
211
0
0
/
/
vmovdqa
0xd336
(
%
rip
)
%
ymm8
#
3cce0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa94
>
.
byte
196
193
85
219
240
/
/
vpand
%
ymm8
%
ymm5
%
ymm6
.
byte
197
213
113
213
8
/
/
vpsrlw
0x8
%
ymm5
%
ymm5
.
byte
197
221
114
212
16
/
/
vpsrld
0x10
%
ymm4
%
ymm4
.
byte
197
197
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm7
.
byte
196
226
69
43
252
/
/
vpackusdw
%
ymm4
%
ymm7
%
ymm7
.
byte
196
193
69
219
224
/
/
vpand
%
ymm8
%
ymm7
%
ymm4
.
byte
197
197
113
215
8
/
/
vpsrlw
0x8
%
ymm7
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
119
254
/
/
ja
2f9d4
<
_sk_load_bgra_dst_hsw_lowp
+
0x1cc
>
.
byte
255
/
/
(
bad
)
.
byte
255
174
254
255
255
152
/
/
ljmp
*
-
0x67000002
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
246
/
/
push
%
rsi
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
254
/
/
fdivrp
%
st
%
st
(
6
)
.
byte
255
/
/
(
bad
)
.
byte
255
200
/
/
dec
%
eax
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
84
255
255
/
/
callq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
71
255
/
/
incl
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
57
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
35
/
/
jmpq
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
154
255
255
255
134
/
/
lcall
*
-
0x79000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_bgra_hsw_lowp
.
globl
_sk_store_bgra_hsw_lowp
FUNCTION
(
_sk_store_bgra_hsw_lowp
)
_sk_store_bgra_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
197
189
113
241
8
/
/
vpsllw
0x8
%
ymm1
%
ymm8
.
byte
197
61
235
194
/
/
vpor
%
ymm2
%
ymm8
%
ymm8
.
byte
196
66
125
51
200
/
/
vpmovzxwd
%
xmm8
%
ymm9
.
byte
196
67
125
57
192
1
/
/
vextracti128
0x1
%
ymm8
%
xmm8
.
byte
196
66
125
51
208
/
/
vpmovzxwd
%
xmm8
%
ymm10
.
byte
197
189
113
243
8
/
/
vpsllw
0x8
%
ymm3
%
ymm8
.
byte
197
61
235
192
/
/
vpor
%
ymm0
%
ymm8
%
ymm8
.
byte
196
67
125
57
195
1
/
/
vextracti128
0x1
%
ymm8
%
xmm11
.
byte
196
66
125
51
219
/
/
vpmovzxwd
%
xmm11
%
ymm11
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
193
61
114
240
16
/
/
vpslld
0x10
%
ymm8
%
ymm8
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
53
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm9
.
byte
196
65
53
235
202
/
/
vpor
%
ymm10
%
ymm9
%
ymm9
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
30
/
/
ja
2fa91
<
_sk_store_bgra_hsw_lowp
+
0x81
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
195
0
0
0
/
/
lea
0xc3
(
%
rip
)
%
r9
#
2fb40
<
_sk_store_bgra_hsw_lowp
+
0x130
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
4
144
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
171
0
0
0
/
/
jmpq
2fb3c
<
_sk_store_bgra_hsw_lowp
+
0x12c
>
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
196
65
126
127
76
144
32
/
/
vmovdqu
%
ymm9
0x20
(
%
r8
%
rdx
4
)
.
byte
233
153
0
0
0
/
/
jmpq
2fb3c
<
_sk_store_bgra_hsw_lowp
+
0x12c
>
.
byte
196
67
121
22
68
144
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
196
65
121
214
4
144
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
233
134
0
0
0
/
/
jmpq
2fb3c
<
_sk_store_bgra_hsw_lowp
+
0x12c
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
76
144
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
76
144
16
/
/
vmovd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
4
144
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
85
/
/
jmp
2fb3c
<
_sk_store_bgra_hsw_lowp
+
0x12c
>
.
byte
196
67
121
22
76
144
40
2
/
/
vpextrd
0x2
%
xmm9
0x28
(
%
r8
%
rdx
4
)
.
byte
196
67
121
22
76
144
36
1
/
/
vpextrd
0x1
%
xmm9
0x24
(
%
r8
%
rdx
4
)
.
byte
196
65
121
126
76
144
32
/
/
vmovd
%
xmm9
0x20
(
%
r8
%
rdx
4
)
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
235
54
/
/
jmp
2fb3c
<
_sk_store_bgra_hsw_lowp
+
0x12c
>
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
84
144
56
2
/
/
vpextrd
0x2
%
xmm10
0x38
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
84
144
52
1
/
/
vpextrd
0x1
%
xmm10
0x34
(
%
r8
%
rdx
4
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
65
121
126
84
144
48
/
/
vmovd
%
xmm10
0x30
(
%
r8
%
rdx
4
)
.
byte
196
65
126
127
4
144
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
4
)
.
byte
196
65
122
127
76
144
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
70
255
/
/
rex
.
RX
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
107
255
/
/
ljmp
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
99
255
/
/
jmpq
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
159
255
255
255
146
/
/
lcall
*
-
0x6d000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
132
255
255
255
118
255
/
/
incl
-
0x890001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
183
/
/
mov
0xb7ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
175
255
255
255
167
/
/
ljmp
*
-
0x58000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
226
/
/
jmpq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_8888_hsw_lowp
.
globl
_sk_gather_8888_hsw_lowp
FUNCTION
(
_sk_gather_8888_hsw_lowp
)
_sk_gather_8888_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
88
64
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm8
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
124
95
194
/
/
vmaxps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
116
95
202
/
/
vmaxps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
88
64
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
100
95
218
/
/
vmaxps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
193
108
95
210
/
/
vmaxps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
219
/
/
vcvttps2dq
%
ymm3
%
ymm3
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
98
125
88
64
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm8
.
byte
196
226
61
64
210
/
/
vpmulld
%
ymm2
%
ymm8
%
ymm2
.
byte
196
226
61
64
219
/
/
vpmulld
%
ymm3
%
ymm8
%
ymm3
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
237
254
192
/
/
vpaddd
%
ymm0
%
ymm2
%
ymm0
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
229
254
201
/
/
vpaddd
%
ymm1
%
ymm3
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
196
194
109
144
28
128
/
/
vpgatherdd
%
ymm2
(
%
r8
%
ymm0
4
)
%
ymm3
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
194
53
144
4
136
/
/
vpgatherdd
%
ymm9
(
%
r8
%
ymm1
4
)
%
ymm0
.
byte
196
227
101
56
208
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm2
.
byte
196
227
101
70
216
49
/
/
vperm2i128
0x31
%
ymm0
%
ymm3
%
ymm3
.
byte
196
195
101
14
194
170
/
/
vpblendw
0xaa
%
ymm10
%
ymm3
%
ymm0
.
byte
196
195
109
14
202
170
/
/
vpblendw
0xaa
%
ymm10
%
ymm2
%
ymm1
.
byte
196
226
117
43
200
/
/
vpackusdw
%
ymm0
%
ymm1
%
ymm1
.
byte
197
125
111
5
210
208
0
0
/
/
vmovdqa
0xd0d2
(
%
rip
)
%
ymm8
#
3cd00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xab4
>
.
byte
196
193
117
219
192
/
/
vpand
%
ymm8
%
ymm1
%
ymm0
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
229
114
211
16
/
/
vpsrld
0x10
%
ymm3
%
ymm3
.
byte
197
237
114
210
16
/
/
vpsrld
0x10
%
ymm2
%
ymm2
.
byte
196
226
109
43
219
/
/
vpackusdw
%
ymm3
%
ymm2
%
ymm3
.
byte
196
193
101
219
208
/
/
vpand
%
ymm8
%
ymm3
%
ymm2
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gather_bgra_hsw_lowp
.
globl
_sk_gather_bgra_hsw_lowp
FUNCTION
(
_sk_gather_bgra_hsw_lowp
)
_sk_gather_bgra_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
88
64
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm8
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
124
95
194
/
/
vmaxps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
116
95
202
/
/
vmaxps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
88
64
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
100
95
218
/
/
vmaxps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
193
108
95
210
/
/
vmaxps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
219
/
/
vcvttps2dq
%
ymm3
%
ymm3
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
98
125
88
64
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm8
.
byte
196
226
61
64
210
/
/
vpmulld
%
ymm2
%
ymm8
%
ymm2
.
byte
196
226
61
64
219
/
/
vpmulld
%
ymm3
%
ymm8
%
ymm3
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
237
254
192
/
/
vpaddd
%
ymm0
%
ymm2
%
ymm0
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
229
254
201
/
/
vpaddd
%
ymm1
%
ymm3
%
ymm1
.
byte
197
237
118
210
/
/
vpcmpeqd
%
ymm2
%
ymm2
%
ymm2
.
byte
197
229
239
219
/
/
vpxor
%
ymm3
%
ymm3
%
ymm3
.
byte
196
194
109
144
28
128
/
/
vpgatherdd
%
ymm2
(
%
r8
%
ymm0
4
)
%
ymm3
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
196
194
53
144
4
136
/
/
vpgatherdd
%
ymm9
(
%
r8
%
ymm1
4
)
%
ymm0
.
byte
196
99
101
56
192
1
/
/
vinserti128
0x1
%
xmm0
%
ymm3
%
ymm8
.
byte
196
227
101
70
192
49
/
/
vperm2i128
0x31
%
ymm0
%
ymm3
%
ymm0
.
byte
196
195
125
14
202
170
/
/
vpblendw
0xaa
%
ymm10
%
ymm0
%
ymm1
.
byte
196
195
61
14
210
170
/
/
vpblendw
0xaa
%
ymm10
%
ymm8
%
ymm2
.
byte
196
226
109
43
201
/
/
vpackusdw
%
ymm1
%
ymm2
%
ymm1
.
byte
197
253
111
29
25
208
0
0
/
/
vmovdqa
0xd019
(
%
rip
)
%
ymm3
#
3cd20
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xad4
>
.
byte
197
245
219
211
/
/
vpand
%
ymm3
%
ymm1
%
ymm2
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
253
114
208
16
/
/
vpsrld
0x10
%
ymm0
%
ymm0
.
byte
196
193
61
114
208
16
/
/
vpsrld
0x10
%
ymm8
%
ymm8
.
byte
196
98
61
43
192
/
/
vpackusdw
%
ymm0
%
ymm8
%
ymm8
.
byte
197
189
219
195
/
/
vpand
%
ymm3
%
ymm8
%
ymm0
.
byte
196
193
101
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_hsw_lowp
.
globl
_sk_load_565_hsw_lowp
FUNCTION
(
_sk_load_565_hsw_lowp
)
_sk_load_565_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
2fd6d
<
_sk_load_565_hsw_lowp
+
0x3f
>
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
130
1
0
0
/
/
lea
0x182
(
%
rip
)
%
r9
#
2fed8
<
_sk_load_565_hsw_lowp
+
0x1aa
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
8
1
0
0
/
/
jmpq
2fe75
<
_sk_load_565_hsw_lowp
+
0x147
>
.
byte
196
193
126
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
ymm0
.
byte
233
253
0
0
0
/
/
jmpq
2fe75
<
_sk_load_565_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
117
56
192
1
/
/
vinserti128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
193
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
125
2
193
1
/
/
vpblendd
0x1
%
ymm1
%
ymm0
%
ymm0
.
byte
233
218
0
0
0
/
/
jmpq
2fe75
<
_sk_load_565_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
117
56
192
1
/
/
vinserti128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
193
121
196
76
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
2
193
15
/
/
vpblendd
0xf
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
121
196
76
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
2
193
15
/
/
vpblendd
0xf
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
125
2
193
3
/
/
vpblendd
0x3
%
ymm1
%
ymm0
%
ymm0
.
byte
233
155
0
0
0
/
/
jmpq
2fe75
<
_sk_load_565_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
20
2
/
/
vpinsrw
0x2
0x14
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
18
1
/
/
vpinsrw
0x1
0x12
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
16
0
/
/
vpinsrw
0x0
0x10
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
122
111
12
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
117
2
192
240
/
/
vpblendd
0xf0
%
ymm0
%
ymm1
%
ymm0
.
byte
235
83
/
/
jmp
2fe75
<
_sk_load_565_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
28
6
/
/
vpinsrw
0x6
0x1c
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
26
5
/
/
vpinsrw
0x5
0x1a
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
24
4
/
/
vpinsrw
0x4
0x18
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
122
126
76
80
16
/
/
vmovq
0x10
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
193
122
111
20
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
196
227
109
56
201
1
/
/
vinserti128
0x1
%
xmm1
%
ymm2
%
ymm1
.
byte
196
227
117
2
192
192
/
/
vpblendd
0xc0
%
ymm0
%
ymm1
%
ymm0
.
byte
196
226
125
121
13
194
206
0
0
/
/
vpbroadcastw
0xcec2
(
%
rip
)
%
ymm1
#
3cd40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xaf4
>
.
byte
197
237
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm2
.
byte
197
237
219
201
/
/
vpand
%
ymm1
%
ymm2
%
ymm1
.
byte
197
237
113
208
5
/
/
vpsrlw
0x5
%
ymm0
%
ymm2
.
byte
196
226
125
121
29
173
206
0
0
/
/
vpbroadcastw
0xcead
(
%
rip
)
%
ymm3
#
3cd42
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xaf6
>
.
byte
197
237
219
211
/
/
vpand
%
ymm3
%
ymm2
%
ymm2
.
byte
196
226
125
121
29
162
206
0
0
/
/
vpbroadcastw
0xcea2
(
%
rip
)
%
ymm3
#
3cd44
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xaf8
>
.
byte
197
253
219
219
/
/
vpand
%
ymm3
%
ymm0
%
ymm3
.
byte
197
253
113
208
13
/
/
vpsrlw
0xd
%
ymm0
%
ymm0
.
byte
197
245
235
192
/
/
vpor
%
ymm0
%
ymm1
%
ymm0
.
byte
197
245
113
242
2
/
/
vpsllw
0x2
%
ymm2
%
ymm1
.
byte
197
237
113
210
4
/
/
vpsrlw
0x4
%
ymm2
%
ymm2
.
byte
197
245
235
202
/
/
vpor
%
ymm2
%
ymm1
%
ymm1
.
byte
197
237
113
243
3
/
/
vpsllw
0x3
%
ymm3
%
ymm2
.
byte
197
229
113
211
2
/
/
vpsrlw
0x2
%
ymm3
%
ymm3
.
byte
197
237
235
211
/
/
vpor
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
29
112
206
0
0
/
/
vpbroadcastw
0xce70
(
%
rip
)
%
ymm3
#
3cd46
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xafa
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
135
254
/
/
xchg
%
edi
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
178
254
255
255
160
/
/
pushq
-
0x5f000002
(
%
rdx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
241
/
/
push
%
rcx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
227
/
/
jmpq
*
%
rbx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
60
255
/
/
cmp
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
40
/
/
ljmp
*
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
20
255
/
/
callq
*
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
2
/
/
incl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
132
255
255
255
112
255
/
/
incl
-
0x8f0001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
74
255
/
/
decl
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_565_dst_hsw_lowp
.
globl
_sk_load_565_dst_hsw_lowp
FUNCTION
(
_sk_load_565_dst_hsw_lowp
)
_sk_load_565_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
2ff53
<
_sk_load_565_dst_hsw_lowp
+
0x3f
>
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
132
1
0
0
/
/
lea
0x184
(
%
rip
)
%
r9
#
300c0
<
_sk_load_565_dst_hsw_lowp
+
0x1ac
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
8
1
0
0
/
/
jmpq
3005b
<
_sk_load_565_dst_hsw_lowp
+
0x147
>
.
byte
196
193
126
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
ymm4
.
byte
233
253
0
0
0
/
/
jmpq
3005b
<
_sk_load_565_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
85
56
228
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
196
193
121
110
44
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
93
2
229
1
/
/
vpblendd
0x1
%
ymm5
%
ymm4
%
ymm4
.
byte
233
218
0
0
0
/
/
jmpq
3005b
<
_sk_load_565_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
85
56
228
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
196
193
89
196
108
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
89
196
108
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
122
126
44
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
93
2
229
3
/
/
vpblendd
0x3
%
ymm5
%
ymm4
%
ymm4
.
byte
233
155
0
0
0
/
/
jmpq
3005b
<
_sk_load_565_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
20
2
/
/
vpinsrw
0x2
0x14
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
18
1
/
/
vpinsrw
0x1
0x12
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
16
0
/
/
vpinsrw
0x0
0x10
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
122
111
44
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
85
2
228
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm5
%
ymm4
.
byte
235
83
/
/
jmp
3005b
<
_sk_load_565_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
28
6
/
/
vpinsrw
0x6
0x1c
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
26
5
/
/
vpinsrw
0x5
0x1a
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
24
4
/
/
vpinsrw
0x4
0x18
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
122
126
108
80
16
/
/
vmovq
0x10
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
193
122
111
52
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
196
227
77
56
237
1
/
/
vinserti128
0x1
%
xmm5
%
ymm6
%
ymm5
.
byte
196
227
85
2
228
192
/
/
vpblendd
0xc0
%
ymm4
%
ymm5
%
ymm4
.
byte
196
226
125
121
45
228
204
0
0
/
/
vpbroadcastw
0xcce4
(
%
rip
)
%
ymm5
#
3cd48
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xafc
>
.
byte
197
205
113
212
8
/
/
vpsrlw
0x8
%
ymm4
%
ymm6
.
byte
197
205
219
237
/
/
vpand
%
ymm5
%
ymm6
%
ymm5
.
byte
197
205
113
212
5
/
/
vpsrlw
0x5
%
ymm4
%
ymm6
.
byte
196
226
125
121
61
207
204
0
0
/
/
vpbroadcastw
0xcccf
(
%
rip
)
%
ymm7
#
3cd4a
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xafe
>
.
byte
197
205
219
247
/
/
vpand
%
ymm7
%
ymm6
%
ymm6
.
byte
196
226
125
121
61
196
204
0
0
/
/
vpbroadcastw
0xccc4
(
%
rip
)
%
ymm7
#
3cd4c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb00
>
.
byte
197
221
219
255
/
/
vpand
%
ymm7
%
ymm4
%
ymm7
.
byte
197
221
113
212
13
/
/
vpsrlw
0xd
%
ymm4
%
ymm4
.
byte
197
213
235
228
/
/
vpor
%
ymm4
%
ymm5
%
ymm4
.
byte
197
213
113
246
2
/
/
vpsllw
0x2
%
ymm6
%
ymm5
.
byte
197
205
113
214
4
/
/
vpsrlw
0x4
%
ymm6
%
ymm6
.
byte
197
213
235
238
/
/
vpor
%
ymm6
%
ymm5
%
ymm5
.
byte
197
205
113
247
3
/
/
vpsllw
0x3
%
ymm7
%
ymm6
.
byte
197
197
113
215
2
/
/
vpsrlw
0x2
%
ymm7
%
ymm7
.
byte
197
205
235
247
/
/
vpor
%
ymm7
%
ymm6
%
ymm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
61
146
204
0
0
/
/
vpbroadcastw
0xcc92
(
%
rip
)
%
ymm7
#
3cd4e
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb02
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
133
254
/
/
test
%
edi
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
176
254
255
255
158
/
/
pushq
-
0x61000002
(
%
rax
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
225
/
/
jmpq
*
%
rcx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
58
255
/
/
cmp
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
38
/
/
jmpq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
130
255
255
255
110
/
/
incl
0x6effffff
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
90
255
/
/
lcall
*
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
72
255
/
/
decl
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_565_hsw_lowp
.
globl
_sk_store_565_hsw_lowp
FUNCTION
(
_sk_store_565_hsw_lowp
)
_sk_store_565_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
98
125
121
5
59
204
0
0
/
/
vpbroadcastw
0xcc3b
(
%
rip
)
%
ymm8
#
3cd50
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb04
>
.
byte
197
181
113
240
8
/
/
vpsllw
0x8
%
ymm0
%
ymm9
.
byte
196
65
53
219
192
/
/
vpand
%
ymm8
%
ymm9
%
ymm8
.
byte
196
98
125
121
13
42
204
0
0
/
/
vpbroadcastw
0xcc2a
(
%
rip
)
%
ymm9
#
3cd52
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb06
>
.
byte
197
173
113
241
3
/
/
vpsllw
0x3
%
ymm1
%
ymm10
.
byte
196
65
45
219
201
/
/
vpand
%
ymm9
%
ymm10
%
ymm9
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
197
181
113
210
3
/
/
vpsrlw
0x3
%
ymm2
%
ymm9
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
31
/
/
ja
3016a
<
_sk_store_565_hsw_lowp
+
0x6e
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
195
0
0
0
/
/
lea
0xc3
(
%
rip
)
%
r9
#
30218
<
_sk_store_565_hsw_lowp
+
0x11c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
233
167
0
0
0
/
/
jmpq
30211
<
_sk_store_565_hsw_lowp
+
0x115
>
.
byte
196
65
126
127
4
80
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
2
)
.
byte
233
156
0
0
0
/
/
jmpq
30211
<
_sk_store_565_hsw_lowp
+
0x115
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
65
121
126
4
80
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
233
137
0
0
0
/
/
jmpq
30211
<
_sk_store_565_hsw_lowp
+
0x115
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
65
121
214
4
80
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
105
/
/
jmp
30211
<
_sk_store_565_hsw_lowp
+
0x115
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
20
2
/
/
vpextrw
0x2
%
xmm9
0x14
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
18
1
/
/
vpextrw
0x1
%
xmm9
0x12
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
16
0
/
/
vpextrw
0x0
%
xmm9
0x10
(
%
r8
%
rdx
2
)
.
byte
235
55
/
/
jmp
3020b
<
_sk_store_565_hsw_lowp
+
0x10f
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
28
6
/
/
vpextrw
0x6
%
xmm9
0x1c
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
26
5
/
/
vpextrw
0x5
%
xmm9
0x1a
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
24
4
/
/
vpextrw
0x4
%
xmm9
0x18
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
214
76
80
16
/
/
vmovq
%
xmm9
0x10
(
%
r8
%
rdx
2
)
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
70
255
/
/
rex
.
RX
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
101
255
/
/
jmpq
*
-
0x1
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
93
255
/
/
lcall
*
-
0x1
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
136
255
255
255
128
/
/
decl
-
0x7f000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
120
255
/
/
js
3022d
<
_sk_store_565_hsw_lowp
+
0x131
>
.
byte
255
/
/
(
bad
)
.
byte
255
112
255
/
/
pushq
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
243
/
/
push
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
172
255
255
255
158
255
/
/
ljmp
*
-
0x610001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
144
255
255
255
230
/
/
callq
*
-
0x19000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
202
/
/
dec
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
/
/
.
byte
0xbc
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_565_hsw_lowp
.
globl
_sk_gather_565_hsw_lowp
FUNCTION
(
_sk_gather_565_hsw_lowp
)
_sk_gather_565_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
88
64
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm8
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
124
95
194
/
/
vmaxps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
116
95
202
/
/
vmaxps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
88
64
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
100
95
218
/
/
vmaxps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
193
108
95
210
/
/
vmaxps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
219
/
/
vcvttps2dq
%
ymm3
%
ymm3
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
98
125
88
64
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm8
.
byte
196
226
61
64
210
/
/
vpmulld
%
ymm2
%
ymm8
%
ymm2
.
byte
196
226
61
64
219
/
/
vpmulld
%
ymm3
%
ymm8
%
ymm3
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
237
254
192
/
/
vpaddd
%
ymm0
%
ymm2
%
ymm0
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
229
254
201
/
/
vpaddd
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
208
1
/
/
vpinsrw
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
2
/
/
vpinsrw
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
233
196
208
3
/
/
vpinsrw
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
4
/
/
vpinsrw
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
200
5
/
/
vpinsrw
0x5
%
eax
%
xmm2
%
xmm1
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
241
196
200
6
/
/
vpinsrw
0x6
%
eax
%
xmm1
%
xmm1
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
241
196
200
7
/
/
vpinsrw
0x7
%
eax
%
xmm1
%
xmm1
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
208
1
/
/
vpinsrw
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
2
/
/
vpinsrw
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
233
196
208
3
/
/
vpinsrw
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
4
/
/
vpinsrw
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm2
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
226
125
121
13
110
201
0
0
/
/
vpbroadcastw
0xc96e
(
%
rip
)
%
ymm1
#
3cd54
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb08
>
.
byte
197
237
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm2
.
byte
197
237
219
201
/
/
vpand
%
ymm1
%
ymm2
%
ymm1
.
byte
197
237
113
208
5
/
/
vpsrlw
0x5
%
ymm0
%
ymm2
.
byte
196
226
125
121
29
89
201
0
0
/
/
vpbroadcastw
0xc959
(
%
rip
)
%
ymm3
#
3cd56
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb0a
>
.
byte
197
237
219
211
/
/
vpand
%
ymm3
%
ymm2
%
ymm2
.
byte
196
226
125
121
29
78
201
0
0
/
/
vpbroadcastw
0xc94e
(
%
rip
)
%
ymm3
#
3cd58
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb0c
>
.
byte
197
253
219
219
/
/
vpand
%
ymm3
%
ymm0
%
ymm3
.
byte
197
253
113
208
13
/
/
vpsrlw
0xd
%
ymm0
%
ymm0
.
byte
197
245
235
192
/
/
vpor
%
ymm0
%
ymm1
%
ymm0
.
byte
197
245
113
242
2
/
/
vpsllw
0x2
%
ymm2
%
ymm1
.
byte
197
237
113
210
4
/
/
vpsrlw
0x4
%
ymm2
%
ymm2
.
byte
197
245
235
202
/
/
vpor
%
ymm2
%
ymm1
%
ymm1
.
byte
197
237
113
243
3
/
/
vpsllw
0x3
%
ymm3
%
ymm2
.
byte
197
229
113
211
2
/
/
vpsrlw
0x2
%
ymm3
%
ymm3
.
byte
197
237
235
211
/
/
vpor
%
ymm3
%
ymm2
%
ymm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
29
28
201
0
0
/
/
vpbroadcastw
0xc91c
(
%
rip
)
%
ymm3
#
3cd5a
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb0e
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_4444_hsw_lowp
.
globl
_sk_load_4444_hsw_lowp
FUNCTION
(
_sk_load_4444_hsw_lowp
)
_sk_load_4444_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
3047f
<
_sk_load_4444_hsw_lowp
+
0x3f
>
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
112
1
0
0
/
/
lea
0x170
(
%
rip
)
%
r9
#
305d8
<
_sk_load_4444_hsw_lowp
+
0x198
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
8
1
0
0
/
/
jmpq
30587
<
_sk_load_4444_hsw_lowp
+
0x147
>
.
byte
196
193
126
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
ymm0
.
byte
233
253
0
0
0
/
/
jmpq
30587
<
_sk_load_4444_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
117
56
192
1
/
/
vinserti128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
193
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
125
2
193
1
/
/
vpblendd
0x1
%
ymm1
%
ymm0
%
ymm0
.
byte
233
218
0
0
0
/
/
jmpq
30587
<
_sk_load_4444_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
117
56
192
1
/
/
vinserti128
0x1
%
xmm0
%
ymm1
%
ymm0
.
byte
196
193
121
196
76
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
2
193
15
/
/
vpblendd
0xf
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
121
196
76
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
2
193
15
/
/
vpblendd
0xf
%
ymm1
%
ymm0
%
ymm0
.
byte
196
193
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
125
2
193
3
/
/
vpblendd
0x3
%
ymm1
%
ymm0
%
ymm0
.
byte
233
155
0
0
0
/
/
jmpq
30587
<
_sk_load_4444_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
20
2
/
/
vpinsrw
0x2
0x14
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
18
1
/
/
vpinsrw
0x1
0x12
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
16
0
/
/
vpinsrw
0x0
0x10
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
122
111
12
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
227
117
2
192
240
/
/
vpblendd
0xf0
%
ymm0
%
ymm1
%
ymm0
.
byte
235
83
/
/
jmp
30587
<
_sk_load_4444_hsw_lowp
+
0x147
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
193
121
196
76
80
28
6
/
/
vpinsrw
0x6
0x1c
(
%
r8
%
rdx
2
)
%
xmm0
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
26
5
/
/
vpinsrw
0x5
0x1a
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
227
125
57
193
1
/
/
vextracti128
0x1
%
ymm0
%
xmm1
.
byte
196
193
113
196
76
80
24
4
/
/
vpinsrw
0x4
0x18
(
%
r8
%
rdx
2
)
%
xmm1
%
xmm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
122
126
76
80
16
/
/
vmovq
0x10
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
196
193
122
111
20
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
196
227
109
56
201
1
/
/
vinserti128
0x1
%
xmm1
%
ymm2
%
ymm1
.
byte
196
227
117
2
192
192
/
/
vpblendd
0xc0
%
ymm0
%
ymm1
%
ymm0
.
byte
197
245
113
208
12
/
/
vpsrlw
0xc
%
ymm0
%
ymm1
.
byte
197
237
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm2
.
byte
196
226
125
121
29
194
199
0
0
/
/
vpbroadcastw
0xc7c2
(
%
rip
)
%
ymm3
#
3cd5c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb10
>
.
byte
197
237
219
211
/
/
vpand
%
ymm3
%
ymm2
%
ymm2
.
byte
197
189
113
208
4
/
/
vpsrlw
0x4
%
ymm0
%
ymm8
.
byte
197
61
219
195
/
/
vpand
%
ymm3
%
ymm8
%
ymm8
.
byte
197
253
219
219
/
/
vpand
%
ymm3
%
ymm0
%
ymm3
.
byte
197
253
113
241
4
/
/
vpsllw
0x4
%
ymm1
%
ymm0
.
byte
197
253
235
193
/
/
vpor
%
ymm1
%
ymm0
%
ymm0
.
byte
197
245
113
242
4
/
/
vpsllw
0x4
%
ymm2
%
ymm1
.
byte
197
245
235
202
/
/
vpor
%
ymm2
%
ymm1
%
ymm1
.
byte
196
193
109
113
240
4
/
/
vpsllw
0x4
%
ymm8
%
ymm2
.
byte
196
193
109
235
208
/
/
vpor
%
ymm8
%
ymm2
%
ymm2
.
byte
197
189
113
243
4
/
/
vpsllw
0x4
%
ymm3
%
ymm8
.
byte
197
189
235
219
/
/
vpor
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
153
/
/
cltd
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
196
/
/
inc
%
esp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
178
254
255
255
3
/
/
pushq
0x3fffffe
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
245
/
/
push
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
78
255
/
/
decl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
58
255
/
/
cmp
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
38
/
/
jmpq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
20
255
/
/
callq
*
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
150
255
255
255
130
/
/
callq
*
-
0x7d000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
110
255
/
/
ljmp
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_dst_hsw_lowp
.
globl
_sk_load_4444_dst_hsw_lowp
FUNCTION
(
_sk_load_4444_dst_hsw_lowp
)
_sk_load_4444_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
30653
<
_sk_load_4444_dst_hsw_lowp
+
0x3f
>
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
112
1
0
0
/
/
lea
0x170
(
%
rip
)
%
r9
#
307ac
<
_sk_load_4444_dst_hsw_lowp
+
0x198
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
8
1
0
0
/
/
jmpq
3075b
<
_sk_load_4444_dst_hsw_lowp
+
0x147
>
.
byte
196
193
126
111
36
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
ymm4
.
byte
233
253
0
0
0
/
/
jmpq
3075b
<
_sk_load_4444_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
85
56
228
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
196
193
121
110
44
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
93
2
229
1
/
/
vpblendd
0x1
%
ymm5
%
ymm4
%
ymm4
.
byte
233
218
0
0
0
/
/
jmpq
3075b
<
_sk_load_4444_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
85
56
228
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm4
.
byte
196
193
89
196
108
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
89
196
108
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
122
126
44
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
93
2
229
3
/
/
vpblendd
0x3
%
ymm5
%
ymm4
%
ymm4
.
byte
233
155
0
0
0
/
/
jmpq
3075b
<
_sk_load_4444_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
20
2
/
/
vpinsrw
0x2
0x14
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
18
1
/
/
vpinsrw
0x1
0x12
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
16
0
/
/
vpinsrw
0x0
0x10
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
122
111
44
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
227
85
2
228
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm5
%
ymm4
.
byte
235
83
/
/
jmp
3075b
<
_sk_load_4444_dst_hsw_lowp
+
0x147
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
193
89
196
108
80
28
6
/
/
vpinsrw
0x6
0x1c
(
%
r8
%
rdx
2
)
%
xmm4
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
26
5
/
/
vpinsrw
0x5
0x1a
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
193
81
196
108
80
24
4
/
/
vpinsrw
0x4
0x18
(
%
r8
%
rdx
2
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
122
126
108
80
16
/
/
vmovq
0x10
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
196
193
122
111
52
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
196
227
77
56
237
1
/
/
vinserti128
0x1
%
xmm5
%
ymm6
%
ymm5
.
byte
196
227
85
2
228
192
/
/
vpblendd
0xc0
%
ymm4
%
ymm5
%
ymm4
.
byte
197
213
113
212
12
/
/
vpsrlw
0xc
%
ymm4
%
ymm5
.
byte
197
205
113
212
8
/
/
vpsrlw
0x8
%
ymm4
%
ymm6
.
byte
196
226
125
121
61
240
197
0
0
/
/
vpbroadcastw
0xc5f0
(
%
rip
)
%
ymm7
#
3cd5e
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb12
>
.
byte
197
205
219
247
/
/
vpand
%
ymm7
%
ymm6
%
ymm6
.
byte
197
189
113
212
4
/
/
vpsrlw
0x4
%
ymm4
%
ymm8
.
byte
197
61
219
199
/
/
vpand
%
ymm7
%
ymm8
%
ymm8
.
byte
197
221
219
255
/
/
vpand
%
ymm7
%
ymm4
%
ymm7
.
byte
197
221
113
245
4
/
/
vpsllw
0x4
%
ymm5
%
ymm4
.
byte
197
221
235
229
/
/
vpor
%
ymm5
%
ymm4
%
ymm4
.
byte
197
213
113
246
4
/
/
vpsllw
0x4
%
ymm6
%
ymm5
.
byte
197
213
235
238
/
/
vpor
%
ymm6
%
ymm5
%
ymm5
.
byte
196
193
77
113
240
4
/
/
vpsllw
0x4
%
ymm8
%
ymm6
.
byte
196
193
77
235
240
/
/
vpor
%
ymm8
%
ymm6
%
ymm6
.
byte
197
189
113
247
4
/
/
vpsllw
0x4
%
ymm7
%
ymm8
.
byte
197
189
235
255
/
/
vpor
%
ymm7
%
ymm8
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
153
/
/
cltd
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
196
/
/
inc
%
esp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
178
254
255
255
3
/
/
pushq
0x3fffffe
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
245
/
/
push
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
231
/
/
jmpq
*
%
rdi
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
78
255
/
/
decl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
58
255
/
/
cmp
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
38
/
/
jmpq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
20
255
/
/
callq
*
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
150
255
255
255
130
/
/
callq
*
-
0x7d000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
110
255
/
/
ljmp
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_4444_hsw_lowp
.
globl
_sk_store_4444_hsw_lowp
FUNCTION
(
_sk_store_4444_hsw_lowp
)
_sk_store_4444_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
98
125
121
5
95
197
0
0
/
/
vpbroadcastw
0xc55f
(
%
rip
)
%
ymm8
#
3cd60
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb14
>
.
byte
197
181
113
240
8
/
/
vpsllw
0x8
%
ymm0
%
ymm9
.
byte
196
65
53
219
192
/
/
vpand
%
ymm8
%
ymm9
%
ymm8
.
byte
197
181
113
241
4
/
/
vpsllw
0x4
%
ymm1
%
ymm9
.
byte
197
53
219
13
104
197
0
0
/
/
vpand
0xc568
(
%
rip
)
%
ymm9
%
ymm9
#
3cd80
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb34
>
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
196
98
125
121
13
122
197
0
0
/
/
vpbroadcastw
0xc57a
(
%
rip
)
%
ymm9
#
3cda0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb54
>
.
byte
196
65
109
219
201
/
/
vpand
%
ymm9
%
ymm2
%
ymm9
.
byte
197
173
113
211
4
/
/
vpsrlw
0x4
%
ymm3
%
ymm10
.
byte
196
65
53
235
202
/
/
vpor
%
ymm10
%
ymm9
%
ymm9
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
31
/
/
ja
30863
<
_sk_store_4444_hsw_lowp
+
0x7b
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
194
0
0
0
/
/
lea
0xc2
(
%
rip
)
%
r9
#
30910
<
_sk_store_4444_hsw_lowp
+
0x128
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
21
4
80
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
233
167
0
0
0
/
/
jmpq
3090a
<
_sk_store_4444_hsw_lowp
+
0x122
>
.
byte
196
65
126
127
4
80
/
/
vmovdqu
%
ymm8
(
%
r8
%
rdx
2
)
.
byte
233
156
0
0
0
/
/
jmpq
3090a
<
_sk_store_4444_hsw_lowp
+
0x122
>
.
byte
196
67
121
21
68
80
4
2
/
/
vpextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
196
65
121
126
4
80
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
233
137
0
0
0
/
/
jmpq
3090a
<
_sk_store_4444_hsw_lowp
+
0x122
>
.
byte
196
67
121
21
68
80
12
6
/
/
vpextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
10
5
/
/
vpextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
196
67
121
21
68
80
8
4
/
/
vpextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
196
65
121
214
4
80
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
105
/
/
jmp
3090a
<
_sk_store_4444_hsw_lowp
+
0x122
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
20
2
/
/
vpextrw
0x2
%
xmm9
0x14
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
18
1
/
/
vpextrw
0x1
%
xmm9
0x12
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
16
0
/
/
vpextrw
0x0
%
xmm9
0x10
(
%
r8
%
rdx
2
)
.
byte
235
55
/
/
jmp
30904
<
_sk_store_4444_hsw_lowp
+
0x11c
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
28
6
/
/
vpextrw
0x6
%
xmm9
0x1c
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
26
5
/
/
vpextrw
0x5
%
xmm9
0x1a
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
21
76
80
24
4
/
/
vpextrw
0x4
%
xmm9
0x18
(
%
r8
%
rdx
2
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
214
76
80
16
/
/
vmovq
%
xmm9
0x10
(
%
r8
%
rdx
2
)
.
byte
196
65
122
127
4
80
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
71
255
/
/
rex
.
RXB
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
102
255
/
/
jmpq
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
94
255
/
/
lcall
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
137
255
255
255
129
/
/
decl
-
0x7e000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
121
255
/
/
jns
30925
<
_sk_store_4444_hsw_lowp
+
0x13d
>
.
byte
255
/
/
(
bad
)
.
byte
255
113
255
/
/
pushq
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
244
/
/
push
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
173
255
255
255
159
/
/
ljmp
*
-
0x60000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
145
255
255
255
231
/
/
callq
*
-
0x18000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
203
/
/
dec
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
189
/
/
.
byte
0xbd
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_4444_hsw_lowp
.
globl
_sk_gather_4444_hsw_lowp
FUNCTION
(
_sk_gather_4444_hsw_lowp
)
_sk_gather_4444_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
88
64
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm8
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
124
95
194
/
/
vmaxps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
116
95
202
/
/
vmaxps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
88
64
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm8
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
100
95
218
/
/
vmaxps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
193
108
95
210
/
/
vmaxps
%
ymm10
%
ymm2
%
ymm2
.
byte
196
193
108
93
208
/
/
vminps
%
ymm8
%
ymm2
%
ymm2
.
byte
196
193
100
93
216
/
/
vminps
%
ymm8
%
ymm3
%
ymm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
219
/
/
vcvttps2dq
%
ymm3
%
ymm3
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
98
125
88
64
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm8
.
byte
196
226
61
64
210
/
/
vpmulld
%
ymm2
%
ymm8
%
ymm2
.
byte
196
226
61
64
219
/
/
vpmulld
%
ymm3
%
ymm8
%
ymm3
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
237
254
192
/
/
vpaddd
%
ymm0
%
ymm2
%
ymm0
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
229
254
201
/
/
vpaddd
%
ymm1
%
ymm3
%
ymm1
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
208
1
/
/
vpinsrw
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
2
/
/
vpinsrw
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
233
196
208
3
/
/
vpinsrw
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
4
/
/
vpinsrw
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
200
5
/
/
vpinsrw
0x5
%
eax
%
xmm2
%
xmm1
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
241
196
200
6
/
/
vpinsrw
0x6
%
eax
%
xmm1
%
xmm1
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
241
196
200
7
/
/
vpinsrw
0x7
%
eax
%
xmm1
%
xmm1
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
208
1
/
/
vpinsrw
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
2
/
/
vpinsrw
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
233
196
208
3
/
/
vpinsrw
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
233
196
208
4
/
/
vpinsrw
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
183
4
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
eax
.
byte
197
233
196
192
5
/
/
vpinsrw
0x5
%
eax
%
xmm2
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
197
249
196
192
6
/
/
vpinsrw
0x6
%
eax
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
183
4
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
eax
.
byte
197
249
196
192
7
/
/
vpinsrw
0x7
%
eax
%
xmm0
%
xmm0
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
197
245
113
208
12
/
/
vpsrlw
0xc
%
ymm0
%
ymm1
.
byte
197
237
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm2
.
byte
196
226
125
121
29
186
194
0
0
/
/
vpbroadcastw
0xc2ba
(
%
rip
)
%
ymm3
#
3cda2
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb56
>
.
byte
197
237
219
211
/
/
vpand
%
ymm3
%
ymm2
%
ymm2
.
byte
197
189
113
208
4
/
/
vpsrlw
0x4
%
ymm0
%
ymm8
.
byte
197
61
219
195
/
/
vpand
%
ymm3
%
ymm8
%
ymm8
.
byte
197
253
219
219
/
/
vpand
%
ymm3
%
ymm0
%
ymm3
.
byte
197
253
113
241
4
/
/
vpsllw
0x4
%
ymm1
%
ymm0
.
byte
197
253
235
193
/
/
vpor
%
ymm1
%
ymm0
%
ymm0
.
byte
197
245
113
242
4
/
/
vpsllw
0x4
%
ymm2
%
ymm1
.
byte
197
245
235
202
/
/
vpor
%
ymm2
%
ymm1
%
ymm1
.
byte
196
193
109
113
240
4
/
/
vpsllw
0x4
%
ymm8
%
ymm2
.
byte
196
193
109
235
208
/
/
vpor
%
ymm8
%
ymm2
%
ymm2
.
byte
197
189
113
243
4
/
/
vpsllw
0x4
%
ymm3
%
ymm8
.
byte
197
189
235
219
/
/
vpor
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_hsw_lowp
.
globl
_sk_load_a8_hsw_lowp
FUNCTION
(
_sk_load_a8_hsw_lowp
)
_sk_load_a8_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
30b5f
<
_sk_load_a8_hsw_lowp
+
0x3c
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
220
0
0
0
/
/
lea
0xdc
(
%
rip
)
%
r9
#
30c24
<
_sk_load_a8_hsw_lowp
+
0x101
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
175
0
0
0
/
/
jmpq
30c0e
<
_sk_load_a8_hsw_lowp
+
0xeb
>
.
byte
196
193
122
111
4
16
/
/
vmovdqu
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
233
164
0
0
0
/
/
jmpq
30c0e
<
_sk_load_a8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
2
2
/
/
vpinsrb
0x2
0x2
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
227
121
14
193
1
/
/
vpblendw
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
132
0
0
0
/
/
jmpq
30c0e
<
_sk_load_a8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
6
6
/
/
vpinsrb
0x6
0x6
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
5
5
/
/
vpinsrb
0x5
0x5
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
4
4
/
/
vpinsrb
0x4
0x4
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
235
90
/
/
jmp
30c0e
<
_sk_load_a8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
10
10
/
/
vpinsrb
0xa
0xa
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
9
9
/
/
vpinsrb
0x9
0x9
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
8
8
/
/
vpinsrb
0x8
0x8
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
235
48
/
/
jmp
30c0e
<
_sk_load_a8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
14
14
/
/
vpinsrb
0xe
0xe
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
13
13
/
/
vpinsrb
0xd
0xd
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
12
12
/
/
vpinsrb
0xc
0xc
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
195
113
34
76
16
8
2
/
/
vpinsrd
0x2
0x8
(
%
r8
%
rdx
1
)
%
xmm1
%
xmm1
.
byte
196
227
113
2
192
8
/
/
vpblendd
0x8
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
125
48
216
/
/
vpmovzxbw
%
xmm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
45
255
255
255
82
/
/
sub
0x52ffffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
70
255
/
/
incl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
130
255
255
255
122
/
/
incl
0x7affffff
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
102
255
/
/
jmpq
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
172
255
255
255
164
255
/
/
ljmp
*
-
0x5b0001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
156
255
255
255
144
255
/
/
lcall
*
-
0x6f0001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
214
/
/
callq
*
%
rsi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
206
/
/
dec
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
186
/
/
.
byte
0xba
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_a8_dst_hsw_lowp
.
globl
_sk_load_a8_dst_hsw_lowp
FUNCTION
(
_sk_load_a8_dst_hsw_lowp
)
_sk_load_a8_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
30c9c
<
_sk_load_a8_dst_hsw_lowp
+
0x3c
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
219
0
0
0
/
/
lea
0xdb
(
%
rip
)
%
r9
#
30d60
<
_sk_load_a8_dst_hsw_lowp
+
0x100
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
175
0
0
0
/
/
jmpq
30d4b
<
_sk_load_a8_dst_hsw_lowp
+
0xeb
>
.
byte
196
193
122
111
36
16
/
/
vmovdqu
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
233
164
0
0
0
/
/
jmpq
30d4b
<
_sk_load_a8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
2
2
/
/
vpinsrb
0x2
0x2
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
232
/
/
vmovd
%
eax
%
xmm5
.
byte
196
227
89
14
229
1
/
/
vpblendw
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
132
0
0
0
/
/
jmpq
30d4b
<
_sk_load_a8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
6
6
/
/
vpinsrb
0x6
0x6
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
5
5
/
/
vpinsrb
0x5
0x5
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
4
4
/
/
vpinsrb
0x4
0x4
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
235
90
/
/
jmp
30d4b
<
_sk_load_a8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
10
10
/
/
vpinsrb
0xa
0xa
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
9
9
/
/
vpinsrb
0x9
0x9
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
8
8
/
/
vpinsrb
0x8
0x8
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
235
48
/
/
jmp
30d4b
<
_sk_load_a8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
14
14
/
/
vpinsrb
0xe
0xe
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
13
13
/
/
vpinsrb
0xd
0xd
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
12
12
/
/
vpinsrb
0xc
0xc
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
195
81
34
108
16
8
2
/
/
vpinsrd
0x2
0x8
(
%
r8
%
rdx
1
)
%
xmm5
%
xmm5
.
byte
196
227
81
2
228
8
/
/
vpblendd
0x8
%
xmm4
%
xmm5
%
xmm4
.
byte
196
226
125
48
252
/
/
vpmovzxbw
%
xmm4
%
ymm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
46
255
/
/
cs
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
83
255
/
/
callq
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
71
255
/
/
incl
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
131
255
255
255
123
/
/
incl
0x7bffffff
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
115
255
/
/
pushq
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
103
255
/
/
jmpq
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
173
255
255
255
165
/
/
ljmp
*
-
0x5a000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
157
255
255
255
145
/
/
lcall
*
-
0x6e000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
215
/
/
callq
*
%
rdi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
207
/
/
dec
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
187
/
/
.
byte
0xbb
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_a8_hsw_lowp
.
globl
_sk_store_a8_hsw_lowp
FUNCTION
(
_sk_store_a8_hsw_lowp
)
_sk_store_a8_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
196
195
125
57
216
1
/
/
vextracti128
0x1
%
ymm3
%
xmm8
.
byte
197
121
111
13
105
200
0
0
/
/
vmovdqa
0xc869
(
%
rip
)
%
xmm9
#
3d620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13d4
>
.
byte
196
66
57
0
193
/
/
vpshufb
%
xmm9
%
xmm8
%
xmm8
.
byte
196
66
97
0
201
/
/
vpshufb
%
xmm9
%
xmm3
%
xmm9
.
byte
196
65
49
108
192
/
/
vpunpcklqdq
%
xmm8
%
xmm9
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
28
/
/
ja
30dec
<
_sk_store_a8_hsw_lowp
+
0x50
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
146
0
0
0
/
/
lea
0x92
(
%
rip
)
%
r9
#
30e6c
<
_sk_store_a8_hsw_lowp
+
0xd0
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
67
121
20
4
16
0
/
/
vpextrb
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
121
/
/
jmp
30e65
<
_sk_store_a8_hsw_lowp
+
0xc9
>
.
byte
196
65
122
127
4
16
/
/
vmovdqu
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
113
/
/
jmp
30e65
<
_sk_store_a8_hsw_lowp
+
0xc9
>
.
byte
196
67
121
20
68
16
2
2
/
/
vpextrb
0x2
%
xmm8
0x2
(
%
r8
%
rdx
1
)
.
byte
196
67
121
21
4
16
0
/
/
vpextrw
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
96
/
/
jmp
30e65
<
_sk_store_a8_hsw_lowp
+
0xc9
>
.
byte
196
67
121
20
68
16
6
6
/
/
vpextrb
0x6
%
xmm8
0x6
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
5
5
/
/
vpextrb
0x5
%
xmm8
0x5
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
4
4
/
/
vpextrb
0x4
%
xmm8
0x4
(
%
r8
%
rdx
1
)
.
byte
196
65
121
126
4
16
/
/
vmovd
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
64
/
/
jmp
30e65
<
_sk_store_a8_hsw_lowp
+
0xc9
>
.
byte
196
67
121
20
68
16
10
10
/
/
vpextrb
0xa
%
xmm8
0xa
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
9
9
/
/
vpextrb
0x9
%
xmm8
0x9
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
8
8
/
/
vpextrb
0x8
%
xmm8
0x8
(
%
r8
%
rdx
1
)
.
byte
235
32
/
/
jmp
30e5f
<
_sk_store_a8_hsw_lowp
+
0xc3
>
.
byte
196
67
121
20
68
16
14
14
/
/
vpextrb
0xe
%
xmm8
0xe
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
13
13
/
/
vpextrb
0xd
%
xmm8
0xd
(
%
r8
%
rdx
1
)
.
byte
196
67
121
20
68
16
12
12
/
/
vpextrb
0xc
%
xmm8
0xc
(
%
r8
%
rdx
1
)
.
byte
196
67
121
22
68
16
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
1
)
.
byte
196
65
121
214
4
16
/
/
vmovq
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
119
255
/
/
ja
30e6d
<
_sk_store_a8_hsw_lowp
+
0xd1
>
.
byte
255
/
/
(
bad
)
.
byte
255
144
255
255
255
136
/
/
callq
*
-
0x77000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
177
255
255
255
169
/
/
pushq
-
0x56000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
161
255
255
255
153
/
/
jmpq
*
-
0x66000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
243
/
/
push
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
193
/
/
inc
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
185
255
255
255
235
/
/
mov
0xebffffff
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
227
/
/
jmpq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_a8_hsw_lowp
.
globl
_sk_gather_a8_hsw_lowp
FUNCTION
(
_sk_gather_a8_hsw_lowp
)
_sk_gather_a8_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
88
64
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm8
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
124
95
194
/
/
vmaxps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
116
95
202
/
/
vmaxps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
116
93
216
/
/
vminps
%
ymm8
%
ymm1
%
ymm11
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
226
125
88
72
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm1
.
byte
196
193
117
254
201
/
/
vpaddd
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
100
95
218
/
/
vmaxps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
193
108
95
210
/
/
vmaxps
%
ymm10
%
ymm2
%
ymm2
.
byte
197
236
93
209
/
/
vminps
%
ymm1
%
ymm2
%
ymm2
.
byte
197
228
93
201
/
/
vminps
%
ymm1
%
ymm3
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
226
125
88
88
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm3
.
byte
196
226
101
64
210
/
/
vpmulld
%
ymm2
%
ymm3
%
ymm2
.
byte
196
226
101
64
217
/
/
vpmulld
%
ymm1
%
ymm3
%
ymm3
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
237
254
200
/
/
vpaddd
%
ymm0
%
ymm2
%
ymm1
.
byte
196
193
126
91
195
/
/
vcvttps2dq
%
ymm11
%
ymm0
.
byte
197
229
254
192
/
/
vpaddd
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
105
32
208
1
/
/
vpinsrb
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
2
/
/
vpinsrb
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
105
32
208
3
/
/
vpinsrb
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
4
/
/
vpinsrb
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
105
32
200
5
/
/
vpinsrb
0x5
%
eax
%
xmm2
%
xmm1
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
6
/
/
vpinsrb
0x6
%
eax
%
xmm1
%
xmm1
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
113
32
200
7
/
/
vpinsrb
0x7
%
eax
%
xmm1
%
xmm1
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
8
/
/
vpinsrb
0x8
%
eax
%
xmm1
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
113
32
200
9
/
/
vpinsrb
0x9
%
eax
%
xmm1
%
xmm1
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
10
/
/
vpinsrb
0xa
%
eax
%
xmm1
%
xmm1
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
113
32
200
11
/
/
vpinsrb
0xb
%
eax
%
xmm1
%
xmm1
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
12
/
/
vpinsrb
0xc
%
eax
%
xmm1
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
113
32
192
13
/
/
vpinsrb
0xd
%
eax
%
xmm1
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
121
32
192
14
/
/
vpinsrb
0xe
%
eax
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
121
32
192
15
/
/
vpinsrb
0xf
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
48
216
/
/
vpmovzxbw
%
xmm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_g8_hsw_lowp
.
globl
_sk_load_g8_hsw_lowp
FUNCTION
(
_sk_load_g8_hsw_lowp
)
_sk_load_g8_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
3108b
<
_sk_load_g8_hsw_lowp
+
0x3c
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
224
0
0
0
/
/
lea
0xe0
(
%
rip
)
%
r9
#
31154
<
_sk_load_g8_hsw_lowp
+
0x105
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
192
/
/
vmovd
%
eax
%
xmm0
.
byte
233
175
0
0
0
/
/
jmpq
3113a
<
_sk_load_g8_hsw_lowp
+
0xeb
>
.
byte
196
193
122
111
4
16
/
/
vmovdqu
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
233
164
0
0
0
/
/
jmpq
3113a
<
_sk_load_g8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
2
2
/
/
vpinsrb
0x2
0x2
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
200
/
/
vmovd
%
eax
%
xmm1
.
byte
196
227
121
14
193
1
/
/
vpblendw
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
233
132
0
0
0
/
/
jmpq
3113a
<
_sk_load_g8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
6
6
/
/
vpinsrb
0x6
0x6
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
5
5
/
/
vpinsrb
0x5
0x5
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
4
4
/
/
vpinsrb
0x4
0x4
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
193
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
227
121
2
193
1
/
/
vpblendd
0x1
%
xmm1
%
xmm0
%
xmm0
.
byte
235
90
/
/
jmp
3113a
<
_sk_load_g8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
10
10
/
/
vpinsrb
0xa
0xa
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
9
9
/
/
vpinsrb
0x9
0x9
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
8
8
/
/
vpinsrb
0x8
0x8
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
227
113
2
192
12
/
/
vpblendd
0xc
%
xmm0
%
xmm1
%
xmm0
.
byte
235
48
/
/
jmp
3113a
<
_sk_load_g8_hsw_lowp
+
0xeb
>
.
byte
197
249
239
192
/
/
vpxor
%
xmm0
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
14
14
/
/
vpinsrb
0xe
0xe
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
13
13
/
/
vpinsrb
0xd
0xd
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
195
121
32
68
16
12
12
/
/
vpinsrb
0xc
0xc
(
%
r8
%
rdx
1
)
%
xmm0
%
xmm0
.
byte
196
193
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
196
195
113
34
76
16
8
2
/
/
vpinsrd
0x2
0x8
(
%
r8
%
rdx
1
)
%
xmm1
%
xmm1
.
byte
196
227
113
2
192
8
/
/
vpblendd
0x8
%
xmm0
%
xmm1
%
xmm0
.
byte
196
226
125
48
192
/
/
vpmovzxbw
%
xmm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
29
90
188
0
0
/
/
vpbroadcastw
0xbc5a
(
%
rip
)
%
ymm3
#
3cda4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb58
>
.
byte
197
253
111
200
/
/
vmovdqa
%
ymm0
%
ymm1
.
byte
197
253
111
208
/
/
vmovdqa
%
ymm0
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
41
255
/
/
sub
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
78
255
/
/
decl
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
66
255
/
/
incl
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
126
255
/
/
jle
31161
<
_sk_load_g8_hsw_lowp
+
0x112
>
.
byte
255
/
/
(
bad
)
.
byte
255
118
255
/
/
pushq
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
110
255
/
/
ljmp
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
98
255
/
/
jmpq
*
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
168
255
255
255
160
/
/
ljmp
*
-
0x5f000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
140
/
/
lcall
*
-
0x73000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
202
/
/
dec
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
182
255
/
/
mov
0xff
%
dh
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_dst_hsw_lowp
.
globl
_sk_load_g8_dst_hsw_lowp
FUNCTION
(
_sk_load_g8_dst_hsw_lowp
)
_sk_load_g8_dst_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
37
/
/
ja
311cc
<
_sk_load_g8_dst_hsw_lowp
+
0x3c
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
227
0
0
0
/
/
lea
0xe3
(
%
rip
)
%
r9
#
31298
<
_sk_load_g8_dst_hsw_lowp
+
0x108
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
224
/
/
vmovd
%
eax
%
xmm4
.
byte
233
175
0
0
0
/
/
jmpq
3127b
<
_sk_load_g8_dst_hsw_lowp
+
0xeb
>
.
byte
196
193
122
111
36
16
/
/
vmovdqu
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
233
164
0
0
0
/
/
jmpq
3127b
<
_sk_load_g8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
2
2
/
/
vpinsrb
0x2
0x2
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
249
110
232
/
/
vmovd
%
eax
%
xmm5
.
byte
196
227
89
14
229
1
/
/
vpblendw
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
233
132
0
0
0
/
/
jmpq
3127b
<
_sk_load_g8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
6
6
/
/
vpinsrb
0x6
0x6
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
5
5
/
/
vpinsrb
0x5
0x5
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
4
4
/
/
vpinsrb
0x4
0x4
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
193
121
110
44
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
227
89
2
229
1
/
/
vpblendd
0x1
%
xmm5
%
xmm4
%
xmm4
.
byte
235
90
/
/
jmp
3127b
<
_sk_load_g8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
10
10
/
/
vpinsrb
0xa
0xa
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
9
9
/
/
vpinsrb
0x9
0x9
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
8
8
/
/
vpinsrb
0x8
0x8
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
227
81
2
228
12
/
/
vpblendd
0xc
%
xmm4
%
xmm5
%
xmm4
.
byte
235
48
/
/
jmp
3127b
<
_sk_load_g8_dst_hsw_lowp
+
0xeb
>
.
byte
197
217
239
228
/
/
vpxor
%
xmm4
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
14
14
/
/
vpinsrb
0xe
0xe
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
13
13
/
/
vpinsrb
0xd
0xd
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
195
89
32
100
16
12
12
/
/
vpinsrb
0xc
0xc
(
%
r8
%
rdx
1
)
%
xmm4
%
xmm4
.
byte
196
193
122
126
44
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
196
195
81
34
108
16
8
2
/
/
vpinsrd
0x2
0x8
(
%
r8
%
rdx
1
)
%
xmm5
%
xmm5
.
byte
196
227
81
2
228
8
/
/
vpblendd
0x8
%
xmm4
%
xmm5
%
xmm4
.
byte
196
226
125
48
228
/
/
vpmovzxbw
%
xmm4
%
ymm4
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
61
27
187
0
0
/
/
vpbroadcastw
0xbb1b
(
%
rip
)
%
ymm7
#
3cda6
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb5a
>
.
byte
197
253
111
236
/
/
vmovdqa
%
ymm4
%
ymm5
.
byte
197
253
111
244
/
/
vmovdqa
%
ymm4
%
ymm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
38
255
/
/
es
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
75
255
/
/
decl
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
63
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
123
255
/
/
jnp
312a5
<
_sk_load_g8_dst_hsw_lowp
+
0x115
>
.
byte
255
/
/
(
bad
)
.
byte
255
115
255
/
/
pushq
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
107
255
/
/
ljmp
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
95
255
/
/
lcall
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
165
255
255
255
157
/
/
jmpq
*
-
0x62000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
149
255
255
255
137
/
/
callq
*
-
0x76000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
207
/
/
dec
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
179
/
/
mov
0xb3ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_luminance_to_alpha_hsw_lowp
.
globl
_sk_luminance_to_alpha_hsw_lowp
FUNCTION
(
_sk_luminance_to_alpha_hsw_lowp
)
_sk_luminance_to_alpha_hsw_lowp
:
.
byte
196
226
125
121
29
203
186
0
0
/
/
vpbroadcastw
0xbacb
(
%
rip
)
%
ymm3
#
3cda8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb5c
>
.
byte
197
253
213
195
/
/
vpmullw
%
ymm3
%
ymm0
%
ymm0
.
byte
196
226
125
121
29
192
186
0
0
/
/
vpbroadcastw
0xbac0
(
%
rip
)
%
ymm3
#
3cdaa
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb5e
>
.
byte
197
245
213
203
/
/
vpmullw
%
ymm3
%
ymm1
%
ymm1
.
byte
197
245
253
192
/
/
vpaddw
%
ymm0
%
ymm1
%
ymm0
.
byte
196
226
125
121
13
177
186
0
0
/
/
vpbroadcastw
0xbab1
(
%
rip
)
%
ymm1
#
3cdac
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb60
>
.
byte
197
237
213
201
/
/
vpmullw
%
ymm1
%
ymm2
%
ymm1
.
byte
197
253
253
193
/
/
vpaddw
%
ymm1
%
ymm0
%
ymm0
.
byte
197
229
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
253
239
192
/
/
vpxor
%
ymm0
%
ymm0
%
ymm0
.
byte
197
245
239
201
/
/
vpxor
%
ymm1
%
ymm1
%
ymm1
.
byte
197
237
239
210
/
/
vpxor
%
ymm2
%
ymm2
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gather_g8_hsw_lowp
.
globl
_sk_gather_g8_hsw_lowp
FUNCTION
(
_sk_gather_g8_hsw_lowp
)
_sk_gather_g8_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
98
125
88
64
12
/
/
vpbroadcastd
0xc
(
%
rax
)
%
ymm8
.
byte
196
65
53
118
201
/
/
vpcmpeqd
%
ymm9
%
ymm9
%
ymm9
.
byte
196
65
61
254
193
/
/
vpaddd
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
193
124
95
194
/
/
vmaxps
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
116
95
202
/
/
vmaxps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
65
116
93
216
/
/
vminps
%
ymm8
%
ymm1
%
ymm11
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
226
125
88
72
16
/
/
vpbroadcastd
0x10
(
%
rax
)
%
ymm1
.
byte
196
193
117
254
201
/
/
vpaddd
%
ymm9
%
ymm1
%
ymm1
.
byte
196
193
100
95
218
/
/
vmaxps
%
ymm10
%
ymm3
%
ymm3
.
byte
196
193
108
95
210
/
/
vmaxps
%
ymm10
%
ymm2
%
ymm2
.
byte
197
236
93
209
/
/
vminps
%
ymm1
%
ymm2
%
ymm2
.
byte
197
228
93
201
/
/
vminps
%
ymm1
%
ymm3
%
ymm1
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
254
91
201
/
/
vcvttps2dq
%
ymm1
%
ymm1
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
226
125
88
88
8
/
/
vpbroadcastd
0x8
(
%
rax
)
%
ymm3
.
byte
196
226
101
64
210
/
/
vpmulld
%
ymm2
%
ymm3
%
ymm2
.
byte
196
226
101
64
217
/
/
vpmulld
%
ymm1
%
ymm3
%
ymm3
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
197
237
254
200
/
/
vpaddd
%
ymm0
%
ymm2
%
ymm1
.
byte
196
193
126
91
195
/
/
vcvttps2dq
%
ymm11
%
ymm0
.
byte
197
229
254
192
/
/
vpaddd
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
197
249
110
208
/
/
vmovd
%
eax
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
105
32
208
1
/
/
vpinsrb
0x1
%
eax
%
xmm2
%
xmm2
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
201
1
/
/
vextracti128
0x1
%
ymm1
%
xmm1
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
2
/
/
vpinsrb
0x2
%
eax
%
xmm2
%
xmm2
.
byte
196
193
249
126
201
/
/
vmovq
%
xmm1
%
r9
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
105
32
208
3
/
/
vpinsrb
0x3
%
eax
%
xmm2
%
xmm2
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
105
32
208
4
/
/
vpinsrb
0x4
%
eax
%
xmm2
%
xmm2
.
byte
196
195
249
22
202
1
/
/
vpextrq
0x1
%
xmm1
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
105
32
200
5
/
/
vpinsrb
0x5
%
eax
%
xmm2
%
xmm1
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
6
/
/
vpinsrb
0x6
%
eax
%
xmm1
%
xmm1
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
113
32
200
7
/
/
vpinsrb
0x7
%
eax
%
xmm1
%
xmm1
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
8
/
/
vpinsrb
0x8
%
eax
%
xmm1
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
113
32
200
9
/
/
vpinsrb
0x9
%
eax
%
xmm1
%
xmm1
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
196
227
125
57
192
1
/
/
vextracti128
0x1
%
ymm0
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
10
/
/
vpinsrb
0xa
%
eax
%
xmm1
%
xmm1
.
byte
196
193
249
126
193
/
/
vmovq
%
xmm0
%
r9
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
113
32
200
11
/
/
vpinsrb
0xb
%
eax
%
xmm1
%
xmm1
.
byte
68
137
200
/
/
mov
%
r9d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
113
32
200
12
/
/
vpinsrb
0xc
%
eax
%
xmm1
%
xmm1
.
byte
196
195
249
22
194
1
/
/
vpextrq
0x1
%
xmm0
%
r10
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
67
15
182
4
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
eax
.
byte
196
227
113
32
192
13
/
/
vpinsrb
0xd
%
eax
%
xmm1
%
xmm0
.
byte
68
137
208
/
/
mov
%
r10d
%
eax
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
196
227
121
32
192
14
/
/
vpinsrb
0xe
%
eax
%
xmm0
%
xmm0
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
67
15
182
4
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
eax
.
byte
196
227
121
32
192
15
/
/
vpinsrb
0xf
%
eax
%
xmm0
%
xmm0
.
byte
196
226
125
48
192
/
/
vpmovzxbw
%
xmm0
%
ymm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
121
29
244
184
0
0
/
/
vpbroadcastw
0xb8f4
(
%
rip
)
%
ymm3
#
3cdae
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb62
>
.
byte
197
253
111
200
/
/
vmovdqa
%
ymm0
%
ymm1
.
byte
197
253
111
208
/
/
vmovdqa
%
ymm0
%
ymm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_hsw_lowp
.
globl
_sk_scale_1_float_hsw_lowp
FUNCTION
(
_sk_scale_1_float_hsw_lowp
)
_sk_scale_1_float_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
13
86
176
0
0
/
/
vmovss
0xb056
(
%
rip
)
%
xmm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
98
57
169
13
29
176
0
0
/
/
vfmadd213ss
0xb01d
(
%
rip
)
%
xmm8
%
xmm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
122
44
193
/
/
vcvttss2si
%
xmm9
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
66
125
121
192
/
/
vpbroadcastw
%
xmm8
%
ymm8
.
byte
197
189
213
192
/
/
vpmullw
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
121
13
186
184
0
0
/
/
vpbroadcastw
0xb8ba
(
%
rip
)
%
ymm9
#
3cdb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb64
>
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
189
213
201
/
/
vpmullw
%
ymm1
%
ymm8
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
189
213
210
/
/
vpmullw
%
ymm2
%
ymm8
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
189
213
219
/
/
vpmullw
%
ymm3
%
ymm8
%
ymm3
.
byte
196
193
101
253
217
/
/
vpaddw
%
ymm9
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_1_float_hsw_lowp
.
globl
_sk_lerp_1_float_hsw_lowp
FUNCTION
(
_sk_lerp_1_float_hsw_lowp
)
_sk_lerp_1_float_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
122
16
0
/
/
vmovss
(
%
rax
)
%
xmm8
.
byte
197
122
16
13
236
175
0
0
/
/
vmovss
0xafec
(
%
rip
)
%
xmm9
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
98
57
169
13
179
175
0
0
/
/
vfmadd213ss
0xafb3
(
%
rip
)
%
xmm8
%
xmm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
193
122
44
193
/
/
vcvttss2si
%
xmm9
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
196
66
125
121
192
/
/
vpbroadcastw
%
xmm8
%
ymm8
.
byte
196
98
125
121
13
86
184
0
0
/
/
vpbroadcastw
0xb856
(
%
rip
)
%
ymm9
#
3cdb2
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb66
>
.
byte
196
65
53
249
208
/
/
vpsubw
%
ymm8
%
ymm9
%
ymm10
.
byte
197
45
213
220
/
/
vpmullw
%
ymm4
%
ymm10
%
ymm11
.
byte
197
189
213
192
/
/
vpmullw
%
ymm0
%
ymm8
%
ymm0
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
165
253
192
/
/
vpaddw
%
ymm0
%
ymm11
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
45
213
221
/
/
vpmullw
%
ymm5
%
ymm10
%
ymm11
.
byte
197
189
213
201
/
/
vpmullw
%
ymm1
%
ymm8
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
165
253
201
/
/
vpaddw
%
ymm1
%
ymm11
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
45
213
222
/
/
vpmullw
%
ymm6
%
ymm10
%
ymm11
.
byte
197
189
213
210
/
/
vpmullw
%
ymm2
%
ymm8
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
165
253
210
/
/
vpaddw
%
ymm2
%
ymm11
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
45
213
215
/
/
vpmullw
%
ymm7
%
ymm10
%
ymm10
.
byte
197
189
213
219
/
/
vpmullw
%
ymm3
%
ymm8
%
ymm3
.
byte
196
193
101
253
217
/
/
vpaddw
%
ymm9
%
ymm3
%
ymm3
.
byte
197
173
253
219
/
/
vpaddw
%
ymm3
%
ymm10
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_hsw_lowp
.
globl
_sk_scale_u8_hsw_lowp
FUNCTION
(
_sk_scale_u8_hsw_lowp
)
_sk_scale_u8_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
38
/
/
ja
315fa
<
_sk_scale_u8_hsw_lowp
+
0x3d
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
21
1
0
0
/
/
lea
0x115
(
%
rip
)
%
r9
#
316f8
<
_sk_scale_u8_hsw_lowp
+
0x13b
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
179
0
0
0
/
/
jmpq
316ad
<
_sk_scale_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
122
111
4
16
/
/
vmovdqu
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
233
168
0
0
0
/
/
jmpq
316ad
<
_sk_scale_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
2
2
/
/
vpinsrb
0x2
0x2
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
200
/
/
vmovd
%
eax
%
xmm9
.
byte
196
67
57
14
193
1
/
/
vpblendw
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
135
0
0
0
/
/
jmpq
316ad
<
_sk_scale_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
6
6
/
/
vpinsrb
0x6
0x6
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
5
5
/
/
vpinsrb
0x5
0x5
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
4
4
/
/
vpinsrb
0x4
0x4
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
235
92
/
/
jmp
316ad
<
_sk_scale_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
10
10
/
/
vpinsrb
0xa
0xa
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
9
9
/
/
vpinsrb
0x9
0x9
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
8
8
/
/
vpinsrb
0x8
0x8
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
235
49
/
/
jmp
316ad
<
_sk_scale_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
14
14
/
/
vpinsrb
0xe
0xe
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
13
13
/
/
vpinsrb
0xd
0xd
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
12
12
/
/
vpinsrb
0xc
0xc
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
67
49
34
76
16
8
2
/
/
vpinsrd
0x2
0x8
(
%
r8
%
rdx
1
)
%
xmm9
%
xmm9
.
byte
196
67
49
2
192
8
/
/
vpblendd
0x8
%
xmm8
%
xmm9
%
xmm8
.
byte
196
66
125
48
192
/
/
vpmovzxbw
%
xmm8
%
ymm8
.
byte
197
189
213
192
/
/
vpmullw
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
121
13
245
182
0
0
/
/
vpbroadcastw
0xb6f5
(
%
rip
)
%
ymm9
#
3cdb4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb68
>
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
189
213
201
/
/
vpmullw
%
ymm1
%
ymm8
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
189
213
210
/
/
vpmullw
%
ymm2
%
ymm8
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
189
213
219
/
/
vpmullw
%
ymm3
%
ymm8
%
ymm3
.
byte
196
193
101
253
217
/
/
vpaddw
%
ymm9
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
244
/
/
hlt
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
75
/
/
decl
0x4bffffff
(
%
rip
)
#
4c031704
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x4bff54b8
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
67
255
/
/
incl
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
59
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
46
/
/
ljmp
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
118
255
/
/
pushq
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
110
255
/
/
ljmp
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
102
255
/
/
jmpq
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
89
255
/
/
lcall
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
161
255
255
255
153
/
/
jmpq
*
-
0x66000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
145
255
255
255
132
/
/
callq
*
-
0x7b000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_u8_hsw_lowp
.
globl
_sk_lerp_u8_hsw_lowp
FUNCTION
(
_sk_lerp_u8_hsw_lowp
)
_sk_lerp_u8_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
38
/
/
ja
31771
<
_sk_lerp_u8_hsw_lowp
+
0x3d
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
58
1
0
0
/
/
lea
0x13a
(
%
rip
)
%
r9
#
31894
<
_sk_lerp_u8_hsw_lowp
+
0x160
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
179
0
0
0
/
/
jmpq
31824
<
_sk_lerp_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
122
111
4
16
/
/
vmovdqu
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
233
168
0
0
0
/
/
jmpq
31824
<
_sk_lerp_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
2
2
/
/
vpinsrb
0x2
0x2
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
197
121
110
200
/
/
vmovd
%
eax
%
xmm9
.
byte
196
67
57
14
193
1
/
/
vpblendw
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
233
135
0
0
0
/
/
jmpq
31824
<
_sk_lerp_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
6
6
/
/
vpinsrb
0x6
0x6
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
5
5
/
/
vpinsrb
0x5
0x5
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
4
4
/
/
vpinsrb
0x4
0x4
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
65
121
110
12
16
/
/
vmovd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
67
57
2
193
1
/
/
vpblendd
0x1
%
xmm9
%
xmm8
%
xmm8
.
byte
235
92
/
/
jmp
31824
<
_sk_lerp_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
10
10
/
/
vpinsrb
0xa
0xa
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
9
9
/
/
vpinsrb
0x9
0x9
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
8
8
/
/
vpinsrb
0x8
0x8
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
67
49
2
192
12
/
/
vpblendd
0xc
%
xmm8
%
xmm9
%
xmm8
.
byte
235
49
/
/
jmp
31824
<
_sk_lerp_u8_hsw_lowp
+
0xf0
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
14
14
/
/
vpinsrb
0xe
0xe
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
13
13
/
/
vpinsrb
0xd
0xd
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
67
57
32
68
16
12
12
/
/
vpinsrb
0xc
0xc
(
%
r8
%
rdx
1
)
%
xmm8
%
xmm8
.
byte
196
65
122
126
12
16
/
/
vmovq
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
196
67
49
34
76
16
8
2
/
/
vpinsrd
0x2
0x8
(
%
r8
%
rdx
1
)
%
xmm9
%
xmm9
.
byte
196
67
49
2
192
8
/
/
vpblendd
0x8
%
xmm8
%
xmm9
%
xmm8
.
byte
196
66
125
48
192
/
/
vpmovzxbw
%
xmm8
%
ymm8
.
byte
196
98
125
121
13
132
181
0
0
/
/
vpbroadcastw
0xb584
(
%
rip
)
%
ymm9
#
3cdb6
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb6a
>
.
byte
196
65
61
239
209
/
/
vpxor
%
ymm9
%
ymm8
%
ymm10
.
byte
197
45
213
220
/
/
vpmullw
%
ymm4
%
ymm10
%
ymm11
.
byte
197
189
213
192
/
/
vpmullw
%
ymm0
%
ymm8
%
ymm0
.
byte
196
193
125
253
193
/
/
vpaddw
%
ymm9
%
ymm0
%
ymm0
.
byte
197
165
253
192
/
/
vpaddw
%
ymm0
%
ymm11
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
45
213
221
/
/
vpmullw
%
ymm5
%
ymm10
%
ymm11
.
byte
197
189
213
201
/
/
vpmullw
%
ymm1
%
ymm8
%
ymm1
.
byte
196
193
117
253
201
/
/
vpaddw
%
ymm9
%
ymm1
%
ymm1
.
byte
197
165
253
201
/
/
vpaddw
%
ymm1
%
ymm11
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
45
213
222
/
/
vpmullw
%
ymm6
%
ymm10
%
ymm11
.
byte
197
189
213
210
/
/
vpmullw
%
ymm2
%
ymm8
%
ymm2
.
byte
196
193
109
253
209
/
/
vpaddw
%
ymm9
%
ymm2
%
ymm2
.
byte
197
165
253
210
/
/
vpaddw
%
ymm2
%
ymm11
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
45
213
215
/
/
vpmullw
%
ymm7
%
ymm10
%
ymm10
.
byte
197
189
213
219
/
/
vpmullw
%
ymm3
%
ymm8
%
ymm3
.
byte
196
193
101
253
217
/
/
vpaddw
%
ymm9
%
ymm3
%
ymm3
.
byte
197
173
253
219
/
/
vpaddw
%
ymm3
%
ymm10
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
207
/
/
iret
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
245
/
/
push
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
254
255
255
38
/
/
callq
2703189f
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x26ff5653
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
9
/
/
decl
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
81
255
/
/
callq
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
73
255
/
/
decl
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
65
255
/
/
incl
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
52
255
/
/
pushq
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
124
255
/
/
jl
318c1
<
_sk_lerp_u8_hsw_lowp
+
0x18d
>
.
byte
255
/
/
(
bad
)
.
byte
255
116
255
255
/
/
pushq
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
108
255
255
/
/
ljmp
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
95
255
/
/
lcall
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_scale_565_hsw_lowp
.
globl
_sk_scale_565_hsw_lowp
FUNCTION
(
_sk_scale_565_hsw_lowp
)
_sk_scale_565_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
38
/
/
ja
31910
<
_sk_scale_565_hsw_lowp
+
0x40
>
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
255
1
0
0
/
/
lea
0x1ff
(
%
rip
)
%
r9
#
31af8
<
_sk_scale_565_hsw_lowp
+
0x228
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
12
1
0
0
/
/
jmpq
31a1c
<
_sk_scale_565_hsw_lowp
+
0x14c
>
.
byte
196
65
126
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
ymm8
.
byte
233
1
1
0
0
/
/
jmpq
31a1c
<
_sk_scale_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
53
56
192
1
/
/
vinserti128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
61
2
193
1
/
/
vpblendd
0x1
%
ymm9
%
ymm8
%
ymm8
.
byte
233
221
0
0
0
/
/
jmpq
31a1c
<
_sk_scale_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
53
56
192
1
/
/
vinserti128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
57
196
76
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
2
193
15
/
/
vpblendd
0xf
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
57
196
76
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
2
193
15
/
/
vpblendd
0xf
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
61
2
193
3
/
/
vpblendd
0x3
%
ymm9
%
ymm8
%
ymm8
.
byte
233
157
0
0
0
/
/
jmpq
31a1c
<
_sk_scale_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
20
2
/
/
vpinsrw
0x2
0x14
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
18
1
/
/
vpinsrw
0x1
0x12
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
16
0
/
/
vpinsrw
0x0
0x10
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
65
122
111
12
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
53
2
192
240
/
/
vpblendd
0xf0
%
ymm8
%
ymm9
%
ymm8
.
byte
235
84
/
/
jmp
31a1c
<
_sk_scale_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
28
6
/
/
vpinsrw
0x6
0x1c
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
26
5
/
/
vpinsrw
0x5
0x1a
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
24
4
/
/
vpinsrw
0x4
0x18
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
65
122
126
76
80
16
/
/
vmovq
0x10
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
65
122
111
20
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm10
.
byte
196
67
45
56
201
1
/
/
vinserti128
0x1
%
xmm9
%
ymm10
%
ymm9
.
byte
196
67
53
2
192
192
/
/
vpblendd
0xc0
%
ymm8
%
ymm9
%
ymm8
.
byte
196
98
125
121
13
147
179
0
0
/
/
vpbroadcastw
0xb393
(
%
rip
)
%
ymm9
#
3cdb8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb6c
>
.
byte
196
193
45
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm10
.
byte
196
65
45
219
201
/
/
vpand
%
ymm9
%
ymm10
%
ymm9
.
byte
196
193
45
113
208
5
/
/
vpsrlw
0x5
%
ymm8
%
ymm10
.
byte
196
98
125
121
29
123
179
0
0
/
/
vpbroadcastw
0xb37b
(
%
rip
)
%
ymm11
#
3cdba
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb6e
>
.
byte
196
65
45
219
211
/
/
vpand
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
121
29
111
179
0
0
/
/
vpbroadcastw
0xb36f
(
%
rip
)
%
ymm11
#
3cdbc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb70
>
.
byte
196
65
61
219
219
/
/
vpand
%
ymm11
%
ymm8
%
ymm11
.
byte
196
193
61
113
208
13
/
/
vpsrlw
0xd
%
ymm8
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
196
193
53
113
242
2
/
/
vpsllw
0x2
%
ymm10
%
ymm9
.
byte
196
193
45
113
210
4
/
/
vpsrlw
0x4
%
ymm10
%
ymm10
.
byte
196
65
53
235
202
/
/
vpor
%
ymm10
%
ymm9
%
ymm9
.
byte
196
193
45
113
243
3
/
/
vpsllw
0x3
%
ymm11
%
ymm10
.
byte
196
193
37
113
211
2
/
/
vpsrlw
0x2
%
ymm11
%
ymm11
.
byte
196
65
45
235
211
/
/
vpor
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
121
29
54
179
0
0
/
/
vpbroadcastw
0xb336
(
%
rip
)
%
ymm11
#
3cdbe
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb72
>
.
byte
196
65
101
239
227
/
/
vpxor
%
ymm11
%
ymm3
%
ymm12
.
byte
196
65
69
239
219
/
/
vpxor
%
ymm11
%
ymm7
%
ymm11
.
byte
196
65
37
101
220
/
/
vpcmpgtw
%
ymm12
%
ymm11
%
ymm11
.
byte
196
66
53
58
226
/
/
vpminuw
%
ymm10
%
ymm9
%
ymm12
.
byte
196
66
29
58
224
/
/
vpminuw
%
ymm8
%
ymm12
%
ymm12
.
byte
196
66
53
62
234
/
/
vpmaxuw
%
ymm10
%
ymm9
%
ymm13
.
byte
196
66
21
62
232
/
/
vpmaxuw
%
ymm8
%
ymm13
%
ymm13
.
byte
196
67
21
76
220
176
/
/
vpblendvb
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
197
189
213
192
/
/
vpmullw
%
ymm0
%
ymm8
%
ymm0
.
byte
196
98
125
121
5
2
179
0
0
/
/
vpbroadcastw
0xb302
(
%
rip
)
%
ymm8
#
3cdc0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb74
>
.
byte
196
193
125
253
192
/
/
vpaddw
%
ymm8
%
ymm0
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
197
181
213
201
/
/
vpmullw
%
ymm1
%
ymm9
%
ymm1
.
byte
196
193
117
253
200
/
/
vpaddw
%
ymm8
%
ymm1
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
197
173
213
210
/
/
vpmullw
%
ymm2
%
ymm10
%
ymm2
.
byte
196
193
109
253
208
/
/
vpaddw
%
ymm8
%
ymm2
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
197
165
213
219
/
/
vpmullw
%
ymm3
%
ymm11
%
ymm3
.
byte
196
193
101
253
216
/
/
vpaddw
%
ymm8
%
ymm3
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
10
254
/
/
or
%
dh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
54
/
/
pushq
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
35
/
/
jmpq
*
(
%
rbx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
118
254
/
/
pushq
-
0x2
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
104
254
/
/
ljmp
*
-
0x2
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
90
254
/
/
lcall
*
-
0x2
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
71
254
/
/
incl
-
0x2
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
174
254
255
255
154
/
/
ljmp
*
-
0x65000002
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
135
254
255
255
11
/
/
incl
0xbfffffe
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
247
/
/
push
%
rdi
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
227
/
/
jmpq
*
%
rbx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_565_hsw_lowp
.
globl
_sk_lerp_565_hsw_lowp
FUNCTION
(
_sk_lerp_565_hsw_lowp
)
_sk_lerp_565_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
15
/
/
and
0xf
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
14
/
/
cmp
0xe
%
al
.
byte
119
38
/
/
ja
31b74
<
_sk_lerp_565_hsw_lowp
+
0x40
>
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
51
2
0
0
/
/
lea
0x233
(
%
rip
)
%
r9
#
31d90
<
_sk_lerp_565_hsw_lowp
+
0x25c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
197
121
110
192
/
/
vmovd
%
eax
%
xmm8
.
byte
233
12
1
0
0
/
/
jmpq
31c80
<
_sk_lerp_565_hsw_lowp
+
0x14c
>
.
byte
196
65
126
111
4
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
ymm8
.
byte
233
1
1
0
0
/
/
jmpq
31c80
<
_sk_lerp_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
4
2
/
/
vpinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
53
56
192
1
/
/
vinserti128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
121
110
12
80
/
/
vmovd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
61
2
193
1
/
/
vpblendd
0x1
%
ymm9
%
ymm8
%
ymm8
.
byte
233
221
0
0
0
/
/
jmpq
31c80
<
_sk_lerp_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
12
6
/
/
vpinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
53
56
192
1
/
/
vinserti128
0x1
%
xmm8
%
ymm9
%
ymm8
.
byte
196
65
57
196
76
80
10
5
/
/
vpinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
2
193
15
/
/
vpblendd
0xf
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
57
196
76
80
8
4
/
/
vpinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
2
193
15
/
/
vpblendd
0xf
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
122
126
12
80
/
/
vmovq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
61
2
193
3
/
/
vpblendd
0x3
%
ymm9
%
ymm8
%
ymm8
.
byte
233
157
0
0
0
/
/
jmpq
31c80
<
_sk_lerp_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
20
2
/
/
vpinsrw
0x2
0x14
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
18
1
/
/
vpinsrw
0x1
0x12
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
16
0
/
/
vpinsrw
0x0
0x10
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
65
122
111
12
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
67
53
2
192
240
/
/
vpblendd
0xf0
%
ymm8
%
ymm9
%
ymm8
.
byte
235
84
/
/
jmp
31c80
<
_sk_lerp_565_hsw_lowp
+
0x14c
>
.
byte
196
65
57
239
192
/
/
vpxor
%
xmm8
%
xmm8
%
xmm8
.
byte
196
65
57
196
76
80
28
6
/
/
vpinsrw
0x6
0x1c
(
%
r8
%
rdx
2
)
%
xmm8
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
26
5
/
/
vpinsrw
0x5
0x1a
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
49
196
76
80
24
4
/
/
vpinsrw
0x4
0x18
(
%
r8
%
rdx
2
)
%
xmm9
%
xmm9
.
byte
196
67
61
56
193
1
/
/
vinserti128
0x1
%
xmm9
%
ymm8
%
ymm8
.
byte
196
65
122
126
76
80
16
/
/
vmovq
0x10
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
196
65
122
111
20
80
/
/
vmovdqu
(
%
r8
%
rdx
2
)
%
xmm10
.
byte
196
67
45
56
201
1
/
/
vinserti128
0x1
%
xmm9
%
ymm10
%
ymm9
.
byte
196
67
53
2
192
192
/
/
vpblendd
0xc0
%
ymm8
%
ymm9
%
ymm8
.
byte
196
98
125
121
13
57
177
0
0
/
/
vpbroadcastw
0xb139
(
%
rip
)
%
ymm9
#
3cdc2
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb76
>
.
byte
196
193
45
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm10
.
byte
196
65
45
219
201
/
/
vpand
%
ymm9
%
ymm10
%
ymm9
.
byte
196
193
45
113
208
5
/
/
vpsrlw
0x5
%
ymm8
%
ymm10
.
byte
196
98
125
121
29
33
177
0
0
/
/
vpbroadcastw
0xb121
(
%
rip
)
%
ymm11
#
3cdc4
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb78
>
.
byte
196
65
45
219
211
/
/
vpand
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
121
29
21
177
0
0
/
/
vpbroadcastw
0xb115
(
%
rip
)
%
ymm11
#
3cdc6
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb7a
>
.
byte
196
65
61
219
219
/
/
vpand
%
ymm11
%
ymm8
%
ymm11
.
byte
196
193
61
113
208
13
/
/
vpsrlw
0xd
%
ymm8
%
ymm8
.
byte
196
65
53
235
192
/
/
vpor
%
ymm8
%
ymm9
%
ymm8
.
byte
196
193
53
113
242
2
/
/
vpsllw
0x2
%
ymm10
%
ymm9
.
byte
196
193
45
113
210
4
/
/
vpsrlw
0x4
%
ymm10
%
ymm10
.
byte
196
65
53
235
202
/
/
vpor
%
ymm10
%
ymm9
%
ymm9
.
byte
196
193
45
113
243
3
/
/
vpsllw
0x3
%
ymm11
%
ymm10
.
byte
196
193
37
113
211
2
/
/
vpsrlw
0x2
%
ymm11
%
ymm11
.
byte
196
65
45
235
211
/
/
vpor
%
ymm11
%
ymm10
%
ymm10
.
byte
196
98
125
121
29
220
176
0
0
/
/
vpbroadcastw
0xb0dc
(
%
rip
)
%
ymm11
#
3cdc8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb7c
>
.
byte
196
65
101
239
227
/
/
vpxor
%
ymm11
%
ymm3
%
ymm12
.
byte
196
65
69
239
219
/
/
vpxor
%
ymm11
%
ymm7
%
ymm11
.
byte
196
65
37
101
220
/
/
vpcmpgtw
%
ymm12
%
ymm11
%
ymm11
.
byte
196
66
53
58
226
/
/
vpminuw
%
ymm10
%
ymm9
%
ymm12
.
byte
196
66
29
58
224
/
/
vpminuw
%
ymm8
%
ymm12
%
ymm12
.
byte
196
66
53
62
234
/
/
vpmaxuw
%
ymm10
%
ymm9
%
ymm13
.
byte
196
66
21
62
232
/
/
vpmaxuw
%
ymm8
%
ymm13
%
ymm13
.
byte
196
67
21
76
220
176
/
/
vpblendvb
%
ymm11
%
ymm12
%
ymm13
%
ymm11
.
byte
196
98
125
121
37
172
176
0
0
/
/
vpbroadcastw
0xb0ac
(
%
rip
)
%
ymm12
#
3cdca
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb7e
>
.
byte
196
65
61
239
236
/
/
vpxor
%
ymm12
%
ymm8
%
ymm13
.
byte
197
21
213
236
/
/
vpmullw
%
ymm4
%
ymm13
%
ymm13
.
byte
197
189
213
192
/
/
vpmullw
%
ymm0
%
ymm8
%
ymm0
.
byte
196
193
125
253
196
/
/
vpaddw
%
ymm12
%
ymm0
%
ymm0
.
byte
197
149
253
192
/
/
vpaddw
%
ymm0
%
ymm13
%
ymm0
.
byte
197
253
113
208
8
/
/
vpsrlw
0x8
%
ymm0
%
ymm0
.
byte
196
65
53
239
196
/
/
vpxor
%
ymm12
%
ymm9
%
ymm8
.
byte
197
61
213
197
/
/
vpmullw
%
ymm5
%
ymm8
%
ymm8
.
byte
197
181
213
201
/
/
vpmullw
%
ymm1
%
ymm9
%
ymm1
.
byte
196
193
117
253
204
/
/
vpaddw
%
ymm12
%
ymm1
%
ymm1
.
byte
197
189
253
201
/
/
vpaddw
%
ymm1
%
ymm8
%
ymm1
.
byte
197
245
113
209
8
/
/
vpsrlw
0x8
%
ymm1
%
ymm1
.
byte
196
65
45
239
196
/
/
vpxor
%
ymm12
%
ymm10
%
ymm8
.
byte
197
61
213
198
/
/
vpmullw
%
ymm6
%
ymm8
%
ymm8
.
byte
197
173
213
210
/
/
vpmullw
%
ymm2
%
ymm10
%
ymm2
.
byte
196
193
109
253
212
/
/
vpaddw
%
ymm12
%
ymm2
%
ymm2
.
byte
197
189
253
210
/
/
vpaddw
%
ymm2
%
ymm8
%
ymm2
.
byte
197
237
113
210
8
/
/
vpsrlw
0x8
%
ymm2
%
ymm2
.
byte
196
65
37
239
196
/
/
vpxor
%
ymm12
%
ymm11
%
ymm8
.
byte
197
61
213
199
/
/
vpmullw
%
ymm7
%
ymm8
%
ymm8
.
byte
197
165
213
219
/
/
vpmullw
%
ymm3
%
ymm11
%
ymm3
.
byte
196
193
101
253
220
/
/
vpaddw
%
ymm12
%
ymm3
%
ymm3
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
197
229
113
211
8
/
/
vpsrlw
0x8
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
214
/
/
(
bad
)
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
2
/
/
incl
(
%
rdx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
66
254
/
/
incl
-
0x2
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
52
254
/
/
pushq
(
%
rsi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
38
/
/
jmpq
*
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
142
254
255
255
122
/
/
decl
0x7afffffe
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
102
254
/
/
jmpq
*
-
0x2
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
83
254
/
/
callq
*
-
0x2
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
215
/
/
callq
*
%
rdi
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
175
254
255
255
156
/
/
ljmp
*
-
0x63000002
(
%
rdi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_clamp_x_1_hsw_lowp
.
globl
_sk_clamp_x_1_hsw_lowp
FUNCTION
(
_sk_clamp_x_1_hsw_lowp
)
_sk_clamp_x_1_hsw_lowp
:
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
193
116
95
200
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
24
167
0
0
/
/
vbroadcastss
0xa718
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_hsw_lowp
.
globl
_sk_repeat_x_1_hsw_lowp
FUNCTION
(
_sk_repeat_x_1_hsw_lowp
)
_sk_repeat_x_1_hsw_lowp
:
.
byte
196
99
125
8
192
1
/
/
vroundps
0x1
%
ymm0
%
ymm8
.
byte
196
99
125
8
201
1
/
/
vroundps
0x1
%
ymm1
%
ymm9
.
byte
196
193
124
92
192
/
/
vsubps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
92
201
/
/
vsubps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
193
116
95
200
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
220
166
0
0
/
/
vbroadcastss
0xa6dc
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_hsw_lowp
.
globl
_sk_mirror_x_1_hsw_lowp
FUNCTION
(
_sk_mirror_x_1_hsw_lowp
)
_sk_mirror_x_1_hsw_lowp
:
.
byte
196
98
125
24
5
221
166
0
0
/
/
vbroadcastss
0xa6dd
(
%
rip
)
%
ymm8
#
3c514
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2c8
>
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
98
125
24
13
174
166
0
0
/
/
vbroadcastss
0xa6ae
(
%
rip
)
%
ymm9
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
65
116
89
209
/
/
vmulps
%
ymm9
%
ymm1
%
ymm10
.
byte
196
65
124
89
201
/
/
vmulps
%
ymm9
%
ymm0
%
ymm9
.
byte
196
67
125
8
201
1
/
/
vroundps
0x1
%
ymm9
%
ymm9
.
byte
196
67
125
8
210
1
/
/
vroundps
0x1
%
ymm10
%
ymm10
.
byte
196
65
44
88
210
/
/
vaddps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
65
52
88
201
/
/
vaddps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
193
124
92
193
/
/
vsubps
%
ymm9
%
ymm0
%
ymm0
.
byte
196
193
116
92
202
/
/
vsubps
%
ymm10
%
ymm1
%
ymm1
.
byte
196
193
116
88
200
/
/
vaddps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
88
192
/
/
vaddps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
17
168
0
0
/
/
vbroadcastss
0xa811
(
%
rip
)
%
ymm8
#
3c698
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x44c
>
.
byte
196
193
124
84
192
/
/
vandps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
84
200
/
/
vandps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
193
116
95
200
/
/
vmaxps
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
95
192
/
/
vmaxps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
98
125
24
5
83
166
0
0
/
/
vbroadcastss
0xa653
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
196
193
124
93
192
/
/
vminps
%
ymm8
%
ymm0
%
ymm0
.
byte
196
193
116
93
200
/
/
vminps
%
ymm8
%
ymm1
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_hsw_lowp
.
globl
_sk_decal_x_hsw_lowp
FUNCTION
(
_sk_decal_x_hsw_lowp
)
_sk_decal_x_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
201
2
/
/
vcmpleps
%
ymm1
%
ymm8
%
ymm9
.
byte
196
67
125
25
202
1
/
/
vextractf128
0x1
%
ymm9
%
xmm10
.
byte
196
65
49
99
202
/
/
vpacksswb
%
xmm10
%
xmm9
%
xmm9
.
byte
197
121
111
21
74
183
0
0
/
/
vmovdqa
0xb74a
(
%
rip
)
%
xmm10
#
3d620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13d4
>
.
byte
196
66
49
0
202
/
/
vpshufb
%
xmm10
%
xmm9
%
xmm9
.
byte
197
60
194
192
2
/
/
vcmpleps
%
ymm0
%
ymm8
%
ymm8
.
byte
196
67
125
25
195
1
/
/
vextractf128
0x1
%
ymm8
%
xmm11
.
byte
196
65
57
99
195
/
/
vpacksswb
%
xmm11
%
xmm8
%
xmm8
.
byte
196
66
57
0
194
/
/
vpshufb
%
xmm10
%
xmm8
%
xmm8
.
byte
196
65
57
108
193
/
/
vpunpcklqdq
%
xmm9
%
xmm8
%
xmm8
.
byte
196
98
125
24
72
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm9
.
byte
196
65
116
194
217
1
/
/
vcmpltps
%
ymm9
%
ymm1
%
ymm11
.
byte
196
67
125
25
220
1
/
/
vextractf128
0x1
%
ymm11
%
xmm12
.
byte
196
65
33
99
220
/
/
vpacksswb
%
xmm12
%
xmm11
%
xmm11
.
byte
196
66
33
0
218
/
/
vpshufb
%
xmm10
%
xmm11
%
xmm11
.
byte
196
65
124
194
201
1
/
/
vcmpltps
%
ymm9
%
ymm0
%
ymm9
.
byte
196
67
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm12
.
byte
196
65
49
99
204
/
/
vpacksswb
%
xmm12
%
xmm9
%
xmm9
.
byte
196
66
49
0
202
/
/
vpshufb
%
xmm10
%
xmm9
%
xmm9
.
byte
196
65
49
108
203
/
/
vpunpcklqdq
%
xmm11
%
xmm9
%
xmm9
.
byte
196
65
49
219
192
/
/
vpand
%
xmm8
%
xmm9
%
xmm8
.
byte
196
66
125
48
192
/
/
vpmovzxbw
%
xmm8
%
ymm8
.
byte
196
193
61
113
240
15
/
/
vpsllw
0xf
%
ymm8
%
ymm8
.
byte
196
193
61
113
224
15
/
/
vpsraw
0xf
%
ymm8
%
ymm8
.
byte
197
126
127
0
/
/
vmovdqu
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_hsw_lowp
.
globl
_sk_decal_y_hsw_lowp
FUNCTION
(
_sk_decal_y_hsw_lowp
)
_sk_decal_y_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
60
194
203
2
/
/
vcmpleps
%
ymm3
%
ymm8
%
ymm9
.
byte
196
67
125
25
202
1
/
/
vextractf128
0x1
%
ymm9
%
xmm10
.
byte
196
65
49
99
202
/
/
vpacksswb
%
xmm10
%
xmm9
%
xmm9
.
byte
197
121
111
21
183
182
0
0
/
/
vmovdqa
0xb6b7
(
%
rip
)
%
xmm10
#
3d620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13d4
>
.
byte
196
66
49
0
202
/
/
vpshufb
%
xmm10
%
xmm9
%
xmm9
.
byte
197
60
194
194
2
/
/
vcmpleps
%
ymm2
%
ymm8
%
ymm8
.
byte
196
67
125
25
195
1
/
/
vextractf128
0x1
%
ymm8
%
xmm11
.
byte
196
65
57
99
195
/
/
vpacksswb
%
xmm11
%
xmm8
%
xmm8
.
byte
196
66
57
0
194
/
/
vpshufb
%
xmm10
%
xmm8
%
xmm8
.
byte
196
65
57
108
193
/
/
vpunpcklqdq
%
xmm9
%
xmm8
%
xmm8
.
byte
196
98
125
24
72
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm9
.
byte
196
65
100
194
217
1
/
/
vcmpltps
%
ymm9
%
ymm3
%
ymm11
.
byte
196
67
125
25
220
1
/
/
vextractf128
0x1
%
ymm11
%
xmm12
.
byte
196
65
33
99
220
/
/
vpacksswb
%
xmm12
%
xmm11
%
xmm11
.
byte
196
66
33
0
218
/
/
vpshufb
%
xmm10
%
xmm11
%
xmm11
.
byte
196
65
108
194
201
1
/
/
vcmpltps
%
ymm9
%
ymm2
%
ymm9
.
byte
196
67
125
25
204
1
/
/
vextractf128
0x1
%
ymm9
%
xmm12
.
byte
196
65
49
99
204
/
/
vpacksswb
%
xmm12
%
xmm9
%
xmm9
.
byte
196
66
49
0
202
/
/
vpshufb
%
xmm10
%
xmm9
%
xmm9
.
byte
196
65
49
108
203
/
/
vpunpcklqdq
%
xmm11
%
xmm9
%
xmm9
.
byte
196
65
49
219
192
/
/
vpand
%
xmm8
%
xmm9
%
xmm8
.
byte
196
66
125
48
192
/
/
vpmovzxbw
%
xmm8
%
ymm8
.
byte
196
193
61
113
240
15
/
/
vpsllw
0xf
%
ymm8
%
ymm8
.
byte
196
193
61
113
224
15
/
/
vpsraw
0xf
%
ymm8
%
ymm8
.
byte
197
126
127
0
/
/
vmovdqu
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_hsw_lowp
.
globl
_sk_decal_x_and_y_hsw_lowp
FUNCTION
(
_sk_decal_x_and_y_hsw_lowp
)
_sk_decal_x_and_y_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
52
194
193
2
/
/
vcmpleps
%
ymm1
%
ymm9
%
ymm8
.
byte
196
67
125
25
194
1
/
/
vextractf128
0x1
%
ymm8
%
xmm10
.
byte
196
65
57
99
210
/
/
vpacksswb
%
xmm10
%
xmm8
%
xmm10
.
byte
197
121
111
5
36
182
0
0
/
/
vmovdqa
0xb624
(
%
rip
)
%
xmm8
#
3d620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13d4
>
.
byte
196
66
41
0
208
/
/
vpshufb
%
xmm8
%
xmm10
%
xmm10
.
byte
197
52
194
216
2
/
/
vcmpleps
%
ymm0
%
ymm9
%
ymm11
.
byte
196
67
125
25
220
1
/
/
vextractf128
0x1
%
ymm11
%
xmm12
.
byte
196
65
33
99
220
/
/
vpacksswb
%
xmm12
%
xmm11
%
xmm11
.
byte
196
66
33
0
216
/
/
vpshufb
%
xmm8
%
xmm11
%
xmm11
.
byte
196
65
33
108
210
/
/
vpunpcklqdq
%
xmm10
%
xmm11
%
xmm10
.
byte
196
98
125
24
88
64
/
/
vbroadcastss
0x40
(
%
rax
)
%
ymm11
.
byte
196
65
116
194
227
1
/
/
vcmpltps
%
ymm11
%
ymm1
%
ymm12
.
byte
196
67
125
25
229
1
/
/
vextractf128
0x1
%
ymm12
%
xmm13
.
byte
196
65
25
99
229
/
/
vpacksswb
%
xmm13
%
xmm12
%
xmm12
.
byte
196
66
25
0
224
/
/
vpshufb
%
xmm8
%
xmm12
%
xmm12
.
byte
196
65
124
194
219
1
/
/
vcmpltps
%
ymm11
%
ymm0
%
ymm11
.
byte
196
67
125
25
221
1
/
/
vextractf128
0x1
%
ymm11
%
xmm13
.
byte
196
65
33
99
221
/
/
vpacksswb
%
xmm13
%
xmm11
%
xmm11
.
byte
196
66
33
0
216
/
/
vpshufb
%
xmm8
%
xmm11
%
xmm11
.
byte
196
65
33
108
220
/
/
vpunpcklqdq
%
xmm12
%
xmm11
%
xmm11
.
byte
197
52
194
227
2
/
/
vcmpleps
%
ymm3
%
ymm9
%
ymm12
.
byte
196
67
125
25
229
1
/
/
vextractf128
0x1
%
ymm12
%
xmm13
.
byte
196
65
25
99
229
/
/
vpacksswb
%
xmm13
%
xmm12
%
xmm12
.
byte
196
66
25
0
224
/
/
vpshufb
%
xmm8
%
xmm12
%
xmm12
.
byte
197
52
194
202
2
/
/
vcmpleps
%
ymm2
%
ymm9
%
ymm9
.
byte
196
67
125
25
205
1
/
/
vextractf128
0x1
%
ymm9
%
xmm13
.
byte
196
65
49
99
205
/
/
vpacksswb
%
xmm13
%
xmm9
%
xmm9
.
byte
196
66
49
0
200
/
/
vpshufb
%
xmm8
%
xmm9
%
xmm9
.
byte
196
65
49
108
204
/
/
vpunpcklqdq
%
xmm12
%
xmm9
%
xmm9
.
byte
196
65
49
219
202
/
/
vpand
%
xmm10
%
xmm9
%
xmm9
.
byte
196
65
49
219
203
/
/
vpand
%
xmm11
%
xmm9
%
xmm9
.
byte
196
98
125
24
80
68
/
/
vbroadcastss
0x44
(
%
rax
)
%
ymm10
.
byte
196
65
100
194
218
1
/
/
vcmpltps
%
ymm10
%
ymm3
%
ymm11
.
byte
196
67
125
25
220
1
/
/
vextractf128
0x1
%
ymm11
%
xmm12
.
byte
196
65
33
99
220
/
/
vpacksswb
%
xmm12
%
xmm11
%
xmm11
.
byte
196
66
33
0
216
/
/
vpshufb
%
xmm8
%
xmm11
%
xmm11
.
byte
196
65
108
194
210
1
/
/
vcmpltps
%
ymm10
%
ymm2
%
ymm10
.
byte
196
67
125
25
212
1
/
/
vextractf128
0x1
%
ymm10
%
xmm12
.
byte
196
65
41
99
212
/
/
vpacksswb
%
xmm12
%
xmm10
%
xmm10
.
byte
196
66
41
0
192
/
/
vpshufb
%
xmm8
%
xmm10
%
xmm8
.
byte
196
65
57
108
195
/
/
vpunpcklqdq
%
xmm11
%
xmm8
%
xmm8
.
byte
196
65
49
219
192
/
/
vpand
%
xmm8
%
xmm9
%
xmm8
.
byte
196
66
125
48
192
/
/
vpmovzxbw
%
xmm8
%
ymm8
.
byte
196
193
61
113
240
15
/
/
vpsllw
0xf
%
ymm8
%
ymm8
.
byte
196
193
61
113
224
15
/
/
vpsraw
0xf
%
ymm8
%
ymm8
.
byte
197
126
127
0
/
/
vmovdqu
%
ymm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_hsw_lowp
.
globl
_sk_check_decal_mask_hsw_lowp
FUNCTION
(
_sk_check_decal_mask_hsw_lowp
)
_sk_check_decal_mask_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
124
16
0
/
/
vmovups
(
%
rax
)
%
ymm8
.
byte
197
188
84
192
/
/
vandps
%
ymm0
%
ymm8
%
ymm0
.
byte
197
188
84
201
/
/
vandps
%
ymm1
%
ymm8
%
ymm1
.
byte
197
188
84
210
/
/
vandps
%
ymm2
%
ymm8
%
ymm2
.
byte
197
188
84
219
/
/
vandps
%
ymm3
%
ymm8
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_hsw_lowp
.
globl
_sk_gradient_hsw_lowp
FUNCTION
(
_sk_gradient_hsw_lowp
)
_sk_gradient_hsw_lowp
:
.
byte
72
129
236
152
0
0
0
/
/
sub
0x98
%
rsp
.
byte
197
254
127
124
36
96
/
/
vmovdqu
%
ymm7
0x60
(
%
rsp
)
.
byte
197
252
17
116
36
64
/
/
vmovups
%
ymm6
0x40
(
%
rsp
)
.
byte
197
254
127
108
36
32
/
/
vmovdqu
%
ymm5
0x20
(
%
rsp
)
.
byte
197
254
127
36
36
/
/
vmovdqu
%
ymm4
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
73
131
248
2
/
/
cmp
0x2
%
r8
.
byte
114
60
/
/
jb
32167
<
_sk_gradient_hsw_lowp
+
0x6d
>
.
byte
76
139
72
72
/
/
mov
0x48
(
%
rax
)
%
r9
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
65
186
1
0
0
0
/
/
mov
0x1
%
r10d
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
130
125
24
20
145
/
/
vbroadcastss
(
%
r9
%
r10
4
)
%
ymm2
.
byte
197
236
194
216
2
/
/
vcmpleps
%
ymm0
%
ymm2
%
ymm3
.
byte
197
197
250
251
/
/
vpsubd
%
ymm3
%
ymm7
%
ymm7
.
byte
197
236
194
209
2
/
/
vcmpleps
%
ymm1
%
ymm2
%
ymm2
.
byte
197
213
250
234
/
/
vpsubd
%
ymm2
%
ymm5
%
ymm5
.
byte
73
255
194
/
/
inc
%
r10
.
byte
77
57
208
/
/
cmp
%
r10
%
r8
.
byte
117
224
/
/
jne
3213d
<
_sk_gradient_hsw_lowp
+
0x43
>
.
byte
73
131
248
8
/
/
cmp
0x8
%
r8
.
byte
15
135
180
0
0
0
/
/
ja
3221b
<
_sk_gradient_hsw_lowp
+
0x121
>
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
196
193
124
16
24
/
/
vmovups
(
%
r8
)
%
ymm3
.
byte
196
226
69
22
211
/
/
vpermps
%
ymm3
%
ymm7
%
ymm2
.
byte
196
226
85
22
219
/
/
vpermps
%
ymm3
%
ymm5
%
ymm3
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
193
124
16
32
/
/
vmovups
(
%
r8
)
%
ymm4
.
byte
196
98
69
22
244
/
/
vpermps
%
ymm4
%
ymm7
%
ymm14
.
byte
196
226
85
22
228
/
/
vpermps
%
ymm4
%
ymm5
%
ymm4
.
byte
197
252
17
100
36
224
/
/
vmovups
%
ymm4
-
0x20
(
%
rsp
)
.
byte
196
193
124
16
33
/
/
vmovups
(
%
r9
)
%
ymm4
.
byte
196
98
69
22
196
/
/
vpermps
%
ymm4
%
ymm7
%
ymm8
.
byte
196
98
85
22
204
/
/
vpermps
%
ymm4
%
ymm5
%
ymm9
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
196
193
124
16
32
/
/
vmovups
(
%
r8
)
%
ymm4
.
byte
196
98
69
22
236
/
/
vpermps
%
ymm4
%
ymm7
%
ymm13
.
byte
196
226
85
22
228
/
/
vpermps
%
ymm4
%
ymm5
%
ymm4
.
byte
197
252
17
100
36
128
/
/
vmovups
%
ymm4
-
0x80
(
%
rsp
)
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
196
193
124
16
32
/
/
vmovups
(
%
r8
)
%
ymm4
.
byte
196
98
69
22
212
/
/
vpermps
%
ymm4
%
ymm7
%
ymm10
.
byte
196
98
85
22
220
/
/
vpermps
%
ymm4
%
ymm5
%
ymm11
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
196
193
124
16
48
/
/
vmovups
(
%
r8
)
%
ymm6
.
byte
196
226
69
22
230
/
/
vpermps
%
ymm6
%
ymm7
%
ymm4
.
byte
196
226
85
22
246
/
/
vpermps
%
ymm6
%
ymm5
%
ymm6
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
65
124
16
32
/
/
vmovups
(
%
r8
)
%
ymm12
.
byte
196
66
69
22
252
/
/
vpermps
%
ymm12
%
ymm7
%
ymm15
.
byte
197
124
17
124
36
192
/
/
vmovups
%
ymm15
-
0x40
(
%
rsp
)
.
byte
196
66
85
22
228
/
/
vpermps
%
ymm12
%
ymm5
%
ymm12
.
byte
197
124
17
100
36
160
/
/
vmovups
%
ymm12
-
0x60
(
%
rsp
)
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
197
124
16
56
/
/
vmovups
(
%
rax
)
%
ymm15
.
byte
196
66
69
22
231
/
/
vpermps
%
ymm15
%
ymm7
%
ymm12
.
byte
196
194
85
22
255
/
/
vpermps
%
ymm15
%
ymm5
%
ymm7
.
byte
233
34
1
0
0
/
/
jmpq
3233d
<
_sk_gradient_hsw_lowp
+
0x243
>
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
196
194
101
146
20
184
/
/
vgatherdps
%
ymm3
(
%
r8
%
ymm7
4
)
%
ymm2
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
196
194
93
146
28
168
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm5
4
)
%
ymm3
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
196
66
93
146
4
185
/
/
vgatherdps
%
ymm4
(
%
r9
%
ymm7
4
)
%
ymm8
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
66
93
146
12
169
/
/
vgatherdps
%
ymm4
(
%
r9
%
ymm5
4
)
%
ymm9
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
196
66
93
146
20
184
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm7
4
)
%
ymm10
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
196
66
93
146
28
168
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm5
4
)
%
ymm11
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
196
194
93
146
52
184
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm7
4
)
%
ymm6
.
byte
197
252
17
116
36
192
/
/
vmovups
%
ymm6
-
0x40
(
%
rsp
)
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
196
194
93
146
52
168
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm5
4
)
%
ymm6
.
byte
197
252
17
116
36
160
/
/
vmovups
%
ymm6
-
0x60
(
%
rsp
)
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
12
87
246
/
/
vxorps
%
ymm14
%
ymm14
%
ymm14
.
byte
196
66
93
146
52
184
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm7
4
)
%
ymm14
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
196
194
93
146
52
168
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm5
4
)
%
ymm6
.
byte
197
252
17
116
36
224
/
/
vmovups
%
ymm6
-
0x20
(
%
rsp
)
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
196
66
93
146
44
184
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm7
4
)
%
ymm13
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
196
194
93
146
52
168
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm5
4
)
%
ymm6
.
byte
197
252
17
116
36
128
/
/
vmovups
%
ymm6
-
0x80
(
%
rsp
)
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
197
205
118
246
/
/
vpcmpeqd
%
ymm6
%
ymm6
%
ymm6
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
196
194
77
146
36
184
/
/
vgatherdps
%
ymm6
(
%
r8
%
ymm7
4
)
%
ymm4
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
196
194
29
146
52
168
/
/
vgatherdps
%
ymm12
(
%
r8
%
ymm5
4
)
%
ymm6
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
196
98
5
146
36
184
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm7
4
)
%
ymm12
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
196
226
5
146
60
168
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm5
4
)
%
ymm7
.
byte
196
194
125
168
214
/
/
vfmadd213ps
%
ymm14
%
ymm0
%
ymm2
.
byte
196
66
125
168
197
/
/
vfmadd213ps
%
ymm13
%
ymm0
%
ymm8
.
byte
196
98
125
168
212
/
/
vfmadd213ps
%
ymm4
%
ymm0
%
ymm10
.
byte
196
98
125
184
100
36
192
/
/
vfmadd231ps
-
0x40
(
%
rsp
)
%
ymm0
%
ymm12
.
byte
196
226
117
168
92
36
224
/
/
vfmadd213ps
-
0x20
(
%
rsp
)
%
ymm1
%
ymm3
.
byte
196
98
117
168
76
36
128
/
/
vfmadd213ps
-
0x80
(
%
rsp
)
%
ymm1
%
ymm9
.
byte
196
98
117
168
222
/
/
vfmadd213ps
%
ymm6
%
ymm1
%
ymm11
.
byte
196
226
117
184
124
36
160
/
/
vfmadd231ps
-
0x60
(
%
rsp
)
%
ymm1
%
ymm7
.
byte
196
226
125
24
5
178
161
0
0
/
/
vbroadcastss
0xa1b2
(
%
rip
)
%
ymm0
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
226
125
24
13
121
161
0
0
/
/
vbroadcastss
0xa179
(
%
rip
)
%
ymm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
226
125
168
217
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm3
.
byte
196
226
125
168
209
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm2
.
byte
196
98
125
168
201
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm9
.
byte
196
98
125
168
193
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm8
.
byte
196
98
125
168
217
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm11
.
byte
196
98
125
168
209
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm10
.
byte
196
226
125
168
249
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm7
.
byte
196
98
125
168
225
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm12
.
byte
197
254
91
194
/
/
vcvttps2dq
%
ymm2
%
ymm0
.
byte
197
253
111
37
45
170
0
0
/
/
vmovdqa
0xaa2d
(
%
rip
)
%
ymm4
#
3cde0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb94
>
.
byte
196
226
125
0
196
/
/
vpshufb
%
ymm4
%
ymm0
%
ymm0
.
byte
196
227
253
0
192
232
/
/
vpermq
0xe8
%
ymm0
%
ymm0
.
byte
197
254
91
203
/
/
vcvttps2dq
%
ymm3
%
ymm1
.
byte
196
226
117
0
204
/
/
vpshufb
%
ymm4
%
ymm1
%
ymm1
.
byte
196
227
253
0
201
232
/
/
vpermq
0xe8
%
ymm1
%
ymm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
126
91
200
/
/
vcvttps2dq
%
ymm8
%
ymm1
.
byte
196
226
117
0
204
/
/
vpshufb
%
ymm4
%
ymm1
%
ymm1
.
byte
196
227
253
0
201
232
/
/
vpermq
0xe8
%
ymm1
%
ymm1
.
byte
196
193
126
91
209
/
/
vcvttps2dq
%
ymm9
%
ymm2
.
byte
196
226
109
0
212
/
/
vpshufb
%
ymm4
%
ymm2
%
ymm2
.
byte
196
227
253
0
210
232
/
/
vpermq
0xe8
%
ymm2
%
ymm2
.
byte
196
227
117
56
202
1
/
/
vinserti128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
196
193
126
91
210
/
/
vcvttps2dq
%
ymm10
%
ymm2
.
byte
196
226
109
0
212
/
/
vpshufb
%
ymm4
%
ymm2
%
ymm2
.
byte
196
227
253
0
210
232
/
/
vpermq
0xe8
%
ymm2
%
ymm2
.
byte
196
193
126
91
219
/
/
vcvttps2dq
%
ymm11
%
ymm3
.
byte
196
226
101
0
220
/
/
vpshufb
%
ymm4
%
ymm3
%
ymm3
.
byte
196
227
253
0
219
232
/
/
vpermq
0xe8
%
ymm3
%
ymm3
.
byte
196
227
109
56
211
1
/
/
vinserti128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
193
126
91
220
/
/
vcvttps2dq
%
ymm12
%
ymm3
.
byte
196
226
101
0
220
/
/
vpshufb
%
ymm4
%
ymm3
%
ymm3
.
byte
197
254
91
239
/
/
vcvttps2dq
%
ymm7
%
ymm5
.
byte
196
226
85
0
228
/
/
vpshufb
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
253
0
219
232
/
/
vpermq
0xe8
%
ymm3
%
ymm3
.
byte
196
227
253
0
228
232
/
/
vpermq
0xe8
%
ymm4
%
ymm4
.
byte
196
227
101
56
220
1
/
/
vinserti128
0x1
%
xmm4
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
36
36
/
/
vmovups
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm7
.
byte
72
129
196
152
0
0
0
/
/
add
0x98
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_hsw_lowp
.
globl
_sk_evenly_spaced_gradient_hsw_lowp
FUNCTION
(
_sk_evenly_spaced_gradient_hsw_lowp
)
_sk_evenly_spaced_gradient_hsw_lowp
:
.
byte
72
129
236
152
0
0
0
/
/
sub
0x98
%
rsp
.
byte
197
252
17
124
36
96
/
/
vmovups
%
ymm7
0x60
(
%
rsp
)
.
byte
197
252
17
116
36
64
/
/
vmovups
%
ymm6
0x40
(
%
rsp
)
.
byte
197
252
17
108
36
32
/
/
vmovups
%
ymm5
0x20
(
%
rsp
)
.
byte
197
254
127
36
36
/
/
vmovdqu
%
ymm4
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
77
137
193
/
/
mov
%
r8
%
r9
.
byte
73
255
201
/
/
dec
%
r9
.
byte
120
7
/
/
js
32498
<
_sk_evenly_spaced_gradient_hsw_lowp
+
0x32
>
.
byte
196
193
234
42
209
/
/
vcvtsi2ss
%
r9
%
xmm2
%
xmm2
.
byte
235
22
/
/
jmp
324ae
<
_sk_evenly_spaced_gradient_hsw_lowp
+
0x48
>
.
byte
77
137
202
/
/
mov
%
r9
%
r10
.
byte
73
209
234
/
/
shr
%
r10
.
byte
65
131
225
1
/
/
and
0x1
%
r9d
.
byte
77
9
209
/
/
or
%
r10
%
r9
.
byte
196
193
234
42
209
/
/
vcvtsi2ss
%
r9
%
xmm2
%
xmm2
.
byte
197
234
88
210
/
/
vaddss
%
xmm2
%
xmm2
%
xmm2
.
byte
196
226
125
24
210
/
/
vbroadcastss
%
xmm2
%
ymm2
.
byte
197
252
89
218
/
/
vmulps
%
ymm2
%
ymm0
%
ymm3
.
byte
197
244
89
210
/
/
vmulps
%
ymm2
%
ymm1
%
ymm2
.
byte
197
126
91
242
/
/
vcvttps2dq
%
ymm2
%
ymm14
.
byte
197
254
91
251
/
/
vcvttps2dq
%
ymm3
%
ymm7
.
byte
73
131
248
8
/
/
cmp
0x8
%
r8
.
byte
15
135
180
0
0
0
/
/
ja
32581
<
_sk_evenly_spaced_gradient_hsw_lowp
+
0x11b
>
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
196
193
124
16
24
/
/
vmovups
(
%
r8
)
%
ymm3
.
byte
196
226
69
22
211
/
/
vpermps
%
ymm3
%
ymm7
%
ymm2
.
byte
196
226
13
22
219
/
/
vpermps
%
ymm3
%
ymm14
%
ymm3
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
193
124
16
32
/
/
vmovups
(
%
r8
)
%
ymm4
.
byte
196
98
69
22
236
/
/
vpermps
%
ymm4
%
ymm7
%
ymm13
.
byte
196
226
13
22
228
/
/
vpermps
%
ymm4
%
ymm14
%
ymm4
.
byte
197
252
17
100
36
224
/
/
vmovups
%
ymm4
-
0x20
(
%
rsp
)
.
byte
196
193
124
16
33
/
/
vmovups
(
%
r9
)
%
ymm4
.
byte
196
98
69
22
196
/
/
vpermps
%
ymm4
%
ymm7
%
ymm8
.
byte
196
98
13
22
204
/
/
vpermps
%
ymm4
%
ymm14
%
ymm9
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
196
193
124
16
40
/
/
vmovups
(
%
r8
)
%
ymm5
.
byte
196
226
69
22
229
/
/
vpermps
%
ymm5
%
ymm7
%
ymm4
.
byte
196
226
13
22
237
/
/
vpermps
%
ymm5
%
ymm14
%
ymm5
.
byte
197
252
17
108
36
128
/
/
vmovups
%
ymm5
-
0x80
(
%
rsp
)
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
196
193
124
16
40
/
/
vmovups
(
%
r8
)
%
ymm5
.
byte
196
98
69
22
213
/
/
vpermps
%
ymm5
%
ymm7
%
ymm10
.
byte
196
98
13
22
221
/
/
vpermps
%
ymm5
%
ymm14
%
ymm11
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
196
193
124
16
48
/
/
vmovups
(
%
r8
)
%
ymm6
.
byte
196
226
69
22
238
/
/
vpermps
%
ymm6
%
ymm7
%
ymm5
.
byte
196
226
13
22
246
/
/
vpermps
%
ymm6
%
ymm14
%
ymm6
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
196
65
124
16
32
/
/
vmovups
(
%
r8
)
%
ymm12
.
byte
196
66
69
22
252
/
/
vpermps
%
ymm12
%
ymm7
%
ymm15
.
byte
197
124
17
124
36
192
/
/
vmovups
%
ymm15
-
0x40
(
%
rsp
)
.
byte
196
66
13
22
228
/
/
vpermps
%
ymm12
%
ymm14
%
ymm12
.
byte
197
124
17
100
36
160
/
/
vmovups
%
ymm12
-
0x60
(
%
rsp
)
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
197
124
16
56
/
/
vmovups
(
%
rax
)
%
ymm15
.
byte
196
66
69
22
231
/
/
vpermps
%
ymm15
%
ymm7
%
ymm12
.
byte
196
194
13
22
255
/
/
vpermps
%
ymm15
%
ymm14
%
ymm7
.
byte
233
33
1
0
0
/
/
jmpq
326a2
<
_sk_evenly_spaced_gradient_hsw_lowp
+
0x23c
>
.
byte
76
139
64
8
/
/
mov
0x8
(
%
rax
)
%
r8
.
byte
76
139
72
16
/
/
mov
0x10
(
%
rax
)
%
r9
.
byte
197
236
87
210
/
/
vxorps
%
ymm2
%
ymm2
%
ymm2
.
byte
197
229
118
219
/
/
vpcmpeqd
%
ymm3
%
ymm3
%
ymm3
.
byte
196
194
101
146
20
184
/
/
vgatherdps
%
ymm3
(
%
r8
%
ymm7
4
)
%
ymm2
.
byte
197
228
87
219
/
/
vxorps
%
ymm3
%
ymm3
%
ymm3
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
130
93
146
28
176
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm14
4
)
%
ymm3
.
byte
196
65
60
87
192
/
/
vxorps
%
ymm8
%
ymm8
%
ymm8
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
66
93
146
4
185
/
/
vgatherdps
%
ymm4
(
%
r9
%
ymm7
4
)
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
2
93
146
12
177
/
/
vgatherdps
%
ymm4
(
%
r9
%
ymm14
4
)
%
ymm9
.
byte
76
139
64
24
/
/
mov
0x18
(
%
rax
)
%
r8
.
byte
196
65
44
87
210
/
/
vxorps
%
ymm10
%
ymm10
%
ymm10
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
66
93
146
20
184
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm7
4
)
%
ymm10
.
byte
196
65
36
87
219
/
/
vxorps
%
ymm11
%
ymm11
%
ymm11
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
2
93
146
28
176
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm14
4
)
%
ymm11
.
byte
76
139
64
32
/
/
mov
0x20
(
%
rax
)
%
r8
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
194
93
146
44
184
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm7
4
)
%
ymm5
.
byte
197
252
17
108
36
192
/
/
vmovups
%
ymm5
-
0x40
(
%
rsp
)
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
130
93
146
44
176
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm14
4
)
%
ymm5
.
byte
197
252
17
108
36
160
/
/
vmovups
%
ymm5
-
0x60
(
%
rsp
)
.
byte
76
139
64
40
/
/
mov
0x28
(
%
rax
)
%
r8
.
byte
196
65
20
87
237
/
/
vxorps
%
ymm13
%
ymm13
%
ymm13
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
66
93
146
44
184
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm7
4
)
%
ymm13
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
221
118
228
/
/
vpcmpeqd
%
ymm4
%
ymm4
%
ymm4
.
byte
196
130
93
146
44
176
/
/
vgatherdps
%
ymm4
(
%
r8
%
ymm14
4
)
%
ymm5
.
byte
197
252
17
108
36
224
/
/
vmovups
%
ymm5
-
0x20
(
%
rsp
)
.
byte
76
139
64
48
/
/
mov
0x30
(
%
rax
)
%
r8
.
byte
197
220
87
228
/
/
vxorps
%
ymm4
%
ymm4
%
ymm4
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
194
85
146
36
184
/
/
vgatherdps
%
ymm5
(
%
r8
%
ymm7
4
)
%
ymm4
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
197
213
118
237
/
/
vpcmpeqd
%
ymm5
%
ymm5
%
ymm5
.
byte
196
130
85
146
52
176
/
/
vgatherdps
%
ymm5
(
%
r8
%
ymm14
4
)
%
ymm6
.
byte
197
252
17
116
36
128
/
/
vmovups
%
ymm6
-
0x80
(
%
rsp
)
.
byte
76
139
64
56
/
/
mov
0x38
(
%
rax
)
%
r8
.
byte
197
212
87
237
/
/
vxorps
%
ymm5
%
ymm5
%
ymm5
.
byte
197
205
118
246
/
/
vpcmpeqd
%
ymm6
%
ymm6
%
ymm6
.
byte
196
194
77
146
44
184
/
/
vgatherdps
%
ymm6
(
%
r8
%
ymm7
4
)
%
ymm5
.
byte
197
204
87
246
/
/
vxorps
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
29
118
228
/
/
vpcmpeqd
%
ymm12
%
ymm12
%
ymm12
.
byte
196
130
29
146
52
176
/
/
vgatherdps
%
ymm12
(
%
r8
%
ymm14
4
)
%
ymm6
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
196
65
28
87
228
/
/
vxorps
%
ymm12
%
ymm12
%
ymm12
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
196
98
5
146
36
184
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm7
4
)
%
ymm12
.
byte
196
65
5
118
255
/
/
vpcmpeqd
%
ymm15
%
ymm15
%
ymm15
.
byte
197
196
87
255
/
/
vxorps
%
ymm7
%
ymm7
%
ymm7
.
byte
196
162
5
146
60
176
/
/
vgatherdps
%
ymm15
(
%
rax
%
ymm14
4
)
%
ymm7
.
byte
196
194
125
168
213
/
/
vfmadd213ps
%
ymm13
%
ymm0
%
ymm2
.
byte
196
98
125
168
196
/
/
vfmadd213ps
%
ymm4
%
ymm0
%
ymm8
.
byte
196
98
125
168
213
/
/
vfmadd213ps
%
ymm5
%
ymm0
%
ymm10
.
byte
196
98
125
184
100
36
192
/
/
vfmadd231ps
-
0x40
(
%
rsp
)
%
ymm0
%
ymm12
.
byte
196
226
117
168
92
36
224
/
/
vfmadd213ps
-
0x20
(
%
rsp
)
%
ymm1
%
ymm3
.
byte
196
98
117
168
76
36
128
/
/
vfmadd213ps
-
0x80
(
%
rsp
)
%
ymm1
%
ymm9
.
byte
196
98
117
168
222
/
/
vfmadd213ps
%
ymm6
%
ymm1
%
ymm11
.
byte
196
226
117
184
124
36
160
/
/
vfmadd231ps
-
0x60
(
%
rsp
)
%
ymm1
%
ymm7
.
byte
196
226
125
24
5
77
158
0
0
/
/
vbroadcastss
0x9e4d
(
%
rip
)
%
ymm0
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
226
125
24
13
20
158
0
0
/
/
vbroadcastss
0x9e14
(
%
rip
)
%
ymm1
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
226
125
168
217
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm3
.
byte
196
226
125
168
209
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm2
.
byte
196
98
125
168
201
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm9
.
byte
196
98
125
168
193
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm8
.
byte
196
98
125
168
217
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm11
.
byte
196
98
125
168
209
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm10
.
byte
196
226
125
168
249
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm7
.
byte
196
98
125
168
225
/
/
vfmadd213ps
%
ymm1
%
ymm0
%
ymm12
.
byte
197
254
91
194
/
/
vcvttps2dq
%
ymm2
%
ymm0
.
byte
197
253
111
37
232
166
0
0
/
/
vmovdqa
0xa6e8
(
%
rip
)
%
ymm4
#
3ce00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xbb4
>
.
byte
196
226
125
0
196
/
/
vpshufb
%
ymm4
%
ymm0
%
ymm0
.
byte
196
227
253
0
192
232
/
/
vpermq
0xe8
%
ymm0
%
ymm0
.
byte
197
254
91
203
/
/
vcvttps2dq
%
ymm3
%
ymm1
.
byte
196
226
117
0
204
/
/
vpshufb
%
ymm4
%
ymm1
%
ymm1
.
byte
196
227
253
0
201
232
/
/
vpermq
0xe8
%
ymm1
%
ymm1
.
byte
196
227
125
56
193
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm0
.
byte
196
193
126
91
200
/
/
vcvttps2dq
%
ymm8
%
ymm1
.
byte
196
226
117
0
204
/
/
vpshufb
%
ymm4
%
ymm1
%
ymm1
.
byte
196
227
253
0
201
232
/
/
vpermq
0xe8
%
ymm1
%
ymm1
.
byte
196
193
126
91
209
/
/
vcvttps2dq
%
ymm9
%
ymm2
.
byte
196
226
109
0
212
/
/
vpshufb
%
ymm4
%
ymm2
%
ymm2
.
byte
196
227
253
0
210
232
/
/
vpermq
0xe8
%
ymm2
%
ymm2
.
byte
196
227
117
56
202
1
/
/
vinserti128
0x1
%
xmm2
%
ymm1
%
ymm1
.
byte
196
193
126
91
210
/
/
vcvttps2dq
%
ymm10
%
ymm2
.
byte
196
226
109
0
212
/
/
vpshufb
%
ymm4
%
ymm2
%
ymm2
.
byte
196
227
253
0
210
232
/
/
vpermq
0xe8
%
ymm2
%
ymm2
.
byte
196
193
126
91
219
/
/
vcvttps2dq
%
ymm11
%
ymm3
.
byte
196
226
101
0
220
/
/
vpshufb
%
ymm4
%
ymm3
%
ymm3
.
byte
196
227
253
0
219
232
/
/
vpermq
0xe8
%
ymm3
%
ymm3
.
byte
196
227
109
56
211
1
/
/
vinserti128
0x1
%
xmm3
%
ymm2
%
ymm2
.
byte
196
193
126
91
220
/
/
vcvttps2dq
%
ymm12
%
ymm3
.
byte
196
226
101
0
220
/
/
vpshufb
%
ymm4
%
ymm3
%
ymm3
.
byte
197
254
91
239
/
/
vcvttps2dq
%
ymm7
%
ymm5
.
byte
196
226
85
0
228
/
/
vpshufb
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
253
0
219
232
/
/
vpermq
0xe8
%
ymm3
%
ymm3
.
byte
196
227
253
0
228
232
/
/
vpermq
0xe8
%
ymm4
%
ymm4
.
byte
196
227
101
56
220
1
/
/
vinserti128
0x1
%
xmm4
%
ymm3
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
16
36
36
/
/
vmovups
(
%
rsp
)
%
ymm4
.
byte
197
252
16
108
36
32
/
/
vmovups
0x20
(
%
rsp
)
%
ymm5
.
byte
197
252
16
116
36
64
/
/
vmovups
0x40
(
%
rsp
)
%
ymm6
.
byte
197
252
16
124
36
96
/
/
vmovups
0x60
(
%
rsp
)
%
ymm7
.
byte
72
129
196
152
0
0
0
/
/
add
0x98
%
rsp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_hsw_lowp
.
globl
_sk_evenly_spaced_2_stop_gradient_hsw_lowp
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_hsw_lowp
)
_sk_evenly_spaced_2_stop_gradient_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
196
226
125
24
16
/
/
vbroadcastss
(
%
rax
)
%
ymm2
.
byte
196
226
125
24
88
16
/
/
vbroadcastss
0x10
(
%
rax
)
%
ymm3
.
byte
197
124
40
194
/
/
vmovaps
%
ymm2
%
ymm8
.
byte
196
98
125
168
195
/
/
vfmadd213ps
%
ymm3
%
ymm0
%
ymm8
.
byte
196
226
117
168
211
/
/
vfmadd213ps
%
ymm3
%
ymm1
%
ymm2
.
byte
196
226
125
24
29
57
157
0
0
/
/
vbroadcastss
0x9d39
(
%
rip
)
%
ymm3
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
196
98
125
24
29
0
157
0
0
/
/
vbroadcastss
0x9d00
(
%
rip
)
%
ymm11
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
196
194
101
168
211
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm2
.
byte
196
66
101
168
195
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm8
.
byte
196
65
126
91
192
/
/
vcvttps2dq
%
ymm8
%
ymm8
.
byte
197
125
111
21
17
166
0
0
/
/
vmovdqa
0xa611
(
%
rip
)
%
ymm10
#
3ce20
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xbd4
>
.
byte
196
66
61
0
194
/
/
vpshufb
%
ymm10
%
ymm8
%
ymm8
.
byte
196
67
253
0
192
232
/
/
vpermq
0xe8
%
ymm8
%
ymm8
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
194
109
0
210
/
/
vpshufb
%
ymm10
%
ymm2
%
ymm2
.
byte
196
227
253
0
210
232
/
/
vpermq
0xe8
%
ymm2
%
ymm2
.
byte
196
99
61
56
194
1
/
/
vinserti128
0x1
%
xmm2
%
ymm8
%
ymm8
.
byte
196
226
125
24
80
4
/
/
vbroadcastss
0x4
(
%
rax
)
%
ymm2
.
byte
196
98
125
24
72
20
/
/
vbroadcastss
0x14
(
%
rax
)
%
ymm9
.
byte
197
124
40
226
/
/
vmovaps
%
ymm2
%
ymm12
.
byte
196
66
125
168
225
/
/
vfmadd213ps
%
ymm9
%
ymm0
%
ymm12
.
byte
196
194
117
168
209
/
/
vfmadd213ps
%
ymm9
%
ymm1
%
ymm2
.
byte
196
194
101
168
211
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm2
.
byte
196
66
101
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm12
.
byte
196
65
126
91
204
/
/
vcvttps2dq
%
ymm12
%
ymm9
.
byte
196
66
53
0
202
/
/
vpshufb
%
ymm10
%
ymm9
%
ymm9
.
byte
196
67
253
0
201
232
/
/
vpermq
0xe8
%
ymm9
%
ymm9
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
194
109
0
210
/
/
vpshufb
%
ymm10
%
ymm2
%
ymm2
.
byte
196
227
253
0
210
232
/
/
vpermq
0xe8
%
ymm2
%
ymm2
.
byte
196
99
53
56
202
1
/
/
vinserti128
0x1
%
xmm2
%
ymm9
%
ymm9
.
byte
196
226
125
24
80
8
/
/
vbroadcastss
0x8
(
%
rax
)
%
ymm2
.
byte
196
98
125
24
96
24
/
/
vbroadcastss
0x18
(
%
rax
)
%
ymm12
.
byte
197
124
40
234
/
/
vmovaps
%
ymm2
%
ymm13
.
byte
196
66
125
168
236
/
/
vfmadd213ps
%
ymm12
%
ymm0
%
ymm13
.
byte
196
194
117
168
212
/
/
vfmadd213ps
%
ymm12
%
ymm1
%
ymm2
.
byte
196
194
101
168
211
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm2
.
byte
196
66
101
168
235
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm13
.
byte
196
65
126
91
229
/
/
vcvttps2dq
%
ymm13
%
ymm12
.
byte
196
66
29
0
226
/
/
vpshufb
%
ymm10
%
ymm12
%
ymm12
.
byte
196
67
253
0
228
232
/
/
vpermq
0xe8
%
ymm12
%
ymm12
.
byte
197
254
91
210
/
/
vcvttps2dq
%
ymm2
%
ymm2
.
byte
196
194
109
0
210
/
/
vpshufb
%
ymm10
%
ymm2
%
ymm2
.
byte
196
227
253
0
210
232
/
/
vpermq
0xe8
%
ymm2
%
ymm2
.
byte
196
227
29
56
210
1
/
/
vinserti128
0x1
%
xmm2
%
ymm12
%
ymm2
.
byte
196
98
125
24
96
12
/
/
vbroadcastss
0xc
(
%
rax
)
%
ymm12
.
byte
196
98
125
24
104
28
/
/
vbroadcastss
0x1c
(
%
rax
)
%
ymm13
.
byte
196
194
29
168
197
/
/
vfmadd213ps
%
ymm13
%
ymm12
%
ymm0
.
byte
196
66
117
168
229
/
/
vfmadd213ps
%
ymm13
%
ymm1
%
ymm12
.
byte
196
66
101
168
227
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm12
.
byte
196
194
101
168
195
/
/
vfmadd213ps
%
ymm11
%
ymm3
%
ymm0
.
byte
197
254
91
192
/
/
vcvttps2dq
%
ymm0
%
ymm0
.
byte
196
194
125
0
194
/
/
vpshufb
%
ymm10
%
ymm0
%
ymm0
.
byte
196
227
253
0
192
232
/
/
vpermq
0xe8
%
ymm0
%
ymm0
.
byte
196
193
126
91
204
/
/
vcvttps2dq
%
ymm12
%
ymm1
.
byte
196
194
117
0
202
/
/
vpshufb
%
ymm10
%
ymm1
%
ymm1
.
byte
196
227
253
0
201
232
/
/
vpermq
0xe8
%
ymm1
%
ymm1
.
byte
196
227
125
56
217
1
/
/
vinserti128
0x1
%
xmm1
%
ymm0
%
ymm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
125
127
192
/
/
vmovdqa
%
ymm8
%
ymm0
.
byte
197
125
127
201
/
/
vmovdqa
%
ymm9
%
ymm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_hsw_lowp
.
globl
_sk_xy_to_unit_angle_hsw_lowp
FUNCTION
(
_sk_xy_to_unit_angle_hsw_lowp
)
_sk_xy_to_unit_angle_hsw_lowp
:
.
byte
197
252
17
124
36
200
/
/
vmovups
%
ymm7
-
0x38
(
%
rsp
)
.
byte
197
252
40
254
/
/
vmovaps
%
ymm6
%
ymm7
.
byte
197
252
40
245
/
/
vmovaps
%
ymm5
%
ymm6
.
byte
197
252
40
236
/
/
vmovaps
%
ymm4
%
ymm5
.
byte
197
252
40
227
/
/
vmovaps
%
ymm3
%
ymm4
.
byte
197
252
40
217
/
/
vmovaps
%
ymm1
%
ymm3
.
byte
197
252
40
200
/
/
vmovaps
%
ymm0
%
ymm1
.
byte
196
98
125
24
5
95
157
0
0
/
/
vbroadcastss
0x9d5f
(
%
rip
)
%
ymm8
#
3c698
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x44c
>
.
byte
196
65
116
84
200
/
/
vandps
%
ymm8
%
ymm1
%
ymm9
.
byte
196
65
100
84
208
/
/
vandps
%
ymm8
%
ymm3
%
ymm10
.
byte
196
65
108
84
216
/
/
vandps
%
ymm8
%
ymm2
%
ymm11
.
byte
196
65
92
84
192
/
/
vandps
%
ymm8
%
ymm4
%
ymm8
.
byte
196
65
44
194
224
1
/
/
vcmpltps
%
ymm8
%
ymm10
%
ymm12
.
byte
196
65
52
194
235
1
/
/
vcmpltps
%
ymm11
%
ymm9
%
ymm13
.
byte
196
67
37
74
241
208
/
/
vblendvps
%
ymm13
%
ymm9
%
ymm11
%
ymm14
.
byte
196
67
61
74
250
192
/
/
vblendvps
%
ymm12
%
ymm10
%
ymm8
%
ymm15
.
byte
196
67
53
74
203
208
/
/
vblendvps
%
ymm13
%
ymm11
%
ymm9
%
ymm9
.
byte
196
65
12
94
201
/
/
vdivps
%
ymm9
%
ymm14
%
ymm9
.
byte
196
67
45
74
192
192
/
/
vblendvps
%
ymm12
%
ymm8
%
ymm10
%
ymm8
.
byte
196
65
4
94
192
/
/
vdivps
%
ymm8
%
ymm15
%
ymm8
.
byte
196
65
60
89
208
/
/
vmulps
%
ymm8
%
ymm8
%
ymm10
.
byte
196
98
125
24
29
175
156
0
0
/
/
vbroadcastss
0x9caf
(
%
rip
)
%
ymm11
#
3c638
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3ec
>
.
byte
196
98
125
24
53
170
156
0
0
/
/
vbroadcastss
0x9caa
(
%
rip
)
%
ymm14
#
3c63c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f0
>
.
byte
196
65
124
40
251
/
/
vmovaps
%
ymm11
%
ymm15
.
byte
196
66
45
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm10
%
ymm15
.
byte
196
193
52
89
193
/
/
vmulps
%
ymm9
%
ymm9
%
ymm0
.
byte
196
66
125
168
222
/
/
vfmadd213ps
%
ymm14
%
ymm0
%
ymm11
.
byte
196
98
125
24
53
145
156
0
0
/
/
vbroadcastss
0x9c91
(
%
rip
)
%
ymm14
#
3c640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f4
>
.
byte
196
66
125
168
222
/
/
vfmadd213ps
%
ymm14
%
ymm0
%
ymm11
.
byte
196
66
45
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm10
%
ymm15
.
byte
196
98
125
24
53
130
156
0
0
/
/
vbroadcastss
0x9c82
(
%
rip
)
%
ymm14
#
3c644
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3f8
>
.
byte
196
66
45
168
254
/
/
vfmadd213ps
%
ymm14
%
ymm10
%
ymm15
.
byte
196
66
125
168
222
/
/
vfmadd213ps
%
ymm14
%
ymm0
%
ymm11
.
byte
196
193
52
89
195
/
/
vmulps
%
ymm11
%
ymm9
%
ymm0
.
byte
196
98
125
24
13
110
156
0
0
/
/
vbroadcastss
0x9c6e
(
%
rip
)
%
ymm9
#
3c648
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3fc
>
.
byte
197
52
92
208
/
/
vsubps
%
ymm0
%
ymm9
%
ymm10
.
byte
196
195
125
74
194
208
/
/
vblendvps
%
ymm13
%
ymm10
%
ymm0
%
ymm0
.
byte
196
65
60
89
199
/
/
vmulps
%
ymm15
%
ymm8
%
ymm8
.
byte
196
65
52
92
200
/
/
vsubps
%
ymm8
%
ymm9
%
ymm9
.
byte
196
67
61
74
193
192
/
/
vblendvps
%
ymm12
%
ymm9
%
ymm8
%
ymm8
.
byte
196
65
52
87
201
/
/
vxorps
%
ymm9
%
ymm9
%
ymm9
.
byte
196
193
116
194
201
1
/
/
vcmpltps
%
ymm9
%
ymm1
%
ymm1
.
byte
196
98
125
24
21
240
154
0
0
/
/
vbroadcastss
0x9af0
(
%
rip
)
%
ymm10
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
197
44
92
216
/
/
vsubps
%
ymm0
%
ymm10
%
ymm11
.
byte
196
195
125
74
195
16
/
/
vblendvps
%
ymm1
%
ymm11
%
ymm0
%
ymm0
.
byte
196
193
100
194
201
1
/
/
vcmpltps
%
ymm9
%
ymm3
%
ymm1
.
byte
196
193
44
92
216
/
/
vsubps
%
ymm8
%
ymm10
%
ymm3
.
byte
196
227
61
74
203
16
/
/
vblendvps
%
ymm1
%
ymm3
%
ymm8
%
ymm1
.
byte
196
193
108
194
217
1
/
/
vcmpltps
%
ymm9
%
ymm2
%
ymm3
.
byte
196
98
125
24
5
202
154
0
0
/
/
vbroadcastss
0x9aca
(
%
rip
)
%
ymm8
#
3c4fc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2b0
>
.
byte
197
60
92
208
/
/
vsubps
%
ymm0
%
ymm8
%
ymm10
.
byte
196
195
125
74
194
48
/
/
vblendvps
%
ymm3
%
ymm10
%
ymm0
%
ymm0
.
byte
196
193
92
194
217
1
/
/
vcmpltps
%
ymm9
%
ymm4
%
ymm3
.
byte
197
60
92
193
/
/
vsubps
%
ymm1
%
ymm8
%
ymm8
.
byte
196
195
117
74
200
48
/
/
vblendvps
%
ymm3
%
ymm8
%
ymm1
%
ymm1
.
byte
196
193
124
194
217
7
/
/
vcmpordps
%
ymm9
%
ymm0
%
ymm3
.
byte
197
228
84
192
/
/
vandps
%
ymm0
%
ymm3
%
ymm0
.
byte
196
193
116
194
217
7
/
/
vcmpordps
%
ymm9
%
ymm1
%
ymm3
.
byte
197
228
84
201
/
/
vandps
%
ymm1
%
ymm3
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
197
252
40
220
/
/
vmovaps
%
ymm4
%
ymm3
.
byte
197
252
40
229
/
/
vmovaps
%
ymm5
%
ymm4
.
byte
197
252
40
238
/
/
vmovaps
%
ymm6
%
ymm5
.
byte
197
252
40
247
/
/
vmovaps
%
ymm7
%
ymm6
.
byte
197
252
16
124
36
200
/
/
vmovups
-
0x38
(
%
rsp
)
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_hsw_lowp
.
globl
_sk_xy_to_radius_hsw_lowp
FUNCTION
(
_sk_xy_to_radius_hsw_lowp
)
_sk_xy_to_radius_hsw_lowp
:
.
byte
197
108
89
194
/
/
vmulps
%
ymm2
%
ymm2
%
ymm8
.
byte
197
100
89
203
/
/
vmulps
%
ymm3
%
ymm3
%
ymm9
.
byte
196
98
117
184
201
/
/
vfmadd231ps
%
ymm1
%
ymm1
%
ymm9
.
byte
196
98
125
184
192
/
/
vfmadd231ps
%
ymm0
%
ymm0
%
ymm8
.
byte
196
193
124
81
192
/
/
vsqrtps
%
ymm8
%
ymm0
.
byte
196
193
124
81
201
/
/
vsqrtps
%
ymm9
%
ymm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_hsw_lowp
.
globl
_sk_srcover_rgba_8888_hsw_lowp
FUNCTION
(
_sk_srcover_rgba_8888_hsw_lowp
)
_sk_srcover_rgba_8888_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
77
141
12
144
/
/
lea
(
%
r8
%
rdx
4
)
%
r9
.
byte
65
137
250
/
/
mov
%
edi
%
r10d
.
byte
65
128
226
15
/
/
and
0xf
%
r10b
.
byte
65
254
202
/
/
dec
%
r10b
.
byte
69
15
182
194
/
/
movzbl
%
r10b
%
r8d
.
byte
65
128
248
14
/
/
cmp
0xe
%
r8b
.
byte
119
51
/
/
ja
32af6
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x5c
>
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
76
141
29
234
2
0
0
/
/
lea
0x2ea
(
%
rip
)
%
r11
#
32db8
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x31e
>
.
byte
75
99
4
131
/
/
movslq
(
%
r11
%
r8
4
)
%
rax
.
byte
76
1
216
/
/
add
%
r11
%
rax
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
205
239
246
/
/
vpxor
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
41
/
/
vmovd
(
%
r9
)
%
xmm5
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
233
22
1
0
0
/
/
jmpq
32c0c
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x172
>
.
byte
196
193
126
111
41
/
/
vmovdqu
(
%
r9
)
%
ymm5
.
byte
196
193
126
111
97
32
/
/
vmovdqu
0x20
(
%
r9
)
%
ymm4
.
byte
233
6
1
0
0
/
/
jmpq
32c0c
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
8
/
/
vmovd
0x8
(
%
r9
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
196
227
69
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm7
%
ymm4
.
byte
196
194
121
53
41
/
/
vpmovzxdq
(
%
r9
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
196
227
93
2
237
3
/
/
vpblendd
0x3
%
ymm5
%
ymm4
%
ymm5
.
byte
197
253
111
231
/
/
vmovdqa
%
ymm7
%
ymm4
.
byte
233
216
0
0
0
/
/
jmpq
32c0c
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
24
/
/
vmovd
0x18
(
%
r9
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
230
1
/
/
vextracti128
0x1
%
ymm4
%
xmm6
.
byte
196
195
73
34
113
20
1
/
/
vpinsrd
0x1
0x14
(
%
r9
)
%
xmm6
%
xmm6
.
byte
196
227
93
56
230
1
/
/
vinserti128
0x1
%
xmm6
%
ymm4
%
ymm4
.
byte
197
253
111
245
/
/
vmovdqa
%
ymm5
%
ymm6
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
105
16
0
/
/
vpinsrd
0x0
0x10
(
%
r9
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
197
125
111
198
/
/
vmovdqa
%
ymm6
%
ymm8
.
byte
196
193
122
111
41
/
/
vmovdqu
(
%
r9
)
%
xmm5
.
byte
196
227
85
2
236
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm5
%
ymm5
.
byte
197
125
127
196
/
/
vmovdqa
%
ymm8
%
ymm4
.
byte
233
129
0
0
0
/
/
jmpq
32c0c
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
40
/
/
vmovd
0x28
(
%
r9
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm4
.
byte
196
195
89
34
105
36
1
/
/
vpinsrd
0x1
0x24
(
%
r9
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
121
110
105
32
/
/
vmovd
0x20
(
%
r9
)
%
xmm5
.
byte
196
227
93
2
229
1
/
/
vpblendd
0x1
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
41
/
/
vmovdqu
(
%
r9
)
%
ymm5
.
byte
235
76
/
/
jmp
32c0c
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
56
/
/
vmovd
0x38
(
%
r9
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
105
52
1
/
/
vpinsrd
0x1
0x34
(
%
r9
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
105
48
0
/
/
vpinsrd
0x0
0x30
(
%
r9
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
41
/
/
vmovdqu
(
%
r9
)
%
ymm5
.
byte
196
193
122
111
113
32
/
/
vmovdqu
0x20
(
%
r9
)
%
xmm6
.
byte
196
227
77
2
228
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm6
%
ymm4
.
byte
196
227
85
56
244
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm6
.
byte
196
227
85
70
252
49
/
/
vperm2i128
0x31
%
ymm4
%
ymm5
%
ymm7
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
196
227
69
14
236
170
/
/
vpblendw
0xaa
%
ymm4
%
ymm7
%
ymm5
.
byte
196
227
77
14
228
170
/
/
vpblendw
0xaa
%
ymm4
%
ymm6
%
ymm4
.
byte
196
226
93
43
237
/
/
vpackusdw
%
ymm5
%
ymm4
%
ymm5
.
byte
197
125
111
5
11
162
0
0
/
/
vmovdqa
0xa20b
(
%
rip
)
%
ymm8
#
3ce40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xbf4
>
.
byte
196
193
85
219
224
/
/
vpand
%
ymm8
%
ymm5
%
ymm4
.
byte
196
98
125
121
13
29
162
0
0
/
/
vpbroadcastw
0xa21d
(
%
rip
)
%
ymm9
#
3ce60
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc14
>
.
byte
197
213
113
213
8
/
/
vpsrlw
0x8
%
ymm5
%
ymm5
.
byte
197
197
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm7
.
byte
197
205
114
214
16
/
/
vpsrld
0x10
%
ymm6
%
ymm6
.
byte
196
226
77
43
255
/
/
vpackusdw
%
ymm7
%
ymm6
%
ymm7
.
byte
196
193
69
219
240
/
/
vpand
%
ymm8
%
ymm7
%
ymm6
.
byte
197
197
113
215
8
/
/
vpsrlw
0x8
%
ymm7
%
ymm7
.
byte
197
53
249
195
/
/
vpsubw
%
ymm3
%
ymm9
%
ymm8
.
byte
196
65
93
213
208
/
/
vpmullw
%
ymm8
%
ymm4
%
ymm10
.
byte
196
65
45
253
209
/
/
vpaddw
%
ymm9
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
192
/
/
vpaddw
%
ymm0
%
ymm10
%
ymm0
.
byte
196
65
85
213
208
/
/
vpmullw
%
ymm8
%
ymm5
%
ymm10
.
byte
196
65
45
253
209
/
/
vpaddw
%
ymm9
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
201
/
/
vpaddw
%
ymm1
%
ymm10
%
ymm1
.
byte
196
65
77
213
208
/
/
vpmullw
%
ymm8
%
ymm6
%
ymm10
.
byte
196
65
45
253
209
/
/
vpaddw
%
ymm9
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
210
/
/
vpaddw
%
ymm2
%
ymm10
%
ymm2
.
byte
196
65
69
213
192
/
/
vpmullw
%
ymm8
%
ymm7
%
ymm8
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
197
189
113
241
8
/
/
vpsllw
0x8
%
ymm1
%
ymm8
.
byte
197
61
235
192
/
/
vpor
%
ymm0
%
ymm8
%
ymm8
.
byte
196
66
125
51
200
/
/
vpmovzxwd
%
xmm8
%
ymm9
.
byte
196
67
125
57
192
1
/
/
vextracti128
0x1
%
ymm8
%
xmm8
.
byte
196
66
125
51
208
/
/
vpmovzxwd
%
xmm8
%
ymm10
.
byte
197
189
113
243
8
/
/
vpsllw
0x8
%
ymm3
%
ymm8
.
byte
197
61
235
194
/
/
vpor
%
ymm2
%
ymm8
%
ymm8
.
byte
196
67
125
57
195
1
/
/
vextracti128
0x1
%
ymm8
%
xmm11
.
byte
196
66
125
51
219
/
/
vpmovzxwd
%
xmm11
%
ymm11
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
193
61
114
240
16
/
/
vpslld
0x10
%
ymm8
%
ymm8
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
53
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm9
.
byte
196
65
53
235
202
/
/
vpor
%
ymm10
%
ymm9
%
ymm9
.
byte
65
128
250
14
/
/
cmp
0xe
%
r10b
.
byte
119
26
/
/
ja
32d1d
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x283
>
.
byte
76
141
21
234
0
0
0
/
/
lea
0xea
(
%
rip
)
%
r10
#
32df4
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x35a
>
.
byte
75
99
4
130
/
/
movslq
(
%
r10
%
r8
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
1
/
/
vmovd
%
xmm8
(
%
r9
)
.
byte
233
151
0
0
0
/
/
jmpq
32db4
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x31a
>
.
byte
196
65
126
127
1
/
/
vmovdqu
%
ymm8
(
%
r9
)
.
byte
196
65
126
127
73
32
/
/
vmovdqu
%
ymm9
0x20
(
%
r9
)
.
byte
233
135
0
0
0
/
/
jmpq
32db4
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
121
22
65
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r9
)
.
byte
196
65
121
214
1
/
/
vmovq
%
xmm8
(
%
r9
)
.
byte
235
121
/
/
jmp
32db4
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
73
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r9
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
73
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r9
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
73
16
/
/
vmovd
%
xmm9
0x10
(
%
r9
)
.
byte
196
65
122
127
1
/
/
vmovdqu
%
xmm8
(
%
r9
)
.
byte
235
76
/
/
jmp
32db4
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
121
22
73
40
2
/
/
vpextrd
0x2
%
xmm9
0x28
(
%
r9
)
.
byte
196
67
121
22
73
36
1
/
/
vpextrd
0x1
%
xmm9
0x24
(
%
r9
)
.
byte
196
65
121
126
73
32
/
/
vmovd
%
xmm9
0x20
(
%
r9
)
.
byte
196
65
126
127
1
/
/
vmovdqu
%
ymm8
(
%
r9
)
.
byte
235
49
/
/
jmp
32db4
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
81
56
2
/
/
vpextrd
0x2
%
xmm10
0x38
(
%
r9
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
81
52
1
/
/
vpextrd
0x1
%
xmm10
0x34
(
%
r9
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
65
121
126
81
48
/
/
vmovd
%
xmm10
0x30
(
%
r9
)
.
byte
196
65
126
127
1
/
/
vmovdqu
%
ymm8
(
%
r9
)
.
byte
196
65
122
127
73
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r9
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
48
253
/
/
xor
%
bh
%
ch
.
byte
255
/
/
(
bad
)
.
byte
255
99
253
/
/
jmpq
*
-
0x3
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
78
253
/
/
decl
-
0x3
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
253
255
255
168
/
/
mov
0xa8fffffd
%
edi
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
145
253
255
255
124
/
/
callq
*
0x7cfffffd
(
%
rcx
)
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
1
/
/
incl
(
%
rcx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
245
/
/
push
%
rbp
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
232
253
255
255
211
/
/
callq
ffffffffd4032dde
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffd3ff6b92
>
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
67
254
/
/
incl
-
0x2
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
48
/
/
pushq
(
%
rax
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
254
255
255
8
/
/
lcall
*
0x8fffffe
(
%
rip
)
#
9032def
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x8ff6ba3
>
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
31
/
/
lcall
*
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
64
255
/
/
incl
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
57
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
109
255
/
/
ljmp
*
-
0x1
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
97
255
/
/
jmpq
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
84
255
255
/
/
callq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
71
255
/
/
incl
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
136
255
255
255
130
/
/
decl
-
0x7d000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
123
255
/
/
jnp
32e19
<
_sk_srcover_rgba_8888_hsw_lowp
+
0x37f
>
.
byte
255
/
/
(
bad
)
.
byte
255
116
255
255
/
/
pushq
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
181
255
255
255
169
/
/
pushq
-
0x56000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
156
255
255
255
143
255
/
/
lcall
*
-
0x700001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_srcover_bgra_8888_hsw_lowp
.
globl
_sk_srcover_bgra_8888_hsw_lowp
FUNCTION
(
_sk_srcover_bgra_8888_hsw_lowp
)
_sk_srcover_bgra_8888_hsw_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
77
141
12
144
/
/
lea
(
%
r8
%
rdx
4
)
%
r9
.
byte
65
137
250
/
/
mov
%
edi
%
r10d
.
byte
65
128
226
15
/
/
and
0xf
%
r10b
.
byte
65
254
202
/
/
dec
%
r10b
.
byte
69
15
182
194
/
/
movzbl
%
r10b
%
r8d
.
byte
65
128
248
14
/
/
cmp
0xe
%
r8b
.
byte
119
51
/
/
ja
32e8c
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x5c
>
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
76
141
29
236
2
0
0
/
/
lea
0x2ec
(
%
rip
)
%
r11
#
33150
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x320
>
.
byte
75
99
4
131
/
/
movslq
(
%
r11
%
r8
4
)
%
rax
.
byte
76
1
216
/
/
add
%
r11
%
rax
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
197
205
239
246
/
/
vpxor
%
ymm6
%
ymm6
%
ymm6
.
byte
196
65
61
239
192
/
/
vpxor
%
ymm8
%
ymm8
%
ymm8
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
193
121
110
41
/
/
vmovd
(
%
r9
)
%
xmm5
.
byte
197
221
239
228
/
/
vpxor
%
ymm4
%
ymm4
%
ymm4
.
byte
233
22
1
0
0
/
/
jmpq
32fa2
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x172
>
.
byte
196
193
126
111
41
/
/
vmovdqu
(
%
r9
)
%
ymm5
.
byte
196
193
126
111
97
32
/
/
vmovdqu
0x20
(
%
r9
)
%
ymm4
.
byte
233
6
1
0
0
/
/
jmpq
32fa2
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
8
/
/
vmovd
0x8
(
%
r9
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
197
239
255
/
/
vpxor
%
ymm7
%
ymm7
%
ymm7
.
byte
196
227
69
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm7
%
ymm4
.
byte
196
194
121
53
41
/
/
vpmovzxdq
(
%
r9
)
%
xmm5
.
byte
197
249
112
237
232
/
/
vpshufd
0xe8
%
xmm5
%
xmm5
.
byte
196
227
93
2
237
3
/
/
vpblendd
0x3
%
ymm5
%
ymm4
%
ymm5
.
byte
197
253
111
231
/
/
vmovdqa
%
ymm7
%
ymm4
.
byte
233
216
0
0
0
/
/
jmpq
32fa2
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
24
/
/
vmovd
0x18
(
%
r9
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
230
1
/
/
vextracti128
0x1
%
ymm4
%
xmm6
.
byte
196
195
73
34
113
20
1
/
/
vpinsrd
0x1
0x14
(
%
r9
)
%
xmm6
%
xmm6
.
byte
196
227
93
56
230
1
/
/
vinserti128
0x1
%
xmm6
%
ymm4
%
ymm4
.
byte
197
253
111
245
/
/
vmovdqa
%
ymm5
%
ymm6
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
105
16
0
/
/
vpinsrd
0x0
0x10
(
%
r9
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
197
125
111
198
/
/
vmovdqa
%
ymm6
%
ymm8
.
byte
196
193
122
111
41
/
/
vmovdqu
(
%
r9
)
%
xmm5
.
byte
196
227
85
2
236
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm5
%
ymm5
.
byte
197
125
127
196
/
/
vmovdqa
%
ymm8
%
ymm4
.
byte
233
129
0
0
0
/
/
jmpq
32fa2
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
40
/
/
vmovd
0x28
(
%
r9
)
%
xmm4
.
byte
196
226
121
89
228
/
/
vpbroadcastq
%
xmm4
%
xmm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
4
/
/
vpblendd
0x4
%
ymm4
%
ymm5
%
ymm4
.
byte
196
195
89
34
105
36
1
/
/
vpinsrd
0x1
0x24
(
%
r9
)
%
xmm4
%
xmm5
.
byte
196
227
93
2
229
15
/
/
vpblendd
0xf
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
121
110
105
32
/
/
vmovd
0x20
(
%
r9
)
%
xmm5
.
byte
196
227
93
2
229
1
/
/
vpblendd
0x1
%
ymm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
41
/
/
vmovdqu
(
%
r9
)
%
ymm5
.
byte
235
76
/
/
jmp
32fa2
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x172
>
.
byte
196
193
121
110
97
56
/
/
vmovd
0x38
(
%
r9
)
%
xmm4
.
byte
196
226
125
89
228
/
/
vpbroadcastq
%
xmm4
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
85
2
228
64
/
/
vpblendd
0x40
%
ymm4
%
ymm5
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
105
52
1
/
/
vpinsrd
0x1
0x34
(
%
r9
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
227
125
57
229
1
/
/
vextracti128
0x1
%
ymm4
%
xmm5
.
byte
196
195
81
34
105
48
0
/
/
vpinsrd
0x0
0x30
(
%
r9
)
%
xmm5
%
xmm5
.
byte
196
227
93
56
229
1
/
/
vinserti128
0x1
%
xmm5
%
ymm4
%
ymm4
.
byte
196
193
126
111
41
/
/
vmovdqu
(
%
r9
)
%
ymm5
.
byte
196
193
122
111
113
32
/
/
vmovdqu
0x20
(
%
r9
)
%
xmm6
.
byte
196
227
77
2
228
240
/
/
vpblendd
0xf0
%
ymm4
%
ymm6
%
ymm4
.
byte
196
227
85
56
252
1
/
/
vinserti128
0x1
%
xmm4
%
ymm5
%
ymm7
.
byte
196
227
85
70
228
49
/
/
vperm2i128
0x31
%
ymm4
%
ymm5
%
ymm4
.
byte
197
213
239
237
/
/
vpxor
%
ymm5
%
ymm5
%
ymm5
.
byte
196
227
93
14
245
170
/
/
vpblendw
0xaa
%
ymm5
%
ymm4
%
ymm6
.
byte
196
227
69
14
237
170
/
/
vpblendw
0xaa
%
ymm5
%
ymm7
%
ymm5
.
byte
196
226
85
43
238
/
/
vpackusdw
%
ymm6
%
ymm5
%
ymm5
.
byte
197
125
111
5
181
158
0
0
/
/
vmovdqa
0x9eb5
(
%
rip
)
%
ymm8
#
3ce80
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc34
>
.
byte
196
193
85
219
240
/
/
vpand
%
ymm8
%
ymm5
%
ymm6
.
byte
196
98
125
121
13
199
158
0
0
/
/
vpbroadcastw
0x9ec7
(
%
rip
)
%
ymm9
#
3cea0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc54
>
.
byte
197
213
113
213
8
/
/
vpsrlw
0x8
%
ymm5
%
ymm5
.
byte
197
221
114
212
16
/
/
vpsrld
0x10
%
ymm4
%
ymm4
.
byte
197
197
114
215
16
/
/
vpsrld
0x10
%
ymm7
%
ymm7
.
byte
196
226
69
43
252
/
/
vpackusdw
%
ymm4
%
ymm7
%
ymm7
.
byte
196
193
69
219
224
/
/
vpand
%
ymm8
%
ymm7
%
ymm4
.
byte
197
197
113
215
8
/
/
vpsrlw
0x8
%
ymm7
%
ymm7
.
byte
197
53
249
195
/
/
vpsubw
%
ymm3
%
ymm9
%
ymm8
.
byte
196
65
93
213
208
/
/
vpmullw
%
ymm8
%
ymm4
%
ymm10
.
byte
196
65
45
253
209
/
/
vpaddw
%
ymm9
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
192
/
/
vpaddw
%
ymm0
%
ymm10
%
ymm0
.
byte
196
65
85
213
208
/
/
vpmullw
%
ymm8
%
ymm5
%
ymm10
.
byte
196
65
45
253
209
/
/
vpaddw
%
ymm9
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
201
/
/
vpaddw
%
ymm1
%
ymm10
%
ymm1
.
byte
196
65
77
213
208
/
/
vpmullw
%
ymm8
%
ymm6
%
ymm10
.
byte
196
65
45
253
209
/
/
vpaddw
%
ymm9
%
ymm10
%
ymm10
.
byte
196
193
45
113
210
8
/
/
vpsrlw
0x8
%
ymm10
%
ymm10
.
byte
197
173
253
210
/
/
vpaddw
%
ymm2
%
ymm10
%
ymm2
.
byte
196
65
69
213
192
/
/
vpmullw
%
ymm8
%
ymm7
%
ymm8
.
byte
196
65
61
253
193
/
/
vpaddw
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
61
113
208
8
/
/
vpsrlw
0x8
%
ymm8
%
ymm8
.
byte
197
189
253
219
/
/
vpaddw
%
ymm3
%
ymm8
%
ymm3
.
byte
197
189
113
241
8
/
/
vpsllw
0x8
%
ymm1
%
ymm8
.
byte
197
61
235
194
/
/
vpor
%
ymm2
%
ymm8
%
ymm8
.
byte
196
66
125
51
200
/
/
vpmovzxwd
%
xmm8
%
ymm9
.
byte
196
67
125
57
192
1
/
/
vextracti128
0x1
%
ymm8
%
xmm8
.
byte
196
66
125
51
208
/
/
vpmovzxwd
%
xmm8
%
ymm10
.
byte
197
189
113
243
8
/
/
vpsllw
0x8
%
ymm3
%
ymm8
.
byte
197
61
235
192
/
/
vpor
%
ymm0
%
ymm8
%
ymm8
.
byte
196
67
125
57
195
1
/
/
vextracti128
0x1
%
ymm8
%
xmm11
.
byte
196
66
125
51
219
/
/
vpmovzxwd
%
xmm11
%
ymm11
.
byte
196
66
125
51
192
/
/
vpmovzxwd
%
xmm8
%
ymm8
.
byte
196
193
61
114
240
16
/
/
vpslld
0x10
%
ymm8
%
ymm8
.
byte
196
65
61
235
193
/
/
vpor
%
ymm9
%
ymm8
%
ymm8
.
byte
196
193
53
114
243
16
/
/
vpslld
0x10
%
ymm11
%
ymm9
.
byte
196
65
53
235
202
/
/
vpor
%
ymm10
%
ymm9
%
ymm9
.
byte
65
128
250
14
/
/
cmp
0xe
%
r10b
.
byte
119
26
/
/
ja
330b3
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x283
>
.
byte
76
141
21
236
0
0
0
/
/
lea
0xec
(
%
rip
)
%
r10
#
3318c
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x35c
>
.
byte
75
99
4
130
/
/
movslq
(
%
r10
%
r8
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
196
65
121
126
1
/
/
vmovd
%
xmm8
(
%
r9
)
.
byte
233
151
0
0
0
/
/
jmpq
3314a
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x31a
>
.
byte
196
65
126
127
1
/
/
vmovdqu
%
ymm8
(
%
r9
)
.
byte
196
65
126
127
73
32
/
/
vmovdqu
%
ymm9
0x20
(
%
r9
)
.
byte
233
135
0
0
0
/
/
jmpq
3314a
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
121
22
65
8
2
/
/
vpextrd
0x2
%
xmm8
0x8
(
%
r9
)
.
byte
196
65
121
214
1
/
/
vmovq
%
xmm8
(
%
r9
)
.
byte
235
121
/
/
jmp
3314a
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
73
24
2
/
/
vpextrd
0x2
%
xmm9
0x18
(
%
r9
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
67
121
22
73
20
1
/
/
vpextrd
0x1
%
xmm9
0x14
(
%
r9
)
.
byte
196
67
125
57
193
1
/
/
vextracti128
0x1
%
ymm8
%
xmm9
.
byte
196
65
121
126
73
16
/
/
vmovd
%
xmm9
0x10
(
%
r9
)
.
byte
196
65
122
127
1
/
/
vmovdqu
%
xmm8
(
%
r9
)
.
byte
235
76
/
/
jmp
3314a
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
121
22
73
40
2
/
/
vpextrd
0x2
%
xmm9
0x28
(
%
r9
)
.
byte
196
67
121
22
73
36
1
/
/
vpextrd
0x1
%
xmm9
0x24
(
%
r9
)
.
byte
196
65
121
126
73
32
/
/
vmovd
%
xmm9
0x20
(
%
r9
)
.
byte
196
65
126
127
1
/
/
vmovdqu
%
ymm8
(
%
r9
)
.
byte
235
49
/
/
jmp
3314a
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x31a
>
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
81
56
2
/
/
vpextrd
0x2
%
xmm10
0x38
(
%
r9
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
67
121
22
81
52
1
/
/
vpextrd
0x1
%
xmm10
0x34
(
%
r9
)
.
byte
196
67
125
57
202
1
/
/
vextracti128
0x1
%
ymm9
%
xmm10
.
byte
196
65
121
126
81
48
/
/
vmovd
%
xmm10
0x30
(
%
r9
)
.
byte
196
65
126
127
1
/
/
vmovdqu
%
ymm8
(
%
r9
)
.
byte
196
65
122
127
73
32
/
/
vmovdqu
%
xmm9
0x20
(
%
r9
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
46
253
/
/
cs
std
.
byte
255
/
/
(
bad
)
.
byte
255
97
253
/
/
jmpq
*
-
0x3
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
76
253
255
/
/
decl
-
0x1
(
%
rbp
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
189
253
255
255
166
/
/
mov
0xa6fffffd
%
ebp
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
143
253
255
255
122
/
/
decl
0x7afffffd
(
%
rdi
)
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
243
/
/
push
%
rbx
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
230
/
/
jmpq
*
%
rsi
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
65
254
/
/
incl
-
0x2
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
46
/
/
ljmp
*
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
27
/
/
lcall
*
(
%
rbx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
62
/
/
lcall
*
0x3effffff
(
%
rip
)
#
3f033190
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3eff6f44
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
55
/
/
pushq
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
107
255
/
/
ljmp
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
95
255
/
/
lcall
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
82
255
/
/
callq
*
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
69
255
/
/
incl
-
0x1
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
134
255
255
255
128
/
/
incl
-
0x7f000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
121
255
/
/
jns
331b1
<
_sk_srcover_bgra_8888_hsw_lowp
+
0x381
>
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
179
255
255
255
167
/
/
pushq
-
0x58000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
154
255
255
255
141
/
/
lcall
*
-
0x72000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_start_pipeline_sse41_lowp
.
globl
_sk_start_pipeline_sse41_lowp
FUNCTION
(
_sk_start_pipeline_sse41_lowp
)
_sk_start_pipeline_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
131
0
0
0
/
/
jae
3327e
<
_sk_start_pipeline_sse41_lowp
+
0xb6
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
8
/
/
lea
0x8
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
119
59
/
/
ja
3324c
<
_sk_start_pipeline_sse41_lowp
+
0x84
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
8
/
/
lea
0x8
(
%
r12
)
%
rdx
.
byte
73
131
196
16
/
/
add
0x10
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
201
/
/
jbe
33215
<
_sk_start_pipeline_sse41_lowp
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
33
/
/
je
33275
<
_sk_start_pipeline_sse41_lowp
+
0xad
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
255
195
/
/
inc
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
117
137
/
/
jne
33207
<
_sk_start_pipeline_sse41_lowp
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_sse41_lowp
.
globl
_sk_just_return_sse41_lowp
FUNCTION
(
_sk_just_return_sse41_lowp
)
_sk_just_return_sse41_lowp
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_sse41_lowp
.
globl
_sk_seed_shader_sse41_lowp
FUNCTION
(
_sk_seed_shader_sse41_lowp
)
_sk_seed_shader_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
208
/
/
cvtdq2ps
%
xmm0
%
xmm2
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
15
16
72
16
/
/
movups
0x10
(
%
rax
)
%
xmm1
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
102
15
110
209
/
/
movd
%
ecx
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
88
21
68
156
0
0
/
/
addps
0x9c44
(
%
rip
)
%
xmm2
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_sse41_lowp
.
globl
_sk_matrix_translate_sse41_lowp
FUNCTION
(
_sk_matrix_translate_sse41_lowp
)
_sk_matrix_translate_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_sse41_lowp
.
globl
_sk_matrix_scale_translate_sse41_lowp
FUNCTION
(
_sk_matrix_scale_translate_sse41_lowp
)
_sk_matrix_scale_translate_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
89
217
/
/
mulps
%
xmm9
%
xmm3
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_sse41_lowp
.
globl
_sk_matrix_2x3_sse41_lowp
FUNCTION
(
_sk_matrix_2x3_sse41_lowp
)
_sk_matrix_2x3_sse41_lowp
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
68
15
16
88
4
/
/
movss
0x4
(
%
rax
)
%
xmm11
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
72
16
/
/
movss
0x10
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
226
/
/
movaps
%
xmm2
%
xmm12
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
69
15
88
209
/
/
addps
%
xmm9
%
xmm10
.
byte
69
15
88
225
/
/
addps
%
xmm9
%
xmm12
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
196
/
/
addps
%
xmm12
%
xmm0
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
96
20
/
/
movss
0x14
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
69
15
88
212
/
/
addps
%
xmm12
%
xmm10
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
68
15
88
209
/
/
addps
%
xmm1
%
xmm10
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_sse41_lowp
.
globl
_sk_matrix_perspective_sse41_lowp
FUNCTION
(
_sk_matrix_perspective_sse41_lowp
)
_sk_matrix_perspective_sse41_lowp
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
8
/
/
movss
(
%
rax
)
%
xmm1
.
byte
243
68
15
16
80
4
/
/
movss
0x4
(
%
rax
)
%
xmm10
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
72
8
/
/
movss
0x8
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
69
15
88
209
/
/
addps
%
xmm9
%
xmm10
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
243
68
15
16
96
12
/
/
movss
0xc
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
104
20
/
/
movss
0x14
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
69
15
88
213
/
/
addps
%
xmm13
%
xmm10
.
byte
69
15
88
221
/
/
addps
%
xmm13
%
xmm11
.
byte
68
15
40
232
/
/
movaps
%
xmm0
%
xmm13
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
69
15
88
213
/
/
addps
%
xmm13
%
xmm10
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
243
68
15
16
96
24
/
/
movss
0x18
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
28
/
/
movss
0x1c
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
243
68
15
16
112
32
/
/
movss
0x20
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
65
15
88
222
/
/
addps
%
xmm14
%
xmm3
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
65
15
88
197
/
/
addps
%
xmm13
%
xmm0
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
68
15
88
227
/
/
addps
%
xmm3
%
xmm12
.
byte
15
83
192
/
/
rcpps
%
xmm0
%
xmm0
.
byte
65
15
83
212
/
/
rcpps
%
xmm12
%
xmm2
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
219
/
/
movaps
%
xmm11
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_sse41_lowp
.
globl
_sk_uniform_color_sse41_lowp
FUNCTION
(
_sk_uniform_color_sse41_lowp
)
_sk_uniform_color_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
110
64
16
/
/
movd
0x10
(
%
rax
)
%
xmm0
.
byte
242
15
112
192
0
/
/
pshuflw
0x0
%
xmm0
%
xmm0
.
byte
102
15
112
192
80
/
/
pshufd
0x50
%
xmm0
%
xmm0
.
byte
68
15
183
64
18
/
/
movzwl
0x12
(
%
rax
)
%
r8d
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
242
15
112
201
0
/
/
pshuflw
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
201
80
/
/
pshufd
0x50
%
xmm1
%
xmm1
.
byte
102
15
110
80
20
/
/
movd
0x14
(
%
rax
)
%
xmm2
.
byte
242
15
112
210
0
/
/
pshuflw
0x0
%
xmm2
%
xmm2
.
byte
102
15
112
210
80
/
/
pshufd
0x50
%
xmm2
%
xmm2
.
byte
15
183
64
22
/
/
movzwl
0x16
(
%
rax
)
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
242
15
112
219
0
/
/
pshuflw
0x0
%
xmm3
%
xmm3
.
byte
102
15
112
219
80
/
/
pshufd
0x50
%
xmm3
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_sse41_lowp
.
globl
_sk_black_color_sse41_lowp
FUNCTION
(
_sk_black_color_sse41_lowp
)
_sk_black_color_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
137
153
0
0
/
/
movaps
0x9989
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_sse41_lowp
.
globl
_sk_white_color_sse41_lowp
FUNCTION
(
_sk_white_color_sse41_lowp
)
_sk_white_color_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
5
117
153
0
0
/
/
movaps
0x9975
(
%
rip
)
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_sse41_lowp
.
globl
_sk_set_rgb_sse41_lowp
FUNCTION
(
_sk_set_rgb_sse41_lowp
)
_sk_set_rgb_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
21
216
143
0
0
/
/
movss
0x8fd8
(
%
rip
)
%
xmm2
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
89
194
/
/
mulss
%
xmm2
%
xmm0
.
byte
243
68
15
16
5
151
143
0
0
/
/
movss
0x8f97
(
%
rip
)
%
xmm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
243
65
15
88
192
/
/
addss
%
xmm8
%
xmm0
.
byte
243
68
15
44
192
/
/
cvttss2si
%
xmm0
%
r8d
.
byte
102
65
15
110
192
/
/
movd
%
r8d
%
xmm0
.
byte
242
15
112
192
0
/
/
pshuflw
0x0
%
xmm0
%
xmm0
.
byte
102
15
112
192
80
/
/
pshufd
0x50
%
xmm0
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
243
15
89
202
/
/
mulss
%
xmm2
%
xmm1
.
byte
243
65
15
88
200
/
/
addss
%
xmm8
%
xmm1
.
byte
243
68
15
44
193
/
/
cvttss2si
%
xmm1
%
r8d
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
242
15
112
201
0
/
/
pshuflw
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
201
80
/
/
pshufd
0x50
%
xmm1
%
xmm1
.
byte
243
15
89
80
8
/
/
mulss
0x8
(
%
rax
)
%
xmm2
.
byte
243
65
15
88
208
/
/
addss
%
xmm8
%
xmm2
.
byte
243
15
44
194
/
/
cvttss2si
%
xmm2
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
242
15
112
210
0
/
/
pshuflw
0x0
%
xmm2
%
xmm2
.
byte
102
15
112
210
80
/
/
pshufd
0x50
%
xmm2
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_sse41_lowp
.
globl
_sk_clamp_a_sse41_lowp
FUNCTION
(
_sk_clamp_a_sse41_lowp
)
_sk_clamp_a_sse41_lowp
:
.
byte
102
15
56
58
195
/
/
pminuw
%
xmm3
%
xmm0
.
byte
102
15
56
58
203
/
/
pminuw
%
xmm3
%
xmm1
.
byte
102
15
56
58
211
/
/
pminuw
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_sse41_lowp
.
globl
_sk_clamp_a_dst_sse41_lowp
FUNCTION
(
_sk_clamp_a_dst_sse41_lowp
)
_sk_clamp_a_dst_sse41_lowp
:
.
byte
102
15
56
58
231
/
/
pminuw
%
xmm7
%
xmm4
.
byte
102
15
56
58
239
/
/
pminuw
%
xmm7
%
xmm5
.
byte
102
15
56
58
247
/
/
pminuw
%
xmm7
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_sse41_lowp
.
globl
_sk_premul_sse41_lowp
FUNCTION
(
_sk_premul_sse41_lowp
)
_sk_premul_sse41_lowp
:
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
68
15
111
5
193
152
0
0
/
/
movdqa
0x98c1
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_sse41_lowp
.
globl
_sk_premul_dst_sse41_lowp
FUNCTION
(
_sk_premul_dst_sse41_lowp
)
_sk_premul_dst_sse41_lowp
:
.
byte
102
15
213
231
/
/
pmullw
%
xmm7
%
xmm4
.
byte
102
68
15
111
5
138
152
0
0
/
/
movdqa
0x988a
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
224
/
/
paddw
%
xmm8
%
xmm4
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
213
239
/
/
pmullw
%
xmm7
%
xmm5
.
byte
102
65
15
253
232
/
/
paddw
%
xmm8
%
xmm5
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
213
247
/
/
pmullw
%
xmm7
%
xmm6
.
byte
102
65
15
253
240
/
/
paddw
%
xmm8
%
xmm6
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_sse41_lowp
.
globl
_sk_force_opaque_sse41_lowp
FUNCTION
(
_sk_force_opaque_sse41_lowp
)
_sk_force_opaque_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
87
152
0
0
/
/
movaps
0x9857
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_sse41_lowp
.
globl
_sk_force_opaque_dst_sse41_lowp
FUNCTION
(
_sk_force_opaque_dst_sse41_lowp
)
_sk_force_opaque_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
76
152
0
0
/
/
movaps
0x984c
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_sse41_lowp
.
globl
_sk_swap_rb_sse41_lowp
FUNCTION
(
_sk_swap_rb_sse41_lowp
)
_sk_swap_rb_sse41_lowp
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_sse41_lowp
.
globl
_sk_move_src_dst_sse41_lowp
FUNCTION
(
_sk_move_src_dst_sse41_lowp
)
_sk_move_src_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_sse41_lowp
.
globl
_sk_move_dst_src_sse41_lowp
FUNCTION
(
_sk_move_dst_src_sse41_lowp
)
_sk_move_dst_src_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_sse41_lowp
.
globl
_sk_invert_sse41_lowp
FUNCTION
(
_sk_invert_sse41_lowp
)
_sk_invert_sse41_lowp
:
.
byte
102
68
15
111
5
18
152
0
0
/
/
movdqa
0x9812
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
68
15
249
200
/
/
psubw
%
xmm0
%
xmm9
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
249
209
/
/
psubw
%
xmm1
%
xmm10
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
68
15
249
218
/
/
psubw
%
xmm2
%
xmm11
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_sse41_lowp
.
globl
_sk_clear_sse41_lowp
FUNCTION
(
_sk_clear_sse41_lowp
)
_sk_clear_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_sse41_lowp
.
globl
_sk_srcatop_sse41_lowp
FUNCTION
(
_sk_srcatop_sse41_lowp
)
_sk_srcatop_sse41_lowp
:
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
111
29
182
151
0
0
/
/
movdqa
0x97b6
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
111
203
/
/
movdqa
%
xmm3
%
xmm9
.
byte
102
69
15
249
200
/
/
psubw
%
xmm8
%
xmm9
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
196
/
/
pmullw
%
xmm4
%
xmm8
.
byte
102
15
253
195
/
/
paddw
%
xmm3
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
15
253
203
/
/
paddw
%
xmm3
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
206
/
/
pmullw
%
xmm6
%
xmm9
.
byte
102
15
253
211
/
/
paddw
%
xmm3
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
111
199
/
/
movdqa
%
xmm7
%
xmm8
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_sse41_lowp
.
globl
_sk_dstatop_sse41_lowp
FUNCTION
(
_sk_dstatop_sse41_lowp
)
_sk_dstatop_sse41_lowp
:
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
68
15
111
13
54
151
0
0
/
/
movdqa
0x9736
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
68
15
249
207
/
/
psubw
%
xmm7
%
xmm9
.
byte
102
65
15
213
193
/
/
pmullw
%
xmm9
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
201
/
/
pmullw
%
xmm9
%
xmm1
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
209
/
/
pmullw
%
xmm9
%
xmm2
.
byte
102
65
15
253
211
/
/
paddw
%
xmm11
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_sse41_lowp
.
globl
_sk_srcin_sse41_lowp
FUNCTION
(
_sk_srcin_sse41_lowp
)
_sk_srcin_sse41_lowp
:
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
68
15
111
5
193
150
0
0
/
/
movdqa
0x96c1
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_sse41_lowp
.
globl
_sk_dstin_sse41_lowp
FUNCTION
(
_sk_dstin_sse41_lowp
)
_sk_dstin_sse41_lowp
:
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
68
15
111
5
120
150
0
0
/
/
movdqa
0x9678
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
111
214
/
/
movdqa
%
xmm6
%
xmm2
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_sse41_lowp
.
globl
_sk_srcout_sse41_lowp
FUNCTION
(
_sk_srcout_sse41_lowp
)
_sk_srcout_sse41_lowp
:
.
byte
102
68
15
111
5
47
150
0
0
/
/
movdqa
0x962f
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
68
15
249
207
/
/
psubw
%
xmm7
%
xmm9
.
byte
102
65
15
213
193
/
/
pmullw
%
xmm9
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
201
/
/
pmullw
%
xmm9
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
209
/
/
pmullw
%
xmm9
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_sse41_lowp
.
globl
_sk_dstout_sse41_lowp
FUNCTION
(
_sk_dstout_sse41_lowp
)
_sk_dstout_sse41_lowp
:
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
68
15
111
5
216
149
0
0
/
/
movdqa
0x95d8
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
249
216
/
/
psubw
%
xmm0
%
xmm3
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_sse41_lowp
.
globl
_sk_srcover_sse41_lowp
FUNCTION
(
_sk_srcover_sse41_lowp
)
_sk_srcover_sse41_lowp
:
.
byte
102
68
15
111
13
126
149
0
0
/
/
movdqa
0x957e
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
212
/
/
pmullw
%
xmm4
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
253
194
/
/
paddw
%
xmm10
%
xmm0
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
213
/
/
pmullw
%
xmm5
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
214
/
/
pmullw
%
xmm6
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_sse41_lowp
.
globl
_sk_dstover_sse41_lowp
FUNCTION
(
_sk_dstover_sse41_lowp
)
_sk_dstover_sse41_lowp
:
.
byte
102
68
15
111
5
4
149
0
0
/
/
movdqa
0x9504
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
68
15
249
207
/
/
psubw
%
xmm7
%
xmm9
.
byte
102
65
15
213
193
/
/
pmullw
%
xmm9
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
196
/
/
paddw
%
xmm4
%
xmm0
.
byte
102
65
15
213
201
/
/
pmullw
%
xmm9
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
253
205
/
/
paddw
%
xmm5
%
xmm1
.
byte
102
65
15
213
209
/
/
pmullw
%
xmm9
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_sse41_lowp
.
globl
_sk_modulate_sse41_lowp
FUNCTION
(
_sk_modulate_sse41_lowp
)
_sk_modulate_sse41_lowp
:
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
68
15
111
5
157
148
0
0
/
/
movdqa
0x949d
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_sse41_lowp
.
globl
_sk_multiply_sse41_lowp
FUNCTION
(
_sk_multiply_sse41_lowp
)
_sk_multiply_sse41_lowp
:
.
byte
102
68
15
111
13
92
148
0
0
/
/
movdqa
0x945c
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
212
/
/
pmullw
%
xmm4
%
xmm10
.
byte
102
68
15
111
220
/
/
movdqa
%
xmm4
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
68
15
249
223
/
/
psubw
%
xmm7
%
xmm11
.
byte
102
65
15
213
195
/
/
pmullw
%
xmm11
%
xmm0
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
253
194
/
/
paddw
%
xmm10
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
213
/
/
pmullw
%
xmm5
%
xmm10
.
byte
102
68
15
111
221
/
/
movdqa
%
xmm5
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
68
15
249
223
/
/
psubw
%
xmm7
%
xmm11
.
byte
102
65
15
213
203
/
/
pmullw
%
xmm11
%
xmm1
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
214
/
/
pmullw
%
xmm6
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
68
15
249
223
/
/
psubw
%
xmm7
%
xmm11
.
byte
102
65
15
213
211
/
/
pmullw
%
xmm11
%
xmm2
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__sse41_lowp
.
globl
_sk_plus__sse41_lowp
FUNCTION
(
_sk_plus__sse41_lowp
)
_sk_plus__sse41_lowp
:
.
byte
102
15
253
196
/
/
paddw
%
xmm4
%
xmm0
.
byte
102
68
15
111
5
161
147
0
0
/
/
movdqa
0x93a1
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
56
58
192
/
/
pminuw
%
xmm8
%
xmm0
.
byte
102
15
253
205
/
/
paddw
%
xmm5
%
xmm1
.
byte
102
65
15
56
58
200
/
/
pminuw
%
xmm8
%
xmm1
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
65
15
56
58
208
/
/
pminuw
%
xmm8
%
xmm2
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
102
65
15
56
58
216
/
/
pminuw
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_sse41_lowp
.
globl
_sk_screen_sse41_lowp
FUNCTION
(
_sk_screen_sse41_lowp
)
_sk_screen_sse41_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
213
204
/
/
pmullw
%
xmm4
%
xmm1
.
byte
102
68
15
111
21
91
147
0
0
/
/
movdqa
0x935b
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
69
15
253
194
/
/
paddw
%
xmm10
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
65
15
249
200
/
/
psubw
%
xmm8
%
xmm1
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
249
194
/
/
psubw
%
xmm2
%
xmm8
.
byte
102
68
15
111
207
/
/
movdqa
%
xmm7
%
xmm9
.
byte
102
68
15
253
203
/
/
paddw
%
xmm3
%
xmm9
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
218
/
/
paddw
%
xmm10
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
68
15
249
203
/
/
psubw
%
xmm3
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__sse41_lowp
.
globl
_sk_xor__sse41_lowp
FUNCTION
(
_sk_xor__sse41_lowp
)
_sk_xor__sse41_lowp
:
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
68
15
111
13
217
146
0
0
/
/
movdqa
0x92d9
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
209
/
/
movdqa
%
xmm9
%
xmm10
.
byte
102
68
15
249
215
/
/
psubw
%
xmm7
%
xmm10
.
byte
102
65
15
213
194
/
/
pmullw
%
xmm10
%
xmm0
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
65
15
249
216
/
/
psubw
%
xmm8
%
xmm3
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
68
15
213
220
/
/
pmullw
%
xmm4
%
xmm11
.
byte
102
65
15
253
195
/
/
paddw
%
xmm11
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
202
/
/
pmullw
%
xmm10
%
xmm1
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
68
15
213
221
/
/
pmullw
%
xmm5
%
xmm11
.
byte
102
65
15
253
203
/
/
paddw
%
xmm11
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
210
/
/
pmullw
%
xmm10
%
xmm2
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
68
15
213
222
/
/
pmullw
%
xmm6
%
xmm11
.
byte
102
65
15
253
211
/
/
paddw
%
xmm11
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
69
15
213
208
/
/
pmullw
%
xmm8
%
xmm10
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
218
/
/
paddw
%
xmm10
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_sse41_lowp
.
globl
_sk_darken_sse41_lowp
FUNCTION
(
_sk_darken_sse41_lowp
)
_sk_darken_sse41_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
68
15
213
207
/
/
pmullw
%
xmm7
%
xmm9
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
65
15
56
62
201
/
/
pmaxuw
%
xmm9
%
xmm1
.
byte
102
68
15
111
13
32
146
0
0
/
/
movdqa
0x9220
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
69
15
56
62
208
/
/
pmaxuw
%
xmm8
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
249
202
/
/
psubw
%
xmm10
%
xmm1
.
byte
102
68
15
111
214
/
/
movdqa
%
xmm6
%
xmm10
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
68
15
56
62
210
/
/
pmaxuw
%
xmm2
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
69
15
249
194
/
/
psubw
%
xmm10
%
xmm8
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_sse41_lowp
.
globl
_sk_lighten_sse41_lowp
FUNCTION
(
_sk_lighten_sse41_lowp
)
_sk_lighten_sse41_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
68
15
213
207
/
/
pmullw
%
xmm7
%
xmm9
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
65
15
56
58
201
/
/
pminuw
%
xmm9
%
xmm1
.
byte
102
68
15
111
13
99
145
0
0
/
/
movdqa
0x9163
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
69
15
56
58
208
/
/
pminuw
%
xmm8
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
249
202
/
/
psubw
%
xmm10
%
xmm1
.
byte
102
68
15
111
214
/
/
movdqa
%
xmm6
%
xmm10
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
68
15
56
58
210
/
/
pminuw
%
xmm2
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
69
15
249
194
/
/
psubw
%
xmm10
%
xmm8
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_sse41_lowp
.
globl
_sk_difference_sse41_lowp
FUNCTION
(
_sk_difference_sse41_lowp
)
_sk_difference_sse41_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
68
15
213
207
/
/
pmullw
%
xmm7
%
xmm9
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
65
15
56
58
201
/
/
pminuw
%
xmm9
%
xmm1
.
byte
102
68
15
111
13
166
144
0
0
/
/
movdqa
0x90a6
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
7
/
/
psrlw
0x7
%
xmm1
.
byte
102
68
15
111
21
19
152
0
0
/
/
movdqa
0x9813
(
%
rip
)
%
xmm10
#
3d630
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13e4
>
.
byte
102
65
15
219
202
/
/
pand
%
xmm10
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
68
15
111
221
/
/
movdqa
%
xmm5
%
xmm11
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
69
15
56
58
216
/
/
pminuw
%
xmm8
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
65
15
113
211
7
/
/
psrlw
0x7
%
xmm11
.
byte
102
69
15
219
218
/
/
pand
%
xmm10
%
xmm11
.
byte
102
65
15
249
203
/
/
psubw
%
xmm11
%
xmm1
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
68
15
56
58
218
/
/
pminuw
%
xmm2
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
65
15
113
211
7
/
/
psrlw
0x7
%
xmm11
.
byte
102
69
15
219
218
/
/
pand
%
xmm10
%
xmm11
.
byte
102
69
15
249
195
/
/
psubw
%
xmm11
%
xmm8
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_sse41_lowp
.
globl
_sk_exclusion_sse41_lowp
FUNCTION
(
_sk_exclusion_sse41_lowp
)
_sk_exclusion_sse41_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
213
204
/
/
pmullw
%
xmm4
%
xmm1
.
byte
102
68
15
111
13
226
143
0
0
/
/
movdqa
0x8fe2
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
7
/
/
psrlw
0x7
%
xmm1
.
byte
102
68
15
111
21
79
151
0
0
/
/
movdqa
0x974f
(
%
rip
)
%
xmm10
#
3d630
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13e4
>
.
byte
102
65
15
219
202
/
/
pand
%
xmm10
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
7
/
/
psrlw
0x7
%
xmm8
.
byte
102
69
15
219
194
/
/
pand
%
xmm10
%
xmm8
.
byte
102
65
15
249
200
/
/
psubw
%
xmm8
%
xmm1
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
7
/
/
psrlw
0x7
%
xmm2
.
byte
102
65
15
219
210
/
/
pand
%
xmm10
%
xmm2
.
byte
102
68
15
249
194
/
/
psubw
%
xmm2
%
xmm8
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_sse41_lowp
.
globl
_sk_hardlight_sse41_lowp
FUNCTION
(
_sk_hardlight_sse41_lowp
)
_sk_hardlight_sse41_lowp
:
.
byte
102
68
15
111
231
/
/
movdqa
%
xmm7
%
xmm12
.
byte
15
41
116
36
232
/
/
movaps
%
xmm6
-
0x18
(
%
rsp
)
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
102
68
15
111
29
61
143
0
0
/
/
movdqa
0x8f3d
(
%
rip
)
%
xmm11
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
249
220
/
/
psubw
%
xmm12
%
xmm11
.
byte
102
65
15
111
227
/
/
movdqa
%
xmm11
%
xmm4
.
byte
102
65
15
213
224
/
/
pmullw
%
xmm8
%
xmm4
.
byte
102
68
15
111
251
/
/
movdqa
%
xmm3
%
xmm15
.
byte
102
69
15
249
248
/
/
psubw
%
xmm8
%
xmm15
.
byte
102
69
15
253
192
/
/
paddw
%
xmm8
%
xmm8
.
byte
102
68
15
111
13
166
150
0
0
/
/
movdqa
0x96a6
(
%
rip
)
%
xmm9
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
68
15
111
243
/
/
movdqa
%
xmm3
%
xmm14
.
byte
102
69
15
239
241
/
/
pxor
%
xmm9
%
xmm14
.
byte
102
65
15
111
196
/
/
movdqa
%
xmm12
%
xmm0
.
byte
102
69
15
111
212
/
/
movdqa
%
xmm12
%
xmm10
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
15
249
197
/
/
psubw
%
xmm5
%
xmm0
.
byte
102
65
15
213
199
/
/
pmullw
%
xmm15
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
69
15
111
250
/
/
movdqa
%
xmm10
%
xmm15
.
byte
102
68
15
249
248
/
/
psubw
%
xmm0
%
xmm15
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
65
15
239
193
/
/
pxor
%
xmm9
%
xmm0
.
byte
102
65
15
101
198
/
/
pcmpgtw
%
xmm14
%
xmm0
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
69
15
56
16
199
/
/
pblendvb
%
xmm0
%
xmm15
%
xmm8
.
byte
102
65
15
111
196
/
/
movdqa
%
xmm12
%
xmm0
.
byte
102
68
15
111
251
/
/
movdqa
%
xmm3
%
xmm15
.
byte
102
68
15
249
249
/
/
psubw
%
xmm1
%
xmm15
.
byte
102
15
249
198
/
/
psubw
%
xmm6
%
xmm0
.
byte
102
65
15
213
199
/
/
pmullw
%
xmm15
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
68
15
249
232
/
/
psubw
%
xmm0
%
xmm13
.
byte
102
69
15
111
251
/
/
movdqa
%
xmm11
%
xmm15
.
byte
102
68
15
213
249
/
/
pmullw
%
xmm1
%
xmm15
.
byte
102
15
253
201
/
/
paddw
%
xmm1
%
xmm1
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
239
193
/
/
pxor
%
xmm9
%
xmm0
.
byte
102
65
15
101
198
/
/
pcmpgtw
%
xmm14
%
xmm0
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
65
15
56
16
205
/
/
pblendvb
%
xmm0
%
xmm13
%
xmm1
.
byte
102
68
15
213
218
/
/
pmullw
%
xmm2
%
xmm11
.
byte
102
68
15
111
235
/
/
movdqa
%
xmm3
%
xmm13
.
byte
102
68
15
249
234
/
/
psubw
%
xmm2
%
xmm13
.
byte
102
15
253
210
/
/
paddw
%
xmm2
%
xmm2
.
byte
102
68
15
239
202
/
/
pxor
%
xmm2
%
xmm9
.
byte
102
69
15
101
206
/
/
pcmpgtw
%
xmm14
%
xmm9
.
byte
102
15
111
68
36
232
/
/
movdqa
-
0x18
(
%
rsp
)
%
xmm0
.
byte
102
15
249
248
/
/
psubw
%
xmm0
%
xmm7
.
byte
102
65
15
213
253
/
/
pmullw
%
xmm13
%
xmm7
.
byte
102
15
253
255
/
/
paddw
%
xmm7
%
xmm7
.
byte
102
68
15
249
215
/
/
psubw
%
xmm7
%
xmm10
.
byte
102
15
213
208
/
/
pmullw
%
xmm0
%
xmm2
.
byte
102
68
15
111
232
/
/
movdqa
%
xmm0
%
xmm13
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
56
16
210
/
/
pblendvb
%
xmm0
%
xmm10
%
xmm2
.
byte
102
68
15
111
13
46
142
0
0
/
/
movdqa
0x8e2e
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
249
195
/
/
psubw
%
xmm3
%
xmm0
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
213
253
/
/
pmullw
%
xmm5
%
xmm7
.
byte
102
15
253
252
/
/
paddw
%
xmm4
%
xmm7
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
68
15
253
199
/
/
paddw
%
xmm7
%
xmm8
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
213
230
/
/
pmullw
%
xmm6
%
xmm4
.
byte
102
65
15
253
231
/
/
paddw
%
xmm15
%
xmm4
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
253
204
/
/
paddw
%
xmm4
%
xmm1
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
65
15
213
229
/
/
pmullw
%
xmm13
%
xmm4
.
byte
102
65
15
253
227
/
/
paddw
%
xmm11
%
xmm4
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
253
212
/
/
paddw
%
xmm4
%
xmm2
.
byte
102
65
15
213
196
/
/
pmullw
%
xmm12
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
252
/
/
movdqa
%
xmm12
%
xmm7
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
65
15
111
245
/
/
movdqa
%
xmm13
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_sse41_lowp
.
globl
_sk_overlay_sse41_lowp
FUNCTION
(
_sk_overlay_sse41_lowp
)
_sk_overlay_sse41_lowp
:
.
byte
102
68
15
111
231
/
/
movdqa
%
xmm7
%
xmm12
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
102
68
15
111
29
145
141
0
0
/
/
movdqa
0x8d91
(
%
rip
)
%
xmm11
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
249
220
/
/
psubw
%
xmm12
%
xmm11
.
byte
102
65
15
111
195
/
/
movdqa
%
xmm11
%
xmm0
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
15
127
68
36
232
/
/
movdqa
%
xmm0
-
0x18
(
%
rsp
)
.
byte
102
68
15
111
243
/
/
movdqa
%
xmm3
%
xmm14
.
byte
102
69
15
249
240
/
/
psubw
%
xmm8
%
xmm14
.
byte
102
68
15
213
196
/
/
pmullw
%
xmm4
%
xmm8
.
byte
102
69
15
111
252
/
/
movdqa
%
xmm12
%
xmm15
.
byte
102
68
15
249
252
/
/
psubw
%
xmm4
%
xmm15
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
68
15
111
13
226
148
0
0
/
/
movdqa
0x94e2
(
%
rip
)
%
xmm9
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
69
15
111
212
/
/
movdqa
%
xmm12
%
xmm10
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
69
15
213
254
/
/
pmullw
%
xmm14
%
xmm15
.
byte
102
69
15
253
255
/
/
paddw
%
xmm15
%
xmm15
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
69
15
249
239
/
/
psubw
%
xmm15
%
xmm13
.
byte
102
69
15
111
244
/
/
movdqa
%
xmm12
%
xmm14
.
byte
102
69
15
239
241
/
/
pxor
%
xmm9
%
xmm14
.
byte
102
65
15
239
193
/
/
pxor
%
xmm9
%
xmm0
.
byte
102
65
15
101
198
/
/
pcmpgtw
%
xmm14
%
xmm0
.
byte
102
69
15
253
192
/
/
paddw
%
xmm8
%
xmm8
.
byte
102
69
15
56
16
197
/
/
pblendvb
%
xmm0
%
xmm13
%
xmm8
.
byte
102
65
15
111
196
/
/
movdqa
%
xmm12
%
xmm0
.
byte
102
68
15
111
235
/
/
movdqa
%
xmm3
%
xmm13
.
byte
102
68
15
249
233
/
/
psubw
%
xmm1
%
xmm13
.
byte
102
15
249
197
/
/
psubw
%
xmm5
%
xmm0
.
byte
102
65
15
213
197
/
/
pmullw
%
xmm13
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
68
15
249
232
/
/
psubw
%
xmm0
%
xmm13
.
byte
102
69
15
111
251
/
/
movdqa
%
xmm11
%
xmm15
.
byte
102
68
15
213
249
/
/
pmullw
%
xmm1
%
xmm15
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
65
15
239
193
/
/
pxor
%
xmm9
%
xmm0
.
byte
102
65
15
101
198
/
/
pcmpgtw
%
xmm14
%
xmm0
.
byte
102
15
253
201
/
/
paddw
%
xmm1
%
xmm1
.
byte
102
65
15
56
16
205
/
/
pblendvb
%
xmm0
%
xmm13
%
xmm1
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
68
15
239
200
/
/
pxor
%
xmm0
%
xmm9
.
byte
102
65
15
111
196
/
/
movdqa
%
xmm12
%
xmm0
.
byte
102
15
249
198
/
/
psubw
%
xmm6
%
xmm0
.
byte
102
69
15
101
206
/
/
pcmpgtw
%
xmm14
%
xmm9
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
102
15
249
250
/
/
psubw
%
xmm2
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
68
15
249
208
/
/
psubw
%
xmm0
%
xmm10
.
byte
102
68
15
213
218
/
/
pmullw
%
xmm2
%
xmm11
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
15
253
210
/
/
paddw
%
xmm2
%
xmm2
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
56
16
210
/
/
pblendvb
%
xmm0
%
xmm10
%
xmm2
.
byte
102
68
15
111
13
116
140
0
0
/
/
movdqa
0x8c74
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
249
195
/
/
psubw
%
xmm3
%
xmm0
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
213
252
/
/
pmullw
%
xmm4
%
xmm7
.
byte
102
15
253
124
36
232
/
/
paddw
-
0x18
(
%
rsp
)
%
xmm7
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
68
15
253
199
/
/
paddw
%
xmm7
%
xmm8
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
213
253
/
/
pmullw
%
xmm5
%
xmm7
.
byte
102
65
15
253
255
/
/
paddw
%
xmm15
%
xmm7
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
213
254
/
/
pmullw
%
xmm6
%
xmm7
.
byte
102
65
15
253
251
/
/
paddw
%
xmm11
%
xmm7
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
65
15
213
196
/
/
pmullw
%
xmm12
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
252
/
/
movdqa
%
xmm12
%
xmm7
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_8888_sse41_lowp
.
globl
_sk_load_8888_sse41_lowp
FUNCTION
(
_sk_load_8888_sse41_lowp
)
_sk_load_8888_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
342fe
<
_sk_load_8888_sse41_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
240
0
0
0
/
/
lea
0xf0
(
%
rip
)
%
r9
#
343d4
<
_sk_load_8888_sse41_lowp
+
0x11a
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
110
20
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm2
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
89
/
/
jmp
34357
<
_sk_load_8888_sse41_lowp
+
0x9d
>
.
byte
243
65
15
111
20
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm2
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
74
/
/
jmp
34357
<
_sk_load_8888_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
243
65
15
126
20
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm2
.
byte
102
65
15
58
14
208
240
/
/
pblendw
0xf0
%
xmm8
%
xmm2
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
235
37
/
/
jmp
34357
<
_sk_load_8888_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
68
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
102
69
15
58
34
68
144
20
1
/
/
pinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
69
15
58
34
68
144
16
0
/
/
pinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
243
65
15
111
20
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm2
.
byte
102
15
111
5
241
146
0
0
/
/
movdqa
0x92f1
(
%
rip
)
%
xmm0
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
56
0
200
/
/
pshufb
%
xmm0
%
xmm1
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
56
0
216
/
/
pshufb
%
xmm0
%
xmm3
.
byte
102
15
108
203
/
/
punpcklqdq
%
xmm3
%
xmm1
.
byte
102
68
15
111
13
49
139
0
0
/
/
movdqa
0x8b31
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
68
15
111
21
202
146
0
0
/
/
movdqa
0x92ca
(
%
rip
)
%
xmm10
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
65
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm3
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
102
65
15
108
219
/
/
punpcklqdq
%
xmm11
%
xmm3
.
byte
102
68
15
111
21
183
146
0
0
/
/
movdqa
0x92b7
(
%
rip
)
%
xmm10
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
210
/
/
pshufb
%
xmm10
%
xmm2
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
65
15
108
208
/
/
punpcklqdq
%
xmm8
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
29
255
255
255
74
/
/
sbb
0x4affffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
57
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
125
255
/
/
jge
343e1
<
_sk_load_8888_sse41_lowp
+
0x127
>
.
byte
255
/
/
(
bad
)
.
byte
255
116
255
255
/
/
pushq
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
107
255
/
/
ljmp
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
94
255
/
/
lcall
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_dst_sse41_lowp
.
globl
_sk_load_8888_dst_sse41_lowp
FUNCTION
(
_sk_load_8888_dst_sse41_lowp
)
_sk_load_8888_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
34434
<
_sk_load_8888_dst_sse41_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
242
0
0
0
/
/
lea
0xf2
(
%
rip
)
%
r9
#
3450c
<
_sk_load_8888_dst_sse41_lowp
+
0x11c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
110
52
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
89
/
/
jmp
3448d
<
_sk_load_8888_dst_sse41_lowp
+
0x9d
>
.
byte
243
65
15
111
52
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
74
/
/
jmp
3448d
<
_sk_load_8888_dst_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
243
65
15
126
52
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
102
65
15
58
14
240
240
/
/
pblendw
0xf0
%
xmm8
%
xmm6
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
235
37
/
/
jmp
3448d
<
_sk_load_8888_dst_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
69
15
58
34
68
144
20
1
/
/
pinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
69
15
58
34
68
144
16
0
/
/
pinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
243
65
15
111
52
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
102
15
111
37
187
145
0
0
/
/
movdqa
0x91bb
(
%
rip
)
%
xmm4
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
56
0
236
/
/
pshufb
%
xmm4
%
xmm5
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
56
0
252
/
/
pshufb
%
xmm4
%
xmm7
.
byte
102
15
108
239
/
/
punpcklqdq
%
xmm7
%
xmm5
.
byte
102
68
15
111
13
251
137
0
0
/
/
movdqa
0x89fb
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
65
15
219
225
/
/
pand
%
xmm9
%
xmm4
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
68
15
111
21
148
145
0
0
/
/
movdqa
0x9194
(
%
rip
)
%
xmm10
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
65
15
56
0
250
/
/
pshufb
%
xmm10
%
xmm7
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
102
65
15
108
251
/
/
punpcklqdq
%
xmm11
%
xmm7
.
byte
102
68
15
111
21
129
145
0
0
/
/
movdqa
0x9181
(
%
rip
)
%
xmm10
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
242
/
/
pshufb
%
xmm10
%
xmm6
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
65
15
108
240
/
/
punpcklqdq
%
xmm8
%
xmm6
.
byte
102
65
15
219
241
/
/
pand
%
xmm9
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
27
255
/
/
sbb
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
72
255
/
/
decl
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
55
/
/
pushq
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
123
255
/
/
jnp
34519
<
_sk_load_8888_dst_sse41_lowp
+
0x129
>
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
105
255
/
/
ljmp
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_8888_sse41_lowp
.
globl
_sk_store_8888_sse41_lowp
FUNCTION
(
_sk_store_8888_sse41_lowp
)
_sk_store_8888_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
65
15
113
240
8
/
/
psllw
0x8
%
xmm8
.
byte
102
68
15
235
192
/
/
por
%
xmm0
%
xmm8
.
byte
102
69
15
112
200
78
/
/
pshufd
0x4e
%
xmm8
%
xmm9
.
byte
102
69
15
56
51
209
/
/
pmovzxwd
%
xmm9
%
xmm10
.
byte
102
69
15
56
51
216
/
/
pmovzxwd
%
xmm8
%
xmm11
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
65
15
113
244
8
/
/
psllw
0x8
%
xmm12
.
byte
102
68
15
235
226
/
/
por
%
xmm2
%
xmm12
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
69
15
105
204
/
/
punpckhwd
%
xmm12
%
xmm9
.
byte
102
69
15
235
202
/
/
por
%
xmm10
%
xmm9
.
byte
102
69
15
97
196
/
/
punpcklwd
%
xmm12
%
xmm8
.
byte
102
69
15
235
195
/
/
por
%
xmm11
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
27
/
/
ja
345ae
<
_sk_store_8888_sse41_lowp
+
0x86
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
87
0
0
0
/
/
lea
0x57
(
%
rip
)
%
r9
#
345f4
<
_sk_store_8888_sse41_lowp
+
0xcc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
63
/
/
jmp
345ed
<
_sk_store_8888_sse41_lowp
+
0xc5
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
76
144
16
/
/
movdqu
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
235
48
/
/
jmp
345ed
<
_sk_store_8888_sse41_lowp
+
0xc5
>
.
byte
102
69
15
58
22
68
144
8
2
/
/
pextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
31
/
/
jmp
345ed
<
_sk_store_8888_sse41_lowp
+
0xc5
>
.
byte
102
69
15
58
22
76
144
24
2
/
/
pextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
102
69
15
58
22
76
144
20
1
/
/
pextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
102
69
15
126
76
144
16
/
/
movd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
178
255
/
/
mov
0xff
%
dl
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
243
/
/
push
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
236
/
/
in
(
%
dx
)
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
227
/
/
jmpq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_sse41_lowp
.
globl
_sk_load_bgra_sse41_lowp
FUNCTION
(
_sk_load_bgra_sse41_lowp
)
_sk_load_bgra_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
34654
<
_sk_load_bgra_sse41_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
242
0
0
0
/
/
lea
0xf2
(
%
rip
)
%
r9
#
3472c
<
_sk_load_bgra_sse41_lowp
+
0x11c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
110
4
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
89
/
/
jmp
346ad
<
_sk_load_bgra_sse41_lowp
+
0x9d
>
.
byte
243
65
15
111
4
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
74
/
/
jmp
346ad
<
_sk_load_bgra_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
243
65
15
126
4
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
65
15
58
14
192
240
/
/
pblendw
0xf0
%
xmm8
%
xmm0
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
235
37
/
/
jmp
346ad
<
_sk_load_bgra_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
68
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
102
69
15
58
34
68
144
20
1
/
/
pinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
69
15
58
34
68
144
16
0
/
/
pinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
243
65
15
111
4
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
15
111
21
155
143
0
0
/
/
movdqa
0x8f9b
(
%
rip
)
%
xmm2
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
56
0
202
/
/
pshufb
%
xmm2
%
xmm1
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
56
0
218
/
/
pshufb
%
xmm2
%
xmm3
.
byte
102
15
108
203
/
/
punpcklqdq
%
xmm3
%
xmm1
.
byte
102
68
15
111
13
219
135
0
0
/
/
movdqa
0x87db
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
68
15
111
21
116
143
0
0
/
/
movdqa
0x8f74
(
%
rip
)
%
xmm10
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
65
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm3
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
102
65
15
108
219
/
/
punpcklqdq
%
xmm11
%
xmm3
.
byte
102
68
15
111
21
97
143
0
0
/
/
movdqa
0x8f61
(
%
rip
)
%
xmm10
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm0
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
65
15
108
192
/
/
punpcklqdq
%
xmm8
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
27
255
/
/
sbb
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
72
255
/
/
decl
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
55
/
/
pushq
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
123
255
/
/
jnp
34739
<
_sk_load_bgra_sse41_lowp
+
0x129
>
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
105
255
/
/
ljmp
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_dst_sse41_lowp
.
globl
_sk_load_bgra_dst_sse41_lowp
FUNCTION
(
_sk_load_bgra_dst_sse41_lowp
)
_sk_load_bgra_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
3478c
<
_sk_load_bgra_dst_sse41_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
242
0
0
0
/
/
lea
0xf2
(
%
rip
)
%
r9
#
34864
<
_sk_load_bgra_dst_sse41_lowp
+
0x11c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
110
36
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
89
/
/
jmp
347e5
<
_sk_load_bgra_dst_sse41_lowp
+
0x9d
>
.
byte
243
65
15
111
36
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
74
/
/
jmp
347e5
<
_sk_load_bgra_dst_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
243
65
15
126
36
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
65
15
58
14
224
240
/
/
pblendw
0xf0
%
xmm8
%
xmm4
.
byte
102
68
15
111
197
/
/
movdqa
%
xmm5
%
xmm8
.
byte
235
37
/
/
jmp
347e5
<
_sk_load_bgra_dst_sse41_lowp
+
0x9d
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
69
15
58
34
68
144
20
1
/
/
pinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
69
15
58
34
68
144
16
0
/
/
pinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
243
65
15
111
36
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
111
53
99
142
0
0
/
/
movdqa
0x8e63
(
%
rip
)
%
xmm6
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
56
0
238
/
/
pshufb
%
xmm6
%
xmm5
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
56
0
254
/
/
pshufb
%
xmm6
%
xmm7
.
byte
102
15
108
239
/
/
punpcklqdq
%
xmm7
%
xmm5
.
byte
102
68
15
111
13
163
134
0
0
/
/
movdqa
0x86a3
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
65
15
219
241
/
/
pand
%
xmm9
%
xmm6
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
68
15
111
21
60
142
0
0
/
/
movdqa
0x8e3c
(
%
rip
)
%
xmm10
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
65
15
56
0
250
/
/
pshufb
%
xmm10
%
xmm7
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
102
65
15
108
251
/
/
punpcklqdq
%
xmm11
%
xmm7
.
byte
102
68
15
111
21
41
142
0
0
/
/
movdqa
0x8e29
(
%
rip
)
%
xmm10
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
226
/
/
pshufb
%
xmm10
%
xmm4
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
65
15
108
224
/
/
punpcklqdq
%
xmm8
%
xmm4
.
byte
102
65
15
219
225
/
/
pand
%
xmm9
%
xmm4
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
27
255
/
/
sbb
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
72
255
/
/
decl
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
55
/
/
pushq
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
123
255
/
/
jnp
34871
<
_sk_load_bgra_dst_sse41_lowp
+
0x129
>
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
105
255
/
/
ljmp
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
92
255
255
/
/
lcall
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_bgra_sse41_lowp
.
globl
_sk_store_bgra_sse41_lowp
FUNCTION
(
_sk_store_bgra_sse41_lowp
)
_sk_store_bgra_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
65
15
113
240
8
/
/
psllw
0x8
%
xmm8
.
byte
102
68
15
235
194
/
/
por
%
xmm2
%
xmm8
.
byte
102
69
15
112
200
78
/
/
pshufd
0x4e
%
xmm8
%
xmm9
.
byte
102
69
15
56
51
209
/
/
pmovzxwd
%
xmm9
%
xmm10
.
byte
102
69
15
56
51
216
/
/
pmovzxwd
%
xmm8
%
xmm11
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
65
15
113
244
8
/
/
psllw
0x8
%
xmm12
.
byte
102
68
15
235
224
/
/
por
%
xmm0
%
xmm12
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
69
15
105
204
/
/
punpckhwd
%
xmm12
%
xmm9
.
byte
102
69
15
235
202
/
/
por
%
xmm10
%
xmm9
.
byte
102
69
15
97
196
/
/
punpcklwd
%
xmm12
%
xmm8
.
byte
102
69
15
235
195
/
/
por
%
xmm11
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
27
/
/
ja
34906
<
_sk_store_bgra_sse41_lowp
+
0x86
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
87
0
0
0
/
/
lea
0x57
(
%
rip
)
%
r9
#
3494c
<
_sk_store_bgra_sse41_lowp
+
0xcc
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
126
4
144
/
/
movd
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
63
/
/
jmp
34945
<
_sk_store_bgra_sse41_lowp
+
0xc5
>
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
76
144
16
/
/
movdqu
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
235
48
/
/
jmp
34945
<
_sk_store_bgra_sse41_lowp
+
0xc5
>
.
byte
102
69
15
58
22
68
144
8
2
/
/
pextrd
0x2
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
4
144
/
/
movq
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
235
31
/
/
jmp
34945
<
_sk_store_bgra_sse41_lowp
+
0xc5
>
.
byte
102
69
15
58
22
76
144
24
2
/
/
pextrd
0x2
%
xmm9
0x18
(
%
r8
%
rdx
4
)
.
byte
102
69
15
58
22
76
144
20
1
/
/
pextrd
0x1
%
xmm9
0x14
(
%
r8
%
rdx
4
)
.
byte
102
69
15
126
76
144
16
/
/
movd
%
xmm9
0x10
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
4
144
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
178
255
/
/
mov
0xff
%
dl
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
243
/
/
push
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
236
/
/
in
(
%
dx
)
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
227
/
/
jmpq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
218
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_8888_sse41_lowp
.
globl
_sk_gather_8888_sse41_lowp
FUNCTION
(
_sk_gather_8888_sse41_lowp
)
_sk_gather_8888_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
15
56
64
211
/
/
pmulld
%
xmm3
%
xmm2
.
byte
102
68
15
56
64
195
/
/
pmulld
%
xmm3
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
72
15
58
22
195
1
/
/
pextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
67
139
44
184
/
/
mov
(
%
r8
%
r15
4
)
%
ebp
.
byte
71
139
52
176
/
/
mov
(
%
r8
%
r14
4
)
%
r14d
.
byte
65
139
28
152
/
/
mov
(
%
r8
%
rbx
4
)
%
ebx
.
byte
71
139
28
152
/
/
mov
(
%
r8
%
r11
4
)
%
r11d
.
byte
71
139
20
144
/
/
mov
(
%
r8
%
r10
4
)
%
r10d
.
byte
71
139
12
136
/
/
mov
(
%
r8
%
r9
4
)
%
r9d
.
byte
65
139
4
128
/
/
mov
(
%
r8
%
rax
4
)
%
eax
.
byte
102
69
15
110
195
/
/
movd
%
r11d
%
xmm8
.
byte
102
69
15
58
34
194
1
/
/
pinsrd
0x1
%
r10d
%
xmm8
.
byte
102
69
15
58
34
193
2
/
/
pinsrd
0x2
%
r9d
%
xmm8
.
byte
102
68
15
58
34
192
3
/
/
pinsrd
0x3
%
eax
%
xmm8
.
byte
102
67
15
110
12
160
/
/
movd
(
%
r8
%
r12
4
)
%
xmm1
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
58
34
213
1
/
/
pinsrd
0x1
%
ebp
%
xmm2
.
byte
102
65
15
58
34
214
2
/
/
pinsrd
0x2
%
r14d
%
xmm2
.
byte
102
15
58
34
211
3
/
/
pinsrd
0x3
%
ebx
%
xmm2
.
byte
102
15
196
205
1
/
/
pinsrw
0x1
%
ebp
%
xmm1
.
byte
102
65
15
196
206
2
/
/
pinsrw
0x2
%
r14d
%
xmm1
.
byte
102
15
196
203
3
/
/
pinsrw
0x3
%
ebx
%
xmm1
.
byte
102
65
15
196
203
4
/
/
pinsrw
0x4
%
r11d
%
xmm1
.
byte
102
65
15
196
202
5
/
/
pinsrw
0x5
%
r10d
%
xmm1
.
byte
102
65
15
196
201
6
/
/
pinsrw
0x6
%
r9d
%
xmm1
.
byte
102
15
196
200
7
/
/
pinsrw
0x7
%
eax
%
xmm1
.
byte
102
68
15
111
13
12
132
0
0
/
/
movdqa
0x840c
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
68
15
111
21
165
139
0
0
/
/
movdqa
0x8ba5
(
%
rip
)
%
xmm10
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
65
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm3
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
102
65
15
108
219
/
/
punpcklqdq
%
xmm11
%
xmm3
.
byte
102
68
15
111
21
146
139
0
0
/
/
movdqa
0x8b92
(
%
rip
)
%
xmm10
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
210
/
/
pshufb
%
xmm10
%
xmm2
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
65
15
108
208
/
/
punpcklqdq
%
xmm8
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gather_bgra_sse41_lowp
.
globl
_sk_gather_bgra_sse41_lowp
FUNCTION
(
_sk_gather_bgra_sse41_lowp
)
_sk_gather_bgra_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
15
56
64
211
/
/
pmulld
%
xmm3
%
xmm2
.
byte
102
68
15
56
64
195
/
/
pmulld
%
xmm3
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
72
15
58
22
195
1
/
/
pextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
67
139
44
184
/
/
mov
(
%
r8
%
r15
4
)
%
ebp
.
byte
71
139
52
176
/
/
mov
(
%
r8
%
r14
4
)
%
r14d
.
byte
65
139
28
152
/
/
mov
(
%
r8
%
rbx
4
)
%
ebx
.
byte
71
139
28
152
/
/
mov
(
%
r8
%
r11
4
)
%
r11d
.
byte
71
139
20
144
/
/
mov
(
%
r8
%
r10
4
)
%
r10d
.
byte
71
139
12
136
/
/
mov
(
%
r8
%
r9
4
)
%
r9d
.
byte
65
139
4
128
/
/
mov
(
%
r8
%
rax
4
)
%
eax
.
byte
102
69
15
110
195
/
/
movd
%
r11d
%
xmm8
.
byte
102
69
15
58
34
194
1
/
/
pinsrd
0x1
%
r10d
%
xmm8
.
byte
102
69
15
58
34
193
2
/
/
pinsrd
0x2
%
r9d
%
xmm8
.
byte
102
68
15
58
34
192
3
/
/
pinsrd
0x3
%
eax
%
xmm8
.
byte
102
67
15
110
12
160
/
/
movd
(
%
r8
%
r12
4
)
%
xmm1
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
58
34
197
1
/
/
pinsrd
0x1
%
ebp
%
xmm0
.
byte
102
65
15
58
34
198
2
/
/
pinsrd
0x2
%
r14d
%
xmm0
.
byte
102
15
58
34
195
3
/
/
pinsrd
0x3
%
ebx
%
xmm0
.
byte
102
15
196
205
1
/
/
pinsrw
0x1
%
ebp
%
xmm1
.
byte
102
65
15
196
206
2
/
/
pinsrw
0x2
%
r14d
%
xmm1
.
byte
102
15
196
203
3
/
/
pinsrw
0x3
%
ebx
%
xmm1
.
byte
102
65
15
196
203
4
/
/
pinsrw
0x4
%
r11d
%
xmm1
.
byte
102
65
15
196
202
5
/
/
pinsrw
0x5
%
r10d
%
xmm1
.
byte
102
65
15
196
201
6
/
/
pinsrw
0x6
%
r9d
%
xmm1
.
byte
102
15
196
200
7
/
/
pinsrw
0x7
%
eax
%
xmm1
.
byte
102
68
15
111
13
116
130
0
0
/
/
movdqa
0x8274
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
68
15
111
21
13
138
0
0
/
/
movdqa
0x8a0d
(
%
rip
)
%
xmm10
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
65
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm3
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
102
65
15
108
219
/
/
punpcklqdq
%
xmm11
%
xmm3
.
byte
102
68
15
111
21
250
137
0
0
/
/
movdqa
0x89fa
(
%
rip
)
%
xmm10
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm0
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
65
15
108
192
/
/
punpcklqdq
%
xmm8
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_sse41_lowp
.
globl
_sk_load_565_sse41_lowp
FUNCTION
(
_sk_load_565_sse41_lowp
)
_sk_load_565_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
34cd4
<
_sk_load_565_sse41_lowp
+
0x3c
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
196
0
0
0
/
/
lea
0xc4
(
%
rip
)
%
r9
#
34d84
<
_sk_load_565_sse41_lowp
+
0xec
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
74
/
/
jmp
34d1e
<
_sk_load_565_sse41_lowp
+
0x86
>
.
byte
243
65
15
111
4
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
235
66
/
/
jmp
34d1e
<
_sk_load_565_sse41_lowp
+
0x86
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
65
15
196
76
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
102
65
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
15
58
14
193
252
/
/
pblendw
0xfc
%
xmm1
%
xmm0
.
byte
235
40
/
/
jmp
34d1e
<
_sk_load_565_sse41_lowp
+
0x86
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
65
15
196
76
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
102
65
15
196
76
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
102
65
15
196
76
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
243
65
15
126
4
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
15
58
14
193
240
/
/
pblendw
0xf0
%
xmm1
%
xmm0
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
219
29
81
137
0
0
/
/
pand
0x8951
(
%
rip
)
%
xmm3
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
113
209
5
/
/
psrlw
0x5
%
xmm1
.
byte
102
15
219
13
80
137
0
0
/
/
pand
0x8950
(
%
rip
)
%
xmm1
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
15
111
21
88
137
0
0
/
/
movdqa
0x8958
(
%
rip
)
%
xmm2
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
113
208
13
/
/
psrlw
0xd
%
xmm0
.
byte
102
15
235
195
/
/
por
%
xmm3
%
xmm0
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
15
113
243
2
/
/
psllw
0x2
%
xmm3
.
byte
102
15
113
209
4
/
/
psrlw
0x4
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
113
243
3
/
/
psllw
0x3
%
xmm3
.
byte
102
15
113
210
2
/
/
psrlw
0x2
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
46
129
0
0
/
/
movaps
0x812e
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
69
255
/
/
rex
.
RB
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
100
255
255
/
/
jmpq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
88
255
/
/
lcall
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
142
255
255
255
134
/
/
decl
-
0x79000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
126
255
/
/
jle
34d99
<
_sk_load_565_sse41_lowp
+
0x101
>
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_565_dst_sse41_lowp
.
globl
_sk_load_565_dst_sse41_lowp
FUNCTION
(
_sk_load_565_dst_sse41_lowp
)
_sk_load_565_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
34ddc
<
_sk_load_565_dst_sse41_lowp
+
0x3c
>
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
196
0
0
0
/
/
lea
0xc4
(
%
rip
)
%
r9
#
34e8c
<
_sk_load_565_dst_sse41_lowp
+
0xec
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
74
/
/
jmp
34e26
<
_sk_load_565_dst_sse41_lowp
+
0x86
>
.
byte
243
65
15
111
36
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
235
66
/
/
jmp
34e26
<
_sk_load_565_dst_sse41_lowp
+
0x86
>
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
102
65
15
196
108
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
102
65
15
110
36
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
15
58
14
229
252
/
/
pblendw
0xfc
%
xmm5
%
xmm4
.
byte
235
40
/
/
jmp
34e26
<
_sk_load_565_dst_sse41_lowp
+
0x86
>
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
102
65
15
196
108
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
102
65
15
196
108
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
102
65
15
196
108
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
243
65
15
126
36
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
15
58
14
229
240
/
/
pblendw
0xf0
%
xmm5
%
xmm4
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
219
61
73
136
0
0
/
/
pand
0x8849
(
%
rip
)
%
xmm7
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
113
213
5
/
/
psrlw
0x5
%
xmm5
.
byte
102
15
219
45
72
136
0
0
/
/
pand
0x8848
(
%
rip
)
%
xmm5
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
15
111
53
80
136
0
0
/
/
movdqa
0x8850
(
%
rip
)
%
xmm6
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
15
219
244
/
/
pand
%
xmm4
%
xmm6
.
byte
102
15
113
212
13
/
/
psrlw
0xd
%
xmm4
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
102
15
111
253
/
/
movdqa
%
xmm5
%
xmm7
.
byte
102
15
113
247
2
/
/
psllw
0x2
%
xmm7
.
byte
102
15
113
213
4
/
/
psrlw
0x4
%
xmm5
.
byte
102
15
235
239
/
/
por
%
xmm7
%
xmm5
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
113
247
3
/
/
psllw
0x3
%
xmm7
.
byte
102
15
113
214
2
/
/
psrlw
0x2
%
xmm6
.
byte
102
15
235
247
/
/
por
%
xmm7
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
38
128
0
0
/
/
movaps
0x8026
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
69
255
/
/
rex
.
RB
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
100
255
255
/
/
jmpq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
88
255
/
/
lcall
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
142
255
255
255
134
/
/
decl
-
0x79000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
126
255
/
/
jle
34ea1
<
_sk_load_565_dst_sse41_lowp
+
0x101
>
.
byte
255
/
/
(
bad
)
.
byte
255
114
255
/
/
pushq
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_565_sse41_lowp
.
globl
_sk_store_565_sse41_lowp
FUNCTION
(
_sk_store_565_sse41_lowp
)
_sk_store_565_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
102
65
15
113
240
8
/
/
psllw
0x8
%
xmm8
.
byte
102
68
15
219
5
228
135
0
0
/
/
pand
0x87e4
(
%
rip
)
%
xmm8
#
3d6b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1464
>
.
byte
102
68
15
111
201
/
/
movdqa
%
xmm1
%
xmm9
.
byte
102
65
15
113
241
3
/
/
psllw
0x3
%
xmm9
.
byte
102
68
15
219
13
224
135
0
0
/
/
pand
0x87e0
(
%
rip
)
%
xmm9
#
3d6c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1474
>
.
byte
102
69
15
235
200
/
/
por
%
xmm8
%
xmm9
.
byte
102
68
15
111
194
/
/
movdqa
%
xmm2
%
xmm8
.
byte
102
65
15
113
208
3
/
/
psrlw
0x3
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
29
/
/
ja
34f1c
<
_sk_store_565_sse41_lowp
+
0x74
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
83
0
0
0
/
/
lea
0x53
(
%
rip
)
%
r9
#
34f5c
<
_sk_store_565_sse41_lowp
+
0xb4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
58
21
4
80
0
/
/
pextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
58
/
/
jmp
34f56
<
_sk_store_565_sse41_lowp
+
0xae
>
.
byte
243
69
15
127
4
80
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
50
/
/
jmp
34f56
<
_sk_store_565_sse41_lowp
+
0xae
>
.
byte
102
69
15
58
21
68
80
4
2
/
/
pextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
33
/
/
jmp
34f56
<
_sk_store_565_sse41_lowp
+
0xae
>
.
byte
102
69
15
58
21
68
80
12
6
/
/
pextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
102
69
15
58
21
68
80
10
5
/
/
pextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
102
69
15
58
21
68
80
8
4
/
/
pextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
182
255
/
/
mov
0xff
%
dh
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
200
/
/
dec
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
244
/
/
push
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
235
255
/
/
jmp
34f6d
<
_sk_store_565_sse41_lowp
+
0xc5
>
.
byte
255
/
/
(
bad
)
.
byte
255
226
/
/
jmpq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
217
255
/
/
fcos
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_565_sse41_lowp
.
globl
_sk_gather_565_sse41_lowp
FUNCTION
(
_sk_gather_565_sse41_lowp
)
_sk_gather_565_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
15
56
64
211
/
/
pmulld
%
xmm3
%
xmm2
.
byte
102
68
15
56
64
195
/
/
pmulld
%
xmm3
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
72
15
58
22
195
1
/
/
pextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
60
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
r15d
.
byte
67
15
183
44
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
ebp
.
byte
102
15
110
197
/
/
movd
%
ebp
%
xmm0
.
byte
102
65
15
196
199
1
/
/
pinsrw
0x1
%
r15d
%
xmm0
.
byte
67
15
183
44
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
ebp
.
byte
102
15
196
197
2
/
/
pinsrw
0x2
%
ebp
%
xmm0
.
byte
65
15
183
28
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
ebx
.
byte
102
15
196
195
3
/
/
pinsrw
0x3
%
ebx
%
xmm0
.
byte
67
15
183
44
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
ebp
.
byte
102
15
196
197
4
/
/
pinsrw
0x4
%
ebp
%
xmm0
.
byte
67
15
183
44
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
ebp
.
byte
102
15
196
197
5
/
/
pinsrw
0x5
%
ebp
%
xmm0
.
byte
67
15
183
44
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
ebp
.
byte
102
15
196
197
6
/
/
pinsrw
0x6
%
ebp
%
xmm0
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
102
15
196
192
7
/
/
pinsrw
0x7
%
eax
%
xmm0
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
219
29
238
133
0
0
/
/
pand
0x85ee
(
%
rip
)
%
xmm3
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
113
209
5
/
/
psrlw
0x5
%
xmm1
.
byte
102
15
219
13
237
133
0
0
/
/
pand
0x85ed
(
%
rip
)
%
xmm1
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
15
111
21
245
133
0
0
/
/
movdqa
0x85f5
(
%
rip
)
%
xmm2
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
113
208
13
/
/
psrlw
0xd
%
xmm0
.
byte
102
15
235
195
/
/
por
%
xmm3
%
xmm0
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
15
113
243
2
/
/
psllw
0x2
%
xmm3
.
byte
102
15
113
209
4
/
/
psrlw
0x4
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
113
243
3
/
/
psllw
0x3
%
xmm3
.
byte
102
15
113
210
2
/
/
psrlw
0x2
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
203
125
0
0
/
/
movaps
0x7dcb
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_4444_sse41_lowp
.
globl
_sk_load_4444_sse41_lowp
FUNCTION
(
_sk_load_4444_sse41_lowp
)
_sk_load_4444_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
35
/
/
ja
3512c
<
_sk_load_4444_sse41_lowp
+
0x3d
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
209
0
0
0
/
/
lea
0xd1
(
%
rip
)
%
r9
#
351e8
<
_sk_load_4444_sse41_lowp
+
0xf9
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
76
/
/
jmp
35178
<
_sk_load_4444_sse41_lowp
+
0x89
>
.
byte
243
69
15
111
4
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
235
68
/
/
jmp
35178
<
_sk_load_4444_sse41_lowp
+
0x89
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
69
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
68
15
58
14
192
252
/
/
pblendw
0xfc
%
xmm0
%
xmm8
.
byte
235
41
/
/
jmp
35178
<
_sk_load_4444_sse41_lowp
+
0x89
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
243
69
15
126
4
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
68
15
58
14
192
240
/
/
pblendw
0xf0
%
xmm0
%
xmm8
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
15
113
209
12
/
/
psrlw
0xc
%
xmm1
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
111
5
60
133
0
0
/
/
movdqa
0x853c
(
%
rip
)
%
xmm0
#
3d6d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1484
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
211
4
/
/
psrlw
0x4
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
102
68
15
219
192
/
/
pand
%
xmm0
%
xmm8
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
4
/
/
psllw
0x4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
4
/
/
psllw
0x4
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
242
4
/
/
psllw
0x4
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
243
4
/
/
psllw
0x4
%
xmm3
.
byte
102
65
15
235
216
/
/
por
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
56
255
/
/
cmp
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
88
255
/
/
lcall
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
76
255
255
/
/
decl
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
131
255
255
255
123
/
/
incl
0x7bffffff
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
115
255
/
/
pushq
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
103
255
/
/
jmpq
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_dst_sse41_lowp
.
globl
_sk_load_4444_dst_sse41_lowp
FUNCTION
(
_sk_load_4444_dst_sse41_lowp
)
_sk_load_4444_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
35
/
/
ja
35241
<
_sk_load_4444_dst_sse41_lowp
+
0x3d
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
208
0
0
0
/
/
lea
0xd0
(
%
rip
)
%
r9
#
352fc
<
_sk_load_4444_dst_sse41_lowp
+
0xf8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
76
/
/
jmp
3528d
<
_sk_load_4444_dst_sse41_lowp
+
0x89
>
.
byte
243
69
15
111
4
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
235
68
/
/
jmp
3528d
<
_sk_load_4444_dst_sse41_lowp
+
0x89
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
65
15
196
100
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
69
15
110
4
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
68
15
58
14
196
252
/
/
pblendw
0xfc
%
xmm4
%
xmm8
.
byte
235
41
/
/
jmp
3528d
<
_sk_load_4444_dst_sse41_lowp
+
0x89
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
65
15
196
100
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
65
15
196
100
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
65
15
196
100
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
243
69
15
126
4
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
68
15
58
14
196
240
/
/
pblendw
0xf0
%
xmm4
%
xmm8
.
byte
102
65
15
111
232
/
/
movdqa
%
xmm8
%
xmm5
.
byte
102
15
113
213
12
/
/
psrlw
0xc
%
xmm5
.
byte
102
65
15
111
240
/
/
movdqa
%
xmm8
%
xmm6
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
102
15
111
37
39
132
0
0
/
/
movdqa
0x8427
(
%
rip
)
%
xmm4
#
3d6d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1484
>
.
byte
102
15
219
244
/
/
pand
%
xmm4
%
xmm6
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
113
215
4
/
/
psrlw
0x4
%
xmm7
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
102
68
15
219
196
/
/
pand
%
xmm4
%
xmm8
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
113
244
4
/
/
psllw
0x4
%
xmm4
.
byte
102
15
235
229
/
/
por
%
xmm5
%
xmm4
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
113
245
4
/
/
psllw
0x4
%
xmm5
.
byte
102
15
235
238
/
/
por
%
xmm6
%
xmm5
.
byte
102
15
111
247
/
/
movdqa
%
xmm7
%
xmm6
.
byte
102
15
113
246
4
/
/
psllw
0x4
%
xmm6
.
byte
102
15
235
247
/
/
por
%
xmm7
%
xmm6
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
113
247
4
/
/
psllw
0x4
%
xmm7
.
byte
102
65
15
235
248
/
/
por
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
57
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
89
255
/
/
lcall
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
77
255
/
/
decl
-
0x1
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
132
255
255
255
124
255
/
/
incl
-
0x830001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
116
255
255
/
/
pushq
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
104
255
/
/
ljmp
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_4444_sse41_lowp
.
globl
_sk_store_4444_sse41_lowp
FUNCTION
(
_sk_store_4444_sse41_lowp
)
_sk_store_4444_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
102
65
15
113
240
8
/
/
psllw
0x8
%
xmm8
.
byte
102
68
15
219
5
164
131
0
0
/
/
pand
0x83a4
(
%
rip
)
%
xmm8
#
3d6e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1494
>
.
byte
102
68
15
111
201
/
/
movdqa
%
xmm1
%
xmm9
.
byte
102
65
15
113
241
4
/
/
psllw
0x4
%
xmm9
.
byte
102
68
15
219
13
160
131
0
0
/
/
pand
0x83a0
(
%
rip
)
%
xmm9
#
3d6f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14a4
>
.
byte
102
69
15
235
200
/
/
por
%
xmm8
%
xmm9
.
byte
102
68
15
111
21
162
131
0
0
/
/
movdqa
0x83a2
(
%
rip
)
%
xmm10
#
3d700
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14b4
>
.
byte
102
68
15
219
210
/
/
pand
%
xmm2
%
xmm10
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
65
15
113
208
4
/
/
psrlw
0x4
%
xmm8
.
byte
102
69
15
235
194
/
/
por
%
xmm10
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
29
/
/
ja
3539f
<
_sk_store_4444_sse41_lowp
+
0x87
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
84
0
0
0
/
/
lea
0x54
(
%
rip
)
%
r9
#
353e0
<
_sk_store_4444_sse41_lowp
+
0xc8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
58
21
4
80
0
/
/
pextrw
0x0
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
58
/
/
jmp
353d9
<
_sk_store_4444_sse41_lowp
+
0xc1
>
.
byte
243
69
15
127
4
80
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
50
/
/
jmp
353d9
<
_sk_store_4444_sse41_lowp
+
0xc1
>
.
byte
102
69
15
58
21
68
80
4
2
/
/
pextrw
0x2
%
xmm8
0x4
(
%
r8
%
rdx
2
)
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
33
/
/
jmp
353d9
<
_sk_store_4444_sse41_lowp
+
0xc1
>
.
byte
102
69
15
58
21
68
80
12
6
/
/
pextrw
0x6
%
xmm8
0xc
(
%
r8
%
rdx
2
)
.
byte
102
69
15
58
21
68
80
10
5
/
/
pextrw
0x5
%
xmm8
0xa
(
%
r8
%
rdx
2
)
.
byte
102
69
15
58
21
68
80
8
4
/
/
pextrw
0x4
%
xmm8
0x8
(
%
r8
%
rdx
2
)
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
181
255
/
/
mov
0xff
%
ch
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
199
/
/
inc
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
243
/
/
push
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
234
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
225
/
/
jmpq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_4444_sse41_lowp
.
globl
_sk_gather_4444_sse41_lowp
FUNCTION
(
_sk_gather_4444_sse41_lowp
)
_sk_gather_4444_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
15
56
64
211
/
/
pmulld
%
xmm3
%
xmm2
.
byte
102
68
15
56
64
195
/
/
pmulld
%
xmm3
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
72
15
58
22
195
1
/
/
pextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
183
60
120
/
/
movzwl
(
%
r8
%
r15
2
)
%
r15d
.
byte
67
15
183
44
96
/
/
movzwl
(
%
r8
%
r12
2
)
%
ebp
.
byte
102
68
15
110
197
/
/
movd
%
ebp
%
xmm8
.
byte
102
69
15
196
199
1
/
/
pinsrw
0x1
%
r15d
%
xmm8
.
byte
67
15
183
44
112
/
/
movzwl
(
%
r8
%
r14
2
)
%
ebp
.
byte
102
68
15
196
197
2
/
/
pinsrw
0x2
%
ebp
%
xmm8
.
byte
65
15
183
28
88
/
/
movzwl
(
%
r8
%
rbx
2
)
%
ebx
.
byte
102
68
15
196
195
3
/
/
pinsrw
0x3
%
ebx
%
xmm8
.
byte
67
15
183
44
88
/
/
movzwl
(
%
r8
%
r11
2
)
%
ebp
.
byte
102
68
15
196
197
4
/
/
pinsrw
0x4
%
ebp
%
xmm8
.
byte
67
15
183
44
80
/
/
movzwl
(
%
r8
%
r10
2
)
%
ebp
.
byte
102
68
15
196
197
5
/
/
pinsrw
0x5
%
ebp
%
xmm8
.
byte
67
15
183
44
72
/
/
movzwl
(
%
r8
%
r9
2
)
%
ebp
.
byte
102
68
15
196
197
6
/
/
pinsrw
0x6
%
ebp
%
xmm8
.
byte
65
15
183
4
64
/
/
movzwl
(
%
r8
%
rax
2
)
%
eax
.
byte
102
68
15
196
192
7
/
/
pinsrw
0x7
%
eax
%
xmm8
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
15
113
209
12
/
/
psrlw
0xc
%
xmm1
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
111
5
168
129
0
0
/
/
movdqa
0x81a8
(
%
rip
)
%
xmm0
#
3d6d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1484
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
211
4
/
/
psrlw
0x4
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
102
68
15
219
192
/
/
pand
%
xmm0
%
xmm8
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
4
/
/
psllw
0x4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
4
/
/
psllw
0x4
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
242
4
/
/
psllw
0x4
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
243
4
/
/
psllw
0x4
%
xmm3
.
byte
102
65
15
235
216
/
/
por
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_sse41_lowp
.
globl
_sk_load_a8_sse41_lowp
FUNCTION
(
_sk_load_a8_sse41_lowp
)
_sk_load_a8_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
355ba
<
_sk_load_a8_sse41_lowp
+
0x39
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
146
0
0
0
/
/
lea
0x92
(
%
rip
)
%
r9
#
35638
<
_sk_load_a8_sse41_lowp
+
0xb7
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
235
100
/
/
jmp
3561e
<
_sk_load_a8_sse41_lowp
+
0x9d
>
.
byte
102
65
15
56
48
28
16
/
/
pmovzxbw
(
%
r8
%
rdx
1
)
%
xmm3
.
byte
235
91
/
/
jmp
3561e
<
_sk_load_a8_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
192
2
/
/
pinsrw
0x2
%
eax
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
200
/
/
movd
%
eax
%
xmm1
.
byte
102
15
56
48
217
/
/
pmovzxbw
%
xmm1
%
xmm3
.
byte
102
15
58
14
216
252
/
/
pblendw
0xfc
%
xmm0
%
xmm3
.
byte
235
54
/
/
jmp
3561e
<
_sk_load_a8_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
192
6
/
/
pinsrw
0x6
%
eax
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
192
5
/
/
pinsrw
0x5
%
eax
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
192
4
/
/
pinsrw
0x4
%
eax
%
xmm0
.
byte
102
65
15
110
12
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
102
15
56
48
217
/
/
pmovzxbw
%
xmm1
%
xmm3
.
byte
102
15
58
14
216
240
/
/
pblendw
0xf0
%
xmm0
%
xmm3
.
byte
102
15
219
29
138
120
0
0
/
/
pand
0x788a
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
119
255
/
/
ja
35639
<
_sk_load_a8_sse41_lowp
+
0xb8
>
.
byte
255
/
/
(
bad
)
.
byte
255
154
255
255
255
139
/
/
lcall
*
-
0x74000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
202
/
/
dec
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
176
/
/
mov
0xb0ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_a8_dst_sse41_lowp
.
globl
_sk_load_a8_dst_sse41_lowp
FUNCTION
(
_sk_load_a8_dst_sse41_lowp
)
_sk_load_a8_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3568d
<
_sk_load_a8_dst_sse41_lowp
+
0x39
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
143
0
0
0
/
/
lea
0x8f
(
%
rip
)
%
r9
#
35708
<
_sk_load_a8_dst_sse41_lowp
+
0xb4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
248
/
/
movd
%
eax
%
xmm7
.
byte
235
100
/
/
jmp
356f1
<
_sk_load_a8_dst_sse41_lowp
+
0x9d
>
.
byte
102
65
15
56
48
60
16
/
/
pmovzxbw
(
%
r8
%
rdx
1
)
%
xmm7
.
byte
235
91
/
/
jmp
356f1
<
_sk_load_a8_dst_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
224
2
/
/
pinsrw
0x2
%
eax
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
102
15
56
48
253
/
/
pmovzxbw
%
xmm5
%
xmm7
.
byte
102
15
58
14
252
252
/
/
pblendw
0xfc
%
xmm4
%
xmm7
.
byte
235
54
/
/
jmp
356f1
<
_sk_load_a8_dst_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
224
6
/
/
pinsrw
0x6
%
eax
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
224
5
/
/
pinsrw
0x5
%
eax
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
224
4
/
/
pinsrw
0x4
%
eax
%
xmm4
.
byte
102
65
15
110
44
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
102
15
56
48
253
/
/
pmovzxbw
%
xmm5
%
xmm7
.
byte
102
15
58
14
252
240
/
/
pblendw
0xf0
%
xmm4
%
xmm7
.
byte
102
15
219
61
183
119
0
0
/
/
pand
0x77b7
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
122
255
/
/
jp
35709
<
_sk_load_a8_dst_sse41_lowp
+
0xb5
>
.
byte
255
/
/
(
bad
)
.
byte
255
157
255
255
255
142
/
/
lcall
*
-
0x71000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
205
/
/
dec
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
194
/
/
inc
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
179
255
/
/
mov
0xff
%
bl
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_a8_sse41_lowp
.
globl
_sk_store_a8_sse41_lowp
FUNCTION
(
_sk_store_a8_sse41_lowp
)
_sk_store_a8_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
29
/
/
ja
35758
<
_sk_store_a8_sse41_lowp
+
0x34
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
131
0
0
0
/
/
lea
0x83
(
%
rip
)
%
r9
#
357c8
<
_sk_store_a8_sse41_lowp
+
0xa4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
58
20
28
16
0
/
/
pextrb
0x0
%
xmm3
(
%
r8
%
rdx
1
)
.
byte
235
105
/
/
jmp
357c1
<
_sk_store_a8_sse41_lowp
+
0x9d
>
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
68
15
56
0
5
185
126
0
0
/
/
pshufb
0x7eb9
(
%
rip
)
%
xmm8
#
3d620
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13d4
>
.
byte
102
69
15
214
4
16
/
/
movq
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
82
/
/
jmp
357c1
<
_sk_store_a8_sse41_lowp
+
0x9d
>
.
byte
102
65
15
58
20
92
16
2
4
/
/
pextrb
0x4
%
xmm3
0x2
(
%
r8
%
rdx
1
)
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
68
15
56
0
5
57
119
0
0
/
/
pshufb
0x7739
(
%
rip
)
%
xmm8
#
3cec0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc74
>
.
byte
102
69
15
58
21
4
16
0
/
/
pextrw
0x0
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
48
/
/
jmp
357c1
<
_sk_store_a8_sse41_lowp
+
0x9d
>
.
byte
102
65
15
58
20
92
16
6
12
/
/
pextrb
0xc
%
xmm3
0x6
(
%
r8
%
rdx
1
)
.
byte
102
65
15
58
20
92
16
5
10
/
/
pextrb
0xa
%
xmm3
0x5
(
%
r8
%
rdx
1
)
.
byte
102
65
15
58
20
92
16
4
8
/
/
pextrb
0x8
%
xmm3
0x4
(
%
r8
%
rdx
1
)
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
68
15
56
0
5
21
119
0
0
/
/
pshufb
0x7715
(
%
rip
)
%
xmm8
#
3ced0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc84
>
.
byte
102
69
15
126
4
16
/
/
movd
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
134
255
/
/
xchg
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
176
255
255
255
167
/
/
pushq
-
0x58000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
228
/
/
jmpq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_a8_sse41_lowp
.
globl
_sk_gather_a8_sse41_lowp
FUNCTION
(
_sk_gather_a8_sse41_lowp
)
_sk_gather_a8_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
15
56
64
211
/
/
pmulld
%
xmm3
%
xmm2
.
byte
102
68
15
56
64
195
/
/
pmulld
%
xmm3
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
72
15
58
22
195
1
/
/
pextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
60
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
r15d
.
byte
67
15
182
44
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
ebp
.
byte
102
15
110
197
/
/
movd
%
ebp
%
xmm0
.
byte
102
65
15
58
32
199
1
/
/
pinsrb
0x1
%
r15d
%
xmm0
.
byte
67
15
182
44
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
ebp
.
byte
102
15
58
32
197
2
/
/
pinsrb
0x2
%
ebp
%
xmm0
.
byte
65
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
ebx
.
byte
102
15
58
32
195
3
/
/
pinsrb
0x3
%
ebx
%
xmm0
.
byte
67
15
182
44
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
ebp
.
byte
102
15
58
32
197
4
/
/
pinsrb
0x4
%
ebp
%
xmm0
.
byte
67
15
182
44
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
ebp
.
byte
102
15
58
32
197
5
/
/
pinsrb
0x5
%
ebp
%
xmm0
.
byte
67
15
182
44
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
ebp
.
byte
102
15
58
32
197
6
/
/
pinsrb
0x6
%
ebp
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
102
15
58
32
192
7
/
/
pinsrb
0x7
%
eax
%
xmm0
.
byte
102
15
56
48
216
/
/
pmovzxbw
%
xmm0
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_g8_sse41_lowp
.
globl
_sk_load_g8_sse41_lowp
FUNCTION
(
_sk_load_g8_sse41_lowp
)
_sk_load_g8_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3594a
<
_sk_load_g8_sse41_lowp
+
0x39
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
150
0
0
0
/
/
lea
0x96
(
%
rip
)
%
r9
#
359cc
<
_sk_load_g8_sse41_lowp
+
0xbb
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
100
/
/
jmp
359ae
<
_sk_load_g8_sse41_lowp
+
0x9d
>
.
byte
102
65
15
56
48
4
16
/
/
pmovzxbw
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
235
91
/
/
jmp
359ae
<
_sk_load_g8_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
196
200
2
/
/
pinsrw
0x2
%
eax
%
xmm1
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
56
48
192
/
/
pmovzxbw
%
xmm0
%
xmm0
.
byte
102
15
58
14
193
252
/
/
pblendw
0xfc
%
xmm1
%
xmm0
.
byte
235
54
/
/
jmp
359ae
<
_sk_load_g8_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
196
200
6
/
/
pinsrw
0x6
%
eax
%
xmm1
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
200
5
/
/
pinsrw
0x5
%
eax
%
xmm1
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
200
4
/
/
pinsrw
0x4
%
eax
%
xmm1
.
byte
102
65
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
102
15
56
48
192
/
/
pmovzxbw
%
xmm0
%
xmm0
.
byte
102
15
58
14
193
240
/
/
pblendw
0xf0
%
xmm1
%
xmm0
.
byte
102
15
219
5
250
116
0
0
/
/
pand
0x74fa
(
%
rip
)
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
241
116
0
0
/
/
movaps
0x74f1
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
115
255
/
/
jae
359cd
<
_sk_load_g8_sse41_lowp
+
0xbc
>
.
byte
255
/
/
(
bad
)
.
byte
255
150
255
255
255
135
/
/
callq
*
-
0x78000001
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
187
255
255
255
172
/
/
mov
0xacffffff
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_dst_sse41_lowp
.
globl
_sk_load_g8_dst_sse41_lowp
FUNCTION
(
_sk_load_g8_dst_sse41_lowp
)
_sk_load_g8_dst_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
35a21
<
_sk_load_g8_dst_sse41_lowp
+
0x39
>
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
147
0
0
0
/
/
lea
0x93
(
%
rip
)
%
r9
#
35aa0
<
_sk_load_g8_dst_sse41_lowp
+
0xb8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
100
/
/
jmp
35a85
<
_sk_load_g8_dst_sse41_lowp
+
0x9d
>
.
byte
102
65
15
56
48
36
16
/
/
pmovzxbw
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
235
91
/
/
jmp
35a85
<
_sk_load_g8_dst_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
102
15
196
232
2
/
/
pinsrw
0x2
%
eax
%
xmm5
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
56
48
228
/
/
pmovzxbw
%
xmm4
%
xmm4
.
byte
102
15
58
14
229
252
/
/
pblendw
0xfc
%
xmm5
%
xmm4
.
byte
235
54
/
/
jmp
35a85
<
_sk_load_g8_dst_sse41_lowp
+
0x9d
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
102
15
196
232
6
/
/
pinsrw
0x6
%
eax
%
xmm5
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
232
5
/
/
pinsrw
0x5
%
eax
%
xmm5
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
232
4
/
/
pinsrw
0x4
%
eax
%
xmm5
.
byte
102
65
15
110
36
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
102
15
56
48
228
/
/
pmovzxbw
%
xmm4
%
xmm4
.
byte
102
15
58
14
229
240
/
/
pblendw
0xf0
%
xmm5
%
xmm4
.
byte
102
15
219
37
35
116
0
0
/
/
pand
0x7423
(
%
rip
)
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
26
116
0
0
/
/
movaps
0x741a
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
118
255
/
/
jbe
35aa1
<
_sk_load_g8_dst_sse41_lowp
+
0xb9
>
.
byte
255
/
/
(
bad
)
.
byte
255
153
255
255
255
138
/
/
lcall
*
-
0x75000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
175
/
/
mov
0xafffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_luminance_to_alpha_sse41_lowp
.
globl
_sk_luminance_to_alpha_sse41_lowp
FUNCTION
(
_sk_luminance_to_alpha_sse41_lowp
)
_sk_luminance_to_alpha_sse41_lowp
:
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
213
5
72
124
0
0
/
/
pmullw
0x7c48
(
%
rip
)
%
xmm0
#
3d710
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14c4
>
.
byte
102
15
213
13
80
124
0
0
/
/
pmullw
0x7c50
(
%
rip
)
%
xmm1
#
3d720
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14d4
>
.
byte
102
15
253
200
/
/
paddw
%
xmm0
%
xmm1
.
byte
102
15
213
29
84
124
0
0
/
/
pmullw
0x7c54
(
%
rip
)
%
xmm3
#
3d730
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14e4
>
.
byte
102
15
253
217
/
/
paddw
%
xmm1
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gather_g8_sse41_lowp
.
globl
_sk_gather_g8_sse41_lowp
FUNCTION
(
_sk_gather_g8_sse41_lowp
)
_sk_gather_g8_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
15
56
64
211
/
/
pmulld
%
xmm3
%
xmm2
.
byte
102
68
15
56
64
195
/
/
pmulld
%
xmm3
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
72
15
58
22
200
1
/
/
pextrq
0x1
%
xmm1
%
rax
.
byte
65
137
193
/
/
mov
%
eax
%
r9d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
72
15
58
22
195
1
/
/
pextrq
0x1
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
60
56
/
/
movzbl
(
%
r8
%
r15
1
)
%
r15d
.
byte
67
15
182
44
32
/
/
movzbl
(
%
r8
%
r12
1
)
%
ebp
.
byte
102
15
110
197
/
/
movd
%
ebp
%
xmm0
.
byte
102
65
15
58
32
199
1
/
/
pinsrb
0x1
%
r15d
%
xmm0
.
byte
67
15
182
44
48
/
/
movzbl
(
%
r8
%
r14
1
)
%
ebp
.
byte
102
15
58
32
197
2
/
/
pinsrb
0x2
%
ebp
%
xmm0
.
byte
65
15
182
28
24
/
/
movzbl
(
%
r8
%
rbx
1
)
%
ebx
.
byte
102
15
58
32
195
3
/
/
pinsrb
0x3
%
ebx
%
xmm0
.
byte
67
15
182
44
24
/
/
movzbl
(
%
r8
%
r11
1
)
%
ebp
.
byte
102
15
58
32
197
4
/
/
pinsrb
0x4
%
ebp
%
xmm0
.
byte
67
15
182
44
16
/
/
movzbl
(
%
r8
%
r10
1
)
%
ebp
.
byte
102
15
58
32
197
5
/
/
pinsrb
0x5
%
ebp
%
xmm0
.
byte
67
15
182
44
8
/
/
movzbl
(
%
r8
%
r9
1
)
%
ebp
.
byte
102
15
58
32
197
6
/
/
pinsrb
0x6
%
ebp
%
xmm0
.
byte
65
15
182
4
0
/
/
movzbl
(
%
r8
%
rax
1
)
%
eax
.
byte
102
15
58
32
192
7
/
/
pinsrb
0x7
%
eax
%
xmm0
.
byte
102
15
56
48
192
/
/
pmovzxbw
%
xmm0
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
157
114
0
0
/
/
movaps
0x729d
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_sse41_lowp
.
globl
_sk_scale_1_float_sse41_lowp
FUNCTION
(
_sk_scale_1_float_sse41_lowp
)
_sk_scale_1_float_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
89
5
243
104
0
0
/
/
mulss
0x68f3
(
%
rip
)
%
xmm8
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
243
68
15
88
5
186
104
0
0
/
/
addss
0x68ba
(
%
rip
)
%
xmm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
243
65
15
44
192
/
/
cvttss2si
%
xmm8
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
242
69
15
112
192
0
/
/
pshuflw
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
80
/
/
pshufd
0x50
%
xmm8
%
xmm8
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
68
15
111
13
78
114
0
0
/
/
movdqa
0x724e
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
65
15
213
216
/
/
pmullw
%
xmm8
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_1_float_sse41_lowp
.
globl
_sk_lerp_1_float_sse41_lowp
FUNCTION
(
_sk_lerp_1_float_sse41_lowp
)
_sk_lerp_1_float_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
89
5
123
104
0
0
/
/
mulss
0x687b
(
%
rip
)
%
xmm8
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
243
68
15
88
5
66
104
0
0
/
/
addss
0x6842
(
%
rip
)
%
xmm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
243
65
15
44
192
/
/
cvttss2si
%
xmm8
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
242
69
15
112
192
0
/
/
pshuflw
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
80
/
/
pshufd
0x50
%
xmm8
%
xmm8
.
byte
102
68
15
111
13
219
113
0
0
/
/
movdqa
0x71db
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
65
15
213
216
/
/
pmullw
%
xmm8
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
69
15
249
200
/
/
psubw
%
xmm8
%
xmm9
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
196
/
/
pmullw
%
xmm4
%
xmm8
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
198
/
/
pmullw
%
xmm6
%
xmm8
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
207
/
/
pmullw
%
xmm7
%
xmm9
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_sse41_lowp
.
globl
_sk_scale_u8_sse41_lowp
FUNCTION
(
_sk_scale_u8_sse41_lowp
)
_sk_scale_u8_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
36
/
/
ja
35d8c
<
_sk_scale_u8_sse41_lowp
+
0x3b
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
221
0
0
0
/
/
lea
0xdd
(
%
rip
)
%
r9
#
35e54
<
_sk_scale_u8_sse41_lowp
+
0x103
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
111
/
/
jmp
35dfb
<
_sk_scale_u8_sse41_lowp
+
0xaa
>
.
byte
102
69
15
56
48
4
16
/
/
pmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
235
102
/
/
jmp
35dfb
<
_sk_scale_u8_sse41_lowp
+
0xaa
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
68
15
196
200
2
/
/
pinsrw
0x2
%
eax
%
xmm9
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
56
48
192
/
/
pmovzxbw
%
xmm8
%
xmm8
.
byte
102
69
15
58
14
193
252
/
/
pblendw
0xfc
%
xmm9
%
xmm8
.
byte
235
60
/
/
jmp
35dfb
<
_sk_scale_u8_sse41_lowp
+
0xaa
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
68
15
196
200
6
/
/
pinsrw
0x6
%
eax
%
xmm9
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
200
5
/
/
pinsrw
0x5
%
eax
%
xmm9
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
200
4
/
/
pinsrw
0x4
%
eax
%
xmm9
.
byte
102
69
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
69
15
56
48
192
/
/
pmovzxbw
%
xmm8
%
xmm8
.
byte
102
69
15
58
14
193
240
/
/
pblendw
0xf0
%
xmm9
%
xmm8
.
byte
102
68
15
219
5
172
112
0
0
/
/
pand
0x70ac
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
68
15
111
13
158
112
0
0
/
/
movdqa
0x709e
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
44
255
/
/
sub
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
82
255
/
/
callq
*
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
65
255
/
/
incl
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
148
255
255
255
136
255
/
/
callq
*
-
0x770001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
124
255
/
/
jl
35e69
<
_sk_scale_u8_sse41_lowp
+
0x118
>
.
byte
255
/
/
(
bad
)
.
byte
255
107
255
/
/
ljmp
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_u8_sse41_lowp
.
globl
_sk_lerp_u8_sse41_lowp
FUNCTION
(
_sk_lerp_u8_sse41_lowp
)
_sk_lerp_u8_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
36
/
/
ja
35eab
<
_sk_lerp_u8_sse41_lowp
+
0x3b
>
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
30
1
0
0
/
/
lea
0x11e
(
%
rip
)
%
r9
#
35fb4
<
_sk_lerp_u8_sse41_lowp
+
0x144
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
111
/
/
jmp
35f1a
<
_sk_lerp_u8_sse41_lowp
+
0xaa
>
.
byte
102
69
15
56
48
4
16
/
/
pmovzxbw
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
235
102
/
/
jmp
35f1a
<
_sk_lerp_u8_sse41_lowp
+
0xaa
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
68
15
196
200
2
/
/
pinsrw
0x2
%
eax
%
xmm9
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
69
15
56
48
192
/
/
pmovzxbw
%
xmm8
%
xmm8
.
byte
102
69
15
58
14
193
252
/
/
pblendw
0xfc
%
xmm9
%
xmm8
.
byte
235
60
/
/
jmp
35f1a
<
_sk_lerp_u8_sse41_lowp
+
0xaa
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
68
15
196
200
6
/
/
pinsrw
0x6
%
eax
%
xmm9
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
200
5
/
/
pinsrw
0x5
%
eax
%
xmm9
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
200
4
/
/
pinsrw
0x4
%
eax
%
xmm9
.
byte
102
69
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
69
15
56
48
192
/
/
pmovzxbw
%
xmm8
%
xmm8
.
byte
102
69
15
58
14
193
240
/
/
pblendw
0xf0
%
xmm9
%
xmm8
.
byte
102
68
15
219
5
141
111
0
0
/
/
pand
0x6f8d
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
111
21
132
111
0
0
/
/
movdqa
0x6f84
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
69
15
239
202
/
/
pxor
%
xmm10
%
xmm9
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
68
15
213
220
/
/
pmullw
%
xmm4
%
xmm11
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
65
15
253
194
/
/
paddw
%
xmm10
%
xmm0
.
byte
102
65
15
253
195
/
/
paddw
%
xmm11
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
68
15
213
221
/
/
pmullw
%
xmm5
%
xmm11
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
65
15
253
203
/
/
paddw
%
xmm11
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
68
15
213
222
/
/
pmullw
%
xmm6
%
xmm11
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
65
15
253
211
/
/
paddw
%
xmm11
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
207
/
/
pmullw
%
xmm7
%
xmm9
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
69
15
253
194
/
/
paddw
%
xmm10
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
235
254
/
/
jmp
35fb4
<
_sk_lerp_u8_sse41_lowp
+
0x144
>
.
byte
255
/
/
(
bad
)
.
byte
255
17
/
/
callq
*
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
83
255
/
/
callq
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
71
255
/
/
incl
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
59
255
/
/
cmp
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
42
/
/
ljmp
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_scale_565_sse41_lowp
.
globl
_sk_scale_565_sse41_lowp
FUNCTION
(
_sk_scale_565_sse41_lowp
)
_sk_scale_565_sse41_lowp
:
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
35
/
/
ja
36012
<
_sk_scale_565_sse41_lowp
+
0x42
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
111
1
0
0
/
/
lea
0x16f
(
%
rip
)
%
r9
#
3616c
<
_sk_scale_565_sse41_lowp
+
0x19c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
235
76
/
/
jmp
3605e
<
_sk_scale_565_sse41_lowp
+
0x8e
>
.
byte
243
69
15
111
12
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
235
68
/
/
jmp
3605e
<
_sk_scale_565_sse41_lowp
+
0x8e
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
69
15
110
12
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
102
68
15
58
14
200
252
/
/
pblendw
0xfc
%
xmm0
%
xmm9
.
byte
235
41
/
/
jmp
3605e
<
_sk_scale_565_sse41_lowp
+
0x8e
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
243
69
15
126
12
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
102
68
15
58
14
200
240
/
/
pblendw
0xf0
%
xmm0
%
xmm9
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
219
5
16
118
0
0
/
/
pand
0x7610
(
%
rip
)
%
xmm0
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
69
15
111
209
/
/
movdqa
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
5
/
/
psrlw
0x5
%
xmm10
.
byte
102
68
15
219
21
12
118
0
0
/
/
pand
0x760c
(
%
rip
)
%
xmm10
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
68
15
111
29
19
118
0
0
/
/
movdqa
0x7613
(
%
rip
)
%
xmm11
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
69
15
219
217
/
/
pand
%
xmm9
%
xmm11
.
byte
102
65
15
113
209
13
/
/
psrlw
0xd
%
xmm9
.
byte
102
68
15
235
200
/
/
por
%
xmm0
%
xmm9
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
15
113
240
2
/
/
psllw
0x2
%
xmm0
.
byte
102
65
15
113
210
4
/
/
psrlw
0x4
%
xmm10
.
byte
102
68
15
235
208
/
/
por
%
xmm0
%
xmm10
.
byte
102
65
15
111
195
/
/
movdqa
%
xmm11
%
xmm0
.
byte
102
15
113
240
3
/
/
psllw
0x3
%
xmm0
.
byte
102
65
15
113
211
2
/
/
psrlw
0x2
%
xmm11
.
byte
102
68
15
235
216
/
/
por
%
xmm0
%
xmm11
.
byte
102
15
111
5
113
117
0
0
/
/
movdqa
0x7571
(
%
rip
)
%
xmm0
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
68
15
239
224
/
/
pxor
%
xmm0
%
xmm12
.
byte
102
15
239
199
/
/
pxor
%
xmm7
%
xmm0
.
byte
102
65
15
101
196
/
/
pcmpgtw
%
xmm12
%
xmm0
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
69
15
56
58
235
/
/
pminuw
%
xmm11
%
xmm13
.
byte
102
69
15
56
58
233
/
/
pminuw
%
xmm9
%
xmm13
.
byte
102
69
15
111
226
/
/
movdqa
%
xmm10
%
xmm12
.
byte
102
69
15
56
62
227
/
/
pmaxuw
%
xmm11
%
xmm12
.
byte
102
69
15
56
62
225
/
/
pmaxuw
%
xmm9
%
xmm12
.
byte
102
69
15
56
16
229
/
/
pblendvb
%
xmm0
%
xmm13
%
xmm12
.
byte
102
69
15
213
200
/
/
pmullw
%
xmm8
%
xmm9
.
byte
102
15
111
5
153
109
0
0
/
/
movdqa
0x6d99
(
%
rip
)
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
68
15
213
209
/
/
pmullw
%
xmm1
%
xmm10
.
byte
102
68
15
253
208
/
/
paddw
%
xmm0
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
68
15
213
218
/
/
pmullw
%
xmm2
%
xmm11
.
byte
102
68
15
253
216
/
/
paddw
%
xmm0
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
68
15
213
227
/
/
pmullw
%
xmm3
%
xmm12
.
byte
102
68
15
253
224
/
/
paddw
%
xmm0
%
xmm12
.
byte
102
65
15
113
212
8
/
/
psrlw
0x8
%
xmm12
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
220
/
/
movdqa
%
xmm12
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
154
/
/
(
bad
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
186
254
255
255
174
/
/
mov
0xaefffffe
%
edx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
229
/
/
jmpq
*
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
221
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
213
/
/
callq
*
%
rbp
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_565_sse41_lowp
.
globl
_sk_lerp_565_sse41_lowp
FUNCTION
(
_sk_lerp_565_sse41_lowp
)
_sk_lerp_565_sse41_lowp
:
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
35
/
/
ja
361ca
<
_sk_lerp_565_sse41_lowp
+
0x42
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
187
1
0
0
/
/
lea
0x1bb
(
%
rip
)
%
r9
#
36370
<
_sk_lerp_565_sse41_lowp
+
0x1e8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
235
76
/
/
jmp
36216
<
_sk_lerp_565_sse41_lowp
+
0x8e
>
.
byte
243
69
15
111
12
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
235
68
/
/
jmp
36216
<
_sk_lerp_565_sse41_lowp
+
0x8e
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
69
15
110
12
80
/
/
movd
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
102
68
15
58
14
200
252
/
/
pblendw
0xfc
%
xmm0
%
xmm9
.
byte
235
41
/
/
jmp
36216
<
_sk_lerp_565_sse41_lowp
+
0x8e
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
243
69
15
126
12
80
/
/
movq
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
102
68
15
58
14
200
240
/
/
pblendw
0xf0
%
xmm0
%
xmm9
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
219
5
88
116
0
0
/
/
pand
0x7458
(
%
rip
)
%
xmm0
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
69
15
111
209
/
/
movdqa
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
5
/
/
psrlw
0x5
%
xmm10
.
byte
102
68
15
219
21
84
116
0
0
/
/
pand
0x7454
(
%
rip
)
%
xmm10
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
68
15
111
29
91
116
0
0
/
/
movdqa
0x745b
(
%
rip
)
%
xmm11
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
69
15
219
217
/
/
pand
%
xmm9
%
xmm11
.
byte
102
65
15
113
209
13
/
/
psrlw
0xd
%
xmm9
.
byte
102
68
15
235
200
/
/
por
%
xmm0
%
xmm9
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
15
113
240
2
/
/
psllw
0x2
%
xmm0
.
byte
102
65
15
113
210
4
/
/
psrlw
0x4
%
xmm10
.
byte
102
68
15
235
208
/
/
por
%
xmm0
%
xmm10
.
byte
102
65
15
111
195
/
/
movdqa
%
xmm11
%
xmm0
.
byte
102
15
113
240
3
/
/
psllw
0x3
%
xmm0
.
byte
102
65
15
113
211
2
/
/
psrlw
0x2
%
xmm11
.
byte
102
68
15
235
216
/
/
por
%
xmm0
%
xmm11
.
byte
102
15
111
5
185
115
0
0
/
/
movdqa
0x73b9
(
%
rip
)
%
xmm0
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
68
15
239
224
/
/
pxor
%
xmm0
%
xmm12
.
byte
102
15
239
199
/
/
pxor
%
xmm7
%
xmm0
.
byte
102
65
15
101
196
/
/
pcmpgtw
%
xmm12
%
xmm0
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
69
15
56
58
235
/
/
pminuw
%
xmm11
%
xmm13
.
byte
102
69
15
56
58
233
/
/
pminuw
%
xmm9
%
xmm13
.
byte
102
69
15
111
226
/
/
movdqa
%
xmm10
%
xmm12
.
byte
102
69
15
56
62
227
/
/
pmaxuw
%
xmm11
%
xmm12
.
byte
102
69
15
56
62
225
/
/
pmaxuw
%
xmm9
%
xmm12
.
byte
102
69
15
56
16
229
/
/
pblendvb
%
xmm0
%
xmm13
%
xmm12
.
byte
102
68
15
111
45
229
107
0
0
/
/
movdqa
0x6be5
(
%
rip
)
%
xmm13
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
239
197
/
/
pxor
%
xmm13
%
xmm0
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
69
15
213
200
/
/
pmullw
%
xmm8
%
xmm9
.
byte
102
69
15
253
205
/
/
paddw
%
xmm13
%
xmm9
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
65
15
239
197
/
/
pxor
%
xmm13
%
xmm0
.
byte
102
15
213
197
/
/
pmullw
%
xmm5
%
xmm0
.
byte
102
68
15
213
209
/
/
pmullw
%
xmm1
%
xmm10
.
byte
102
69
15
253
213
/
/
paddw
%
xmm13
%
xmm10
.
byte
102
68
15
253
208
/
/
paddw
%
xmm0
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
111
195
/
/
movdqa
%
xmm11
%
xmm0
.
byte
102
65
15
239
197
/
/
pxor
%
xmm13
%
xmm0
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
68
15
213
218
/
/
pmullw
%
xmm2
%
xmm11
.
byte
102
69
15
253
221
/
/
paddw
%
xmm13
%
xmm11
.
byte
102
68
15
253
216
/
/
paddw
%
xmm0
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
65
15
111
196
/
/
movdqa
%
xmm12
%
xmm0
.
byte
102
65
15
239
197
/
/
pxor
%
xmm13
%
xmm0
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
68
15
213
227
/
/
pmullw
%
xmm3
%
xmm12
.
byte
102
69
15
253
229
/
/
paddw
%
xmm13
%
xmm12
.
byte
102
68
15
253
224
/
/
paddw
%
xmm0
%
xmm12
.
byte
102
65
15
113
212
8
/
/
psrlw
0x8
%
xmm12
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
220
/
/
movdqa
%
xmm12
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
78
254
/
/
rex
.
WRX
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
110
254
/
/
ljmp
*
-
0x2
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
98
254
/
/
jmpq
*
-
0x2
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
153
254
255
255
145
/
/
lcall
*
-
0x6e000002
(
%
rcx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
137
254
255
255
125
/
/
decl
0x7dfffffe
(
%
rcx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_clamp_x_1_sse41_lowp
.
globl
_sk_clamp_x_1_sse41_lowp
FUNCTION
(
_sk_clamp_x_1_sse41_lowp
)
_sk_clamp_x_1_sse41_lowp
:
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
68
15
40
5
112
107
0
0
/
/
movaps
0x6b70
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_sse41_lowp
.
globl
_sk_repeat_x_1_sse41_lowp
FUNCTION
(
_sk_repeat_x_1_sse41_lowp
)
_sk_repeat_x_1_sse41_lowp
:
.
byte
102
68
15
58
8
192
1
/
/
roundps
0x1
%
xmm0
%
xmm8
.
byte
102
68
15
58
8
201
1
/
/
roundps
0x1
%
xmm1
%
xmm9
.
byte
65
15
92
192
/
/
subps
%
xmm8
%
xmm0
.
byte
65
15
92
201
/
/
subps
%
xmm9
%
xmm1
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
68
15
40
5
58
107
0
0
/
/
movaps
0x6b3a
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_sse41_lowp
.
globl
_sk_mirror_x_1_sse41_lowp
FUNCTION
(
_sk_mirror_x_1_sse41_lowp
)
_sk_mirror_x_1_sse41_lowp
:
.
byte
68
15
40
5
134
107
0
0
/
/
movaps
0x6b86
(
%
rip
)
%
xmm8
#
3cf70
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd24
>
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
68
15
40
13
6
107
0
0
/
/
movaps
0x6b06
(
%
rip
)
%
xmm9
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
69
15
89
209
/
/
mulps
%
xmm9
%
xmm10
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
102
69
15
58
8
201
1
/
/
roundps
0x1
%
xmm9
%
xmm9
.
byte
102
69
15
58
8
210
1
/
/
roundps
0x1
%
xmm10
%
xmm10
.
byte
69
15
88
210
/
/
addps
%
xmm10
%
xmm10
.
byte
69
15
88
201
/
/
addps
%
xmm9
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
92
202
/
/
subps
%
xmm10
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
5
60
112
0
0
/
/
movaps
0x703c
(
%
rip
)
%
xmm8
#
3d470
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1224
>
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
68
15
40
5
192
106
0
0
/
/
movaps
0x6ac0
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_sse41_lowp
.
globl
_sk_decal_x_sse41_lowp
FUNCTION
(
_sk_decal_x_sse41_lowp
)
_sk_decal_x_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
201
2
/
/
cmpleps
%
xmm1
%
xmm9
.
byte
102
68
15
111
21
220
113
0
0
/
/
movdqa
0x71dc
(
%
rip
)
%
xmm10
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
69
15
56
0
202
/
/
pshufb
%
xmm10
%
xmm9
.
byte
68
15
194
192
2
/
/
cmpleps
%
xmm0
%
xmm8
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
69
15
108
193
/
/
punpcklqdq
%
xmm9
%
xmm8
.
byte
102
65
15
113
240
15
/
/
psllw
0xf
%
xmm8
.
byte
102
65
15
113
224
15
/
/
psraw
0xf
%
xmm8
.
byte
243
68
15
16
72
64
/
/
movss
0x40
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
217
/
/
movaps
%
xmm1
%
xmm11
.
byte
69
15
194
217
1
/
/
cmpltps
%
xmm9
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
68
15
40
224
/
/
movaps
%
xmm0
%
xmm12
.
byte
69
15
194
225
1
/
/
cmpltps
%
xmm9
%
xmm12
.
byte
102
69
15
56
0
226
/
/
pshufb
%
xmm10
%
xmm12
.
byte
102
69
15
108
227
/
/
punpcklqdq
%
xmm11
%
xmm12
.
byte
102
65
15
113
244
15
/
/
psllw
0xf
%
xmm12
.
byte
102
65
15
113
228
15
/
/
psraw
0xf
%
xmm12
.
byte
102
69
15
219
224
/
/
pand
%
xmm8
%
xmm12
.
byte
243
68
15
127
32
/
/
movdqu
%
xmm12
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_sse41_lowp
.
globl
_sk_decal_y_sse41_lowp
FUNCTION
(
_sk_decal_y_sse41_lowp
)
_sk_decal_y_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
203
2
/
/
cmpleps
%
xmm3
%
xmm9
.
byte
102
68
15
111
21
90
113
0
0
/
/
movdqa
0x715a
(
%
rip
)
%
xmm10
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
69
15
56
0
202
/
/
pshufb
%
xmm10
%
xmm9
.
byte
68
15
194
194
2
/
/
cmpleps
%
xmm2
%
xmm8
.
byte
102
69
15
56
0
194
/
/
pshufb
%
xmm10
%
xmm8
.
byte
102
69
15
108
193
/
/
punpcklqdq
%
xmm9
%
xmm8
.
byte
102
65
15
113
240
15
/
/
psllw
0xf
%
xmm8
.
byte
102
65
15
113
224
15
/
/
psraw
0xf
%
xmm8
.
byte
243
68
15
16
72
68
/
/
movss
0x44
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
69
15
194
217
1
/
/
cmpltps
%
xmm9
%
xmm11
.
byte
102
69
15
56
0
218
/
/
pshufb
%
xmm10
%
xmm11
.
byte
68
15
40
226
/
/
movaps
%
xmm2
%
xmm12
.
byte
69
15
194
225
1
/
/
cmpltps
%
xmm9
%
xmm12
.
byte
102
69
15
56
0
226
/
/
pshufb
%
xmm10
%
xmm12
.
byte
102
69
15
108
227
/
/
punpcklqdq
%
xmm11
%
xmm12
.
byte
102
65
15
113
244
15
/
/
psllw
0xf
%
xmm12
.
byte
102
65
15
113
228
15
/
/
psraw
0xf
%
xmm12
.
byte
102
69
15
219
224
/
/
pand
%
xmm8
%
xmm12
.
byte
243
68
15
127
32
/
/
movdqu
%
xmm12
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_sse41_lowp
.
globl
_sk_decal_x_and_y_sse41_lowp
FUNCTION
(
_sk_decal_x_and_y_sse41_lowp
)
_sk_decal_x_and_y_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
219
/
/
xorps
%
xmm11
%
xmm11
.
byte
68
15
194
217
2
/
/
cmpleps
%
xmm1
%
xmm11
.
byte
102
68
15
111
13
216
112
0
0
/
/
movdqa
0x70d8
(
%
rip
)
%
xmm9
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
69
15
56
0
217
/
/
pshufb
%
xmm9
%
xmm11
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
68
15
194
208
2
/
/
cmpleps
%
xmm0
%
xmm10
.
byte
102
69
15
56
0
209
/
/
pshufb
%
xmm9
%
xmm10
.
byte
102
69
15
108
211
/
/
punpcklqdq
%
xmm11
%
xmm10
.
byte
102
65
15
113
242
15
/
/
psllw
0xf
%
xmm10
.
byte
102
65
15
113
226
15
/
/
psraw
0xf
%
xmm10
.
byte
243
68
15
16
96
64
/
/
movss
0x40
(
%
rax
)
%
xmm12
.
byte
243
68
15
16
88
68
/
/
movss
0x44
(
%
rax
)
%
xmm11
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
68
15
40
233
/
/
movaps
%
xmm1
%
xmm13
.
byte
69
15
194
236
1
/
/
cmpltps
%
xmm12
%
xmm13
.
byte
102
69
15
56
0
233
/
/
pshufb
%
xmm9
%
xmm13
.
byte
68
15
40
240
/
/
movaps
%
xmm0
%
xmm14
.
byte
69
15
194
244
1
/
/
cmpltps
%
xmm12
%
xmm14
.
byte
102
69
15
56
0
241
/
/
pshufb
%
xmm9
%
xmm14
.
byte
102
69
15
108
245
/
/
punpcklqdq
%
xmm13
%
xmm14
.
byte
102
65
15
113
246
15
/
/
psllw
0xf
%
xmm14
.
byte
102
65
15
113
230
15
/
/
psraw
0xf
%
xmm14
.
byte
69
15
87
228
/
/
xorps
%
xmm12
%
xmm12
.
byte
68
15
194
227
2
/
/
cmpleps
%
xmm3
%
xmm12
.
byte
102
69
15
56
0
225
/
/
pshufb
%
xmm9
%
xmm12
.
byte
68
15
194
194
2
/
/
cmpleps
%
xmm2
%
xmm8
.
byte
102
69
15
56
0
193
/
/
pshufb
%
xmm9
%
xmm8
.
byte
102
69
15
108
196
/
/
punpcklqdq
%
xmm12
%
xmm8
.
byte
102
65
15
113
240
15
/
/
psllw
0xf
%
xmm8
.
byte
102
65
15
113
224
15
/
/
psraw
0xf
%
xmm8
.
byte
102
69
15
219
194
/
/
pand
%
xmm10
%
xmm8
.
byte
102
69
15
219
198
/
/
pand
%
xmm14
%
xmm8
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
69
15
194
211
1
/
/
cmpltps
%
xmm11
%
xmm10
.
byte
102
69
15
56
0
209
/
/
pshufb
%
xmm9
%
xmm10
.
byte
68
15
40
226
/
/
movaps
%
xmm2
%
xmm12
.
byte
69
15
194
227
1
/
/
cmpltps
%
xmm11
%
xmm12
.
byte
102
69
15
56
0
225
/
/
pshufb
%
xmm9
%
xmm12
.
byte
102
69
15
108
226
/
/
punpcklqdq
%
xmm10
%
xmm12
.
byte
102
65
15
113
244
15
/
/
psllw
0xf
%
xmm12
.
byte
102
65
15
113
228
15
/
/
psraw
0xf
%
xmm12
.
byte
102
69
15
219
224
/
/
pand
%
xmm8
%
xmm12
.
byte
243
68
15
127
32
/
/
movdqu
%
xmm12
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_sse41_lowp
.
globl
_sk_check_decal_mask_sse41_lowp
FUNCTION
(
_sk_check_decal_mask_sse41_lowp
)
_sk_check_decal_mask_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
0
/
/
movups
(
%
rax
)
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
65
15
84
208
/
/
andps
%
xmm8
%
xmm2
.
byte
65
15
84
216
/
/
andps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_sse41_lowp
.
globl
_sk_gradient_sse41_lowp
FUNCTION
(
_sk_gradient_sse41_lowp
)
_sk_gradient_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
72
137
76
36
208
/
/
mov
%
rcx
-
0x30
(
%
rsp
)
.
byte
72
137
84
36
200
/
/
mov
%
rdx
-
0x38
(
%
rsp
)
.
byte
72
137
124
36
192
/
/
mov
%
rdi
-
0x40
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
137
116
36
184
/
/
mov
%
rsi
-
0x48
(
%
rsp
)
.
byte
72
139
8
/
/
mov
(
%
rax
)
%
rcx
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
72
131
249
2
/
/
cmp
0x2
%
rcx
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
114
57
/
/
jb
366e4
<
_sk_gradient_sse41_lowp
+
0x75
>
.
byte
72
139
80
72
/
/
mov
0x48
(
%
rax
)
%
rdx
.
byte
72
255
201
/
/
dec
%
rcx
.
byte
72
131
194
4
/
/
add
0x4
%
rdx
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
243
15
16
18
/
/
movss
(
%
rdx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
194
248
2
/
/
cmpleps
%
xmm0
%
xmm7
.
byte
102
68
15
250
207
/
/
psubd
%
xmm7
%
xmm9
.
byte
15
194
209
2
/
/
cmpleps
%
xmm1
%
xmm2
.
byte
102
15
250
218
/
/
psubd
%
xmm2
%
xmm3
.
byte
72
131
194
4
/
/
add
0x4
%
rdx
.
byte
72
255
201
/
/
dec
%
rcx
.
byte
117
219
/
/
jne
366bf
<
_sk_gradient_sse41_lowp
+
0x50
>
.
byte
102
72
15
58
22
219
1
/
/
pextrq
0x1
%
xmm3
%
rbx
.
byte
65
137
221
/
/
mov
%
ebx
%
r13d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
72
15
126
221
/
/
movq
%
xmm3
%
rbp
.
byte
137
239
/
/
mov
%
ebp
%
edi
.
byte
72
193
237
32
/
/
shr
0x20
%
rbp
.
byte
102
69
15
126
200
/
/
movd
%
xmm9
%
r8d
.
byte
102
69
15
58
22
201
1
/
/
pextrd
0x1
%
xmm9
%
r9d
.
byte
102
69
15
58
22
202
2
/
/
pextrd
0x2
%
xmm9
%
r10d
.
byte
72
139
72
8
/
/
mov
0x8
(
%
rax
)
%
rcx
.
byte
243
68
15
16
44
185
/
/
movss
(
%
rcx
%
rdi
4
)
%
xmm13
.
byte
102
68
15
58
33
44
169
16
/
/
insertps
0x10
(
%
rcx
%
rbp
4
)
%
xmm13
.
byte
243
66
15
16
20
169
/
/
movss
(
%
rcx
%
r13
4
)
%
xmm2
.
byte
243
68
15
16
4
153
/
/
movss
(
%
rcx
%
rbx
4
)
%
xmm8
.
byte
243
66
15
16
28
129
/
/
movss
(
%
rcx
%
r8
4
)
%
xmm3
.
byte
102
66
15
58
33
28
137
16
/
/
insertps
0x10
(
%
rcx
%
r9
4
)
%
xmm3
.
byte
102
66
15
58
33
28
145
32
/
/
insertps
0x20
(
%
rcx
%
r10
4
)
%
xmm3
.
byte
102
69
15
58
22
203
3
/
/
pextrd
0x3
%
xmm9
%
r11d
.
byte
102
66
15
58
33
28
153
48
/
/
insertps
0x30
(
%
rcx
%
r11
4
)
%
xmm3
.
byte
102
68
15
58
33
234
32
/
/
insertps
0x20
%
xmm2
%
xmm13
.
byte
76
139
96
16
/
/
mov
0x10
(
%
rax
)
%
r12
.
byte
243
69
15
16
12
188
/
/
movss
(
%
r12
%
rdi
4
)
%
xmm9
.
byte
76
139
120
24
/
/
mov
0x18
(
%
rax
)
%
r15
.
byte
243
69
15
16
28
191
/
/
movss
(
%
r15
%
rdi
4
)
%
xmm11
.
byte
76
139
112
32
/
/
mov
0x20
(
%
rax
)
%
r14
.
byte
243
69
15
16
20
190
/
/
movss
(
%
r14
%
rdi
4
)
%
xmm10
.
byte
72
139
112
40
/
/
mov
0x28
(
%
rax
)
%
rsi
.
byte
243
68
15
16
60
190
/
/
movss
(
%
rsi
%
rdi
4
)
%
xmm15
.
byte
72
139
80
48
/
/
mov
0x30
(
%
rax
)
%
rdx
.
byte
243
68
15
16
52
186
/
/
movss
(
%
rdx
%
rdi
4
)
%
xmm14
.
byte
72
139
72
56
/
/
mov
0x38
(
%
rax
)
%
rcx
.
byte
243
15
16
20
185
/
/
movss
(
%
rcx
%
rdi
4
)
%
xmm2
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
68
15
16
36
184
/
/
movss
(
%
rax
%
rdi
4
)
%
xmm12
.
byte
102
69
15
58
33
12
172
16
/
/
insertps
0x10
(
%
r12
%
rbp
4
)
%
xmm9
.
byte
102
69
15
58
33
28
175
16
/
/
insertps
0x10
(
%
r15
%
rbp
4
)
%
xmm11
.
byte
102
69
15
58
33
20
174
16
/
/
insertps
0x10
(
%
r14
%
rbp
4
)
%
xmm10
.
byte
102
68
15
58
33
60
174
16
/
/
insertps
0x10
(
%
rsi
%
rbp
4
)
%
xmm15
.
byte
102
68
15
58
33
52
170
16
/
/
insertps
0x10
(
%
rdx
%
rbp
4
)
%
xmm14
.
byte
102
15
58
33
20
169
16
/
/
insertps
0x10
(
%
rcx
%
rbp
4
)
%
xmm2
.
byte
102
68
15
58
33
36
168
16
/
/
insertps
0x10
(
%
rax
%
rbp
4
)
%
xmm12
.
byte
102
69
15
58
33
232
48
/
/
insertps
0x30
%
xmm8
%
xmm13
.
byte
243
67
15
16
60
172
/
/
movss
(
%
r12
%
r13
4
)
%
xmm7
.
byte
102
68
15
58
33
207
32
/
/
insertps
0x20
%
xmm7
%
xmm9
.
byte
243
65
15
16
60
156
/
/
movss
(
%
r12
%
rbx
4
)
%
xmm7
.
byte
102
68
15
58
33
207
48
/
/
insertps
0x30
%
xmm7
%
xmm9
.
byte
243
67
15
16
60
175
/
/
movss
(
%
r15
%
r13
4
)
%
xmm7
.
byte
102
68
15
58
33
223
32
/
/
insertps
0x20
%
xmm7
%
xmm11
.
byte
243
65
15
16
60
159
/
/
movss
(
%
r15
%
rbx
4
)
%
xmm7
.
byte
102
68
15
58
33
223
48
/
/
insertps
0x30
%
xmm7
%
xmm11
.
byte
243
67
15
16
60
174
/
/
movss
(
%
r14
%
r13
4
)
%
xmm7
.
byte
102
68
15
58
33
215
32
/
/
insertps
0x20
%
xmm7
%
xmm10
.
byte
243
65
15
16
60
158
/
/
movss
(
%
r14
%
rbx
4
)
%
xmm7
.
byte
102
68
15
58
33
215
48
/
/
insertps
0x30
%
xmm7
%
xmm10
.
byte
243
66
15
16
60
174
/
/
movss
(
%
rsi
%
r13
4
)
%
xmm7
.
byte
102
68
15
58
33
255
32
/
/
insertps
0x20
%
xmm7
%
xmm15
.
byte
243
15
16
60
158
/
/
movss
(
%
rsi
%
rbx
4
)
%
xmm7
.
byte
102
68
15
58
33
255
48
/
/
insertps
0x30
%
xmm7
%
xmm15
.
byte
243
66
15
16
60
170
/
/
movss
(
%
rdx
%
r13
4
)
%
xmm7
.
byte
102
68
15
58
33
247
32
/
/
insertps
0x20
%
xmm7
%
xmm14
.
byte
243
15
16
60
154
/
/
movss
(
%
rdx
%
rbx
4
)
%
xmm7
.
byte
102
68
15
58
33
247
48
/
/
insertps
0x30
%
xmm7
%
xmm14
.
byte
243
66
15
16
60
169
/
/
movss
(
%
rcx
%
r13
4
)
%
xmm7
.
byte
102
15
58
33
215
32
/
/
insertps
0x20
%
xmm7
%
xmm2
.
byte
243
15
16
60
153
/
/
movss
(
%
rcx
%
rbx
4
)
%
xmm7
.
byte
102
15
58
33
215
48
/
/
insertps
0x30
%
xmm7
%
xmm2
.
byte
243
66
15
16
60
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm7
.
byte
102
68
15
58
33
231
32
/
/
insertps
0x20
%
xmm7
%
xmm12
.
byte
243
15
16
60
152
/
/
movss
(
%
rax
%
rbx
4
)
%
xmm7
.
byte
102
68
15
58
33
231
48
/
/
insertps
0x30
%
xmm7
%
xmm12
.
byte
243
66
15
16
60
134
/
/
movss
(
%
rsi
%
r8
4
)
%
xmm7
.
byte
102
66
15
58
33
60
142
16
/
/
insertps
0x10
(
%
rsi
%
r9
4
)
%
xmm7
.
byte
102
66
15
58
33
60
150
32
/
/
insertps
0x20
(
%
rsi
%
r10
4
)
%
xmm7
.
byte
102
66
15
58
33
60
158
48
/
/
insertps
0x30
(
%
rsi
%
r11
4
)
%
xmm7
.
byte
68
15
89
233
/
/
mulps
%
xmm1
%
xmm13
.
byte
69
15
88
239
/
/
addps
%
xmm15
%
xmm13
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
40
61
48
103
0
0
/
/
movaps
0x6730
(
%
rip
)
%
xmm7
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
68
15
89
239
/
/
mulps
%
xmm7
%
xmm13
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
68
15
40
61
49
102
0
0
/
/
movaps
0x6631
(
%
rip
)
%
xmm15
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
88
239
/
/
addps
%
xmm15
%
xmm13
.
byte
65
15
88
223
/
/
addps
%
xmm15
%
xmm3
.
byte
243
65
15
91
245
/
/
cvttps2dq
%
xmm13
%
xmm6
.
byte
102
68
15
111
45
107
109
0
0
/
/
movdqa
0x6d6b
(
%
rip
)
%
xmm13
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
65
15
56
0
245
/
/
pshufb
%
xmm13
%
xmm6
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
102
69
15
56
0
197
/
/
pshufb
%
xmm13
%
xmm8
.
byte
102
68
15
108
198
/
/
punpcklqdq
%
xmm6
%
xmm8
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
243
67
15
16
28
132
/
/
movss
(
%
r12
%
r8
4
)
%
xmm3
.
byte
102
67
15
58
33
28
140
16
/
/
insertps
0x10
(
%
r12
%
r9
4
)
%
xmm3
.
byte
102
67
15
58
33
28
148
32
/
/
insertps
0x20
(
%
r12
%
r10
4
)
%
xmm3
.
byte
102
67
15
58
33
28
156
48
/
/
insertps
0x30
(
%
r12
%
r11
4
)
%
xmm3
.
byte
243
66
15
16
52
130
/
/
movss
(
%
rdx
%
r8
4
)
%
xmm6
.
byte
102
66
15
58
33
52
138
16
/
/
insertps
0x10
(
%
rdx
%
r9
4
)
%
xmm6
.
byte
102
66
15
58
33
52
146
32
/
/
insertps
0x20
(
%
rdx
%
r10
4
)
%
xmm6
.
byte
102
66
15
58
33
52
154
48
/
/
insertps
0x30
(
%
rdx
%
r11
4
)
%
xmm6
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
68
15
89
207
/
/
mulps
%
xmm7
%
xmm9
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
69
15
88
207
/
/
addps
%
xmm15
%
xmm9
.
byte
65
15
88
223
/
/
addps
%
xmm15
%
xmm3
.
byte
243
65
15
91
241
/
/
cvttps2dq
%
xmm9
%
xmm6
.
byte
102
65
15
56
0
245
/
/
pshufb
%
xmm13
%
xmm6
.
byte
243
68
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm9
.
byte
102
69
15
56
0
205
/
/
pshufb
%
xmm13
%
xmm9
.
byte
102
68
15
108
206
/
/
punpcklqdq
%
xmm6
%
xmm9
.
byte
243
67
15
16
28
135
/
/
movss
(
%
r15
%
r8
4
)
%
xmm3
.
byte
102
67
15
58
33
28
143
16
/
/
insertps
0x10
(
%
r15
%
r9
4
)
%
xmm3
.
byte
102
67
15
58
33
28
151
32
/
/
insertps
0x20
(
%
r15
%
r10
4
)
%
xmm3
.
byte
102
67
15
58
33
28
159
48
/
/
insertps
0x30
(
%
r15
%
r11
4
)
%
xmm3
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
68
15
88
218
/
/
addps
%
xmm2
%
xmm11
.
byte
243
66
15
16
20
129
/
/
movss
(
%
rcx
%
r8
4
)
%
xmm2
.
byte
102
66
15
58
33
20
137
16
/
/
insertps
0x10
(
%
rcx
%
r9
4
)
%
xmm2
.
byte
102
66
15
58
33
20
145
32
/
/
insertps
0x20
(
%
rcx
%
r10
4
)
%
xmm2
.
byte
102
66
15
58
33
20
153
48
/
/
insertps
0x30
(
%
rcx
%
r11
4
)
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
68
15
89
223
/
/
mulps
%
xmm7
%
xmm11
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
69
15
88
223
/
/
addps
%
xmm15
%
xmm11
.
byte
65
15
88
223
/
/
addps
%
xmm15
%
xmm3
.
byte
243
65
15
91
243
/
/
cvttps2dq
%
xmm11
%
xmm6
.
byte
102
65
15
56
0
245
/
/
pshufb
%
xmm13
%
xmm6
.
byte
243
15
91
211
/
/
cvttps2dq
%
xmm3
%
xmm2
.
byte
102
65
15
56
0
213
/
/
pshufb
%
xmm13
%
xmm2
.
byte
102
15
108
214
/
/
punpcklqdq
%
xmm6
%
xmm2
.
byte
68
15
89
209
/
/
mulps
%
xmm1
%
xmm10
.
byte
69
15
88
212
/
/
addps
%
xmm12
%
xmm10
.
byte
243
67
15
16
12
134
/
/
movss
(
%
r14
%
r8
4
)
%
xmm1
.
byte
102
67
15
58
33
12
142
16
/
/
insertps
0x10
(
%
r14
%
r9
4
)
%
xmm1
.
byte
102
67
15
58
33
12
150
32
/
/
insertps
0x20
(
%
r14
%
r10
4
)
%
xmm1
.
byte
102
67
15
58
33
12
158
48
/
/
insertps
0x30
(
%
r14
%
r11
4
)
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
243
66
15
16
4
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm0
.
byte
102
66
15
58
33
4
136
16
/
/
insertps
0x10
(
%
rax
%
r9
4
)
%
xmm0
.
byte
102
66
15
58
33
4
144
32
/
/
insertps
0x20
(
%
rax
%
r10
4
)
%
xmm0
.
byte
102
66
15
58
33
4
152
48
/
/
insertps
0x30
(
%
rax
%
r11
4
)
%
xmm0
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
68
15
89
215
/
/
mulps
%
xmm7
%
xmm10
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
69
15
88
215
/
/
addps
%
xmm15
%
xmm10
.
byte
65
15
88
207
/
/
addps
%
xmm15
%
xmm1
.
byte
243
65
15
91
194
/
/
cvttps2dq
%
xmm10
%
xmm0
.
byte
102
65
15
56
0
197
/
/
pshufb
%
xmm13
%
xmm0
.
byte
243
15
91
217
/
/
cvttps2dq
%
xmm1
%
xmm3
.
byte
102
65
15
56
0
221
/
/
pshufb
%
xmm13
%
xmm3
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
72
139
116
36
184
/
/
mov
-
0x48
(
%
rsp
)
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
139
124
36
192
/
/
mov
-
0x40
(
%
rsp
)
%
rdi
.
byte
72
139
84
36
200
/
/
mov
-
0x38
(
%
rsp
)
%
rdx
.
byte
72
139
76
36
208
/
/
mov
-
0x30
(
%
rsp
)
%
rcx
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
15
40
116
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_sse41_lowp
.
globl
_sk_evenly_spaced_gradient_sse41_lowp
FUNCTION
(
_sk_evenly_spaced_gradient_sse41_lowp
)
_sk_evenly_spaced_gradient_sse41_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
80
/
/
push
%
rax
.
byte
15
41
124
36
240
/
/
movaps
%
xmm7
-
0x10
(
%
rsp
)
.
byte
15
41
116
36
224
/
/
movaps
%
xmm6
-
0x20
(
%
rsp
)
.
byte
15
41
108
36
208
/
/
movaps
%
xmm5
-
0x30
(
%
rsp
)
.
byte
15
41
100
36
192
/
/
movaps
%
xmm4
-
0x40
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
72
139
104
8
/
/
mov
0x8
(
%
rax
)
%
rbp
.
byte
72
255
203
/
/
dec
%
rbx
.
byte
120
7
/
/
js
36abd
<
_sk_evenly_spaced_gradient_sse41_lowp
+
0x34
>
.
byte
243
72
15
42
211
/
/
cvtsi2ss
%
rbx
%
xmm2
.
byte
235
21
/
/
jmp
36ad2
<
_sk_evenly_spaced_gradient_sse41_lowp
+
0x49
>
.
byte
73
137
216
/
/
mov
%
rbx
%
r8
.
byte
73
209
232
/
/
shr
%
r8
.
byte
131
227
1
/
/
and
0x1
%
ebx
.
byte
76
9
195
/
/
or
%
r8
%
rbx
.
byte
243
72
15
42
211
/
/
cvtsi2ss
%
rbx
%
xmm2
.
byte
243
15
88
210
/
/
addss
%
xmm2
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
41
68
36
176
/
/
movaps
%
xmm0
-
0x50
(
%
rsp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
73
15
58
22
194
1
/
/
pextrq
0x1
%
xmm0
%
r10
.
byte
69
137
214
/
/
mov
%
r10d
%
r14d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
73
15
126
197
/
/
movq
%
xmm0
%
r13
.
byte
69
137
236
/
/
mov
%
r13d
%
r12d
.
byte
73
193
237
32
/
/
shr
0x20
%
r13
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
102
73
15
58
22
192
1
/
/
pextrq
0x1
%
xmm0
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
73
15
126
195
/
/
movq
%
xmm0
%
r11
.
byte
69
137
223
/
/
mov
%
r11d
%
r15d
.
byte
73
193
235
32
/
/
shr
0x20
%
r11
.
byte
243
70
15
16
68
189
0
/
/
movss
0x0
(
%
rbp
%
r15
4
)
%
xmm8
.
byte
102
70
15
58
33
68
157
0
16
/
/
insertps
0x10
0x0
(
%
rbp
%
r11
4
)
%
xmm8
.
byte
243
66
15
16
68
141
0
/
/
movss
0x0
(
%
rbp
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
84
133
0
/
/
movss
0x0
(
%
rbp
%
r8
4
)
%
xmm2
.
byte
15
41
76
36
160
/
/
movaps
%
xmm1
-
0x60
(
%
rsp
)
.
byte
243
66
15
16
92
165
0
/
/
movss
0x0
(
%
rbp
%
r12
4
)
%
xmm3
.
byte
102
66
15
58
33
92
173
0
16
/
/
insertps
0x10
0x0
(
%
rbp
%
r13
4
)
%
xmm3
.
byte
243
66
15
16
100
181
0
/
/
movss
0x0
(
%
rbp
%
r14
4
)
%
xmm4
.
byte
243
66
15
16
108
149
0
/
/
movss
0x0
(
%
rbp
%
r10
4
)
%
xmm5
.
byte
102
68
15
58
33
192
32
/
/
insertps
0x20
%
xmm0
%
xmm8
.
byte
102
68
15
58
33
194
48
/
/
insertps
0x30
%
xmm2
%
xmm8
.
byte
72
139
88
16
/
/
mov
0x10
(
%
rax
)
%
rbx
.
byte
243
70
15
16
12
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm9
.
byte
102
70
15
58
33
12
155
16
/
/
insertps
0x10
(
%
rbx
%
r11
4
)
%
xmm9
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
243
66
15
16
20
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm2
.
byte
102
66
15
58
33
20
171
16
/
/
insertps
0x10
(
%
rbx
%
r13
4
)
%
xmm2
.
byte
243
66
15
16
52
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm6
.
byte
243
66
15
16
60
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm7
.
byte
102
15
58
33
220
32
/
/
insertps
0x20
%
xmm4
%
xmm3
.
byte
102
15
58
33
221
48
/
/
insertps
0x30
%
xmm5
%
xmm3
.
byte
102
68
15
58
33
200
32
/
/
insertps
0x20
%
xmm0
%
xmm9
.
byte
102
68
15
58
33
201
48
/
/
insertps
0x30
%
xmm1
%
xmm9
.
byte
102
15
58
33
214
32
/
/
insertps
0x20
%
xmm6
%
xmm2
.
byte
102
15
58
33
215
48
/
/
insertps
0x30
%
xmm7
%
xmm2
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
243
70
15
16
28
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm11
.
byte
102
70
15
58
33
28
155
16
/
/
insertps
0x10
(
%
rbx
%
r11
4
)
%
xmm11
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
243
70
15
16
44
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm13
.
byte
102
70
15
58
33
44
171
16
/
/
insertps
0x10
(
%
rbx
%
r13
4
)
%
xmm13
.
byte
243
66
15
16
36
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm4
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
102
68
15
58
33
216
32
/
/
insertps
0x20
%
xmm0
%
xmm11
.
byte
102
68
15
58
33
217
48
/
/
insertps
0x30
%
xmm1
%
xmm11
.
byte
102
68
15
58
33
236
32
/
/
insertps
0x20
%
xmm4
%
xmm13
.
byte
102
68
15
58
33
237
48
/
/
insertps
0x30
%
xmm5
%
xmm13
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
243
70
15
16
20
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm10
.
byte
102
70
15
58
33
20
155
16
/
/
insertps
0x10
(
%
rbx
%
r11
4
)
%
xmm10
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
102
68
15
58
33
208
32
/
/
insertps
0x20
%
xmm0
%
xmm10
.
byte
243
66
15
16
4
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm0
.
byte
102
68
15
58
33
208
48
/
/
insertps
0x30
%
xmm0
%
xmm10
.
byte
243
70
15
16
36
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm12
.
byte
102
70
15
58
33
36
171
16
/
/
insertps
0x10
(
%
rbx
%
r13
4
)
%
xmm12
.
byte
243
66
15
16
4
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm0
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
102
68
15
58
33
224
32
/
/
insertps
0x20
%
xmm0
%
xmm12
.
byte
102
68
15
58
33
225
48
/
/
insertps
0x30
%
xmm1
%
xmm12
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
243
66
15
16
36
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm4
.
byte
102
66
15
58
33
36
171
16
/
/
insertps
0x10
(
%
rbx
%
r13
4
)
%
xmm4
.
byte
243
66
15
16
4
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm0
.
byte
102
15
58
33
224
32
/
/
insertps
0x20
%
xmm0
%
xmm4
.
byte
243
66
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm0
.
byte
102
15
58
33
224
48
/
/
insertps
0x30
%
xmm0
%
xmm4
.
byte
243
66
15
16
4
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm0
.
byte
102
66
15
58
33
4
155
16
/
/
insertps
0x10
(
%
rbx
%
r11
4
)
%
xmm0
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
102
15
58
33
193
32
/
/
insertps
0x20
%
xmm1
%
xmm0
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
102
15
58
33
193
48
/
/
insertps
0x30
%
xmm1
%
xmm0
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
243
66
15
16
60
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm7
.
byte
102
66
15
58
33
60
171
16
/
/
insertps
0x10
(
%
rbx
%
r13
4
)
%
xmm7
.
byte
243
66
15
16
12
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm1
.
byte
102
15
58
33
249
32
/
/
insertps
0x20
%
xmm1
%
xmm7
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
102
15
58
33
249
48
/
/
insertps
0x30
%
xmm1
%
xmm7
.
byte
243
66
15
16
12
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm1
.
byte
102
66
15
58
33
12
155
16
/
/
insertps
0x10
(
%
rbx
%
r11
4
)
%
xmm1
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
102
15
58
33
205
32
/
/
insertps
0x20
%
xmm5
%
xmm1
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
102
15
58
33
205
48
/
/
insertps
0x30
%
xmm5
%
xmm1
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
243
66
15
16
52
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm6
.
byte
102
66
15
58
33
52
171
16
/
/
insertps
0x10
(
%
rbx
%
r13
4
)
%
xmm6
.
byte
243
66
15
16
44
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm5
.
byte
102
15
58
33
245
32
/
/
insertps
0x20
%
xmm5
%
xmm6
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
102
15
58
33
245
48
/
/
insertps
0x30
%
xmm5
%
xmm6
.
byte
15
41
116
36
128
/
/
movaps
%
xmm6
-
0x80
(
%
rsp
)
.
byte
243
66
15
16
44
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm5
.
byte
102
66
15
58
33
44
155
16
/
/
insertps
0x10
(
%
rbx
%
r11
4
)
%
xmm5
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
102
15
58
33
238
32
/
/
insertps
0x20
%
xmm6
%
xmm5
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
102
15
58
33
238
48
/
/
insertps
0x30
%
xmm6
%
xmm5
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
70
15
16
52
160
/
/
movss
(
%
rax
%
r12
4
)
%
xmm14
.
byte
102
70
15
58
33
52
168
16
/
/
insertps
0x10
(
%
rax
%
r13
4
)
%
xmm14
.
byte
243
66
15
16
52
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm6
.
byte
102
68
15
58
33
246
32
/
/
insertps
0x20
%
xmm6
%
xmm14
.
byte
243
66
15
16
52
144
/
/
movss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
102
68
15
58
33
246
48
/
/
insertps
0x30
%
xmm6
%
xmm14
.
byte
68
15
41
116
36
144
/
/
movaps
%
xmm14
-
0x70
(
%
rsp
)
.
byte
243
70
15
16
60
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm15
.
byte
102
70
15
58
33
60
152
16
/
/
insertps
0x10
(
%
rax
%
r11
4
)
%
xmm15
.
byte
243
70
15
16
52
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm14
.
byte
102
69
15
58
33
254
32
/
/
insertps
0x20
%
xmm14
%
xmm15
.
byte
243
66
15
16
52
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm6
.
byte
102
68
15
58
33
254
48
/
/
insertps
0x30
%
xmm6
%
xmm15
.
byte
15
40
116
36
160
/
/
movaps
-
0x60
(
%
rsp
)
%
xmm6
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
15
88
220
/
/
addps
%
xmm4
%
xmm3
.
byte
15
40
100
36
176
/
/
movaps
-
0x50
(
%
rsp
)
%
xmm4
.
byte
68
15
89
196
/
/
mulps
%
xmm4
%
xmm8
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
15
40
5
13
98
0
0
/
/
movaps
0x620d
(
%
rip
)
%
xmm0
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
68
15
40
53
14
97
0
0
/
/
movaps
0x610e
(
%
rip
)
%
xmm14
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
69
15
88
198
/
/
addps
%
xmm14
%
xmm8
.
byte
65
15
88
222
/
/
addps
%
xmm14
%
xmm3
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
102
15
111
5
74
104
0
0
/
/
movdqa
0x684a
(
%
rip
)
%
xmm0
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
15
56
0
216
/
/
pshufb
%
xmm0
%
xmm3
.
byte
243
69
15
91
192
/
/
cvttps2dq
%
xmm8
%
xmm8
.
byte
102
68
15
56
0
192
/
/
pshufb
%
xmm0
%
xmm8
.
byte
102
68
15
108
195
/
/
punpcklqdq
%
xmm3
%
xmm8
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
68
15
89
203
/
/
mulps
%
xmm3
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
15
40
13
189
97
0
0
/
/
movaps
0x61bd
(
%
rip
)
%
xmm1
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
69
15
88
206
/
/
addps
%
xmm14
%
xmm9
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
243
15
91
202
/
/
cvttps2dq
%
xmm2
%
xmm1
.
byte
102
15
56
0
200
/
/
pshufb
%
xmm0
%
xmm1
.
byte
243
69
15
91
201
/
/
cvttps2dq
%
xmm9
%
xmm9
.
byte
102
68
15
56
0
200
/
/
pshufb
%
xmm0
%
xmm9
.
byte
102
68
15
108
201
/
/
punpcklqdq
%
xmm1
%
xmm9
.
byte
68
15
89
238
/
/
mulps
%
xmm6
%
xmm13
.
byte
68
15
88
108
36
128
/
/
addps
-
0x80
(
%
rsp
)
%
xmm13
.
byte
68
15
89
219
/
/
mulps
%
xmm3
%
xmm11
.
byte
68
15
88
221
/
/
addps
%
xmm5
%
xmm11
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
68
15
89
233
/
/
mulps
%
xmm1
%
xmm13
.
byte
68
15
89
217
/
/
mulps
%
xmm1
%
xmm11
.
byte
69
15
88
222
/
/
addps
%
xmm14
%
xmm11
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
243
65
15
91
205
/
/
cvttps2dq
%
xmm13
%
xmm1
.
byte
102
15
56
0
200
/
/
pshufb
%
xmm0
%
xmm1
.
byte
243
65
15
91
211
/
/
cvttps2dq
%
xmm11
%
xmm2
.
byte
102
15
56
0
208
/
/
pshufb
%
xmm0
%
xmm2
.
byte
102
15
108
209
/
/
punpcklqdq
%
xmm1
%
xmm2
.
byte
68
15
89
230
/
/
mulps
%
xmm6
%
xmm12
.
byte
68
15
88
100
36
144
/
/
addps
-
0x70
(
%
rsp
)
%
xmm12
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
69
15
88
215
/
/
addps
%
xmm15
%
xmm10
.
byte
68
15
89
228
/
/
mulps
%
xmm4
%
xmm12
.
byte
68
15
89
212
/
/
mulps
%
xmm4
%
xmm10
.
byte
69
15
88
214
/
/
addps
%
xmm14
%
xmm10
.
byte
69
15
88
230
/
/
addps
%
xmm14
%
xmm12
.
byte
243
65
15
91
204
/
/
cvttps2dq
%
xmm12
%
xmm1
.
byte
102
15
56
0
200
/
/
pshufb
%
xmm0
%
xmm1
.
byte
243
65
15
91
218
/
/
cvttps2dq
%
xmm10
%
xmm3
.
byte
102
15
56
0
216
/
/
pshufb
%
xmm0
%
xmm3
.
byte
102
15
108
217
/
/
punpcklqdq
%
xmm1
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
15
40
100
36
192
/
/
movaps
-
0x40
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
208
/
/
movaps
-
0x30
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
224
/
/
movaps
-
0x20
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm7
.
byte
72
131
196
8
/
/
add
0x8
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_sse41_lowp
.
globl
_sk_evenly_spaced_2_stop_gradient_sse41_lowp
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_sse41_lowp
)
_sk_evenly_spaced_2_stop_gradient_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
24
/
/
movss
(
%
rax
)
%
xmm3
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
68
15
40
21
183
96
0
0
/
/
movaps
0x60b7
(
%
rip
)
%
xmm10
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
68
15
40
37
183
95
0
0
/
/
movaps
0x5fb7
(
%
rip
)
%
xmm12
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
68
15
111
29
242
102
0
0
/
/
movdqa
0x66f2
(
%
rip
)
%
xmm11
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
65
15
56
0
211
/
/
pshufb
%
xmm11
%
xmm2
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
102
69
15
56
0
195
/
/
pshufb
%
xmm11
%
xmm8
.
byte
102
68
15
108
194
/
/
punpcklqdq
%
xmm2
%
xmm8
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
65
15
89
217
/
/
mulps
%
xmm9
%
xmm3
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
68
15
88
202
/
/
addps
%
xmm2
%
xmm9
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
69
15
89
202
/
/
mulps
%
xmm10
%
xmm9
.
byte
69
15
88
204
/
/
addps
%
xmm12
%
xmm9
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
243
15
91
211
/
/
cvttps2dq
%
xmm3
%
xmm2
.
byte
102
65
15
56
0
211
/
/
pshufb
%
xmm11
%
xmm2
.
byte
243
69
15
91
201
/
/
cvttps2dq
%
xmm9
%
xmm9
.
byte
102
69
15
56
0
203
/
/
pshufb
%
xmm11
%
xmm9
.
byte
102
68
15
108
202
/
/
punpcklqdq
%
xmm2
%
xmm9
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
68
15
16
104
24
/
/
movss
0x18
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
65
15
88
213
/
/
addps
%
xmm13
%
xmm2
.
byte
65
15
88
221
/
/
addps
%
xmm13
%
xmm3
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
102
65
15
56
0
219
/
/
pshufb
%
xmm11
%
xmm3
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
65
15
56
0
211
/
/
pshufb
%
xmm11
%
xmm2
.
byte
102
15
108
211
/
/
punpcklqdq
%
xmm3
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
104
28
/
/
movss
0x1c
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
221
/
/
addps
%
xmm13
%
xmm3
.
byte
65
15
88
205
/
/
addps
%
xmm13
%
xmm1
.
byte
65
15
89
202
/
/
mulps
%
xmm10
%
xmm1
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
65
15
88
204
/
/
addps
%
xmm12
%
xmm1
.
byte
243
15
91
193
/
/
cvttps2dq
%
xmm1
%
xmm0
.
byte
102
65
15
56
0
195
/
/
pshufb
%
xmm11
%
xmm0
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
102
65
15
56
0
219
/
/
pshufb
%
xmm11
%
xmm3
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_sse41_lowp
.
globl
_sk_xy_to_unit_angle_sse41_lowp
FUNCTION
(
_sk_xy_to_unit_angle_sse41_lowp
)
_sk_xy_to_unit_angle_sse41_lowp
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
15
41
108
36
200
/
/
movaps
%
xmm5
-
0x38
(
%
rsp
)
.
byte
15
41
100
36
184
/
/
movaps
%
xmm4
-
0x48
(
%
rsp
)
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
15
40
37
232
99
0
0
/
/
movaps
0x63e8
(
%
rip
)
%
xmm4
#
3d470
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1224
>
.
byte
68
15
40
233
/
/
movaps
%
xmm1
%
xmm13
.
byte
68
15
84
236
/
/
andps
%
xmm4
%
xmm13
.
byte
69
15
40
216
/
/
movaps
%
xmm8
%
xmm11
.
byte
68
15
84
220
/
/
andps
%
xmm4
%
xmm11
.
byte
68
15
40
231
/
/
movaps
%
xmm7
%
xmm12
.
byte
68
15
84
228
/
/
andps
%
xmm4
%
xmm12
.
byte
15
84
230
/
/
andps
%
xmm6
%
xmm4
.
byte
69
15
40
211
/
/
movaps
%
xmm11
%
xmm10
.
byte
68
15
194
212
1
/
/
cmpltps
%
xmm4
%
xmm10
.
byte
69
15
40
205
/
/
movaps
%
xmm13
%
xmm9
.
byte
69
15
194
204
1
/
/
cmpltps
%
xmm12
%
xmm9
.
byte
69
15
40
244
/
/
movaps
%
xmm12
%
xmm14
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
245
/
/
blendvps
%
xmm0
%
xmm13
%
xmm14
.
byte
68
15
40
252
/
/
movaps
%
xmm4
%
xmm15
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
69
15
56
20
251
/
/
blendvps
%
xmm0
%
xmm11
%
xmm15
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
69
15
56
20
236
/
/
blendvps
%
xmm0
%
xmm12
%
xmm13
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
68
15
56
20
220
/
/
blendvps
%
xmm0
%
xmm4
%
xmm11
.
byte
69
15
94
251
/
/
divps
%
xmm11
%
xmm15
.
byte
65
15
40
199
/
/
movaps
%
xmm15
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
29
232
99
0
0
/
/
movaps
0x63e8
(
%
rip
)
%
xmm11
#
3d4e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1294
>
.
byte
68
15
40
224
/
/
movaps
%
xmm0
%
xmm12
.
byte
69
15
89
227
/
/
mulps
%
xmm11
%
xmm12
.
byte
68
15
88
37
232
99
0
0
/
/
addps
0x63e8
(
%
rip
)
%
xmm12
#
3d4f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12a4
>
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
15
40
45
237
99
0
0
/
/
movaps
0x63ed
(
%
rip
)
%
xmm5
#
3d500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12b4
>
.
byte
68
15
88
229
/
/
addps
%
xmm5
%
xmm12
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
15
40
37
238
99
0
0
/
/
movaps
0x63ee
(
%
rip
)
%
xmm4
#
3d510
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12c4
>
.
byte
68
15
88
228
/
/
addps
%
xmm4
%
xmm12
.
byte
69
15
89
231
/
/
mulps
%
xmm15
%
xmm12
.
byte
15
40
21
239
99
0
0
/
/
movaps
0x63ef
(
%
rip
)
%
xmm2
#
3d520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12d4
>
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
65
15
92
212
/
/
subps
%
xmm12
%
xmm2
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
102
68
15
56
20
226
/
/
blendvps
%
xmm0
%
xmm2
%
xmm12
.
byte
69
15
94
245
/
/
divps
%
xmm13
%
xmm14
.
byte
65
15
40
198
/
/
movaps
%
xmm14
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
89
216
/
/
mulps
%
xmm0
%
xmm11
.
byte
68
15
88
29
151
99
0
0
/
/
addps
0x6397
(
%
rip
)
%
xmm11
#
3d4f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12a4
>
.
byte
68
15
89
216
/
/
mulps
%
xmm0
%
xmm11
.
byte
68
15
88
221
/
/
addps
%
xmm5
%
xmm11
.
byte
68
15
89
216
/
/
mulps
%
xmm0
%
xmm11
.
byte
68
15
88
220
/
/
addps
%
xmm4
%
xmm11
.
byte
69
15
89
222
/
/
mulps
%
xmm14
%
xmm11
.
byte
65
15
92
219
/
/
subps
%
xmm11
%
xmm3
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
102
68
15
56
20
219
/
/
blendvps
%
xmm0
%
xmm3
%
xmm11
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
69
15
194
193
1
/
/
cmpltps
%
xmm9
%
xmm8
.
byte
15
40
21
117
93
0
0
/
/
movaps
0x5d75
(
%
rip
)
%
xmm2
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
65
15
92
212
/
/
subps
%
xmm12
%
xmm2
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
102
68
15
56
20
226
/
/
blendvps
%
xmm0
%
xmm2
%
xmm12
.
byte
65
15
194
201
1
/
/
cmpltps
%
xmm9
%
xmm1
.
byte
65
15
92
219
/
/
subps
%
xmm11
%
xmm3
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
102
68
15
56
20
219
/
/
blendvps
%
xmm0
%
xmm3
%
xmm11
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
65
15
194
193
1
/
/
cmpltps
%
xmm9
%
xmm0
.
byte
15
40
13
83
93
0
0
/
/
movaps
0x5d53
(
%
rip
)
%
xmm1
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
65
15
92
204
/
/
subps
%
xmm12
%
xmm1
.
byte
102
68
15
56
20
225
/
/
blendvps
%
xmm0
%
xmm1
%
xmm12
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
65
15
194
193
1
/
/
cmpltps
%
xmm9
%
xmm0
.
byte
65
15
92
211
/
/
subps
%
xmm11
%
xmm2
.
byte
102
68
15
56
20
218
/
/
blendvps
%
xmm0
%
xmm2
%
xmm11
.
byte
65
15
40
203
/
/
movaps
%
xmm11
%
xmm1
.
byte
65
15
194
201
7
/
/
cmpordps
%
xmm9
%
xmm1
.
byte
69
15
194
204
7
/
/
cmpordps
%
xmm12
%
xmm9
.
byte
69
15
84
204
/
/
andps
%
xmm12
%
xmm9
.
byte
65
15
84
203
/
/
andps
%
xmm11
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
15
40
100
36
184
/
/
movaps
-
0x48
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
200
/
/
movaps
-
0x38
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_sse41_lowp
.
globl
_sk_xy_to_radius_sse41_lowp
FUNCTION
(
_sk_xy_to_radius_sse41_lowp
)
_sk_xy_to_radius_sse41_lowp
:
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
89
201
/
/
mulps
%
xmm1
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
65
15
81
192
/
/
sqrtps
%
xmm8
%
xmm0
.
byte
65
15
81
201
/
/
sqrtps
%
xmm9
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_sse41_lowp
.
globl
_sk_srcover_rgba_8888_sse41_lowp
FUNCTION
(
_sk_srcover_rgba_8888_sse41_lowp
)
_sk_srcover_rgba_8888_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
65
137
250
/
/
mov
%
edi
%
r10d
.
byte
65
128
226
7
/
/
and
0x7
%
r10b
.
byte
65
254
202
/
/
dec
%
r10b
.
byte
69
15
182
202
/
/
movzbl
%
r10b
%
r9d
.
byte
65
128
249
6
/
/
cmp
0x6
%
r9b
.
byte
119
38
/
/
ja
37289
<
_sk_srcover_rgba_8888_sse41_lowp
+
0x4b
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
76
141
29
21
2
0
0
/
/
lea
0x215
(
%
rip
)
%
r11
#
37484
<
_sk_srcover_rgba_8888_sse41_lowp
+
0x246
>
.
byte
75
99
4
139
/
/
movslq
(
%
r11
%
r9
4
)
%
rax
.
byte
76
1
216
/
/
add
%
r11
%
rax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
110
52
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
89
/
/
jmp
372e2
<
_sk_srcover_rgba_8888_sse41_lowp
+
0xa4
>
.
byte
243
65
15
111
52
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
74
/
/
jmp
372e2
<
_sk_srcover_rgba_8888_sse41_lowp
+
0xa4
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
243
65
15
126
52
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
102
65
15
58
14
240
240
/
/
pblendw
0xf0
%
xmm8
%
xmm6
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
235
37
/
/
jmp
372e2
<
_sk_srcover_rgba_8888_sse41_lowp
+
0xa4
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
69
15
58
34
68
144
20
1
/
/
pinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
69
15
58
34
68
144
16
0
/
/
pinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
243
65
15
111
52
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm6
.
byte
102
15
111
37
102
99
0
0
/
/
movdqa
0x6366
(
%
rip
)
%
xmm4
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
56
0
236
/
/
pshufb
%
xmm4
%
xmm5
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
56
0
252
/
/
pshufb
%
xmm4
%
xmm7
.
byte
102
15
108
239
/
/
punpcklqdq
%
xmm7
%
xmm5
.
byte
102
68
15
111
37
166
91
0
0
/
/
movdqa
0x5ba6
(
%
rip
)
%
xmm12
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
65
15
219
228
/
/
pand
%
xmm12
%
xmm4
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
68
15
111
13
63
99
0
0
/
/
movdqa
0x633f
(
%
rip
)
%
xmm9
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
65
15
56
0
249
/
/
pshufb
%
xmm9
%
xmm7
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
69
15
56
0
209
/
/
pshufb
%
xmm9
%
xmm10
.
byte
102
65
15
108
250
/
/
punpcklqdq
%
xmm10
%
xmm7
.
byte
102
68
15
111
13
44
99
0
0
/
/
movdqa
0x632c
(
%
rip
)
%
xmm9
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
241
/
/
pshufb
%
xmm9
%
xmm6
.
byte
102
69
15
56
0
193
/
/
pshufb
%
xmm9
%
xmm8
.
byte
102
65
15
108
240
/
/
punpcklqdq
%
xmm8
%
xmm6
.
byte
102
65
15
219
244
/
/
pand
%
xmm12
%
xmm6
.
byte
102
69
15
111
196
/
/
movdqa
%
xmm12
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
68
15
111
204
/
/
movdqa
%
xmm4
%
xmm9
.
byte
102
69
15
213
200
/
/
pmullw
%
xmm8
%
xmm9
.
byte
102
69
15
253
204
/
/
paddw
%
xmm12
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
69
15
213
208
/
/
pmullw
%
xmm8
%
xmm10
.
byte
102
69
15
253
212
/
/
paddw
%
xmm12
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
68
15
253
209
/
/
paddw
%
xmm1
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
69
15
213
216
/
/
pmullw
%
xmm8
%
xmm11
.
byte
102
69
15
253
220
/
/
paddw
%
xmm12
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
68
15
253
218
/
/
paddw
%
xmm2
%
xmm11
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
69
15
253
196
/
/
paddw
%
xmm12
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
68
15
253
195
/
/
paddw
%
xmm3
%
xmm8
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
65
15
235
193
/
/
por
%
xmm9
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
56
51
209
/
/
pmovzxwd
%
xmm1
%
xmm2
.
byte
102
68
15
56
51
224
/
/
pmovzxwd
%
xmm0
%
xmm12
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
65
15
235
219
/
/
por
%
xmm11
%
xmm3
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
105
203
/
/
punpckhwd
%
xmm3
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
97
195
/
/
punpcklwd
%
xmm3
%
xmm0
.
byte
102
65
15
235
196
/
/
por
%
xmm12
%
xmm0
.
byte
65
128
250
6
/
/
cmp
0x6
%
r10b
.
byte
119
24
/
/
ja
3742c
<
_sk_srcover_rgba_8888_sse41_lowp
+
0x1ee
>
.
byte
76
141
21
133
0
0
0
/
/
lea
0x85
(
%
rip
)
%
r10
#
374a0
<
_sk_srcover_rgba_8888_sse41_lowp
+
0x262
>
.
byte
75
99
4
138
/
/
movslq
(
%
r10
%
r9
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
126
4
144
/
/
movd
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
235
63
/
/
jmp
3746b
<
_sk_srcover_rgba_8888_sse41_lowp
+
0x22d
>
.
byte
243
65
15
127
4
144
/
/
movdqu
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
243
65
15
127
76
144
16
/
/
movdqu
%
xmm1
0x10
(
%
r8
%
rdx
4
)
.
byte
235
48
/
/
jmp
3746b
<
_sk_srcover_rgba_8888_sse41_lowp
+
0x22d
>
.
byte
102
65
15
58
22
68
144
8
2
/
/
pextrd
0x2
%
xmm0
0x8
(
%
r8
%
rdx
4
)
.
byte
102
65
15
214
4
144
/
/
movq
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
235
31
/
/
jmp
3746b
<
_sk_srcover_rgba_8888_sse41_lowp
+
0x22d
>
.
byte
102
65
15
58
22
76
144
24
2
/
/
pextrd
0x2
%
xmm1
0x18
(
%
r8
%
rdx
4
)
.
byte
102
65
15
58
22
76
144
20
1
/
/
pextrd
0x1
%
xmm1
0x14
(
%
r8
%
rdx
4
)
.
byte
102
65
15
126
76
144
16
/
/
movd
%
xmm1
0x10
(
%
r8
%
rdx
4
)
.
byte
243
65
15
127
4
144
/
/
movdqu
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
248
/
/
clc
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
37
254
255
255
20
/
/
jmpq
*
0x14fffffe
(
%
rip
)
#
1503748b
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14ffb23f
>
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
88
254
/
/
lcall
*
-
0x2
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
79
254
/
/
decl
-
0x2
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
70
254
/
/
incl
-
0x2
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
57
254
/
/
cmp
%
edi
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
132
255
255
255
164
255
/
/
incl
-
0x5b0001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
155
255
255
255
197
/
/
lcall
*
-
0x3a000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
181
/
/
mov
0xb5ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
172
/
/
lods
%
ds
:
(
%
rsi
)
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_srcover_bgra_8888_sse41_lowp
.
globl
_sk_srcover_bgra_8888_sse41_lowp
FUNCTION
(
_sk_srcover_bgra_8888_sse41_lowp
)
_sk_srcover_bgra_8888_sse41_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
65
137
250
/
/
mov
%
edi
%
r10d
.
byte
65
128
226
7
/
/
and
0x7
%
r10b
.
byte
65
254
202
/
/
dec
%
r10b
.
byte
69
15
182
202
/
/
movzbl
%
r10b
%
r9d
.
byte
65
128
249
6
/
/
cmp
0x6
%
r9b
.
byte
119
38
/
/
ja
37507
<
_sk_srcover_bgra_8888_sse41_lowp
+
0x4b
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
76
141
29
23
2
0
0
/
/
lea
0x217
(
%
rip
)
%
r11
#
37704
<
_sk_srcover_bgra_8888_sse41_lowp
+
0x248
>
.
byte
75
99
4
139
/
/
movslq
(
%
r11
%
r9
4
)
%
rax
.
byte
76
1
216
/
/
add
%
r11
%
rax
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
110
36
144
/
/
movd
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
89
/
/
jmp
37560
<
_sk_srcover_bgra_8888_sse41_lowp
+
0xa4
>
.
byte
243
65
15
111
36
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
74
/
/
jmp
37560
<
_sk_srcover_bgra_8888_sse41_lowp
+
0xa4
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
243
65
15
126
36
144
/
/
movq
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
65
15
58
14
224
240
/
/
pblendw
0xf0
%
xmm8
%
xmm4
.
byte
102
68
15
111
197
/
/
movdqa
%
xmm5
%
xmm8
.
byte
235
37
/
/
jmp
37560
<
_sk_srcover_bgra_8888_sse41_lowp
+
0xa4
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
69
15
58
34
68
144
20
1
/
/
pinsrd
0x1
0x14
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
69
15
58
34
68
144
16
0
/
/
pinsrd
0x0
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
243
65
15
111
36
144
/
/
movdqu
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
15
111
53
232
96
0
0
/
/
movdqa
0x60e8
(
%
rip
)
%
xmm6
#
3d650
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1404
>
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
56
0
238
/
/
pshufb
%
xmm6
%
xmm5
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
56
0
254
/
/
pshufb
%
xmm6
%
xmm7
.
byte
102
15
108
239
/
/
punpcklqdq
%
xmm7
%
xmm5
.
byte
102
68
15
111
37
40
89
0
0
/
/
movdqa
0x5928
(
%
rip
)
%
xmm12
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
65
15
219
244
/
/
pand
%
xmm12
%
xmm6
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
68
15
111
13
193
96
0
0
/
/
movdqa
0x60c1
(
%
rip
)
%
xmm9
#
3d660
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1414
>
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
65
15
56
0
249
/
/
pshufb
%
xmm9
%
xmm7
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
69
15
56
0
209
/
/
pshufb
%
xmm9
%
xmm10
.
byte
102
65
15
108
250
/
/
punpcklqdq
%
xmm10
%
xmm7
.
byte
102
68
15
111
13
174
96
0
0
/
/
movdqa
0x60ae
(
%
rip
)
%
xmm9
#
3d670
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1424
>
.
byte
102
65
15
56
0
225
/
/
pshufb
%
xmm9
%
xmm4
.
byte
102
69
15
56
0
193
/
/
pshufb
%
xmm9
%
xmm8
.
byte
102
65
15
108
224
/
/
punpcklqdq
%
xmm8
%
xmm4
.
byte
102
65
15
219
228
/
/
pand
%
xmm12
%
xmm4
.
byte
102
69
15
111
196
/
/
movdqa
%
xmm12
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
68
15
111
204
/
/
movdqa
%
xmm4
%
xmm9
.
byte
102
69
15
213
200
/
/
pmullw
%
xmm8
%
xmm9
.
byte
102
69
15
253
204
/
/
paddw
%
xmm12
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
69
15
213
208
/
/
pmullw
%
xmm8
%
xmm10
.
byte
102
69
15
253
212
/
/
paddw
%
xmm12
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
68
15
253
209
/
/
paddw
%
xmm1
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
69
15
213
216
/
/
pmullw
%
xmm8
%
xmm11
.
byte
102
69
15
253
220
/
/
paddw
%
xmm12
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
68
15
253
218
/
/
paddw
%
xmm2
%
xmm11
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
69
15
253
196
/
/
paddw
%
xmm12
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
68
15
253
195
/
/
paddw
%
xmm3
%
xmm8
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
65
15
235
195
/
/
por
%
xmm11
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
56
51
209
/
/
pmovzxwd
%
xmm1
%
xmm2
.
byte
102
68
15
56
51
224
/
/
pmovzxwd
%
xmm0
%
xmm12
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
65
15
235
217
/
/
por
%
xmm9
%
xmm3
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
105
203
/
/
punpckhwd
%
xmm3
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
97
195
/
/
punpcklwd
%
xmm3
%
xmm0
.
byte
102
65
15
235
196
/
/
por
%
xmm12
%
xmm0
.
byte
65
128
250
6
/
/
cmp
0x6
%
r10b
.
byte
119
24
/
/
ja
376aa
<
_sk_srcover_bgra_8888_sse41_lowp
+
0x1ee
>
.
byte
76
141
21
135
0
0
0
/
/
lea
0x87
(
%
rip
)
%
r10
#
37720
<
_sk_srcover_bgra_8888_sse41_lowp
+
0x264
>
.
byte
75
99
4
138
/
/
movslq
(
%
r10
%
r9
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
65
15
126
4
144
/
/
movd
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
235
63
/
/
jmp
376e9
<
_sk_srcover_bgra_8888_sse41_lowp
+
0x22d
>
.
byte
243
65
15
127
4
144
/
/
movdqu
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
243
65
15
127
76
144
16
/
/
movdqu
%
xmm1
0x10
(
%
r8
%
rdx
4
)
.
byte
235
48
/
/
jmp
376e9
<
_sk_srcover_bgra_8888_sse41_lowp
+
0x22d
>
.
byte
102
65
15
58
22
68
144
8
2
/
/
pextrd
0x2
%
xmm0
0x8
(
%
r8
%
rdx
4
)
.
byte
102
65
15
214
4
144
/
/
movq
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
235
31
/
/
jmp
376e9
<
_sk_srcover_bgra_8888_sse41_lowp
+
0x22d
>
.
byte
102
65
15
58
22
76
144
24
2
/
/
pextrd
0x2
%
xmm1
0x18
(
%
r8
%
rdx
4
)
.
byte
102
65
15
58
22
76
144
20
1
/
/
pextrd
0x1
%
xmm1
0x14
(
%
r8
%
rdx
4
)
.
byte
102
65
15
126
76
144
16
/
/
movd
%
xmm1
0x10
(
%
r8
%
rdx
4
)
.
byte
243
65
15
127
4
144
/
/
movdqu
%
xmm0
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
246
253
/
/
idiv
%
ch
.
byte
255
/
/
(
bad
)
.
byte
255
35
/
/
jmpq
*
(
%
rbx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
86
254
/
/
callq
*
-
0x2
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
77
254
/
/
decl
-
0x2
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
68
254
255
/
/
incl
-
0x1
(
%
rsi
%
rdi
8
)
.
byte
255
55
/
/
pushq
(
%
rdi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
130
255
255
255
162
/
/
incl
-
0x5d000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
153
255
255
255
195
/
/
lcall
*
-
0x3c000001
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
255
255
255
179
/
/
mov
0xb3ffffff
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_start_pipeline_sse2_lowp
.
globl
_sk_start_pipeline_sse2_lowp
FUNCTION
(
_sk_start_pipeline_sse2_lowp
)
_sk_start_pipeline_sse2_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
72
137
229
/
/
mov
%
rsp
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
131
236
24
/
/
sub
0x18
%
rsp
.
byte
73
137
215
/
/
mov
%
rdx
%
r15
.
byte
72
137
243
/
/
mov
%
rsi
%
rbx
.
byte
72
137
125
208
/
/
mov
%
rdi
-
0x30
(
%
rbp
)
.
byte
76
137
198
/
/
mov
%
r8
%
rsi
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
73
137
197
/
/
mov
%
rax
%
r13
.
byte
73
137
246
/
/
mov
%
rsi
%
r14
.
byte
72
137
77
192
/
/
mov
%
rcx
-
0x40
(
%
rbp
)
.
byte
72
57
203
/
/
cmp
%
rcx
%
rbx
.
byte
15
131
131
0
0
0
/
/
jae
377f2
<
_sk_start_pipeline_sse2_lowp
+
0xb6
>
.
byte
72
139
69
208
/
/
mov
-
0x30
(
%
rbp
)
%
rax
.
byte
72
141
64
8
/
/
lea
0x8
(
%
rax
)
%
rax
.
byte
72
137
69
200
/
/
mov
%
rax
-
0x38
(
%
rbp
)
.
byte
76
57
125
200
/
/
cmp
%
r15
-
0x38
(
%
rbp
)
.
byte
72
139
85
208
/
/
mov
-
0x30
(
%
rbp
)
%
rdx
.
byte
119
59
/
/
ja
377c0
<
_sk_start_pipeline_sse2_lowp
+
0x84
>
.
byte
76
139
101
208
/
/
mov
-
0x30
(
%
rbp
)
%
r12
.
byte
49
255
/
/
xor
%
edi
%
edi
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
76
137
226
/
/
mov
%
r12
%
rdx
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
73
141
84
36
8
/
/
lea
0x8
(
%
r12
)
%
rdx
.
byte
73
131
196
16
/
/
add
0x10
%
r12
.
byte
77
57
252
/
/
cmp
%
r15
%
r12
.
byte
73
137
212
/
/
mov
%
rdx
%
r12
.
byte
118
201
/
/
jbe
37789
<
_sk_start_pipeline_sse2_lowp
+
0x4d
>
.
byte
76
137
255
/
/
mov
%
r15
%
rdi
.
byte
72
41
215
/
/
sub
%
rdx
%
rdi
.
byte
116
33
/
/
je
377e9
<
_sk_start_pipeline_sse2_lowp
+
0xad
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
76
137
246
/
/
mov
%
r14
%
rsi
.
byte
72
137
217
/
/
mov
%
rbx
%
rcx
.
byte
65
255
213
/
/
callq
*
%
r13
.
byte
72
255
195
/
/
inc
%
rbx
.
byte
72
59
93
192
/
/
cmp
-
0x40
(
%
rbp
)
%
rbx
.
byte
117
137
/
/
jne
3777b
<
_sk_start_pipeline_sse2_lowp
+
0x3f
>
.
byte
72
131
196
24
/
/
add
0x18
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
195
/
/
retq
HIDDEN
_sk_just_return_sse2_lowp
.
globl
_sk_just_return_sse2_lowp
FUNCTION
(
_sk_just_return_sse2_lowp
)
_sk_just_return_sse2_lowp
:
.
byte
195
/
/
retq
HIDDEN
_sk_seed_shader_sse2_lowp
.
globl
_sk_seed_shader_sse2_lowp
FUNCTION
(
_sk_seed_shader_sse2_lowp
)
_sk_seed_shader_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
208
/
/
cvtdq2ps
%
xmm0
%
xmm2
.
byte
15
16
0
/
/
movups
(
%
rax
)
%
xmm0
.
byte
15
16
72
16
/
/
movups
0x10
(
%
rax
)
%
xmm1
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
102
15
110
209
/
/
movd
%
ecx
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
88
21
208
86
0
0
/
/
addps
0x56d0
(
%
rip
)
%
xmm2
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_translate_sse2_lowp
.
globl
_sk_matrix_translate_sse2_lowp
FUNCTION
(
_sk_matrix_translate_sse2_lowp
)
_sk_matrix_translate_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
65
15
88
209
/
/
addps
%
xmm9
%
xmm2
.
byte
65
15
88
217
/
/
addps
%
xmm9
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_scale_translate_sse2_lowp
.
globl
_sk_matrix_scale_translate_sse2_lowp
FUNCTION
(
_sk_matrix_scale_translate_sse2_lowp
)
_sk_matrix_scale_translate_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
194
/
/
addps
%
xmm10
%
xmm0
.
byte
65
15
88
202
/
/
addps
%
xmm10
%
xmm1
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
65
15
89
217
/
/
mulps
%
xmm9
%
xmm3
.
byte
65
15
89
209
/
/
mulps
%
xmm9
%
xmm2
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_2x3_sse2_lowp
.
globl
_sk_matrix_2x3_sse2_lowp
FUNCTION
(
_sk_matrix_2x3_sse2_lowp
)
_sk_matrix_2x3_sse2_lowp
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
68
15
16
88
4
/
/
movss
0x4
(
%
rax
)
%
xmm11
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
68
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
72
16
/
/
movss
0x10
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
226
/
/
movaps
%
xmm2
%
xmm12
.
byte
69
15
89
226
/
/
mulps
%
xmm10
%
xmm12
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
69
15
88
209
/
/
addps
%
xmm9
%
xmm10
.
byte
69
15
88
225
/
/
addps
%
xmm9
%
xmm12
.
byte
68
15
40
201
/
/
movaps
%
xmm1
%
xmm9
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
65
15
89
192
/
/
mulps
%
xmm8
%
xmm0
.
byte
65
15
88
196
/
/
addps
%
xmm12
%
xmm0
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
243
68
15
16
80
12
/
/
movss
0xc
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
96
20
/
/
movss
0x14
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
68
15
89
211
/
/
mulps
%
xmm3
%
xmm10
.
byte
69
15
88
212
/
/
addps
%
xmm12
%
xmm10
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
65
15
89
203
/
/
mulps
%
xmm11
%
xmm1
.
byte
68
15
88
209
/
/
addps
%
xmm1
%
xmm10
.
byte
69
15
89
216
/
/
mulps
%
xmm8
%
xmm11
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
201
/
/
movaps
%
xmm9
%
xmm1
.
byte
65
15
40
218
/
/
movaps
%
xmm10
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_matrix_perspective_sse2_lowp
.
globl
_sk_matrix_perspective_sse2_lowp
FUNCTION
(
_sk_matrix_perspective_sse2_lowp
)
_sk_matrix_perspective_sse2_lowp
:
.
byte
68
15
40
193
/
/
movaps
%
xmm1
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
8
/
/
movss
(
%
rax
)
%
xmm1
.
byte
243
68
15
16
80
4
/
/
movss
0x4
(
%
rax
)
%
xmm10
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
72
8
/
/
movss
0x8
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
69
15
88
209
/
/
addps
%
xmm9
%
xmm10
.
byte
69
15
88
217
/
/
addps
%
xmm9
%
xmm11
.
byte
68
15
40
200
/
/
movaps
%
xmm0
%
xmm9
.
byte
68
15
89
201
/
/
mulps
%
xmm1
%
xmm9
.
byte
69
15
88
202
/
/
addps
%
xmm10
%
xmm9
.
byte
65
15
89
200
/
/
mulps
%
xmm8
%
xmm1
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
243
68
15
16
96
12
/
/
movss
0xc
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
80
16
/
/
movss
0x10
(
%
rax
)
%
xmm10
.
byte
69
15
198
210
0
/
/
shufps
0x0
%
xmm10
%
xmm10
.
byte
243
68
15
16
104
20
/
/
movss
0x14
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
68
15
89
210
/
/
mulps
%
xmm2
%
xmm10
.
byte
69
15
88
213
/
/
addps
%
xmm13
%
xmm10
.
byte
69
15
88
221
/
/
addps
%
xmm13
%
xmm11
.
byte
68
15
40
232
/
/
movaps
%
xmm0
%
xmm13
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
69
15
88
213
/
/
addps
%
xmm13
%
xmm10
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
69
15
88
220
/
/
addps
%
xmm12
%
xmm11
.
byte
243
68
15
16
96
24
/
/
movss
0x18
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
243
68
15
16
104
28
/
/
movss
0x1c
(
%
rax
)
%
xmm13
.
byte
69
15
198
237
0
/
/
shufps
0x0
%
xmm13
%
xmm13
.
byte
243
68
15
16
112
32
/
/
movss
0x20
(
%
rax
)
%
xmm14
.
byte
69
15
198
246
0
/
/
shufps
0x0
%
xmm14
%
xmm14
.
byte
65
15
89
221
/
/
mulps
%
xmm13
%
xmm3
.
byte
68
15
89
234
/
/
mulps
%
xmm2
%
xmm13
.
byte
69
15
88
238
/
/
addps
%
xmm14
%
xmm13
.
byte
65
15
88
222
/
/
addps
%
xmm14
%
xmm3
.
byte
65
15
89
196
/
/
mulps
%
xmm12
%
xmm0
.
byte
65
15
88
197
/
/
addps
%
xmm13
%
xmm0
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
68
15
88
227
/
/
addps
%
xmm3
%
xmm12
.
byte
15
83
192
/
/
rcpps
%
xmm0
%
xmm0
.
byte
65
15
83
212
/
/
rcpps
%
xmm12
%
xmm2
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
68
15
89
218
/
/
mulps
%
xmm2
%
xmm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
65
15
40
193
/
/
movaps
%
xmm9
%
xmm0
.
byte
65
15
40
210
/
/
movaps
%
xmm10
%
xmm2
.
byte
65
15
40
219
/
/
movaps
%
xmm11
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_uniform_color_sse2_lowp
.
globl
_sk_uniform_color_sse2_lowp
FUNCTION
(
_sk_uniform_color_sse2_lowp
)
_sk_uniform_color_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
110
64
16
/
/
movd
0x10
(
%
rax
)
%
xmm0
.
byte
242
15
112
192
0
/
/
pshuflw
0x0
%
xmm0
%
xmm0
.
byte
102
15
112
192
80
/
/
pshufd
0x50
%
xmm0
%
xmm0
.
byte
68
15
183
64
18
/
/
movzwl
0x12
(
%
rax
)
%
r8d
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
242
15
112
201
0
/
/
pshuflw
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
201
80
/
/
pshufd
0x50
%
xmm1
%
xmm1
.
byte
102
15
110
80
20
/
/
movd
0x14
(
%
rax
)
%
xmm2
.
byte
242
15
112
210
0
/
/
pshuflw
0x0
%
xmm2
%
xmm2
.
byte
102
15
112
210
80
/
/
pshufd
0x50
%
xmm2
%
xmm2
.
byte
15
183
64
22
/
/
movzwl
0x16
(
%
rax
)
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
242
15
112
219
0
/
/
pshuflw
0x0
%
xmm3
%
xmm3
.
byte
102
15
112
219
80
/
/
pshufd
0x50
%
xmm3
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_black_color_sse2_lowp
.
globl
_sk_black_color_sse2_lowp
FUNCTION
(
_sk_black_color_sse2_lowp
)
_sk_black_color_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
21
84
0
0
/
/
movaps
0x5415
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_white_color_sse2_lowp
.
globl
_sk_white_color_sse2_lowp
FUNCTION
(
_sk_white_color_sse2_lowp
)
_sk_white_color_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
5
1
84
0
0
/
/
movaps
0x5401
(
%
rip
)
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_set_rgb_sse2_lowp
.
globl
_sk_set_rgb_sse2_lowp
FUNCTION
(
_sk_set_rgb_sse2_lowp
)
_sk_set_rgb_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
21
100
74
0
0
/
/
movss
0x4a64
(
%
rip
)
%
xmm2
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
243
15
16
0
/
/
movss
(
%
rax
)
%
xmm0
.
byte
243
15
89
194
/
/
mulss
%
xmm2
%
xmm0
.
byte
243
68
15
16
5
35
74
0
0
/
/
movss
0x4a23
(
%
rip
)
%
xmm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
243
65
15
88
192
/
/
addss
%
xmm8
%
xmm0
.
byte
243
68
15
44
192
/
/
cvttss2si
%
xmm0
%
r8d
.
byte
102
65
15
110
192
/
/
movd
%
r8d
%
xmm0
.
byte
242
15
112
192
0
/
/
pshuflw
0x0
%
xmm0
%
xmm0
.
byte
102
15
112
192
80
/
/
pshufd
0x50
%
xmm0
%
xmm0
.
byte
243
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm1
.
byte
243
15
89
202
/
/
mulss
%
xmm2
%
xmm1
.
byte
243
65
15
88
200
/
/
addss
%
xmm8
%
xmm1
.
byte
243
68
15
44
193
/
/
cvttss2si
%
xmm1
%
r8d
.
byte
102
65
15
110
200
/
/
movd
%
r8d
%
xmm1
.
byte
242
15
112
201
0
/
/
pshuflw
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
201
80
/
/
pshufd
0x50
%
xmm1
%
xmm1
.
byte
243
15
89
80
8
/
/
mulss
0x8
(
%
rax
)
%
xmm2
.
byte
243
65
15
88
208
/
/
addss
%
xmm8
%
xmm2
.
byte
243
15
44
194
/
/
cvttss2si
%
xmm2
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
242
15
112
210
0
/
/
pshuflw
0x0
%
xmm2
%
xmm2
.
byte
102
15
112
210
80
/
/
pshufd
0x50
%
xmm2
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_sse2_lowp
.
globl
_sk_clamp_a_sse2_lowp
FUNCTION
(
_sk_clamp_a_sse2_lowp
)
_sk_clamp_a_sse2_lowp
:
.
byte
102
68
15
111
5
7
91
0
0
/
/
movdqa
0x5b07
(
%
rip
)
%
xmm8
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
102
69
15
239
200
/
/
pxor
%
xmm8
%
xmm9
.
byte
102
68
15
111
211
/
/
movdqa
%
xmm3
%
xmm10
.
byte
102
69
15
239
208
/
/
pxor
%
xmm8
%
xmm10
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
69
15
101
217
/
/
pcmpgtw
%
xmm9
%
xmm11
.
byte
102
65
15
219
195
/
/
pand
%
xmm11
%
xmm0
.
byte
102
68
15
223
219
/
/
pandn
%
xmm3
%
xmm11
.
byte
102
65
15
235
195
/
/
por
%
xmm11
%
xmm0
.
byte
102
68
15
111
201
/
/
movdqa
%
xmm1
%
xmm9
.
byte
102
69
15
239
200
/
/
pxor
%
xmm8
%
xmm9
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
69
15
101
217
/
/
pcmpgtw
%
xmm9
%
xmm11
.
byte
102
65
15
219
203
/
/
pand
%
xmm11
%
xmm1
.
byte
102
68
15
223
219
/
/
pandn
%
xmm3
%
xmm11
.
byte
102
65
15
235
203
/
/
por
%
xmm11
%
xmm1
.
byte
102
68
15
239
194
/
/
pxor
%
xmm2
%
xmm8
.
byte
102
69
15
101
208
/
/
pcmpgtw
%
xmm8
%
xmm10
.
byte
102
65
15
219
210
/
/
pand
%
xmm10
%
xmm2
.
byte
102
68
15
223
211
/
/
pandn
%
xmm3
%
xmm10
.
byte
102
65
15
235
210
/
/
por
%
xmm10
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clamp_a_dst_sse2_lowp
.
globl
_sk_clamp_a_dst_sse2_lowp
FUNCTION
(
_sk_clamp_a_dst_sse2_lowp
)
_sk_clamp_a_dst_sse2_lowp
:
.
byte
102
68
15
111
5
145
90
0
0
/
/
movdqa
0x5a91
(
%
rip
)
%
xmm8
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
68
15
111
204
/
/
movdqa
%
xmm4
%
xmm9
.
byte
102
69
15
239
200
/
/
pxor
%
xmm8
%
xmm9
.
byte
102
68
15
111
215
/
/
movdqa
%
xmm7
%
xmm10
.
byte
102
69
15
239
208
/
/
pxor
%
xmm8
%
xmm10
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
69
15
101
217
/
/
pcmpgtw
%
xmm9
%
xmm11
.
byte
102
65
15
219
227
/
/
pand
%
xmm11
%
xmm4
.
byte
102
68
15
223
223
/
/
pandn
%
xmm7
%
xmm11
.
byte
102
65
15
235
227
/
/
por
%
xmm11
%
xmm4
.
byte
102
68
15
111
205
/
/
movdqa
%
xmm5
%
xmm9
.
byte
102
69
15
239
200
/
/
pxor
%
xmm8
%
xmm9
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
69
15
101
217
/
/
pcmpgtw
%
xmm9
%
xmm11
.
byte
102
65
15
219
235
/
/
pand
%
xmm11
%
xmm5
.
byte
102
68
15
223
223
/
/
pandn
%
xmm7
%
xmm11
.
byte
102
65
15
235
235
/
/
por
%
xmm11
%
xmm5
.
byte
102
68
15
239
198
/
/
pxor
%
xmm6
%
xmm8
.
byte
102
69
15
101
208
/
/
pcmpgtw
%
xmm8
%
xmm10
.
byte
102
65
15
219
242
/
/
pand
%
xmm10
%
xmm6
.
byte
102
68
15
223
215
/
/
pandn
%
xmm7
%
xmm10
.
byte
102
65
15
235
242
/
/
por
%
xmm10
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_sse2_lowp
.
globl
_sk_premul_sse2_lowp
FUNCTION
(
_sk_premul_sse2_lowp
)
_sk_premul_sse2_lowp
:
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
68
15
111
5
135
82
0
0
/
/
movdqa
0x5287
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_premul_dst_sse2_lowp
.
globl
_sk_premul_dst_sse2_lowp
FUNCTION
(
_sk_premul_dst_sse2_lowp
)
_sk_premul_dst_sse2_lowp
:
.
byte
102
15
213
231
/
/
pmullw
%
xmm7
%
xmm4
.
byte
102
68
15
111
5
80
82
0
0
/
/
movdqa
0x5250
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
224
/
/
paddw
%
xmm8
%
xmm4
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
213
239
/
/
pmullw
%
xmm7
%
xmm5
.
byte
102
65
15
253
232
/
/
paddw
%
xmm8
%
xmm5
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
213
247
/
/
pmullw
%
xmm7
%
xmm6
.
byte
102
65
15
253
240
/
/
paddw
%
xmm8
%
xmm6
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_sse2_lowp
.
globl
_sk_force_opaque_sse2_lowp
FUNCTION
(
_sk_force_opaque_sse2_lowp
)
_sk_force_opaque_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
29
82
0
0
/
/
movaps
0x521d
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_force_opaque_dst_sse2_lowp
.
globl
_sk_force_opaque_dst_sse2_lowp
FUNCTION
(
_sk_force_opaque_dst_sse2_lowp
)
_sk_force_opaque_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
18
82
0
0
/
/
movaps
0x5212
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_swap_rb_sse2_lowp
.
globl
_sk_swap_rb_sse2_lowp
FUNCTION
(
_sk_swap_rb_sse2_lowp
)
_sk_swap_rb_sse2_lowp
:
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
65
15
40
208
/
/
movaps
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_src_dst_sse2_lowp
.
globl
_sk_move_src_dst_sse2_lowp
FUNCTION
(
_sk_move_src_dst_sse2_lowp
)
_sk_move_src_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_move_dst_src_sse2_lowp
.
globl
_sk_move_dst_src_sse2_lowp
FUNCTION
(
_sk_move_dst_src_sse2_lowp
)
_sk_move_dst_src_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_invert_sse2_lowp
.
globl
_sk_invert_sse2_lowp
FUNCTION
(
_sk_invert_sse2_lowp
)
_sk_invert_sse2_lowp
:
.
byte
102
68
15
111
5
216
81
0
0
/
/
movdqa
0x51d8
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
68
15
249
200
/
/
psubw
%
xmm0
%
xmm9
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
249
209
/
/
psubw
%
xmm1
%
xmm10
.
byte
102
69
15
111
216
/
/
movdqa
%
xmm8
%
xmm11
.
byte
102
68
15
249
218
/
/
psubw
%
xmm2
%
xmm11
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_clear_sse2_lowp
.
globl
_sk_clear_sse2_lowp
FUNCTION
(
_sk_clear_sse2_lowp
)
_sk_clear_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcatop_sse2_lowp
.
globl
_sk_srcatop_sse2_lowp
FUNCTION
(
_sk_srcatop_sse2_lowp
)
_sk_srcatop_sse2_lowp
:
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
111
29
124
81
0
0
/
/
movdqa
0x517c
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
111
203
/
/
movdqa
%
xmm3
%
xmm9
.
byte
102
69
15
249
200
/
/
psubw
%
xmm8
%
xmm9
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
196
/
/
pmullw
%
xmm4
%
xmm8
.
byte
102
15
253
195
/
/
paddw
%
xmm3
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
15
253
203
/
/
paddw
%
xmm3
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
206
/
/
pmullw
%
xmm6
%
xmm9
.
byte
102
15
253
211
/
/
paddw
%
xmm3
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
111
199
/
/
movdqa
%
xmm7
%
xmm8
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstatop_sse2_lowp
.
globl
_sk_dstatop_sse2_lowp
FUNCTION
(
_sk_dstatop_sse2_lowp
)
_sk_dstatop_sse2_lowp
:
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
68
15
111
13
252
80
0
0
/
/
movdqa
0x50fc
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
68
15
249
207
/
/
psubw
%
xmm7
%
xmm9
.
byte
102
65
15
213
193
/
/
pmullw
%
xmm9
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
201
/
/
pmullw
%
xmm9
%
xmm1
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
209
/
/
pmullw
%
xmm9
%
xmm2
.
byte
102
65
15
253
211
/
/
paddw
%
xmm11
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcin_sse2_lowp
.
globl
_sk_srcin_sse2_lowp
FUNCTION
(
_sk_srcin_sse2_lowp
)
_sk_srcin_sse2_lowp
:
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
68
15
111
5
135
80
0
0
/
/
movdqa
0x5087
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstin_sse2_lowp
.
globl
_sk_dstin_sse2_lowp
FUNCTION
(
_sk_dstin_sse2_lowp
)
_sk_dstin_sse2_lowp
:
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
68
15
111
5
62
80
0
0
/
/
movdqa
0x503e
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
111
214
/
/
movdqa
%
xmm6
%
xmm2
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcout_sse2_lowp
.
globl
_sk_srcout_sse2_lowp
FUNCTION
(
_sk_srcout_sse2_lowp
)
_sk_srcout_sse2_lowp
:
.
byte
102
68
15
111
5
245
79
0
0
/
/
movdqa
0x4ff5
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
68
15
249
207
/
/
psubw
%
xmm7
%
xmm9
.
byte
102
65
15
213
193
/
/
pmullw
%
xmm9
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
201
/
/
pmullw
%
xmm9
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
209
/
/
pmullw
%
xmm9
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstout_sse2_lowp
.
globl
_sk_dstout_sse2_lowp
FUNCTION
(
_sk_dstout_sse2_lowp
)
_sk_dstout_sse2_lowp
:
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
68
15
111
5
158
79
0
0
/
/
movdqa
0x4f9e
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
249
216
/
/
psubw
%
xmm0
%
xmm3
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_sse2_lowp
.
globl
_sk_srcover_sse2_lowp
FUNCTION
(
_sk_srcover_sse2_lowp
)
_sk_srcover_sse2_lowp
:
.
byte
102
68
15
111
13
68
79
0
0
/
/
movdqa
0x4f44
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
212
/
/
pmullw
%
xmm4
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
253
194
/
/
paddw
%
xmm10
%
xmm0
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
213
/
/
pmullw
%
xmm5
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
214
/
/
pmullw
%
xmm6
%
xmm10
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_dstover_sse2_lowp
.
globl
_sk_dstover_sse2_lowp
FUNCTION
(
_sk_dstover_sse2_lowp
)
_sk_dstover_sse2_lowp
:
.
byte
102
68
15
111
5
202
78
0
0
/
/
movdqa
0x4eca
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
68
15
249
207
/
/
psubw
%
xmm7
%
xmm9
.
byte
102
65
15
213
193
/
/
pmullw
%
xmm9
%
xmm0
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
196
/
/
paddw
%
xmm4
%
xmm0
.
byte
102
65
15
213
201
/
/
pmullw
%
xmm9
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
253
205
/
/
paddw
%
xmm5
%
xmm1
.
byte
102
65
15
213
209
/
/
pmullw
%
xmm9
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_modulate_sse2_lowp
.
globl
_sk_modulate_sse2_lowp
FUNCTION
(
_sk_modulate_sse2_lowp
)
_sk_modulate_sse2_lowp
:
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
68
15
111
5
99
78
0
0
/
/
movdqa
0x4e63
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_multiply_sse2_lowp
.
globl
_sk_multiply_sse2_lowp
FUNCTION
(
_sk_multiply_sse2_lowp
)
_sk_multiply_sse2_lowp
:
.
byte
102
68
15
111
13
34
78
0
0
/
/
movdqa
0x4e22
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
212
/
/
pmullw
%
xmm4
%
xmm10
.
byte
102
68
15
111
220
/
/
movdqa
%
xmm4
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
68
15
249
223
/
/
psubw
%
xmm7
%
xmm11
.
byte
102
65
15
213
195
/
/
pmullw
%
xmm11
%
xmm0
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
253
194
/
/
paddw
%
xmm10
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
213
/
/
pmullw
%
xmm5
%
xmm10
.
byte
102
68
15
111
221
/
/
movdqa
%
xmm5
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
68
15
249
223
/
/
psubw
%
xmm7
%
xmm11
.
byte
102
65
15
213
203
/
/
pmullw
%
xmm11
%
xmm1
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
68
15
213
214
/
/
pmullw
%
xmm6
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
69
15
253
217
/
/
paddw
%
xmm9
%
xmm11
.
byte
102
68
15
249
223
/
/
psubw
%
xmm7
%
xmm11
.
byte
102
65
15
213
211
/
/
pmullw
%
xmm11
%
xmm2
.
byte
102
69
15
253
209
/
/
paddw
%
xmm9
%
xmm10
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
65
15
213
217
/
/
pmullw
%
xmm9
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
65
15
253
216
/
/
paddw
%
xmm8
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_plus__sse2_lowp
.
globl
_sk_plus__sse2_lowp
FUNCTION
(
_sk_plus__sse2_lowp
)
_sk_plus__sse2_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
253
204
/
/
paddw
%
xmm4
%
xmm1
.
byte
102
68
15
111
21
238
85
0
0
/
/
movdqa
0x55ee
(
%
rip
)
%
xmm10
#
3d740
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14f4
>
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
217
194
/
/
psubusw
%
xmm10
%
xmm0
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
102
65
15
117
196
/
/
pcmpeqw
%
xmm12
%
xmm0
.
byte
102
68
15
111
29
66
77
0
0
/
/
movdqa
0x4d42
(
%
rip
)
%
xmm11
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
102
65
15
223
195
/
/
pandn
%
xmm11
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
68
15
253
197
/
/
paddw
%
xmm5
%
xmm8
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
65
15
217
202
/
/
psubusw
%
xmm10
%
xmm1
.
byte
102
65
15
117
204
/
/
pcmpeqw
%
xmm12
%
xmm1
.
byte
102
68
15
219
193
/
/
pand
%
xmm1
%
xmm8
.
byte
102
65
15
223
203
/
/
pandn
%
xmm11
%
xmm1
.
byte
102
65
15
235
200
/
/
por
%
xmm8
%
xmm1
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
68
15
111
194
/
/
movdqa
%
xmm2
%
xmm8
.
byte
102
69
15
217
194
/
/
psubusw
%
xmm10
%
xmm8
.
byte
102
69
15
117
196
/
/
pcmpeqw
%
xmm12
%
xmm8
.
byte
102
65
15
219
208
/
/
pand
%
xmm8
%
xmm2
.
byte
102
69
15
223
195
/
/
pandn
%
xmm11
%
xmm8
.
byte
102
68
15
235
194
/
/
por
%
xmm2
%
xmm8
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
102
68
15
111
203
/
/
movdqa
%
xmm3
%
xmm9
.
byte
102
69
15
217
202
/
/
psubusw
%
xmm10
%
xmm9
.
byte
102
69
15
117
204
/
/
pcmpeqw
%
xmm12
%
xmm9
.
byte
102
65
15
219
217
/
/
pand
%
xmm9
%
xmm3
.
byte
102
69
15
223
203
/
/
pandn
%
xmm11
%
xmm9
.
byte
102
68
15
235
203
/
/
por
%
xmm3
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_screen_sse2_lowp
.
globl
_sk_screen_sse2_lowp
FUNCTION
(
_sk_screen_sse2_lowp
)
_sk_screen_sse2_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
213
204
/
/
pmullw
%
xmm4
%
xmm1
.
byte
102
68
15
111
21
162
76
0
0
/
/
movdqa
0x4ca2
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
69
15
253
194
/
/
paddw
%
xmm10
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
65
15
249
200
/
/
psubw
%
xmm8
%
xmm1
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
249
194
/
/
psubw
%
xmm2
%
xmm8
.
byte
102
68
15
111
207
/
/
movdqa
%
xmm7
%
xmm9
.
byte
102
68
15
253
203
/
/
paddw
%
xmm3
%
xmm9
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
218
/
/
paddw
%
xmm10
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
68
15
249
203
/
/
psubw
%
xmm3
%
xmm9
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xor__sse2_lowp
.
globl
_sk_xor__sse2_lowp
FUNCTION
(
_sk_xor__sse2_lowp
)
_sk_xor__sse2_lowp
:
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
68
15
111
13
32
76
0
0
/
/
movdqa
0x4c20
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
209
/
/
movdqa
%
xmm9
%
xmm10
.
byte
102
68
15
249
215
/
/
psubw
%
xmm7
%
xmm10
.
byte
102
65
15
213
194
/
/
pmullw
%
xmm10
%
xmm0
.
byte
102
65
15
111
217
/
/
movdqa
%
xmm9
%
xmm3
.
byte
102
65
15
249
216
/
/
psubw
%
xmm8
%
xmm3
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
68
15
213
220
/
/
pmullw
%
xmm4
%
xmm11
.
byte
102
65
15
253
195
/
/
paddw
%
xmm11
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
202
/
/
pmullw
%
xmm10
%
xmm1
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
68
15
213
221
/
/
pmullw
%
xmm5
%
xmm11
.
byte
102
65
15
253
203
/
/
paddw
%
xmm11
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
210
/
/
pmullw
%
xmm10
%
xmm2
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
68
15
213
222
/
/
pmullw
%
xmm6
%
xmm11
.
byte
102
65
15
253
211
/
/
paddw
%
xmm11
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
69
15
213
208
/
/
pmullw
%
xmm8
%
xmm10
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
65
15
253
218
/
/
paddw
%
xmm10
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_darken_sse2_lowp
.
globl
_sk_darken_sse2_lowp
FUNCTION
(
_sk_darken_sse2_lowp
)
_sk_darken_sse2_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
68
15
111
208
/
/
movdqa
%
xmm0
%
xmm10
.
byte
102
68
15
111
228
/
/
movdqa
%
xmm4
%
xmm12
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
65
15
253
194
/
/
paddw
%
xmm10
%
xmm0
.
byte
102
68
15
213
215
/
/
pmullw
%
xmm7
%
xmm10
.
byte
102
68
15
213
227
/
/
pmullw
%
xmm3
%
xmm12
.
byte
102
68
15
111
13
251
82
0
0
/
/
movdqa
0x52fb
(
%
rip
)
%
xmm9
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
69
15
239
217
/
/
pxor
%
xmm9
%
xmm11
.
byte
102
65
15
111
204
/
/
movdqa
%
xmm12
%
xmm1
.
byte
102
65
15
239
201
/
/
pxor
%
xmm9
%
xmm1
.
byte
102
65
15
101
203
/
/
pcmpgtw
%
xmm11
%
xmm1
.
byte
102
68
15
219
225
/
/
pand
%
xmm1
%
xmm12
.
byte
102
65
15
223
202
/
/
pandn
%
xmm10
%
xmm1
.
byte
102
65
15
235
204
/
/
por
%
xmm12
%
xmm1
.
byte
102
68
15
111
21
58
75
0
0
/
/
movdqa
0x4b3a
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
68
15
111
221
/
/
movdqa
%
xmm5
%
xmm11
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
69
15
111
224
/
/
movdqa
%
xmm8
%
xmm12
.
byte
102
69
15
239
225
/
/
pxor
%
xmm9
%
xmm12
.
byte
102
69
15
111
235
/
/
movdqa
%
xmm11
%
xmm13
.
byte
102
69
15
239
233
/
/
pxor
%
xmm9
%
xmm13
.
byte
102
69
15
101
236
/
/
pcmpgtw
%
xmm12
%
xmm13
.
byte
102
69
15
219
221
/
/
pand
%
xmm13
%
xmm11
.
byte
102
69
15
223
232
/
/
pandn
%
xmm8
%
xmm13
.
byte
102
69
15
235
235
/
/
por
%
xmm11
%
xmm13
.
byte
102
69
15
253
234
/
/
paddw
%
xmm10
%
xmm13
.
byte
102
65
15
113
213
8
/
/
psrlw
0x8
%
xmm13
.
byte
102
65
15
249
205
/
/
psubw
%
xmm13
%
xmm1
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
68
15
111
226
/
/
movdqa
%
xmm2
%
xmm12
.
byte
102
69
15
239
225
/
/
pxor
%
xmm9
%
xmm12
.
byte
102
69
15
239
203
/
/
pxor
%
xmm11
%
xmm9
.
byte
102
69
15
101
204
/
/
pcmpgtw
%
xmm12
%
xmm9
.
byte
102
69
15
219
217
/
/
pand
%
xmm9
%
xmm11
.
byte
102
68
15
223
202
/
/
pandn
%
xmm2
%
xmm9
.
byte
102
69
15
235
203
/
/
por
%
xmm11
%
xmm9
.
byte
102
69
15
253
202
/
/
paddw
%
xmm10
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
69
15
249
193
/
/
psubw
%
xmm9
%
xmm8
.
byte
102
65
15
111
210
/
/
movdqa
%
xmm10
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lighten_sse2_lowp
.
globl
_sk_lighten_sse2_lowp
FUNCTION
(
_sk_lighten_sse2_lowp
)
_sk_lighten_sse2_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
68
15
111
224
/
/
movdqa
%
xmm0
%
xmm12
.
byte
102
68
15
111
212
/
/
movdqa
%
xmm4
%
xmm10
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
65
15
253
196
/
/
paddw
%
xmm12
%
xmm0
.
byte
102
68
15
213
231
/
/
pmullw
%
xmm7
%
xmm12
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
68
15
111
13
210
81
0
0
/
/
movdqa
0x51d2
(
%
rip
)
%
xmm9
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
69
15
111
220
/
/
movdqa
%
xmm12
%
xmm11
.
byte
102
69
15
239
217
/
/
pxor
%
xmm9
%
xmm11
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
239
201
/
/
pxor
%
xmm9
%
xmm1
.
byte
102
65
15
101
203
/
/
pcmpgtw
%
xmm11
%
xmm1
.
byte
102
68
15
219
225
/
/
pand
%
xmm1
%
xmm12
.
byte
102
65
15
223
202
/
/
pandn
%
xmm10
%
xmm1
.
byte
102
65
15
235
204
/
/
por
%
xmm12
%
xmm1
.
byte
102
68
15
111
21
17
74
0
0
/
/
movdqa
0x4a11
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
68
15
111
221
/
/
movdqa
%
xmm5
%
xmm11
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
69
15
111
224
/
/
movdqa
%
xmm8
%
xmm12
.
byte
102
69
15
239
225
/
/
pxor
%
xmm9
%
xmm12
.
byte
102
69
15
111
235
/
/
movdqa
%
xmm11
%
xmm13
.
byte
102
69
15
239
233
/
/
pxor
%
xmm9
%
xmm13
.
byte
102
69
15
101
236
/
/
pcmpgtw
%
xmm12
%
xmm13
.
byte
102
69
15
219
197
/
/
pand
%
xmm13
%
xmm8
.
byte
102
69
15
223
235
/
/
pandn
%
xmm11
%
xmm13
.
byte
102
69
15
235
232
/
/
por
%
xmm8
%
xmm13
.
byte
102
69
15
253
234
/
/
paddw
%
xmm10
%
xmm13
.
byte
102
65
15
113
213
8
/
/
psrlw
0x8
%
xmm13
.
byte
102
65
15
249
205
/
/
psubw
%
xmm13
%
xmm1
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
68
15
111
226
/
/
movdqa
%
xmm2
%
xmm12
.
byte
102
69
15
239
225
/
/
pxor
%
xmm9
%
xmm12
.
byte
102
69
15
239
203
/
/
pxor
%
xmm11
%
xmm9
.
byte
102
69
15
101
204
/
/
pcmpgtw
%
xmm12
%
xmm9
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
102
69
15
223
203
/
/
pandn
%
xmm11
%
xmm9
.
byte
102
68
15
235
202
/
/
por
%
xmm2
%
xmm9
.
byte
102
69
15
253
202
/
/
paddw
%
xmm10
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
69
15
249
193
/
/
psubw
%
xmm9
%
xmm8
.
byte
102
65
15
111
210
/
/
movdqa
%
xmm10
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_difference_sse2_lowp
.
globl
_sk_difference_sse2_lowp
FUNCTION
(
_sk_difference_sse2_lowp
)
_sk_difference_sse2_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
68
15
111
224
/
/
movdqa
%
xmm0
%
xmm12
.
byte
102
68
15
111
212
/
/
movdqa
%
xmm4
%
xmm10
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
65
15
253
196
/
/
paddw
%
xmm12
%
xmm0
.
byte
102
68
15
213
231
/
/
pmullw
%
xmm7
%
xmm12
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
68
15
111
13
169
80
0
0
/
/
movdqa
0x50a9
(
%
rip
)
%
xmm9
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
69
15
111
220
/
/
movdqa
%
xmm12
%
xmm11
.
byte
102
69
15
239
217
/
/
pxor
%
xmm9
%
xmm11
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
239
201
/
/
pxor
%
xmm9
%
xmm1
.
byte
102
65
15
101
203
/
/
pcmpgtw
%
xmm11
%
xmm1
.
byte
102
68
15
219
225
/
/
pand
%
xmm1
%
xmm12
.
byte
102
65
15
223
202
/
/
pandn
%
xmm10
%
xmm1
.
byte
102
65
15
235
204
/
/
por
%
xmm12
%
xmm1
.
byte
102
68
15
111
21
232
72
0
0
/
/
movdqa
0x48e8
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
15
113
209
7
/
/
psrlw
0x7
%
xmm1
.
byte
102
68
15
111
29
85
80
0
0
/
/
movdqa
0x5055
(
%
rip
)
%
xmm11
#
3d630
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13e4
>
.
byte
102
65
15
219
203
/
/
pand
%
xmm11
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
68
15
111
229
/
/
movdqa
%
xmm5
%
xmm12
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
68
15
213
227
/
/
pmullw
%
xmm3
%
xmm12
.
byte
102
69
15
111
232
/
/
movdqa
%
xmm8
%
xmm13
.
byte
102
69
15
239
233
/
/
pxor
%
xmm9
%
xmm13
.
byte
102
69
15
111
244
/
/
movdqa
%
xmm12
%
xmm14
.
byte
102
69
15
239
241
/
/
pxor
%
xmm9
%
xmm14
.
byte
102
69
15
101
245
/
/
pcmpgtw
%
xmm13
%
xmm14
.
byte
102
69
15
219
198
/
/
pand
%
xmm14
%
xmm8
.
byte
102
69
15
223
244
/
/
pandn
%
xmm12
%
xmm14
.
byte
102
69
15
235
240
/
/
por
%
xmm8
%
xmm14
.
byte
102
69
15
253
242
/
/
paddw
%
xmm10
%
xmm14
.
byte
102
65
15
113
214
7
/
/
psrlw
0x7
%
xmm14
.
byte
102
69
15
219
243
/
/
pand
%
xmm11
%
xmm14
.
byte
102
65
15
249
206
/
/
psubw
%
xmm14
%
xmm1
.
byte
102
68
15
111
230
/
/
movdqa
%
xmm6
%
xmm12
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
68
15
213
227
/
/
pmullw
%
xmm3
%
xmm12
.
byte
102
68
15
111
234
/
/
movdqa
%
xmm2
%
xmm13
.
byte
102
69
15
239
233
/
/
pxor
%
xmm9
%
xmm13
.
byte
102
69
15
239
204
/
/
pxor
%
xmm12
%
xmm9
.
byte
102
69
15
101
205
/
/
pcmpgtw
%
xmm13
%
xmm9
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
102
69
15
223
204
/
/
pandn
%
xmm12
%
xmm9
.
byte
102
68
15
235
202
/
/
por
%
xmm2
%
xmm9
.
byte
102
69
15
253
202
/
/
paddw
%
xmm10
%
xmm9
.
byte
102
65
15
113
209
7
/
/
psrlw
0x7
%
xmm9
.
byte
102
69
15
219
203
/
/
pand
%
xmm11
%
xmm9
.
byte
102
69
15
249
193
/
/
psubw
%
xmm9
%
xmm8
.
byte
102
65
15
111
210
/
/
movdqa
%
xmm10
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_exclusion_sse2_lowp
.
globl
_sk_exclusion_sse2_lowp
FUNCTION
(
_sk_exclusion_sse2_lowp
)
_sk_exclusion_sse2_lowp
:
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
213
204
/
/
pmullw
%
xmm4
%
xmm1
.
byte
102
68
15
111
13
229
71
0
0
/
/
movdqa
0x47e5
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
7
/
/
psrlw
0x7
%
xmm1
.
byte
102
68
15
111
21
82
79
0
0
/
/
movdqa
0x4f52
(
%
rip
)
%
xmm10
#
3d630
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13e4
>
.
byte
102
65
15
219
202
/
/
pand
%
xmm10
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
7
/
/
psrlw
0x7
%
xmm8
.
byte
102
69
15
219
194
/
/
pand
%
xmm10
%
xmm8
.
byte
102
65
15
249
200
/
/
psubw
%
xmm8
%
xmm1
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
68
15
253
194
/
/
paddw
%
xmm2
%
xmm8
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
7
/
/
psrlw
0x7
%
xmm2
.
byte
102
65
15
219
210
/
/
pand
%
xmm10
%
xmm2
.
byte
102
68
15
249
194
/
/
psubw
%
xmm2
%
xmm8
.
byte
102
65
15
111
209
/
/
movdqa
%
xmm9
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_hardlight_sse2_lowp
.
globl
_sk_hardlight_sse2_lowp
FUNCTION
(
_sk_hardlight_sse2_lowp
)
_sk_hardlight_sse2_lowp
:
.
byte
15
41
116
36
232
/
/
movaps
%
xmm6
-
0x18
(
%
rsp
)
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
68
15
111
194
/
/
movdqa
%
xmm2
%
xmm8
.
byte
102
68
15
111
200
/
/
movdqa
%
xmm0
%
xmm9
.
byte
102
15
111
21
65
71
0
0
/
/
movdqa
0x4741
(
%
rip
)
%
xmm2
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
111
226
/
/
movdqa
%
xmm2
%
xmm12
.
byte
102
68
15
249
231
/
/
psubw
%
xmm7
%
xmm12
.
byte
102
65
15
111
196
/
/
movdqa
%
xmm12
%
xmm0
.
byte
102
65
15
213
193
/
/
pmullw
%
xmm9
%
xmm0
.
byte
102
68
15
111
218
/
/
movdqa
%
xmm2
%
xmm11
.
byte
102
68
15
249
219
/
/
psubw
%
xmm3
%
xmm11
.
byte
102
69
15
111
251
/
/
movdqa
%
xmm11
%
xmm15
.
byte
102
68
15
213
253
/
/
pmullw
%
xmm5
%
xmm15
.
byte
102
68
15
253
248
/
/
paddw
%
xmm0
%
xmm15
.
byte
102
68
15
111
243
/
/
movdqa
%
xmm3
%
xmm14
.
byte
102
69
15
249
241
/
/
psubw
%
xmm9
%
xmm14
.
byte
102
69
15
253
201
/
/
paddw
%
xmm9
%
xmm9
.
byte
102
15
111
21
141
78
0
0
/
/
movdqa
0x4e8d
(
%
rip
)
%
xmm2
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
15
111
199
/
/
movdqa
%
xmm7
%
xmm0
.
byte
102
68
15
111
215
/
/
movdqa
%
xmm7
%
xmm10
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
68
15
213
211
/
/
pmullw
%
xmm3
%
xmm10
.
byte
102
15
249
197
/
/
psubw
%
xmm5
%
xmm0
.
byte
102
65
15
213
198
/
/
pmullw
%
xmm14
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
68
15
249
232
/
/
psubw
%
xmm0
%
xmm13
.
byte
102
68
15
111
243
/
/
movdqa
%
xmm3
%
xmm14
.
byte
102
68
15
239
242
/
/
pxor
%
xmm2
%
xmm14
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
15
239
194
/
/
pxor
%
xmm2
%
xmm0
.
byte
102
65
15
101
198
/
/
pcmpgtw
%
xmm14
%
xmm0
.
byte
102
68
15
213
205
/
/
pmullw
%
xmm5
%
xmm9
.
byte
102
68
15
219
232
/
/
pand
%
xmm0
%
xmm13
.
byte
102
65
15
223
193
/
/
pandn
%
xmm9
%
xmm0
.
byte
102
65
15
235
197
/
/
por
%
xmm13
%
xmm0
.
byte
102
15
253
5
160
70
0
0
/
/
paddw
0x46a0
(
%
rip
)
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
199
/
/
paddw
%
xmm15
%
xmm0
.
byte
102
69
15
111
204
/
/
movdqa
%
xmm12
%
xmm9
.
byte
102
68
15
213
201
/
/
pmullw
%
xmm1
%
xmm9
.
byte
102
69
15
111
235
/
/
movdqa
%
xmm11
%
xmm13
.
byte
102
68
15
213
238
/
/
pmullw
%
xmm6
%
xmm13
.
byte
102
69
15
253
233
/
/
paddw
%
xmm9
%
xmm13
.
byte
102
68
15
111
203
/
/
movdqa
%
xmm3
%
xmm9
.
byte
102
68
15
249
201
/
/
psubw
%
xmm1
%
xmm9
.
byte
102
68
15
111
252
/
/
movdqa
%
xmm4
%
xmm15
.
byte
102
68
15
249
254
/
/
psubw
%
xmm6
%
xmm15
.
byte
102
69
15
213
249
/
/
pmullw
%
xmm9
%
xmm15
.
byte
102
69
15
253
255
/
/
paddw
%
xmm15
%
xmm15
.
byte
102
65
15
111
250
/
/
movdqa
%
xmm10
%
xmm7
.
byte
102
65
15
249
255
/
/
psubw
%
xmm15
%
xmm7
.
byte
102
15
253
201
/
/
paddw
%
xmm1
%
xmm1
.
byte
102
68
15
111
201
/
/
movdqa
%
xmm1
%
xmm9
.
byte
102
68
15
239
202
/
/
pxor
%
xmm2
%
xmm9
.
byte
102
69
15
101
206
/
/
pcmpgtw
%
xmm14
%
xmm9
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
65
15
219
249
/
/
pand
%
xmm9
%
xmm7
.
byte
102
68
15
223
201
/
/
pandn
%
xmm1
%
xmm9
.
byte
102
68
15
235
207
/
/
por
%
xmm7
%
xmm9
.
byte
102
68
15
253
13
43
70
0
0
/
/
paddw
0x462b
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
253
205
/
/
paddw
%
xmm13
%
xmm9
.
byte
102
69
15
213
224
/
/
pmullw
%
xmm8
%
xmm12
.
byte
102
69
15
111
251
/
/
movdqa
%
xmm11
%
xmm15
.
byte
102
15
111
76
36
232
/
/
movdqa
-
0x18
(
%
rsp
)
%
xmm1
.
byte
102
68
15
213
249
/
/
pmullw
%
xmm1
%
xmm15
.
byte
102
69
15
253
252
/
/
paddw
%
xmm12
%
xmm15
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
69
15
249
224
/
/
psubw
%
xmm8
%
xmm12
.
byte
102
69
15
253
192
/
/
paddw
%
xmm8
%
xmm8
.
byte
102
65
15
239
208
/
/
pxor
%
xmm8
%
xmm2
.
byte
102
65
15
101
214
/
/
pcmpgtw
%
xmm14
%
xmm2
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
249
249
/
/
psubw
%
xmm1
%
xmm7
.
byte
102
65
15
213
252
/
/
pmullw
%
xmm12
%
xmm7
.
byte
102
15
253
255
/
/
paddw
%
xmm7
%
xmm7
.
byte
102
68
15
249
215
/
/
psubw
%
xmm7
%
xmm10
.
byte
102
68
15
213
193
/
/
pmullw
%
xmm1
%
xmm8
.
byte
102
68
15
111
225
/
/
movdqa
%
xmm1
%
xmm12
.
byte
102
68
15
219
210
/
/
pand
%
xmm2
%
xmm10
.
byte
102
65
15
223
208
/
/
pandn
%
xmm8
%
xmm2
.
byte
102
65
15
235
210
/
/
por
%
xmm10
%
xmm2
.
byte
102
68
15
111
5
187
69
0
0
/
/
movdqa
0x45bb
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
65
15
253
215
/
/
paddw
%
xmm15
%
xmm2
.
byte
102
68
15
213
220
/
/
pmullw
%
xmm4
%
xmm11
.
byte
102
69
15
253
216
/
/
paddw
%
xmm8
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
65
15
253
219
/
/
paddw
%
xmm11
%
xmm3
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
65
15
111
244
/
/
movdqa
%
xmm12
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_overlay_sse2_lowp
.
globl
_sk_overlay_sse2_lowp
FUNCTION
(
_sk_overlay_sse2_lowp
)
_sk_overlay_sse2_lowp
:
.
byte
102
68
15
111
247
/
/
movdqa
%
xmm7
%
xmm14
.
byte
15
41
116
36
216
/
/
movaps
%
xmm6
-
0x28
(
%
rsp
)
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
127
124
36
232
/
/
movdqa
%
xmm7
-
0x18
(
%
rsp
)
.
byte
102
68
15
111
193
/
/
movdqa
%
xmm1
%
xmm8
.
byte
102
68
15
111
248
/
/
movdqa
%
xmm0
%
xmm15
.
byte
102
15
111
53
76
69
0
0
/
/
movdqa
0x454c
(
%
rip
)
%
xmm6
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
69
15
249
222
/
/
psubw
%
xmm14
%
xmm11
.
byte
102
65
15
111
203
/
/
movdqa
%
xmm11
%
xmm1
.
byte
102
65
15
213
207
/
/
pmullw
%
xmm15
%
xmm1
.
byte
102
68
15
111
214
/
/
movdqa
%
xmm6
%
xmm10
.
byte
102
68
15
249
211
/
/
psubw
%
xmm3
%
xmm10
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
68
15
111
235
/
/
movdqa
%
xmm3
%
xmm13
.
byte
102
69
15
249
239
/
/
psubw
%
xmm15
%
xmm13
.
byte
102
68
15
213
255
/
/
pmullw
%
xmm7
%
xmm15
.
byte
102
65
15
111
206
/
/
movdqa
%
xmm14
%
xmm1
.
byte
102
15
249
207
/
/
psubw
%
xmm7
%
xmm1
.
byte
102
15
253
255
/
/
paddw
%
xmm7
%
xmm7
.
byte
102
69
15
111
206
/
/
movdqa
%
xmm14
%
xmm9
.
byte
102
68
15
213
203
/
/
pmullw
%
xmm3
%
xmm9
.
byte
102
65
15
213
205
/
/
pmullw
%
xmm13
%
xmm1
.
byte
102
15
253
201
/
/
paddw
%
xmm1
%
xmm1
.
byte
102
69
15
111
225
/
/
movdqa
%
xmm9
%
xmm12
.
byte
102
68
15
249
225
/
/
psubw
%
xmm1
%
xmm12
.
byte
102
15
111
13
112
76
0
0
/
/
movdqa
0x4c70
(
%
rip
)
%
xmm1
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
69
15
111
238
/
/
movdqa
%
xmm14
%
xmm13
.
byte
102
68
15
239
233
/
/
pxor
%
xmm1
%
xmm13
.
byte
102
15
239
249
/
/
pxor
%
xmm1
%
xmm7
.
byte
102
65
15
101
253
/
/
pcmpgtw
%
xmm13
%
xmm7
.
byte
102
69
15
253
255
/
/
paddw
%
xmm15
%
xmm15
.
byte
102
68
15
219
231
/
/
pand
%
xmm7
%
xmm12
.
byte
102
65
15
223
255
/
/
pandn
%
xmm15
%
xmm7
.
byte
102
65
15
235
252
/
/
por
%
xmm12
%
xmm7
.
byte
102
15
253
254
/
/
paddw
%
xmm6
%
xmm7
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
65
15
111
227
/
/
movdqa
%
xmm11
%
xmm4
.
byte
102
65
15
213
224
/
/
pmullw
%
xmm8
%
xmm4
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
15
253
204
/
/
paddw
%
xmm4
%
xmm1
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
69
15
249
224
/
/
psubw
%
xmm8
%
xmm12
.
byte
102
65
15
111
230
/
/
movdqa
%
xmm14
%
xmm4
.
byte
102
15
249
229
/
/
psubw
%
xmm5
%
xmm4
.
byte
102
65
15
213
228
/
/
pmullw
%
xmm12
%
xmm4
.
byte
102
15
253
228
/
/
paddw
%
xmm4
%
xmm4
.
byte
102
69
15
111
225
/
/
movdqa
%
xmm9
%
xmm12
.
byte
102
68
15
249
228
/
/
psubw
%
xmm4
%
xmm12
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
253
228
/
/
paddw
%
xmm4
%
xmm4
.
byte
102
15
111
61
239
75
0
0
/
/
movdqa
0x4bef
(
%
rip
)
%
xmm7
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
15
239
231
/
/
pxor
%
xmm7
%
xmm4
.
byte
102
65
15
101
229
/
/
pcmpgtw
%
xmm13
%
xmm4
.
byte
102
69
15
253
192
/
/
paddw
%
xmm8
%
xmm8
.
byte
102
68
15
219
228
/
/
pand
%
xmm4
%
xmm12
.
byte
102
65
15
223
224
/
/
pandn
%
xmm8
%
xmm4
.
byte
102
65
15
235
228
/
/
por
%
xmm12
%
xmm4
.
byte
102
15
253
230
/
/
paddw
%
xmm6
%
xmm4
.
byte
102
68
15
111
230
/
/
movdqa
%
xmm6
%
xmm12
.
byte
102
15
253
204
/
/
paddw
%
xmm4
%
xmm1
.
byte
102
68
15
213
218
/
/
pmullw
%
xmm2
%
xmm11
.
byte
102
69
15
111
194
/
/
movdqa
%
xmm10
%
xmm8
.
byte
102
15
111
116
36
216
/
/
movdqa
-
0x28
(
%
rsp
)
%
xmm6
.
byte
102
68
15
213
198
/
/
pmullw
%
xmm6
%
xmm8
.
byte
102
69
15
253
195
/
/
paddw
%
xmm11
%
xmm8
.
byte
102
15
111
230
/
/
movdqa
%
xmm6
%
xmm4
.
byte
102
15
253
228
/
/
paddw
%
xmm4
%
xmm4
.
byte
102
15
239
231
/
/
pxor
%
xmm7
%
xmm4
.
byte
102
69
15
111
222
/
/
movdqa
%
xmm14
%
xmm11
.
byte
102
68
15
249
222
/
/
psubw
%
xmm6
%
xmm11
.
byte
102
65
15
101
229
/
/
pcmpgtw
%
xmm13
%
xmm4
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
102
15
249
250
/
/
psubw
%
xmm2
%
xmm7
.
byte
102
68
15
213
223
/
/
pmullw
%
xmm7
%
xmm11
.
byte
102
69
15
253
219
/
/
paddw
%
xmm11
%
xmm11
.
byte
102
69
15
249
203
/
/
psubw
%
xmm11
%
xmm9
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
15
253
210
/
/
paddw
%
xmm2
%
xmm2
.
byte
102
68
15
219
204
/
/
pand
%
xmm4
%
xmm9
.
byte
102
15
223
226
/
/
pandn
%
xmm2
%
xmm4
.
byte
102
65
15
235
225
/
/
por
%
xmm9
%
xmm4
.
byte
102
65
15
111
212
/
/
movdqa
%
xmm12
%
xmm2
.
byte
102
15
253
226
/
/
paddw
%
xmm2
%
xmm4
.
byte
102
68
15
253
196
/
/
paddw
%
xmm4
%
xmm8
.
byte
102
69
15
213
214
/
/
pmullw
%
xmm14
%
xmm10
.
byte
102
68
15
253
210
/
/
paddw
%
xmm2
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
253
218
/
/
paddw
%
xmm10
%
xmm3
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
100
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm4
.
byte
102
65
15
111
254
/
/
movdqa
%
xmm14
%
xmm7
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_8888_sse2_lowp
.
globl
_sk_load_8888_sse2_lowp
FUNCTION
(
_sk_load_8888_sse2_lowp
)
_sk_load_8888_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
38b67
<
_sk_load_8888_sse2_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
23
1
0
0
/
/
lea
0x117
(
%
rip
)
%
r9
#
38c64
<
_sk_load_8888_sse2_lowp
+
0x141
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
243
65
15
16
28
144
/
/
movss
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
102
/
/
jmp
38bcd
<
_sk_load_8888_sse2_lowp
+
0xaa
>
.
byte
102
65
15
16
28
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
87
/
/
jmp
38bcd
<
_sk_load_8888_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
69
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
65
15
40
216
/
/
movapd
%
xmm8
%
xmm3
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
235
52
/
/
jmp
38bcd
<
_sk_load_8888_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
68
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
243
65
15
16
68
144
20
/
/
movss
0x14
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
65
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm0
.
byte
65
15
198
192
226
/
/
shufps
0xe2
%
xmm8
%
xmm0
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
243
65
15
16
68
144
16
/
/
movss
0x10
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
243
68
15
16
192
/
/
movss
%
xmm0
%
xmm8
.
byte
102
65
15
16
28
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
102
15
40
203
/
/
movapd
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
68
15
111
13
185
66
0
0
/
/
movdqa
0x42b9
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
65
15
114
210
16
/
/
psrld
0x10
%
xmm10
.
byte
102
15
40
211
/
/
movapd
%
xmm3
%
xmm2
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
65
15
107
216
/
/
packssdw
%
xmm8
%
xmm3
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
65
15
114
242
16
/
/
pslld
0x10
%
xmm10
.
byte
102
65
15
114
226
16
/
/
psrad
0x10
%
xmm10
.
byte
102
65
15
107
210
/
/
packssdw
%
xmm10
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
246
254
/
/
idiv
%
dh
.
byte
255
/
/
(
bad
)
.
byte
255
35
/
/
jmpq
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
99
255
/
/
jmpq
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
87
255
/
/
callq
*
-
0x1
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
66
255
/
/
incl
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
53
/
/
.
byte
0x35
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_8888_dst_sse2_lowp
.
globl
_sk_load_8888_dst_sse2_lowp
FUNCTION
(
_sk_load_8888_dst_sse2_lowp
)
_sk_load_8888_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
38cc4
<
_sk_load_8888_dst_sse2_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
22
1
0
0
/
/
lea
0x116
(
%
rip
)
%
r9
#
38dc0
<
_sk_load_8888_dst_sse2_lowp
+
0x140
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
243
65
15
16
60
144
/
/
movss
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
102
/
/
jmp
38d2a
<
_sk_load_8888_dst_sse2_lowp
+
0xaa
>
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
87
/
/
jmp
38d2a
<
_sk_load_8888_dst_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
69
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
65
15
40
248
/
/
movapd
%
xmm8
%
xmm7
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
235
52
/
/
jmp
38d2a
<
_sk_load_8888_dst_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
20
/
/
movss
0x14
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
65
15
198
224
0
/
/
shufps
0x0
%
xmm8
%
xmm4
.
byte
65
15
198
224
226
/
/
shufps
0xe2
%
xmm8
%
xmm4
.
byte
68
15
40
196
/
/
movaps
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
16
/
/
movss
0x10
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
243
68
15
16
196
/
/
movss
%
xmm4
%
xmm8
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
15
40
239
/
/
movapd
%
xmm7
%
xmm5
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
114
229
16
/
/
psrad
0x10
%
xmm5
.
byte
102
65
15
111
224
/
/
movdqa
%
xmm8
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
236
/
/
packssdw
%
xmm4
%
xmm5
.
byte
102
68
15
111
13
92
65
0
0
/
/
movdqa
0x415c
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
65
15
219
225
/
/
pand
%
xmm9
%
xmm4
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
65
15
114
210
16
/
/
psrld
0x10
%
xmm10
.
byte
102
15
40
247
/
/
movapd
%
xmm7
%
xmm6
.
byte
102
15
114
214
16
/
/
psrld
0x10
%
xmm6
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
65
15
107
248
/
/
packssdw
%
xmm8
%
xmm7
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
65
15
114
242
16
/
/
pslld
0x10
%
xmm10
.
byte
102
65
15
114
226
16
/
/
psrad
0x10
%
xmm10
.
byte
102
65
15
107
242
/
/
packssdw
%
xmm10
%
xmm6
.
byte
102
65
15
219
241
/
/
pand
%
xmm9
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
247
254
/
/
idiv
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
36
255
/
/
jmpq
*
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
100
255
255
/
/
jmpq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
88
255
/
/
lcall
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
67
255
/
/
incl
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
54
/
/
pushq
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_8888_sse2_lowp
.
globl
_sk_store_8888_sse2_lowp
FUNCTION
(
_sk_store_8888_sse2_lowp
)
_sk_store_8888_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
209
/
/
movdqa
%
xmm1
%
xmm10
.
byte
102
65
15
113
242
8
/
/
psllw
0x8
%
xmm10
.
byte
102
68
15
235
208
/
/
por
%
xmm0
%
xmm10
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
69
15
97
216
/
/
punpcklwd
%
xmm8
%
xmm11
.
byte
102
69
15
105
208
/
/
punpckhwd
%
xmm8
%
xmm10
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
65
15
113
244
8
/
/
psllw
0x8
%
xmm12
.
byte
102
68
15
235
226
/
/
por
%
xmm2
%
xmm12
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
69
15
97
204
/
/
punpcklwd
%
xmm12
%
xmm9
.
byte
102
69
15
235
203
/
/
por
%
xmm11
%
xmm9
.
byte
102
69
15
105
196
/
/
punpckhwd
%
xmm12
%
xmm8
.
byte
102
69
15
235
194
/
/
por
%
xmm10
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
27
/
/
ja
38e5f
<
_sk_store_8888_sse2_lowp
+
0x83
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
98
0
0
0
/
/
lea
0x62
(
%
rip
)
%
r9
#
38eb0
<
_sk_store_8888_sse2_lowp
+
0xd4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
126
12
144
/
/
movd
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
235
75
/
/
jmp
38eaa
<
_sk_store_8888_sse2_lowp
+
0xce
>
.
byte
243
69
15
127
12
144
/
/
movdqu
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
68
144
16
/
/
movdqu
%
xmm8
0x10
(
%
r8
%
rdx
4
)
.
byte
235
60
/
/
jmp
38eaa
<
_sk_store_8888_sse2_lowp
+
0xce
>
.
byte
102
69
15
112
193
78
/
/
pshufd
0x4e
%
xmm9
%
xmm8
.
byte
102
69
15
126
68
144
8
/
/
movd
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
12
144
/
/
movq
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
235
39
/
/
jmp
38eaa
<
_sk_store_8888_sse2_lowp
+
0xce
>
.
byte
102
69
15
112
208
78
/
/
pshufd
0x4e
%
xmm8
%
xmm10
.
byte
102
69
15
126
84
144
24
/
/
movd
%
xmm10
0x18
(
%
r8
%
rdx
4
)
.
byte
102
69
15
112
208
229
/
/
pshufd
0xe5
%
xmm8
%
xmm10
.
byte
102
69
15
126
84
144
20
/
/
movd
%
xmm10
0x14
(
%
r8
%
rdx
4
)
.
byte
102
69
15
126
68
144
16
/
/
movd
%
xmm8
0x10
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
12
144
/
/
movdqu
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
167
/
/
cmpsl
%
es
:
(
%
rdi
)
%
ds
:
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
203
/
/
dec
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
244
/
/
mov
0xf4ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_sse2_lowp
.
globl
_sk_load_bgra_sse2_lowp
FUNCTION
(
_sk_load_bgra_sse2_lowp
)
_sk_load_bgra_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
38f10
<
_sk_load_bgra_sse2_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
22
1
0
0
/
/
lea
0x116
(
%
rip
)
%
r9
#
3900c
<
_sk_load_bgra_sse2_lowp
+
0x140
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
243
65
15
16
28
144
/
/
movss
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
102
/
/
jmp
38f76
<
_sk_load_bgra_sse2_lowp
+
0xaa
>
.
byte
102
65
15
16
28
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
87
/
/
jmp
38f76
<
_sk_load_bgra_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
68
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
69
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
65
15
40
216
/
/
movapd
%
xmm8
%
xmm3
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
235
52
/
/
jmp
38f76
<
_sk_load_bgra_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
68
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
102
68
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm8
.
byte
243
65
15
16
68
144
20
/
/
movss
0x14
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
65
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm0
.
byte
65
15
198
192
226
/
/
shufps
0xe2
%
xmm8
%
xmm0
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
243
65
15
16
68
144
16
/
/
movss
0x10
(
%
r8
%
rdx
4
)
%
xmm0
.
byte
243
68
15
16
192
/
/
movss
%
xmm0
%
xmm8
.
byte
102
65
15
16
28
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm3
.
byte
102
15
40
203
/
/
movapd
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
68
15
111
13
16
63
0
0
/
/
movdqa
0x3f10
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
65
15
114
210
16
/
/
psrld
0x10
%
xmm10
.
byte
102
15
40
195
/
/
movapd
%
xmm3
%
xmm0
.
byte
102
15
114
208
16
/
/
psrld
0x10
%
xmm0
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
65
15
107
216
/
/
packssdw
%
xmm8
%
xmm3
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
65
15
114
242
16
/
/
pslld
0x10
%
xmm10
.
byte
102
65
15
114
226
16
/
/
psrad
0x10
%
xmm10
.
byte
102
65
15
107
194
/
/
packssdw
%
xmm10
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
247
254
/
/
idiv
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
36
255
/
/
jmpq
*
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
100
255
255
/
/
jmpq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
88
255
/
/
lcall
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
67
255
/
/
incl
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
54
/
/
pushq
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_bgra_dst_sse2_lowp
.
globl
_sk_load_bgra_dst_sse2_lowp
FUNCTION
(
_sk_load_bgra_dst_sse2_lowp
)
_sk_load_bgra_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
41
/
/
ja
3906c
<
_sk_load_bgra_dst_sse2_lowp
+
0x44
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
22
1
0
0
/
/
lea
0x116
(
%
rip
)
%
r9
#
39168
<
_sk_load_bgra_dst_sse2_lowp
+
0x140
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
243
65
15
16
60
144
/
/
movss
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
102
/
/
jmp
390d2
<
_sk_load_bgra_dst_sse2_lowp
+
0xaa
>
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
87
/
/
jmp
390d2
<
_sk_load_bgra_dst_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
69
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
65
15
40
248
/
/
movapd
%
xmm8
%
xmm7
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
235
52
/
/
jmp
390d2
<
_sk_load_bgra_dst_sse2_lowp
+
0xaa
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
20
/
/
movss
0x14
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
65
15
198
224
0
/
/
shufps
0x0
%
xmm8
%
xmm4
.
byte
65
15
198
224
226
/
/
shufps
0xe2
%
xmm8
%
xmm4
.
byte
68
15
40
196
/
/
movaps
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
16
/
/
movss
0x10
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
243
68
15
16
196
/
/
movss
%
xmm4
%
xmm8
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
15
40
239
/
/
movapd
%
xmm7
%
xmm5
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
114
229
16
/
/
psrad
0x10
%
xmm5
.
byte
102
65
15
111
224
/
/
movdqa
%
xmm8
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
236
/
/
packssdw
%
xmm4
%
xmm5
.
byte
102
68
15
111
13
180
61
0
0
/
/
movdqa
0x3db4
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
65
15
219
241
/
/
pand
%
xmm9
%
xmm6
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
65
15
114
210
16
/
/
psrld
0x10
%
xmm10
.
byte
102
15
40
231
/
/
movapd
%
xmm7
%
xmm4
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
65
15
107
248
/
/
packssdw
%
xmm8
%
xmm7
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
65
15
114
242
16
/
/
pslld
0x10
%
xmm10
.
byte
102
65
15
114
226
16
/
/
psrad
0x10
%
xmm10
.
byte
102
65
15
107
226
/
/
packssdw
%
xmm10
%
xmm4
.
byte
102
65
15
219
225
/
/
pand
%
xmm9
%
xmm4
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
247
254
/
/
idiv
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
36
255
/
/
jmpq
*
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
100
255
255
/
/
jmpq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
88
255
/
/
lcall
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
67
255
/
/
incl
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
54
/
/
pushq
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_bgra_sse2_lowp
.
globl
_sk_store_bgra_sse2_lowp
FUNCTION
(
_sk_store_bgra_sse2_lowp
)
_sk_store_bgra_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
209
/
/
movdqa
%
xmm1
%
xmm10
.
byte
102
65
15
113
242
8
/
/
psllw
0x8
%
xmm10
.
byte
102
68
15
235
210
/
/
por
%
xmm2
%
xmm10
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
69
15
97
216
/
/
punpcklwd
%
xmm8
%
xmm11
.
byte
102
69
15
105
208
/
/
punpckhwd
%
xmm8
%
xmm10
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
65
15
113
244
8
/
/
psllw
0x8
%
xmm12
.
byte
102
68
15
235
224
/
/
por
%
xmm0
%
xmm12
.
byte
102
69
15
239
201
/
/
pxor
%
xmm9
%
xmm9
.
byte
102
69
15
97
204
/
/
punpcklwd
%
xmm12
%
xmm9
.
byte
102
69
15
235
203
/
/
por
%
xmm11
%
xmm9
.
byte
102
69
15
105
196
/
/
punpckhwd
%
xmm12
%
xmm8
.
byte
102
69
15
235
194
/
/
por
%
xmm10
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
27
/
/
ja
39207
<
_sk_store_bgra_sse2_lowp
+
0x83
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
98
0
0
0
/
/
lea
0x62
(
%
rip
)
%
r9
#
39258
<
_sk_store_bgra_sse2_lowp
+
0xd4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
126
12
144
/
/
movd
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
235
75
/
/
jmp
39252
<
_sk_store_bgra_sse2_lowp
+
0xce
>
.
byte
243
69
15
127
12
144
/
/
movdqu
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
68
144
16
/
/
movdqu
%
xmm8
0x10
(
%
r8
%
rdx
4
)
.
byte
235
60
/
/
jmp
39252
<
_sk_store_bgra_sse2_lowp
+
0xce
>
.
byte
102
69
15
112
193
78
/
/
pshufd
0x4e
%
xmm9
%
xmm8
.
byte
102
69
15
126
68
144
8
/
/
movd
%
xmm8
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
12
144
/
/
movq
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
235
39
/
/
jmp
39252
<
_sk_store_bgra_sse2_lowp
+
0xce
>
.
byte
102
69
15
112
208
78
/
/
pshufd
0x4e
%
xmm8
%
xmm10
.
byte
102
69
15
126
84
144
24
/
/
movd
%
xmm10
0x18
(
%
r8
%
rdx
4
)
.
byte
102
69
15
112
208
229
/
/
pshufd
0xe5
%
xmm8
%
xmm10
.
byte
102
69
15
126
84
144
20
/
/
movd
%
xmm10
0x14
(
%
r8
%
rdx
4
)
.
byte
102
69
15
126
68
144
16
/
/
movd
%
xmm8
0x10
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
12
144
/
/
movdqu
%
xmm9
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
167
/
/
cmpsl
%
es
:
(
%
rdi
)
%
ds
:
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
203
/
/
dec
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
244
/
/
mov
0xf4ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
237
/
/
in
(
%
dx
)
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
211
/
/
callq
*
%
rbx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_8888_sse2_lowp
.
globl
_sk_gather_8888_sse2_lowp
FUNCTION
(
_sk_gather_8888_sse2_lowp
)
_sk_gather_8888_sse2_lowp
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
68
15
91
202
/
/
cvttps2dq
%
xmm2
%
xmm9
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
112
209
245
/
/
pshufd
0xf5
%
xmm9
%
xmm10
.
byte
102
68
15
244
211
/
/
pmuludq
%
xmm3
%
xmm10
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
69
15
112
224
245
/
/
pshufd
0xf5
%
xmm8
%
xmm12
.
byte
102
68
15
244
227
/
/
pmuludq
%
xmm3
%
xmm12
.
byte
102
65
15
244
217
/
/
pmuludq
%
xmm9
%
xmm3
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
65
15
112
210
232
/
/
pshufd
0xe8
%
xmm10
%
xmm2
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
102
69
15
244
216
/
/
pmuludq
%
xmm8
%
xmm11
.
byte
102
69
15
112
195
232
/
/
pshufd
0xe8
%
xmm11
%
xmm8
.
byte
102
65
15
112
212
232
/
/
pshufd
0xe8
%
xmm12
%
xmm2
.
byte
102
68
15
98
194
/
/
punpckldq
%
xmm2
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
102
66
15
110
4
184
/
/
movd
(
%
rax
%
r15
4
)
%
xmm0
.
byte
102
66
15
110
28
160
/
/
movd
(
%
rax
%
r12
4
)
%
xmm3
.
byte
102
15
98
216
/
/
punpckldq
%
xmm0
%
xmm3
.
byte
102
15
110
4
152
/
/
movd
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
102
66
15
110
12
176
/
/
movd
(
%
rax
%
r14
4
)
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
15
108
217
/
/
punpcklqdq
%
xmm1
%
xmm3
.
byte
102
66
15
110
4
144
/
/
movd
(
%
rax
%
r10
4
)
%
xmm0
.
byte
102
70
15
110
4
152
/
/
movd
(
%
rax
%
r11
4
)
%
xmm8
.
byte
102
68
15
98
192
/
/
punpckldq
%
xmm0
%
xmm8
.
byte
102
66
15
110
4
128
/
/
movd
(
%
rax
%
r8
4
)
%
xmm0
.
byte
102
66
15
110
12
136
/
/
movd
(
%
rax
%
r9
4
)
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
68
15
108
193
/
/
punpcklqdq
%
xmm1
%
xmm8
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
68
15
111
13
208
58
0
0
/
/
movdqa
0x3ad0
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
65
15
114
210
16
/
/
psrld
0x10
%
xmm10
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
65
15
107
216
/
/
packssdw
%
xmm8
%
xmm3
.
byte
102
65
15
114
242
16
/
/
pslld
0x10
%
xmm10
.
byte
102
65
15
114
226
16
/
/
psrad
0x10
%
xmm10
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
65
15
107
210
/
/
packssdw
%
xmm10
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gather_bgra_sse2_lowp
.
globl
_sk_gather_bgra_sse2_lowp
FUNCTION
(
_sk_gather_bgra_sse2_lowp
)
_sk_gather_bgra_sse2_lowp
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
243
68
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm8
.
byte
243
68
15
91
202
/
/
cvttps2dq
%
xmm2
%
xmm9
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
112
209
245
/
/
pshufd
0xf5
%
xmm9
%
xmm10
.
byte
102
68
15
244
211
/
/
pmuludq
%
xmm3
%
xmm10
.
byte
102
68
15
111
219
/
/
movdqa
%
xmm3
%
xmm11
.
byte
102
69
15
112
224
245
/
/
pshufd
0xf5
%
xmm8
%
xmm12
.
byte
102
68
15
244
227
/
/
pmuludq
%
xmm3
%
xmm12
.
byte
102
65
15
244
217
/
/
pmuludq
%
xmm9
%
xmm3
.
byte
72
139
0
/
/
mov
(
%
rax
)
%
rax
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
65
15
112
210
232
/
/
pshufd
0xe8
%
xmm10
%
xmm2
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
102
69
15
244
216
/
/
pmuludq
%
xmm8
%
xmm11
.
byte
102
69
15
112
195
232
/
/
pshufd
0xe8
%
xmm11
%
xmm8
.
byte
102
65
15
112
212
232
/
/
pshufd
0xe8
%
xmm12
%
xmm2
.
byte
102
68
15
98
194
/
/
punpckldq
%
xmm2
%
xmm8
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
200
/
/
paddd
%
xmm8
%
xmm1
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
73
15
126
202
/
/
movq
%
xmm1
%
r10
.
byte
69
137
211
/
/
mov
%
r10d
%
r11d
.
byte
73
193
234
32
/
/
shr
0x20
%
r10
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
72
15
126
203
/
/
movq
%
xmm1
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
102
66
15
110
4
184
/
/
movd
(
%
rax
%
r15
4
)
%
xmm0
.
byte
102
66
15
110
28
160
/
/
movd
(
%
rax
%
r12
4
)
%
xmm3
.
byte
102
15
98
216
/
/
punpckldq
%
xmm0
%
xmm3
.
byte
102
15
110
4
152
/
/
movd
(
%
rax
%
rbx
4
)
%
xmm0
.
byte
102
66
15
110
12
176
/
/
movd
(
%
rax
%
r14
4
)
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
15
108
217
/
/
punpcklqdq
%
xmm1
%
xmm3
.
byte
102
66
15
110
4
144
/
/
movd
(
%
rax
%
r10
4
)
%
xmm0
.
byte
102
70
15
110
4
152
/
/
movd
(
%
rax
%
r11
4
)
%
xmm8
.
byte
102
68
15
98
192
/
/
punpckldq
%
xmm0
%
xmm8
.
byte
102
66
15
110
4
128
/
/
movd
(
%
rax
%
r8
4
)
%
xmm0
.
byte
102
66
15
110
12
136
/
/
movd
(
%
rax
%
r9
4
)
%
xmm1
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
68
15
108
193
/
/
punpcklqdq
%
xmm1
%
xmm8
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
68
15
111
13
241
56
0
0
/
/
movdqa
0x38f1
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
65
15
219
209
/
/
pand
%
xmm9
%
xmm2
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
114
208
16
/
/
psrld
0x10
%
xmm0
.
byte
102
69
15
111
208
/
/
movdqa
%
xmm8
%
xmm10
.
byte
102
65
15
114
210
16
/
/
psrld
0x10
%
xmm10
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
65
15
107
216
/
/
packssdw
%
xmm8
%
xmm3
.
byte
102
65
15
114
242
16
/
/
pslld
0x10
%
xmm10
.
byte
102
65
15
114
226
16
/
/
psrad
0x10
%
xmm10
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
65
15
107
194
/
/
packssdw
%
xmm10
%
xmm0
.
byte
102
65
15
219
193
/
/
pand
%
xmm9
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_565_sse2_lowp
.
globl
_sk_load_565_sse2_lowp
FUNCTION
(
_sk_load_565_sse2_lowp
)
_sk_load_565_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3966e
<
_sk_load_565_sse2_lowp
+
0x3c
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
190
0
0
0
/
/
lea
0xbe
(
%
rip
)
%
r9
#
39718
<
_sk_load_565_sse2_lowp
+
0xe6
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
66
/
/
jmp
396b0
<
_sk_load_565_sse2_lowp
+
0x7e
>
.
byte
243
65
15
111
4
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
235
58
/
/
jmp
396b0
<
_sk_load_565_sse2_lowp
+
0x7e
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
243
65
15
16
12
80
/
/
movss
(
%
r8
%
rdx
2
)
%
xmm1
.
byte
243
15
16
193
/
/
movss
%
xmm1
%
xmm0
.
byte
235
34
/
/
jmp
396b0
<
_sk_load_565_sse2_lowp
+
0x7e
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
65
15
196
68
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
196
68
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
65
15
18
4
80
/
/
movlpd
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
219
29
191
63
0
0
/
/
pand
0x3fbf
(
%
rip
)
%
xmm3
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
113
209
5
/
/
psrlw
0x5
%
xmm1
.
byte
102
15
219
13
190
63
0
0
/
/
pand
0x3fbe
(
%
rip
)
%
xmm1
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
15
111
21
198
63
0
0
/
/
movdqa
0x3fc6
(
%
rip
)
%
xmm2
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
113
208
13
/
/
psrlw
0xd
%
xmm0
.
byte
102
15
235
195
/
/
por
%
xmm3
%
xmm0
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
15
113
243
2
/
/
psllw
0x2
%
xmm3
.
byte
102
15
113
209
4
/
/
psrlw
0x4
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
113
243
3
/
/
psllw
0x3
%
xmm3
.
byte
102
15
113
210
2
/
/
psrlw
0x2
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
156
55
0
0
/
/
movaps
0x379c
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
75
255
/
/
rex
.
WXB
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
106
255
/
/
ljmp
*
-
0x1
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
94
255
/
/
lcall
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
146
255
255
255
138
/
/
callq
*
-
0x75000001
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
130
255
255
255
118
/
/
incl
0x76ffffff
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_565_dst_sse2_lowp
.
globl
_sk_load_565_dst_sse2_lowp
FUNCTION
(
_sk_load_565_dst_sse2_lowp
)
_sk_load_565_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
39770
<
_sk_load_565_dst_sse2_lowp
+
0x3c
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
188
0
0
0
/
/
lea
0xbc
(
%
rip
)
%
r9
#
39818
<
_sk_load_565_dst_sse2_lowp
+
0xe4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
66
/
/
jmp
397b2
<
_sk_load_565_dst_sse2_lowp
+
0x7e
>
.
byte
243
65
15
111
36
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
235
58
/
/
jmp
397b2
<
_sk_load_565_dst_sse2_lowp
+
0x7e
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
65
15
196
100
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
243
65
15
16
44
80
/
/
movss
(
%
r8
%
rdx
2
)
%
xmm5
.
byte
243
15
16
229
/
/
movss
%
xmm5
%
xmm4
.
byte
235
34
/
/
jmp
397b2
<
_sk_load_565_dst_sse2_lowp
+
0x7e
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
65
15
196
100
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
65
15
196
100
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
65
15
196
100
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
65
15
18
36
80
/
/
movlpd
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
219
61
189
62
0
0
/
/
pand
0x3ebd
(
%
rip
)
%
xmm7
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
113
213
5
/
/
psrlw
0x5
%
xmm5
.
byte
102
15
219
45
188
62
0
0
/
/
pand
0x3ebc
(
%
rip
)
%
xmm5
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
15
111
53
196
62
0
0
/
/
movdqa
0x3ec4
(
%
rip
)
%
xmm6
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
15
219
244
/
/
pand
%
xmm4
%
xmm6
.
byte
102
15
113
212
13
/
/
psrlw
0xd
%
xmm4
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
102
15
111
253
/
/
movdqa
%
xmm5
%
xmm7
.
byte
102
15
113
247
2
/
/
psllw
0x2
%
xmm7
.
byte
102
15
113
213
4
/
/
psrlw
0x4
%
xmm5
.
byte
102
15
235
239
/
/
por
%
xmm7
%
xmm5
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
113
247
3
/
/
psllw
0x3
%
xmm7
.
byte
102
15
113
214
2
/
/
psrlw
0x2
%
xmm6
.
byte
102
15
235
247
/
/
por
%
xmm7
%
xmm6
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
154
54
0
0
/
/
movaps
0x369a
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
77
255
/
/
rex
.
WRB
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
108
255
255
/
/
ljmp
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
96
255
/
/
jmpq
*
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
148
255
255
255
140
255
/
/
callq
*
-
0x730001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
132
255
255
255
120
255
/
/
incl
-
0x870001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_565_sse2_lowp
.
globl
_sk_store_565_sse2_lowp
FUNCTION
(
_sk_store_565_sse2_lowp
)
_sk_store_565_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
102
65
15
113
240
8
/
/
psllw
0x8
%
xmm8
.
byte
102
68
15
219
5
88
62
0
0
/
/
pand
0x3e58
(
%
rip
)
%
xmm8
#
3d6b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1464
>
.
byte
102
68
15
111
201
/
/
movdqa
%
xmm1
%
xmm9
.
byte
102
65
15
113
241
3
/
/
psllw
0x3
%
xmm9
.
byte
102
68
15
219
13
84
62
0
0
/
/
pand
0x3e54
(
%
rip
)
%
xmm9
#
3d6c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1474
>
.
byte
102
69
15
235
200
/
/
por
%
xmm8
%
xmm9
.
byte
102
68
15
111
194
/
/
movdqa
%
xmm2
%
xmm8
.
byte
102
65
15
113
208
3
/
/
psrlw
0x3
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
31
/
/
ja
398aa
<
_sk_store_565_sse2_lowp
+
0x76
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
95
0
0
0
/
/
lea
0x5f
(
%
rip
)
%
r9
#
398f4
<
_sk_store_565_sse2_lowp
+
0xc0
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
102
65
137
4
80
/
/
mov
%
ax
(
%
r8
%
rdx
2
)
.
byte
235
70
/
/
jmp
398f0
<
_sk_store_565_sse2_lowp
+
0xbc
>
.
byte
243
69
15
127
4
80
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
62
/
/
jmp
398f0
<
_sk_store_565_sse2_lowp
+
0xbc
>
.
byte
102
65
15
197
192
2
/
/
pextrw
0x2
%
xmm8
%
eax
.
byte
102
65
137
68
80
4
/
/
mov
%
ax
0x4
(
%
r8
%
rdx
2
)
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
42
/
/
jmp
398f0
<
_sk_store_565_sse2_lowp
+
0xbc
>
.
byte
102
65
15
197
192
6
/
/
pextrw
0x6
%
xmm8
%
eax
.
byte
102
65
137
68
80
12
/
/
mov
%
ax
0xc
(
%
r8
%
rdx
2
)
.
byte
102
65
15
197
192
5
/
/
pextrw
0x5
%
xmm8
%
eax
.
byte
102
65
137
68
80
10
/
/
mov
%
ax
0xa
(
%
r8
%
rdx
2
)
.
byte
102
65
15
197
192
4
/
/
pextrw
0x4
%
xmm8
%
eax
.
byte
102
65
137
68
80
8
/
/
mov
%
ax
0x8
(
%
r8
%
rdx
2
)
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
202
/
/
dec
%
edx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
246
/
/
mov
0xf6ffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
234
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
222
255
/
/
fdivrp
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
210
/
/
callq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_565_sse2_lowp
.
globl
_sk_gather_565_sse2_lowp
FUNCTION
(
_sk_gather_565_sse2_lowp
)
_sk_gather_565_sse2_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
243
68
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm8
.
byte
243
68
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm9
.
byte
102
15
110
80
8
/
/
movd
0x8
(
%
rax
)
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
112
209
245
/
/
pshufd
0xf5
%
xmm9
%
xmm10
.
byte
102
68
15
244
210
/
/
pmuludq
%
xmm2
%
xmm10
.
byte
102
68
15
111
218
/
/
movdqa
%
xmm2
%
xmm11
.
byte
102
69
15
112
224
245
/
/
pshufd
0xf5
%
xmm8
%
xmm12
.
byte
102
68
15
244
226
/
/
pmuludq
%
xmm2
%
xmm12
.
byte
102
65
15
244
209
/
/
pmuludq
%
xmm9
%
xmm2
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
65
15
112
218
232
/
/
pshufd
0xe8
%
xmm10
%
xmm3
.
byte
102
15
98
211
/
/
punpckldq
%
xmm3
%
xmm2
.
byte
102
69
15
244
216
/
/
pmuludq
%
xmm8
%
xmm11
.
byte
102
69
15
112
195
232
/
/
pshufd
0xe8
%
xmm11
%
xmm8
.
byte
102
65
15
112
220
232
/
/
pshufd
0xe8
%
xmm12
%
xmm3
.
byte
102
68
15
98
195
/
/
punpckldq
%
xmm3
%
xmm8
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
254
202
/
/
paddd
%
xmm2
%
xmm1
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
195
/
/
mov
%
eax
%
r11d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
207
/
/
movq
%
xmm1
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
67
15
183
44
122
/
/
movzwl
(
%
r10
%
r15
2
)
%
ebp
.
byte
102
15
110
197
/
/
movd
%
ebp
%
xmm0
.
byte
67
15
183
44
98
/
/
movzwl
(
%
r10
%
r12
2
)
%
ebp
.
byte
102
15
110
205
/
/
movd
%
ebp
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
65
15
183
28
90
/
/
movzwl
(
%
r10
%
rbx
2
)
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
67
15
183
28
114
/
/
movzwl
(
%
r10
%
r14
2
)
%
ebx
.
byte
102
15
110
211
/
/
movd
%
ebx
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
15
98
202
/
/
punpckldq
%
xmm2
%
xmm1
.
byte
65
15
183
4
66
/
/
movzwl
(
%
r10
%
rax
2
)
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
67
15
183
4
90
/
/
movzwl
(
%
r10
%
r11
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
97
194
/
/
punpcklwd
%
xmm2
%
xmm0
.
byte
67
15
183
4
66
/
/
movzwl
(
%
r10
%
r8
2
)
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
67
15
183
4
74
/
/
movzwl
(
%
r10
%
r9
2
)
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
102
15
97
218
/
/
punpcklwd
%
xmm2
%
xmm3
.
byte
102
15
98
195
/
/
punpckldq
%
xmm3
%
xmm0
.
byte
102
15
108
193
/
/
punpcklqdq
%
xmm1
%
xmm0
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
219
29
1
60
0
0
/
/
pand
0x3c01
(
%
rip
)
%
xmm3
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
113
209
5
/
/
psrlw
0x5
%
xmm1
.
byte
102
15
219
13
0
60
0
0
/
/
pand
0x3c00
(
%
rip
)
%
xmm1
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
15
111
21
8
60
0
0
/
/
movdqa
0x3c08
(
%
rip
)
%
xmm2
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
113
208
13
/
/
psrlw
0xd
%
xmm0
.
byte
102
15
235
195
/
/
por
%
xmm3
%
xmm0
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
15
113
243
2
/
/
psllw
0x2
%
xmm3
.
byte
102
15
113
209
4
/
/
psrlw
0x4
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
113
243
3
/
/
psllw
0x3
%
xmm3
.
byte
102
15
113
210
2
/
/
psrlw
0x2
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
222
51
0
0
/
/
movaps
0x33de
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_4444_sse2_lowp
.
globl
_sk_load_4444_sse2_lowp
FUNCTION
(
_sk_load_4444_sse2_lowp
)
_sk_load_4444_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
36
/
/
ja
39b1a
<
_sk_load_4444_sse2_lowp
+
0x3e
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
199
0
0
0
/
/
lea
0xc7
(
%
rip
)
%
r9
#
39bcc
<
_sk_load_4444_sse2_lowp
+
0xf0
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
69
/
/
jmp
39b5f
<
_sk_load_4444_sse2_lowp
+
0x83
>
.
byte
243
69
15
111
4
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
235
61
/
/
jmp
39b5f
<
_sk_load_4444_sse2_lowp
+
0x83
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
196
68
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
243
65
15
16
4
80
/
/
movss
(
%
r8
%
rdx
2
)
%
xmm0
.
byte
243
68
15
16
192
/
/
movss
%
xmm0
%
xmm8
.
byte
235
35
/
/
jmp
39b5f
<
_sk_load_4444_sse2_lowp
+
0x83
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
196
68
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
69
15
196
68
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
69
15
196
68
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
69
15
18
4
80
/
/
movlpd
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
15
113
209
12
/
/
psrlw
0xc
%
xmm1
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
111
5
85
59
0
0
/
/
movdqa
0x3b55
(
%
rip
)
%
xmm0
#
3d6d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1484
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
211
4
/
/
psrlw
0x4
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
102
68
15
219
192
/
/
pand
%
xmm0
%
xmm8
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
4
/
/
psllw
0x4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
4
/
/
psllw
0x4
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
242
4
/
/
psllw
0x4
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
243
4
/
/
psllw
0x4
%
xmm3
.
byte
102
65
15
235
216
/
/
por
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
66
255
/
/
rex
.
X
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
99
255
/
/
jmpq
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
86
255
/
/
callq
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
141
255
255
255
133
/
/
decl
-
0x7a000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
125
255
/
/
jge
39be1
<
_sk_load_4444_sse2_lowp
+
0x105
>
.
byte
255
/
/
(
bad
)
.
byte
255
112
255
/
/
pushq
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_4444_dst_sse2_lowp
.
globl
_sk_load_4444_dst_sse2_lowp
FUNCTION
(
_sk_load_4444_dst_sse2_lowp
)
_sk_load_4444_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
36
/
/
ja
39c26
<
_sk_load_4444_dst_sse2_lowp
+
0x3e
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
199
0
0
0
/
/
lea
0xc7
(
%
rip
)
%
r9
#
39cd8
<
_sk_load_4444_dst_sse2_lowp
+
0xf0
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
69
/
/
jmp
39c6b
<
_sk_load_4444_dst_sse2_lowp
+
0x83
>
.
byte
243
69
15
111
4
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
235
61
/
/
jmp
39c6b
<
_sk_load_4444_dst_sse2_lowp
+
0x83
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
196
68
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
243
65
15
16
36
80
/
/
movss
(
%
r8
%
rdx
2
)
%
xmm4
.
byte
243
68
15
16
196
/
/
movss
%
xmm4
%
xmm8
.
byte
235
35
/
/
jmp
39c6b
<
_sk_load_4444_dst_sse2_lowp
+
0x83
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
69
15
196
68
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
69
15
196
68
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
69
15
196
68
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
69
15
18
4
80
/
/
movlpd
(
%
r8
%
rdx
2
)
%
xmm8
.
byte
102
65
15
111
232
/
/
movdqa
%
xmm8
%
xmm5
.
byte
102
15
113
213
12
/
/
psrlw
0xc
%
xmm5
.
byte
102
65
15
111
240
/
/
movdqa
%
xmm8
%
xmm6
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
102
15
111
37
73
58
0
0
/
/
movdqa
0x3a49
(
%
rip
)
%
xmm4
#
3d6d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1484
>
.
byte
102
15
219
244
/
/
pand
%
xmm4
%
xmm6
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
113
215
4
/
/
psrlw
0x4
%
xmm7
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
102
68
15
219
196
/
/
pand
%
xmm4
%
xmm8
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
113
244
4
/
/
psllw
0x4
%
xmm4
.
byte
102
15
235
229
/
/
por
%
xmm5
%
xmm4
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
113
245
4
/
/
psllw
0x4
%
xmm5
.
byte
102
15
235
238
/
/
por
%
xmm6
%
xmm5
.
byte
102
15
111
247
/
/
movdqa
%
xmm7
%
xmm6
.
byte
102
15
113
246
4
/
/
psllw
0x4
%
xmm6
.
byte
102
15
235
247
/
/
por
%
xmm7
%
xmm6
.
byte
102
65
15
111
248
/
/
movdqa
%
xmm8
%
xmm7
.
byte
102
15
113
247
4
/
/
psllw
0x4
%
xmm7
.
byte
102
65
15
235
248
/
/
por
%
xmm8
%
xmm7
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
66
255
/
/
rex
.
X
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
99
255
/
/
jmpq
*
-
0x1
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
86
255
/
/
callq
*
-
0x1
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
141
255
255
255
133
/
/
decl
-
0x7a000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
125
255
/
/
jge
39ced
<
_sk_load_4444_dst_sse2_lowp
+
0x105
>
.
byte
255
/
/
(
bad
)
.
byte
255
112
255
/
/
pushq
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_4444_sse2_lowp
.
globl
_sk_store_4444_sse2_lowp
FUNCTION
(
_sk_store_4444_sse2_lowp
)
_sk_store_4444_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
102
68
15
111
192
/
/
movdqa
%
xmm0
%
xmm8
.
byte
102
65
15
113
240
8
/
/
psllw
0x8
%
xmm8
.
byte
102
68
15
219
5
200
57
0
0
/
/
pand
0x39c8
(
%
rip
)
%
xmm8
#
3d6e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1494
>
.
byte
102
68
15
111
201
/
/
movdqa
%
xmm1
%
xmm9
.
byte
102
65
15
113
241
4
/
/
psllw
0x4
%
xmm9
.
byte
102
68
15
219
13
196
57
0
0
/
/
pand
0x39c4
(
%
rip
)
%
xmm9
#
3d6f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14a4
>
.
byte
102
69
15
235
200
/
/
por
%
xmm8
%
xmm9
.
byte
102
68
15
111
21
198
57
0
0
/
/
movdqa
0x39c6
(
%
rip
)
%
xmm10
#
3d700
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14b4
>
.
byte
102
68
15
219
210
/
/
pand
%
xmm2
%
xmm10
.
byte
102
68
15
111
195
/
/
movdqa
%
xmm3
%
xmm8
.
byte
102
65
15
113
208
4
/
/
psrlw
0x4
%
xmm8
.
byte
102
69
15
235
194
/
/
por
%
xmm10
%
xmm8
.
byte
102
69
15
235
193
/
/
por
%
xmm9
%
xmm8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
31
/
/
ja
39d7d
<
_sk_store_4444_sse2_lowp
+
0x89
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
96
0
0
0
/
/
lea
0x60
(
%
rip
)
%
r9
#
39dc8
<
_sk_store_4444_sse2_lowp
+
0xd4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
102
65
137
4
80
/
/
mov
%
ax
(
%
r8
%
rdx
2
)
.
byte
235
70
/
/
jmp
39dc3
<
_sk_store_4444_sse2_lowp
+
0xcf
>
.
byte
243
69
15
127
4
80
/
/
movdqu
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
62
/
/
jmp
39dc3
<
_sk_store_4444_sse2_lowp
+
0xcf
>
.
byte
102
65
15
197
192
2
/
/
pextrw
0x2
%
xmm8
%
eax
.
byte
102
65
137
68
80
4
/
/
mov
%
ax
0x4
(
%
r8
%
rdx
2
)
.
byte
102
69
15
126
4
80
/
/
movd
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
235
42
/
/
jmp
39dc3
<
_sk_store_4444_sse2_lowp
+
0xcf
>
.
byte
102
65
15
197
192
6
/
/
pextrw
0x6
%
xmm8
%
eax
.
byte
102
65
137
68
80
12
/
/
mov
%
ax
0xc
(
%
r8
%
rdx
2
)
.
byte
102
65
15
197
192
5
/
/
pextrw
0x5
%
xmm8
%
eax
.
byte
102
65
137
68
80
10
/
/
mov
%
ax
0xa
(
%
r8
%
rdx
2
)
.
byte
102
65
15
197
192
4
/
/
pextrw
0x4
%
xmm8
%
eax
.
byte
102
65
137
68
80
8
/
/
mov
%
ax
0x8
(
%
r8
%
rdx
2
)
.
byte
102
69
15
214
4
80
/
/
movq
%
xmm8
(
%
r8
%
rdx
2
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
169
255
255
255
201
/
/
test
0xc9ffffff
%
eax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
189
255
255
255
245
/
/
mov
0xf5ffffff
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
233
255
255
255
221
/
/
jmpq
ffffffffde039ddc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffddffdb90
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_4444_sse2_lowp
.
globl
_sk_gather_4444_sse2_lowp
FUNCTION
(
_sk_gather_4444_sse2_lowp
)
_sk_gather_4444_sse2_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
64
12
/
/
movss
0xc
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
118
201
/
/
pcmpeqd
%
xmm9
%
xmm9
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
65
15
95
202
/
/
maxps
%
xmm10
%
xmm1
.
byte
65
15
95
194
/
/
maxps
%
xmm10
%
xmm0
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
254
193
/
/
paddd
%
xmm9
%
xmm8
.
byte
65
15
95
210
/
/
maxps
%
xmm10
%
xmm2
.
byte
65
15
95
218
/
/
maxps
%
xmm10
%
xmm3
.
byte
65
15
93
216
/
/
minps
%
xmm8
%
xmm3
.
byte
65
15
93
208
/
/
minps
%
xmm8
%
xmm2
.
byte
243
68
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm8
.
byte
243
68
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm9
.
byte
102
15
110
80
8
/
/
movd
0x8
(
%
rax
)
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
102
69
15
112
209
245
/
/
pshufd
0xf5
%
xmm9
%
xmm10
.
byte
102
68
15
244
210
/
/
pmuludq
%
xmm2
%
xmm10
.
byte
102
68
15
111
218
/
/
movdqa
%
xmm2
%
xmm11
.
byte
102
69
15
112
224
245
/
/
pshufd
0xf5
%
xmm8
%
xmm12
.
byte
102
68
15
244
226
/
/
pmuludq
%
xmm2
%
xmm12
.
byte
102
65
15
244
209
/
/
pmuludq
%
xmm9
%
xmm2
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
65
15
112
218
232
/
/
pshufd
0xe8
%
xmm10
%
xmm3
.
byte
102
15
98
211
/
/
punpckldq
%
xmm3
%
xmm2
.
byte
102
69
15
244
216
/
/
pmuludq
%
xmm8
%
xmm11
.
byte
102
69
15
112
195
232
/
/
pshufd
0xe8
%
xmm11
%
xmm8
.
byte
102
65
15
112
220
232
/
/
pshufd
0xe8
%
xmm12
%
xmm3
.
byte
102
68
15
98
195
/
/
punpckldq
%
xmm3
%
xmm8
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
254
202
/
/
paddd
%
xmm2
%
xmm1
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
65
15
254
192
/
/
paddd
%
xmm8
%
xmm0
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
72
15
126
192
/
/
movq
%
xmm0
%
rax
.
byte
65
137
195
/
/
mov
%
eax
%
r11d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
73
15
126
207
/
/
movq
%
xmm1
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
67
15
183
44
122
/
/
movzwl
(
%
r10
%
r15
2
)
%
ebp
.
byte
102
15
110
197
/
/
movd
%
ebp
%
xmm0
.
byte
67
15
183
44
98
/
/
movzwl
(
%
r10
%
r12
2
)
%
ebp
.
byte
102
15
110
205
/
/
movd
%
ebp
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
65
15
183
28
90
/
/
movzwl
(
%
r10
%
rbx
2
)
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
67
15
183
28
114
/
/
movzwl
(
%
r10
%
r14
2
)
%
ebx
.
byte
102
15
110
211
/
/
movd
%
ebx
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
15
98
202
/
/
punpckldq
%
xmm2
%
xmm1
.
byte
65
15
183
4
66
/
/
movzwl
(
%
r10
%
rax
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
67
15
183
4
90
/
/
movzwl
(
%
r10
%
r11
2
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
102
68
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm8
.
byte
67
15
183
4
66
/
/
movzwl
(
%
r10
%
r8
2
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
67
15
183
4
74
/
/
movzwl
(
%
r10
%
r9
2
)
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
68
15
98
194
/
/
punpckldq
%
xmm2
%
xmm8
.
byte
102
68
15
108
193
/
/
punpcklqdq
%
xmm1
%
xmm8
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
15
113
209
12
/
/
psrlw
0xc
%
xmm1
.
byte
102
65
15
111
208
/
/
movdqa
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
111
5
110
55
0
0
/
/
movdqa
0x376e
(
%
rip
)
%
xmm0
#
3d6d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1484
>
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
211
4
/
/
psrlw
0x4
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
102
68
15
219
192
/
/
pand
%
xmm0
%
xmm8
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
4
/
/
psllw
0x4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
4
/
/
psllw
0x4
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
242
4
/
/
psllw
0x4
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
102
15
113
243
4
/
/
psllw
0x4
%
xmm3
.
byte
102
65
15
235
216
/
/
por
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_a8_sse2_lowp
.
globl
_sk_load_a8_sse2_lowp
FUNCTION
(
_sk_load_a8_sse2_lowp
)
_sk_load_a8_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
39ff4
<
_sk_load_a8_sse2_lowp
+
0x39
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
140
0
0
0
/
/
lea
0x8c
(
%
rip
)
%
r9
#
3a06c
<
_sk_load_a8_sse2_lowp
+
0xb1
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
235
97
/
/
jmp
3a055
<
_sk_load_a8_sse2_lowp
+
0x9a
>
.
byte
243
65
15
126
28
16
/
/
movq
(
%
r8
%
rdx
1
)
%
xmm3
.
byte
102
15
96
216
/
/
punpcklbw
%
xmm0
%
xmm3
.
byte
235
85
/
/
jmp
3a055
<
_sk_load_a8_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
216
2
/
/
pinsrw
0x2
%
eax
%
xmm3
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
243
15
16
216
/
/
movss
%
xmm0
%
xmm3
.
byte
235
51
/
/
jmp
3a055
<
_sk_load_a8_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
216
6
/
/
pinsrw
0x6
%
eax
%
xmm3
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
216
5
/
/
pinsrw
0x5
%
eax
%
xmm3
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
216
4
/
/
pinsrw
0x4
%
eax
%
xmm3
.
byte
102
65
15
110
4
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
242
15
16
216
/
/
movsd
%
xmm0
%
xmm3
.
byte
102
15
219
29
83
46
0
0
/
/
pand
0x2e53
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
125
255
/
/
jge
3a06d
<
_sk_load_a8_sse2_lowp
+
0xb2
>
.
byte
255
/
/
(
bad
)
.
byte
255
163
255
255
255
148
/
/
jmpq
*
-
0x6b000001
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
219
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
208
/
/
callq
*
%
rax
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
197
/
/
inc
%
ebp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
182
255
/
/
mov
0xff
%
dh
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_a8_dst_sse2_lowp
.
globl
_sk_load_a8_dst_sse2_lowp
FUNCTION
(
_sk_load_a8_dst_sse2_lowp
)
_sk_load_a8_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3a0c1
<
_sk_load_a8_dst_sse2_lowp
+
0x39
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
139
0
0
0
/
/
lea
0x8b
(
%
rip
)
%
r9
#
3a138
<
_sk_load_a8_dst_sse2_lowp
+
0xb0
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
248
/
/
movd
%
eax
%
xmm7
.
byte
235
97
/
/
jmp
3a122
<
_sk_load_a8_dst_sse2_lowp
+
0x9a
>
.
byte
243
65
15
126
60
16
/
/
movq
(
%
r8
%
rdx
1
)
%
xmm7
.
byte
102
15
96
248
/
/
punpcklbw
%
xmm0
%
xmm7
.
byte
235
85
/
/
jmp
3a122
<
_sk_load_a8_dst_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
196
248
2
/
/
pinsrw
0x2
%
eax
%
xmm7
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
243
15
16
252
/
/
movss
%
xmm4
%
xmm7
.
byte
235
51
/
/
jmp
3a122
<
_sk_load_a8_dst_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
196
248
6
/
/
pinsrw
0x6
%
eax
%
xmm7
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
248
5
/
/
pinsrw
0x5
%
eax
%
xmm7
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
248
4
/
/
pinsrw
0x4
%
eax
%
xmm7
.
byte
102
65
15
110
36
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
242
15
16
252
/
/
movsd
%
xmm4
%
xmm7
.
byte
102
15
219
61
134
45
0
0
/
/
pand
0x2d86
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
126
255
/
/
jle
3a139
<
_sk_load_a8_dst_sse2_lowp
+
0xb1
>
.
byte
255
/
/
(
bad
)
.
byte
255
164
255
255
255
149
255
/
/
jmpq
*
-
0x6a0001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
220
255
/
/
fdivr
%
st
%
st
(
7
)
.
byte
255
/
/
(
bad
)
.
byte
255
209
/
/
callq
*
%
rcx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
183
255
/
/
mov
0xff
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_store_a8_sse2_lowp
.
globl
_sk_store_a8_sse2_lowp
FUNCTION
(
_sk_store_a8_sse2_lowp
)
_sk_store_a8_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
29
/
/
ja
3a188
<
_sk_store_a8_sse2_lowp
+
0x34
>
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
147
0
0
0
/
/
lea
0x93
(
%
rip
)
%
r9
#
3a208
<
_sk_store_a8_sse2_lowp
+
0xb4
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
65
136
4
16
/
/
mov
%
al
(
%
r8
%
rdx
1
)
.
byte
235
123
/
/
jmp
3a203
<
_sk_store_a8_sse2_lowp
+
0xaf
>
.
byte
102
68
15
111
5
31
45
0
0
/
/
movdqa
0x2d1f
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
219
195
/
/
pand
%
xmm3
%
xmm8
.
byte
102
69
15
103
192
/
/
packuswb
%
xmm8
%
xmm8
.
byte
102
69
15
214
4
16
/
/
movq
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
235
96
/
/
jmp
3a203
<
_sk_store_a8_sse2_lowp
+
0xaf
>
.
byte
102
15
197
195
2
/
/
pextrw
0x2
%
xmm3
%
eax
.
byte
65
136
68
16
2
/
/
mov
%
al
0x2
(
%
r8
%
rdx
1
)
.
byte
102
68
15
111
5
250
44
0
0
/
/
movdqa
0x2cfa
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
219
195
/
/
pand
%
xmm3
%
xmm8
.
byte
102
69
15
103
192
/
/
packuswb
%
xmm8
%
xmm8
.
byte
102
68
15
126
192
/
/
movd
%
xmm8
%
eax
.
byte
102
65
137
4
16
/
/
mov
%
ax
(
%
r8
%
rdx
1
)
.
byte
235
55
/
/
jmp
3a203
<
_sk_store_a8_sse2_lowp
+
0xaf
>
.
byte
102
15
197
195
6
/
/
pextrw
0x6
%
xmm3
%
eax
.
byte
65
136
68
16
6
/
/
mov
%
al
0x6
(
%
r8
%
rdx
1
)
.
byte
102
15
197
195
5
/
/
pextrw
0x5
%
xmm3
%
eax
.
byte
65
136
68
16
5
/
/
mov
%
al
0x5
(
%
r8
%
rdx
1
)
.
byte
102
15
197
195
4
/
/
pextrw
0x4
%
xmm3
%
eax
.
byte
65
136
68
16
4
/
/
mov
%
al
0x4
(
%
r8
%
rdx
1
)
.
byte
102
68
15
111
5
189
44
0
0
/
/
movdqa
0x2cbd
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
219
195
/
/
pand
%
xmm3
%
xmm8
.
byte
102
69
15
103
192
/
/
packuswb
%
xmm8
%
xmm8
.
byte
102
69
15
126
4
16
/
/
movd
%
xmm8
(
%
r8
%
rdx
1
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
144
/
/
nop
.
byte
118
255
/
/
jbe
3a209
<
_sk_store_a8_sse2_lowp
+
0xb5
>
.
byte
255
/
/
(
bad
)
.
byte
255
165
255
255
255
155
/
/
jmpq
*
-
0x64000001
(
%
rbp
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
226
/
/
jmpq
*
%
rdx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
216
255
/
/
fdivr
%
st
(
7
)
%
st
.
byte
255
/
/
(
bad
)
.
byte
255
206
/
/
dec
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
196
/
/
inc
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_gather_a8_sse2_lowp
.
globl
_sk_gather_a8_sse2_lowp
FUNCTION
(
_sk_gather_a8_sse2_lowp
)
_sk_gather_a8_sse2_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
102
69
15
118
210
/
/
pcmpeqd
%
xmm10
%
xmm10
.
byte
102
69
15
254
202
/
/
paddd
%
xmm10
%
xmm9
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
93
201
/
/
minps
%
xmm9
%
xmm1
.
byte
65
15
93
193
/
/
minps
%
xmm9
%
xmm0
.
byte
243
68
15
16
72
16
/
/
movss
0x10
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
102
69
15
254
202
/
/
paddd
%
xmm10
%
xmm9
.
byte
65
15
95
216
/
/
maxps
%
xmm8
%
xmm3
.
byte
65
15
95
208
/
/
maxps
%
xmm8
%
xmm2
.
byte
65
15
93
209
/
/
minps
%
xmm9
%
xmm2
.
byte
65
15
93
217
/
/
minps
%
xmm9
%
xmm3
.
byte
243
68
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm9
.
byte
243
68
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm10
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
112
218
245
/
/
pshufd
0xf5
%
xmm10
%
xmm11
.
byte
102
68
15
244
219
/
/
pmuludq
%
xmm3
%
xmm11
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
69
15
112
233
245
/
/
pshufd
0xf5
%
xmm9
%
xmm13
.
byte
102
68
15
244
235
/
/
pmuludq
%
xmm3
%
xmm13
.
byte
102
65
15
244
218
/
/
pmuludq
%
xmm10
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
65
15
112
211
232
/
/
pshufd
0xe8
%
xmm11
%
xmm2
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
102
69
15
244
225
/
/
pmuludq
%
xmm9
%
xmm12
.
byte
102
69
15
112
204
232
/
/
pshufd
0xe8
%
xmm12
%
xmm9
.
byte
102
65
15
112
213
232
/
/
pshufd
0xe8
%
xmm13
%
xmm2
.
byte
102
68
15
98
202
/
/
punpckldq
%
xmm2
%
xmm9
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
201
/
/
paddd
%
xmm9
%
xmm1
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
195
/
/
mov
%
eax
%
r11d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
15
112
192
78
/
/
pshufd
0x4e
%
xmm0
%
xmm0
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
36
34
/
/
movzbl
(
%
r10
%
r12
1
)
%
r12d
.
byte
67
15
182
44
58
/
/
movzbl
(
%
r10
%
r15
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
68
9
229
/
/
or
%
r12d
%
ebp
.
byte
71
15
182
52
50
/
/
movzbl
(
%
r10
%
r14
1
)
%
r14d
.
byte
65
15
182
28
26
/
/
movzbl
(
%
r10
%
rbx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
243
/
/
or
%
r14d
%
ebx
.
byte
102
15
110
219
/
/
movd
%
ebx
%
xmm3
.
byte
102
15
196
221
1
/
/
pinsrw
0x1
%
ebp
%
xmm3
.
byte
67
15
182
44
26
/
/
movzbl
(
%
r10
%
r11
1
)
%
ebp
.
byte
65
15
182
4
2
/
/
movzbl
(
%
r10
%
rax
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
232
/
/
or
%
ebp
%
eax
.
byte
102
15
196
216
2
/
/
pinsrw
0x2
%
eax
%
xmm3
.
byte
67
15
182
4
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
eax
.
byte
67
15
182
44
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
9
197
/
/
or
%
eax
%
ebp
.
byte
102
15
196
221
3
/
/
pinsrw
0x3
%
ebp
%
xmm3
.
byte
102
65
15
96
216
/
/
punpcklbw
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_load_g8_sse2_lowp
.
globl
_sk_load_g8_sse2_lowp
FUNCTION
(
_sk_load_g8_sse2_lowp
)
_sk_load_g8_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3a3c5
<
_sk_load_g8_sse2_lowp
+
0x39
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
147
0
0
0
/
/
lea
0x93
(
%
rip
)
%
r9
#
3a444
<
_sk_load_g8_sse2_lowp
+
0xb8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
192
/
/
movd
%
eax
%
xmm0
.
byte
235
97
/
/
jmp
3a426
<
_sk_load_g8_sse2_lowp
+
0x9a
>
.
byte
243
65
15
126
4
16
/
/
movq
(
%
r8
%
rdx
1
)
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
235
85
/
/
jmp
3a426
<
_sk_load_g8_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
192
2
/
/
pinsrw
0x2
%
eax
%
xmm0
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
200
/
/
movd
%
eax
%
xmm1
.
byte
102
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm1
.
byte
243
15
16
193
/
/
movss
%
xmm1
%
xmm0
.
byte
235
51
/
/
jmp
3a426
<
_sk_load_g8_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
192
6
/
/
pinsrw
0x6
%
eax
%
xmm0
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
192
5
/
/
pinsrw
0x5
%
eax
%
xmm0
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
192
4
/
/
pinsrw
0x4
%
eax
%
xmm0
.
byte
102
65
15
110
12
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm1
.
byte
102
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm1
.
byte
242
15
16
193
/
/
movsd
%
xmm1
%
xmm0
.
byte
102
15
219
5
130
42
0
0
/
/
pand
0x2a82
(
%
rip
)
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
121
42
0
0
/
/
movaps
0x2a79
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
118
255
/
/
jbe
3a445
<
_sk_load_g8_sse2_lowp
+
0xb9
>
.
byte
255
/
/
(
bad
)
.
byte
255
156
255
255
255
141
255
/
/
lcall
*
-
0x720001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
175
/
/
mov
0xafffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_load_g8_dst_sse2_lowp
.
globl
_sk_load_g8_dst_sse2_lowp
FUNCTION
(
_sk_load_g8_dst_sse2_lowp
)
_sk_load_g8_dst_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3a499
<
_sk_load_g8_dst_sse2_lowp
+
0x39
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
147
0
0
0
/
/
lea
0x93
(
%
rip
)
%
r9
#
3a518
<
_sk_load_g8_dst_sse2_lowp
+
0xb8
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
235
97
/
/
jmp
3a4fa
<
_sk_load_g8_dst_sse2_lowp
+
0x9a
>
.
byte
243
65
15
126
36
16
/
/
movq
(
%
r8
%
rdx
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
235
85
/
/
jmp
3a4fa
<
_sk_load_g8_dst_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
224
2
/
/
pinsrw
0x2
%
eax
%
xmm4
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
243
15
16
229
/
/
movss
%
xmm5
%
xmm4
.
byte
235
51
/
/
jmp
3a4fa
<
_sk_load_g8_dst_sse2_lowp
+
0x9a
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
224
6
/
/
pinsrw
0x6
%
eax
%
xmm4
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
224
5
/
/
pinsrw
0x5
%
eax
%
xmm4
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
15
196
224
4
/
/
pinsrw
0x4
%
eax
%
xmm4
.
byte
102
65
15
110
44
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
102
15
219
37
174
41
0
0
/
/
pand
0x29ae
(
%
rip
)
%
xmm4
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
61
165
41
0
0
/
/
movaps
0x29a5
(
%
rip
)
%
xmm7
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
118
255
/
/
jbe
3a519
<
_sk_load_g8_dst_sse2_lowp
+
0xb9
>
.
byte
255
/
/
(
bad
)
.
byte
255
156
255
255
255
141
255
/
/
lcall
*
-
0x720001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
212
/
/
callq
*
%
rsp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
201
/
/
dec
%
ecx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
190
255
255
255
175
/
/
mov
0xafffffff
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_luminance_to_alpha_sse2_lowp
.
globl
_sk_luminance_to_alpha_sse2_lowp
FUNCTION
(
_sk_luminance_to_alpha_sse2_lowp
)
_sk_luminance_to_alpha_sse2_lowp
:
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
213
5
208
49
0
0
/
/
pmullw
0x31d0
(
%
rip
)
%
xmm0
#
3d710
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14c4
>
.
byte
102
15
213
13
216
49
0
0
/
/
pmullw
0x31d8
(
%
rip
)
%
xmm1
#
3d720
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14d4
>
.
byte
102
15
253
200
/
/
paddw
%
xmm0
%
xmm1
.
byte
102
15
213
29
220
49
0
0
/
/
pmullw
0x31dc
(
%
rip
)
%
xmm3
#
3d730
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x14e4
>
.
byte
102
15
253
217
/
/
paddw
%
xmm1
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gather_g8_sse2_lowp
.
globl
_sk_gather_g8_sse2_lowp
FUNCTION
(
_sk_gather_g8_sse2_lowp
)
_sk_gather_g8_sse2_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
72
12
/
/
movss
0xc
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
102
69
15
118
210
/
/
pcmpeqd
%
xmm10
%
xmm10
.
byte
102
69
15
254
202
/
/
paddd
%
xmm10
%
xmm9
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
93
201
/
/
minps
%
xmm9
%
xmm1
.
byte
65
15
93
193
/
/
minps
%
xmm9
%
xmm0
.
byte
243
68
15
16
72
16
/
/
movss
0x10
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
102
69
15
254
202
/
/
paddd
%
xmm10
%
xmm9
.
byte
65
15
95
216
/
/
maxps
%
xmm8
%
xmm3
.
byte
65
15
95
208
/
/
maxps
%
xmm8
%
xmm2
.
byte
65
15
93
209
/
/
minps
%
xmm9
%
xmm2
.
byte
65
15
93
217
/
/
minps
%
xmm9
%
xmm3
.
byte
243
68
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm9
.
byte
243
68
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm10
.
byte
102
15
110
88
8
/
/
movd
0x8
(
%
rax
)
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
102
69
15
112
218
245
/
/
pshufd
0xf5
%
xmm10
%
xmm11
.
byte
102
68
15
244
219
/
/
pmuludq
%
xmm3
%
xmm11
.
byte
102
68
15
111
227
/
/
movdqa
%
xmm3
%
xmm12
.
byte
102
69
15
112
233
245
/
/
pshufd
0xf5
%
xmm9
%
xmm13
.
byte
102
68
15
244
235
/
/
pmuludq
%
xmm3
%
xmm13
.
byte
102
65
15
244
218
/
/
pmuludq
%
xmm10
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
65
15
112
211
232
/
/
pshufd
0xe8
%
xmm11
%
xmm2
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
76
139
16
/
/
mov
(
%
rax
)
%
r10
.
byte
102
69
15
244
225
/
/
pmuludq
%
xmm9
%
xmm12
.
byte
102
69
15
112
204
232
/
/
pshufd
0xe8
%
xmm12
%
xmm9
.
byte
102
65
15
112
213
232
/
/
pshufd
0xe8
%
xmm13
%
xmm2
.
byte
102
68
15
98
202
/
/
punpckldq
%
xmm2
%
xmm9
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
65
15
254
201
/
/
paddd
%
xmm9
%
xmm1
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
73
15
126
208
/
/
movq
%
xmm2
%
r8
.
byte
69
137
193
/
/
mov
%
r8d
%
r9d
.
byte
73
193
232
32
/
/
shr
0x20
%
r8
.
byte
102
72
15
126
200
/
/
movq
%
xmm1
%
rax
.
byte
65
137
195
/
/
mov
%
eax
%
r11d
.
byte
72
193
232
32
/
/
shr
0x20
%
rax
.
byte
102
72
15
126
195
/
/
movq
%
xmm0
%
rbx
.
byte
65
137
222
/
/
mov
%
ebx
%
r14d
.
byte
72
193
235
32
/
/
shr
0x20
%
rbx
.
byte
102
15
112
192
78
/
/
pshufd
0x4e
%
xmm0
%
xmm0
.
byte
102
73
15
126
199
/
/
movq
%
xmm0
%
r15
.
byte
69
137
252
/
/
mov
%
r15d
%
r12d
.
byte
73
193
239
32
/
/
shr
0x20
%
r15
.
byte
71
15
182
36
34
/
/
movzbl
(
%
r10
%
r12
1
)
%
r12d
.
byte
67
15
182
44
58
/
/
movzbl
(
%
r10
%
r15
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
68
9
229
/
/
or
%
r12d
%
ebp
.
byte
71
15
182
52
50
/
/
movzbl
(
%
r10
%
r14
1
)
%
r14d
.
byte
65
15
182
28
26
/
/
movzbl
(
%
r10
%
rbx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
68
9
243
/
/
or
%
r14d
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
102
15
196
197
1
/
/
pinsrw
0x1
%
ebp
%
xmm0
.
byte
67
15
182
44
26
/
/
movzbl
(
%
r10
%
r11
1
)
%
ebp
.
byte
65
15
182
4
2
/
/
movzbl
(
%
r10
%
rax
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
232
/
/
or
%
ebp
%
eax
.
byte
102
15
196
192
2
/
/
pinsrw
0x2
%
eax
%
xmm0
.
byte
67
15
182
4
10
/
/
movzbl
(
%
r10
%
r9
1
)
%
eax
.
byte
67
15
182
44
2
/
/
movzbl
(
%
r10
%
r8
1
)
%
ebp
.
byte
193
229
8
/
/
shl
0x8
%
ebp
.
byte
9
197
/
/
or
%
eax
%
ebp
.
byte
102
15
196
197
3
/
/
pinsrw
0x3
%
ebp
%
xmm0
.
byte
102
65
15
96
192
/
/
punpcklbw
%
xmm8
%
xmm0
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
29
234
39
0
0
/
/
movaps
0x27ea
(
%
rip
)
%
xmm3
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_1_float_sse2_lowp
.
globl
_sk_scale_1_float_sse2_lowp
FUNCTION
(
_sk_scale_1_float_sse2_lowp
)
_sk_scale_1_float_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
89
5
64
30
0
0
/
/
mulss
0x1e40
(
%
rip
)
%
xmm8
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
243
68
15
88
5
7
30
0
0
/
/
addss
0x1e07
(
%
rip
)
%
xmm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
243
65
15
44
192
/
/
cvttss2si
%
xmm8
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
242
69
15
112
192
0
/
/
pshuflw
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
80
/
/
pshufd
0x50
%
xmm8
%
xmm8
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
68
15
111
13
155
39
0
0
/
/
movdqa
0x279b
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
65
15
213
216
/
/
pmullw
%
xmm8
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_lerp_1_float_sse2_lowp
.
globl
_sk_lerp_1_float_sse2_lowp
FUNCTION
(
_sk_lerp_1_float_sse2_lowp
)
_sk_lerp_1_float_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
68
15
16
0
/
/
movss
(
%
rax
)
%
xmm8
.
byte
243
68
15
89
5
200
29
0
0
/
/
mulss
0x1dc8
(
%
rip
)
%
xmm8
#
3c528
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc
>
.
byte
243
68
15
88
5
143
29
0
0
/
/
addss
0x1d8f
(
%
rip
)
%
xmm8
#
3c4f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2ac
>
.
byte
243
65
15
44
192
/
/
cvttss2si
%
xmm8
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
242
69
15
112
192
0
/
/
pshuflw
0x0
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
80
/
/
pshufd
0x50
%
xmm8
%
xmm8
.
byte
102
68
15
111
13
40
39
0
0
/
/
movdqa
0x2728
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
65
15
213
216
/
/
pmullw
%
xmm8
%
xmm3
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
69
15
249
200
/
/
psubw
%
xmm8
%
xmm9
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
196
/
/
pmullw
%
xmm4
%
xmm8
.
byte
102
65
15
253
192
/
/
paddw
%
xmm8
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
197
/
/
pmullw
%
xmm5
%
xmm8
.
byte
102
65
15
253
200
/
/
paddw
%
xmm8
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
193
/
/
movdqa
%
xmm9
%
xmm8
.
byte
102
68
15
213
198
/
/
pmullw
%
xmm6
%
xmm8
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
207
/
/
pmullw
%
xmm7
%
xmm9
.
byte
102
65
15
253
217
/
/
paddw
%
xmm9
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_scale_u8_sse2_lowp
.
globl
_sk_scale_u8_sse2_lowp
FUNCTION
(
_sk_scale_u8_sse2_lowp
)
_sk_scale_u8_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
36
/
/
ja
3a83f
<
_sk_scale_u8_sse2_lowp
+
0x3b
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
218
0
0
0
/
/
lea
0xda
(
%
rip
)
%
r9
#
3a904
<
_sk_scale_u8_sse2_lowp
+
0x100
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
109
/
/
jmp
3a8ac
<
_sk_scale_u8_sse2_lowp
+
0xa8
>
.
byte
243
69
15
126
4
16
/
/
movq
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
68
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm8
.
byte
235
96
/
/
jmp
3a8ac
<
_sk_scale_u8_sse2_lowp
+
0xa8
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
68
15
196
192
2
/
/
pinsrw
0x2
%
eax
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
102
68
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm9
.
byte
243
69
15
16
193
/
/
movss
%
xmm9
%
xmm8
.
byte
235
57
/
/
jmp
3a8ac
<
_sk_scale_u8_sse2_lowp
+
0xa8
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
68
15
196
192
6
/
/
pinsrw
0x6
%
eax
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
192
5
/
/
pinsrw
0x5
%
eax
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
192
4
/
/
pinsrw
0x4
%
eax
%
xmm8
.
byte
102
69
15
110
12
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
102
68
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm9
.
byte
242
69
15
16
193
/
/
movsd
%
xmm9
%
xmm8
.
byte
102
68
15
219
5
251
37
0
0
/
/
pand
0x25fb
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
68
15
111
13
237
37
0
0
/
/
movdqa
0x25ed
(
%
rip
)
%
xmm9
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
253
193
/
/
paddw
%
xmm9
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
201
/
/
paddw
%
xmm9
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
209
/
/
paddw
%
xmm9
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
47
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
89
255
/
/
lcall
*
-
0x1
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
72
255
/
/
decl
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
152
255
255
255
140
/
/
lcall
*
-
0x73000001
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
128
255
255
255
111
/
/
incl
0x6fffffff
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_u8_sse2_lowp
.
globl
_sk_lerp_u8_sse2_lowp
FUNCTION
(
_sk_lerp_u8_sse2_lowp
)
_sk_lerp_u8_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
36
/
/
ja
3a95b
<
_sk_lerp_u8_sse2_lowp
+
0x3b
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
30
1
0
0
/
/
lea
0x11e
(
%
rip
)
%
r9
#
3aa64
<
_sk_lerp_u8_sse2_lowp
+
0x144
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
182
4
16
/
/
movzbl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
192
/
/
movd
%
eax
%
xmm8
.
byte
235
109
/
/
jmp
3a9c8
<
_sk_lerp_u8_sse2_lowp
+
0xa8
>
.
byte
243
69
15
126
4
16
/
/
movq
(
%
r8
%
rdx
1
)
%
xmm8
.
byte
102
68
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm8
.
byte
235
96
/
/
jmp
3a9c8
<
_sk_lerp_u8_sse2_lowp
+
0xa8
>
.
byte
65
15
182
68
16
2
/
/
movzbl
0x2
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
68
15
196
192
2
/
/
pinsrw
0x2
%
eax
%
xmm8
.
byte
65
15
183
4
16
/
/
movzwl
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
110
200
/
/
movd
%
eax
%
xmm9
.
byte
102
68
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm9
.
byte
243
69
15
16
193
/
/
movss
%
xmm9
%
xmm8
.
byte
235
57
/
/
jmp
3a9c8
<
_sk_lerp_u8_sse2_lowp
+
0xa8
>
.
byte
65
15
182
68
16
6
/
/
movzbl
0x6
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
102
68
15
196
192
6
/
/
pinsrw
0x6
%
eax
%
xmm8
.
byte
65
15
182
68
16
5
/
/
movzbl
0x5
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
192
5
/
/
pinsrw
0x5
%
eax
%
xmm8
.
byte
65
15
182
68
16
4
/
/
movzbl
0x4
(
%
r8
%
rdx
1
)
%
eax
.
byte
102
68
15
196
192
4
/
/
pinsrw
0x4
%
eax
%
xmm8
.
byte
102
69
15
110
12
16
/
/
movd
(
%
r8
%
rdx
1
)
%
xmm9
.
byte
102
68
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm9
.
byte
242
69
15
16
193
/
/
movsd
%
xmm9
%
xmm8
.
byte
102
68
15
219
5
223
36
0
0
/
/
pand
0x24df
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
68
15
111
21
214
36
0
0
/
/
movdqa
0x24d6
(
%
rip
)
%
xmm10
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
69
15
239
202
/
/
pxor
%
xmm10
%
xmm9
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
68
15
213
220
/
/
pmullw
%
xmm4
%
xmm11
.
byte
102
65
15
213
192
/
/
pmullw
%
xmm8
%
xmm0
.
byte
102
65
15
253
194
/
/
paddw
%
xmm10
%
xmm0
.
byte
102
65
15
253
195
/
/
paddw
%
xmm11
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
68
15
213
221
/
/
pmullw
%
xmm5
%
xmm11
.
byte
102
65
15
213
200
/
/
pmullw
%
xmm8
%
xmm1
.
byte
102
65
15
253
202
/
/
paddw
%
xmm10
%
xmm1
.
byte
102
65
15
253
203
/
/
paddw
%
xmm11
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
68
15
213
222
/
/
pmullw
%
xmm6
%
xmm11
.
byte
102
65
15
213
208
/
/
pmullw
%
xmm8
%
xmm2
.
byte
102
65
15
253
210
/
/
paddw
%
xmm10
%
xmm2
.
byte
102
65
15
253
211
/
/
paddw
%
xmm11
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
68
15
213
207
/
/
pmullw
%
xmm7
%
xmm9
.
byte
102
68
15
213
195
/
/
pmullw
%
xmm3
%
xmm8
.
byte
102
69
15
253
194
/
/
paddw
%
xmm10
%
xmm8
.
byte
102
69
15
253
193
/
/
paddw
%
xmm9
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
235
254
/
/
jmp
3aa64
<
_sk_lerp_u8_sse2_lowp
+
0x144
>
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
4
/
/
callq
*
0x4ffffff
(
%
rip
)
#
503aa6c
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x4ffe820
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
84
255
255
/
/
callq
*
-
0x1
(
%
rdi
%
rdi
8
)
.
byte
255
72
255
/
/
decl
-
0x1
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
60
255
/
/
cmp
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
43
/
/
ljmp
*
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_scale_565_sse2_lowp
.
globl
_sk_scale_565_sse2_lowp
FUNCTION
(
_sk_scale_565_sse2_lowp
)
_sk_scale_565_sse2_lowp
:
.
byte
102
15
127
116
36
232
/
/
movdqa
%
xmm6
-
0x18
(
%
rsp
)
.
byte
15
41
108
36
216
/
/
movaps
%
xmm5
-
0x28
(
%
rsp
)
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3aacb
<
_sk_scale_565_sse2_lowp
+
0x4b
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
229
1
0
0
/
/
lea
0x1e5
(
%
rip
)
%
r9
#
3ac9c
<
_sk_scale_565_sse2_lowp
+
0x21c
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
240
/
/
movd
%
eax
%
xmm6
.
byte
235
67
/
/
jmp
3ab0e
<
_sk_scale_565_sse2_lowp
+
0x8e
>
.
byte
243
65
15
111
52
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
235
59
/
/
jmp
3ab0e
<
_sk_scale_565_sse2_lowp
+
0x8e
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
102
65
15
196
116
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
243
69
15
16
12
80
/
/
movss
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
243
65
15
16
241
/
/
movss
%
xmm9
%
xmm6
.
byte
235
34
/
/
jmp
3ab0e
<
_sk_scale_565_sse2_lowp
+
0x8e
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
102
65
15
196
116
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
102
65
15
196
116
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
102
65
15
196
116
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
102
65
15
18
52
80
/
/
movlpd
(
%
r8
%
rdx
2
)
%
xmm6
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
68
15
219
29
94
43
0
0
/
/
pand
0x2b5e
(
%
rip
)
%
xmm11
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
68
15
111
206
/
/
movdqa
%
xmm6
%
xmm9
.
byte
102
65
15
113
209
5
/
/
psrlw
0x5
%
xmm9
.
byte
102
68
15
219
13
90
43
0
0
/
/
pand
0x2b5a
(
%
rip
)
%
xmm9
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
68
15
111
21
97
43
0
0
/
/
movdqa
0x2b61
(
%
rip
)
%
xmm10
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
68
15
219
214
/
/
pand
%
xmm6
%
xmm10
.
byte
102
15
113
214
13
/
/
psrlw
0xd
%
xmm6
.
byte
102
65
15
235
243
/
/
por
%
xmm11
%
xmm6
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
65
15
113
243
2
/
/
psllw
0x2
%
xmm11
.
byte
102
65
15
113
209
4
/
/
psrlw
0x4
%
xmm9
.
byte
102
69
15
235
203
/
/
por
%
xmm11
%
xmm9
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
65
15
113
243
3
/
/
psllw
0x3
%
xmm11
.
byte
102
65
15
113
210
2
/
/
psrlw
0x2
%
xmm10
.
byte
102
69
15
235
211
/
/
por
%
xmm11
%
xmm10
.
byte
102
68
15
111
37
189
42
0
0
/
/
movdqa
0x2abd
(
%
rip
)
%
xmm12
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
68
15
111
235
/
/
movdqa
%
xmm3
%
xmm13
.
byte
102
69
15
239
236
/
/
pxor
%
xmm12
%
xmm13
.
byte
102
68
15
111
223
/
/
movdqa
%
xmm7
%
xmm11
.
byte
102
69
15
239
220
/
/
pxor
%
xmm12
%
xmm11
.
byte
102
69
15
101
221
/
/
pcmpgtw
%
xmm13
%
xmm11
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
69
15
239
236
/
/
pxor
%
xmm12
%
xmm13
.
byte
102
69
15
111
241
/
/
movdqa
%
xmm9
%
xmm14
.
byte
102
69
15
239
244
/
/
pxor
%
xmm12
%
xmm14
.
byte
102
69
15
101
238
/
/
pcmpgtw
%
xmm14
%
xmm13
.
byte
102
69
15
111
245
/
/
movdqa
%
xmm13
%
xmm14
.
byte
102
69
15
223
242
/
/
pandn
%
xmm10
%
xmm14
.
byte
102
69
15
111
249
/
/
movdqa
%
xmm9
%
xmm15
.
byte
102
69
15
219
253
/
/
pand
%
xmm13
%
xmm15
.
byte
102
69
15
235
254
/
/
por
%
xmm14
%
xmm15
.
byte
102
69
15
111
247
/
/
movdqa
%
xmm15
%
xmm14
.
byte
102
69
15
239
244
/
/
pxor
%
xmm12
%
xmm14
.
byte
102
68
15
111
198
/
/
movdqa
%
xmm6
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
102
69
15
101
240
/
/
pcmpgtw
%
xmm8
%
xmm14
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
65
15
219
198
/
/
pand
%
xmm14
%
xmm0
.
byte
102
69
15
223
247
/
/
pandn
%
xmm15
%
xmm14
.
byte
102
68
15
235
240
/
/
por
%
xmm0
%
xmm14
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
65
15
219
197
/
/
pand
%
xmm13
%
xmm0
.
byte
102
69
15
223
233
/
/
pandn
%
xmm9
%
xmm13
.
byte
102
68
15
235
232
/
/
por
%
xmm0
%
xmm13
.
byte
102
69
15
239
229
/
/
pxor
%
xmm13
%
xmm12
.
byte
102
69
15
101
224
/
/
pcmpgtw
%
xmm8
%
xmm12
.
byte
102
69
15
219
236
/
/
pand
%
xmm12
%
xmm13
.
byte
102
68
15
223
230
/
/
pandn
%
xmm6
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
102
69
15
219
243
/
/
pand
%
xmm11
%
xmm14
.
byte
102
69
15
223
220
/
/
pandn
%
xmm12
%
xmm11
.
byte
102
69
15
235
222
/
/
por
%
xmm14
%
xmm11
.
byte
102
15
213
245
/
/
pmullw
%
xmm5
%
xmm6
.
byte
102
68
15
213
201
/
/
pmullw
%
xmm1
%
xmm9
.
byte
102
68
15
213
210
/
/
pmullw
%
xmm2
%
xmm10
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
15
111
5
95
34
0
0
/
/
movdqa
0x225f
(
%
rip
)
%
xmm0
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
253
240
/
/
paddw
%
xmm0
%
xmm6
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
68
15
253
208
/
/
paddw
%
xmm0
%
xmm10
.
byte
102
68
15
253
216
/
/
paddw
%
xmm0
%
xmm11
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
65
15
111
210
/
/
movdqa
%
xmm10
%
xmm2
.
byte
102
65
15
111
219
/
/
movdqa
%
xmm11
%
xmm3
.
byte
15
40
108
36
216
/
/
movaps
-
0x28
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm6
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
36
254
/
/
and
0xfe
%
al
.
byte
255
/
/
(
bad
)
.
byte
255
67
254
/
/
incl
-
0x2
(
%
rbx
)
.
byte
255
/
/
(
bad
)
.
byte
255
55
/
/
pushq
(
%
rdi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
108
254
255
/
/
ljmp
*
-
0x1
(
%
rsi
%
rdi
8
)
.
byte
255
100
254
255
/
/
jmpq
*
-
0x1
(
%
rsi
%
rdi
8
)
.
byte
255
92
254
255
/
/
lcall
*
-
0x1
(
%
rsi
%
rdi
8
)
.
byte
255
80
254
/
/
callq
*
-
0x2
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_lerp_565_sse2_lowp
.
globl
_sk_lerp_565_sse2_lowp
FUNCTION
(
_sk_lerp_565_sse2_lowp
)
_sk_lerp_565_sse2_lowp
:
.
byte
102
15
127
84
36
232
/
/
movdqa
%
xmm2
-
0x18
(
%
rsp
)
.
byte
15
41
76
36
216
/
/
movaps
%
xmm1
-
0x28
(
%
rsp
)
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
77
1
192
/
/
add
%
r8
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
137
248
/
/
mov
%
edi
%
eax
.
byte
36
7
/
/
and
0x7
%
al
.
byte
254
200
/
/
dec
%
al
.
byte
60
6
/
/
cmp
0x6
%
al
.
byte
119
34
/
/
ja
3ad03
<
_sk_lerp_565_sse2_lowp
+
0x4b
>
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
15
182
192
/
/
movzbl
%
al
%
eax
.
byte
76
141
13
45
2
0
0
/
/
lea
0x22d
(
%
rip
)
%
r9
#
3af1c
<
_sk_lerp_565_sse2_lowp
+
0x264
>
.
byte
73
99
4
129
/
/
movslq
(
%
r9
%
rax
4
)
%
rax
.
byte
76
1
200
/
/
add
%
r9
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
65
15
183
4
80
/
/
movzwl
(
%
r8
%
rdx
2
)
%
eax
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
235
67
/
/
jmp
3ad46
<
_sk_lerp_565_sse2_lowp
+
0x8e
>
.
byte
243
65
15
111
20
80
/
/
movdqu
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
235
59
/
/
jmp
3ad46
<
_sk_lerp_565_sse2_lowp
+
0x8e
>
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
65
15
196
84
80
4
2
/
/
pinsrw
0x2
0x4
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
243
69
15
16
12
80
/
/
movss
(
%
r8
%
rdx
2
)
%
xmm9
.
byte
243
65
15
16
209
/
/
movss
%
xmm9
%
xmm2
.
byte
235
34
/
/
jmp
3ad46
<
_sk_lerp_565_sse2_lowp
+
0x8e
>
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
65
15
196
84
80
12
6
/
/
pinsrw
0x6
0xc
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
102
65
15
196
84
80
10
5
/
/
pinsrw
0x5
0xa
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
102
65
15
196
84
80
8
4
/
/
pinsrw
0x4
0x8
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
102
65
15
18
20
80
/
/
movlpd
(
%
r8
%
rdx
2
)
%
xmm2
.
byte
102
68
15
111
218
/
/
movdqa
%
xmm2
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
68
15
219
29
38
41
0
0
/
/
pand
0x2926
(
%
rip
)
%
xmm11
#
3d680
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1434
>
.
byte
102
68
15
111
202
/
/
movdqa
%
xmm2
%
xmm9
.
byte
102
65
15
113
209
5
/
/
psrlw
0x5
%
xmm9
.
byte
102
68
15
219
13
34
41
0
0
/
/
pand
0x2922
(
%
rip
)
%
xmm9
#
3d690
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1444
>
.
byte
102
68
15
111
21
41
41
0
0
/
/
movdqa
0x2929
(
%
rip
)
%
xmm10
#
3d6a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1454
>
.
byte
102
68
15
219
210
/
/
pand
%
xmm2
%
xmm10
.
byte
102
15
113
210
13
/
/
psrlw
0xd
%
xmm2
.
byte
102
65
15
235
211
/
/
por
%
xmm11
%
xmm2
.
byte
102
69
15
111
217
/
/
movdqa
%
xmm9
%
xmm11
.
byte
102
65
15
113
243
2
/
/
psllw
0x2
%
xmm11
.
byte
102
65
15
113
209
4
/
/
psrlw
0x4
%
xmm9
.
byte
102
69
15
235
203
/
/
por
%
xmm11
%
xmm9
.
byte
102
69
15
111
218
/
/
movdqa
%
xmm10
%
xmm11
.
byte
102
65
15
113
243
3
/
/
psllw
0x3
%
xmm11
.
byte
102
65
15
113
210
2
/
/
psrlw
0x2
%
xmm10
.
byte
102
69
15
235
211
/
/
por
%
xmm11
%
xmm10
.
byte
102
68
15
111
37
133
40
0
0
/
/
movdqa
0x2885
(
%
rip
)
%
xmm12
#
3d640
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x13f4
>
.
byte
102
68
15
111
235
/
/
movdqa
%
xmm3
%
xmm13
.
byte
102
69
15
239
236
/
/
pxor
%
xmm12
%
xmm13
.
byte
102
68
15
111
223
/
/
movdqa
%
xmm7
%
xmm11
.
byte
102
69
15
239
220
/
/
pxor
%
xmm12
%
xmm11
.
byte
102
69
15
101
221
/
/
pcmpgtw
%
xmm13
%
xmm11
.
byte
102
69
15
111
234
/
/
movdqa
%
xmm10
%
xmm13
.
byte
102
69
15
239
236
/
/
pxor
%
xmm12
%
xmm13
.
byte
102
69
15
111
241
/
/
movdqa
%
xmm9
%
xmm14
.
byte
102
69
15
239
244
/
/
pxor
%
xmm12
%
xmm14
.
byte
102
69
15
101
238
/
/
pcmpgtw
%
xmm14
%
xmm13
.
byte
102
69
15
111
245
/
/
movdqa
%
xmm13
%
xmm14
.
byte
102
69
15
223
242
/
/
pandn
%
xmm10
%
xmm14
.
byte
102
69
15
111
249
/
/
movdqa
%
xmm9
%
xmm15
.
byte
102
69
15
219
253
/
/
pand
%
xmm13
%
xmm15
.
byte
102
69
15
235
254
/
/
por
%
xmm14
%
xmm15
.
byte
102
69
15
111
247
/
/
movdqa
%
xmm15
%
xmm14
.
byte
102
69
15
239
244
/
/
pxor
%
xmm12
%
xmm14
.
byte
102
68
15
111
194
/
/
movdqa
%
xmm2
%
xmm8
.
byte
102
69
15
235
196
/
/
por
%
xmm12
%
xmm8
.
byte
102
69
15
101
240
/
/
pcmpgtw
%
xmm8
%
xmm14
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
65
15
219
198
/
/
pand
%
xmm14
%
xmm0
.
byte
102
69
15
223
247
/
/
pandn
%
xmm15
%
xmm14
.
byte
102
68
15
235
240
/
/
por
%
xmm0
%
xmm14
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
65
15
219
197
/
/
pand
%
xmm13
%
xmm0
.
byte
102
69
15
223
233
/
/
pandn
%
xmm9
%
xmm13
.
byte
102
68
15
235
232
/
/
por
%
xmm0
%
xmm13
.
byte
102
69
15
239
229
/
/
pxor
%
xmm13
%
xmm12
.
byte
102
69
15
101
224
/
/
pcmpgtw
%
xmm8
%
xmm12
.
byte
102
69
15
219
236
/
/
pand
%
xmm12
%
xmm13
.
byte
102
68
15
223
226
/
/
pandn
%
xmm2
%
xmm12
.
byte
102
69
15
235
229
/
/
por
%
xmm13
%
xmm12
.
byte
102
69
15
219
243
/
/
pand
%
xmm11
%
xmm14
.
byte
102
69
15
223
220
/
/
pandn
%
xmm12
%
xmm11
.
byte
102
69
15
235
222
/
/
por
%
xmm14
%
xmm11
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
213
209
/
/
pmullw
%
xmm1
%
xmm2
.
byte
102
68
15
111
5
49
32
0
0
/
/
movdqa
0x2031
(
%
rip
)
%
xmm8
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
65
15
239
192
/
/
pxor
%
xmm8
%
xmm0
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
65
15
253
208
/
/
paddw
%
xmm8
%
xmm2
.
byte
102
15
253
208
/
/
paddw
%
xmm0
%
xmm2
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
68
15
213
76
36
216
/
/
pmullw
-
0x28
(
%
rsp
)
%
xmm9
.
byte
102
65
15
239
192
/
/
pxor
%
xmm8
%
xmm0
.
byte
102
15
213
197
/
/
pmullw
%
xmm5
%
xmm0
.
byte
102
69
15
253
200
/
/
paddw
%
xmm8
%
xmm9
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
65
15
111
194
/
/
movdqa
%
xmm10
%
xmm0
.
byte
102
68
15
213
84
36
232
/
/
pmullw
-
0x18
(
%
rsp
)
%
xmm10
.
byte
102
65
15
239
192
/
/
pxor
%
xmm8
%
xmm0
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
69
15
253
208
/
/
paddw
%
xmm8
%
xmm10
.
byte
102
68
15
253
208
/
/
paddw
%
xmm0
%
xmm10
.
byte
102
65
15
111
195
/
/
movdqa
%
xmm11
%
xmm0
.
byte
102
68
15
213
219
/
/
pmullw
%
xmm3
%
xmm11
.
byte
102
65
15
239
192
/
/
pxor
%
xmm8
%
xmm0
.
byte
102
69
15
253
216
/
/
paddw
%
xmm8
%
xmm11
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
68
15
253
216
/
/
paddw
%
xmm0
%
xmm11
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
102
65
15
111
210
/
/
movdqa
%
xmm10
%
xmm2
.
byte
102
65
15
111
219
/
/
movdqa
%
xmm11
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
220
253
/
/
fdivr
%
st
%
st
(
5
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
251
/
/
sti
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
239
/
/
out
%
eax
(
%
dx
)
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
36
254
/
/
jmpq
*
(
%
rsi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
28
254
/
/
lcall
*
(
%
rsi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
20
254
/
/
callq
*
(
%
rsi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
8
/
/
decl
(
%
rax
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_clamp_x_1_sse2_lowp
.
globl
_sk_clamp_x_1_sse2_lowp
FUNCTION
(
_sk_clamp_x_1_sse2_lowp
)
_sk_clamp_x_1_sse2_lowp
:
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
95
200
/
/
maxps
%
xmm8
%
xmm1
.
byte
65
15
95
192
/
/
maxps
%
xmm8
%
xmm0
.
byte
68
15
40
5
196
31
0
0
/
/
movaps
0x1fc4
(
%
rip
)
%
xmm8
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
65
15
93
192
/
/
minps
%
xmm8
%
xmm0
.
byte
65
15
93
200
/
/
minps
%
xmm8
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_repeat_x_1_sse2_lowp
.
globl
_sk_repeat_x_1_sse2_lowp
FUNCTION
(
_sk_repeat_x_1_sse2_lowp
)
_sk_repeat_x_1_sse2_lowp
:
.
byte
243
68
15
91
193
/
/
cvttps2dq
%
xmm1
%
xmm8
.
byte
243
68
15
91
200
/
/
cvttps2dq
%
xmm0
%
xmm9
.
byte
69
15
91
201
/
/
cvtdq2ps
%
xmm9
%
xmm9
.
byte
69
15
91
192
/
/
cvtdq2ps
%
xmm8
%
xmm8
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
69
15
194
208
1
/
/
cmpltps
%
xmm8
%
xmm10
.
byte
68
15
40
216
/
/
movaps
%
xmm0
%
xmm11
.
byte
69
15
194
217
1
/
/
cmpltps
%
xmm9
%
xmm11
.
byte
68
15
40
37
140
31
0
0
/
/
movaps
0x1f8c
(
%
rip
)
%
xmm12
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
220
/
/
andps
%
xmm12
%
xmm11
.
byte
69
15
84
212
/
/
andps
%
xmm12
%
xmm10
.
byte
69
15
87
237
/
/
xorps
%
xmm13
%
xmm13
.
byte
69
15
92
194
/
/
subps
%
xmm10
%
xmm8
.
byte
69
15
92
203
/
/
subps
%
xmm11
%
xmm9
.
byte
65
15
92
193
/
/
subps
%
xmm9
%
xmm0
.
byte
65
15
92
200
/
/
subps
%
xmm8
%
xmm1
.
byte
65
15
95
205
/
/
maxps
%
xmm13
%
xmm1
.
byte
65
15
95
197
/
/
maxps
%
xmm13
%
xmm0
.
byte
65
15
93
196
/
/
minps
%
xmm12
%
xmm0
.
byte
65
15
93
204
/
/
minps
%
xmm12
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_mirror_x_1_sse2_lowp
.
globl
_sk_mirror_x_1_sse2_lowp
FUNCTION
(
_sk_mirror_x_1_sse2_lowp
)
_sk_mirror_x_1_sse2_lowp
:
.
byte
68
15
40
5
180
31
0
0
/
/
movaps
0x1fb4
(
%
rip
)
%
xmm8
#
3cf70
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xd24
>
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
68
15
40
21
52
31
0
0
/
/
movaps
0x1f34
(
%
rip
)
%
xmm10
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
40
217
/
/
movaps
%
xmm1
%
xmm11
.
byte
69
15
89
218
/
/
mulps
%
xmm10
%
xmm11
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
243
69
15
91
202
/
/
cvttps2dq
%
xmm10
%
xmm9
.
byte
243
69
15
91
227
/
/
cvttps2dq
%
xmm11
%
xmm12
.
byte
69
15
91
228
/
/
cvtdq2ps
%
xmm12
%
xmm12
.
byte
69
15
91
233
/
/
cvtdq2ps
%
xmm9
%
xmm13
.
byte
69
15
194
213
1
/
/
cmpltps
%
xmm13
%
xmm10
.
byte
69
15
194
220
1
/
/
cmpltps
%
xmm12
%
xmm11
.
byte
68
15
40
13
20
31
0
0
/
/
movaps
0x1f14
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
84
217
/
/
andps
%
xmm9
%
xmm11
.
byte
69
15
84
209
/
/
andps
%
xmm9
%
xmm10
.
byte
69
15
87
246
/
/
xorps
%
xmm14
%
xmm14
.
byte
69
15
92
234
/
/
subps
%
xmm10
%
xmm13
.
byte
69
15
92
227
/
/
subps
%
xmm11
%
xmm12
.
byte
69
15
88
228
/
/
addps
%
xmm12
%
xmm12
.
byte
69
15
88
237
/
/
addps
%
xmm13
%
xmm13
.
byte
65
15
92
197
/
/
subps
%
xmm13
%
xmm0
.
byte
65
15
92
204
/
/
subps
%
xmm12
%
xmm1
.
byte
65
15
88
200
/
/
addps
%
xmm8
%
xmm1
.
byte
65
15
88
192
/
/
addps
%
xmm8
%
xmm0
.
byte
68
15
40
5
64
36
0
0
/
/
movaps
0x2440
(
%
rip
)
%
xmm8
#
3d470
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1224
>
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
65
15
95
206
/
/
maxps
%
xmm14
%
xmm1
.
byte
65
15
95
198
/
/
maxps
%
xmm14
%
xmm0
.
byte
65
15
93
193
/
/
minps
%
xmm9
%
xmm0
.
byte
65
15
93
201
/
/
minps
%
xmm9
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_sse2_lowp
.
globl
_sk_decal_x_sse2_lowp
FUNCTION
(
_sk_decal_x_sse2_lowp
)
_sk_decal_x_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
201
2
/
/
cmpleps
%
xmm1
%
xmm9
.
byte
242
69
15
112
201
232
/
/
pshuflw
0xe8
%
xmm9
%
xmm9
.
byte
243
69
15
112
201
232
/
/
pshufhw
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
112
201
232
/
/
pshufd
0xe8
%
xmm9
%
xmm9
.
byte
68
15
194
192
2
/
/
cmpleps
%
xmm0
%
xmm8
.
byte
242
69
15
112
192
232
/
/
pshuflw
0xe8
%
xmm8
%
xmm8
.
byte
243
69
15
112
192
232
/
/
pshufhw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
232
/
/
pshufd
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
108
193
/
/
punpcklqdq
%
xmm9
%
xmm8
.
byte
102
65
15
113
240
15
/
/
psllw
0xf
%
xmm8
.
byte
102
65
15
113
224
15
/
/
psraw
0xf
%
xmm8
.
byte
243
68
15
16
72
64
/
/
movss
0x40
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
209
/
/
movaps
%
xmm1
%
xmm10
.
byte
69
15
194
209
1
/
/
cmpltps
%
xmm9
%
xmm10
.
byte
242
69
15
112
210
232
/
/
pshuflw
0xe8
%
xmm10
%
xmm10
.
byte
243
69
15
112
210
232
/
/
pshufhw
0xe8
%
xmm10
%
xmm10
.
byte
102
69
15
112
210
232
/
/
pshufd
0xe8
%
xmm10
%
xmm10
.
byte
68
15
40
216
/
/
movaps
%
xmm0
%
xmm11
.
byte
69
15
194
217
1
/
/
cmpltps
%
xmm9
%
xmm11
.
byte
242
69
15
112
203
232
/
/
pshuflw
0xe8
%
xmm11
%
xmm9
.
byte
243
69
15
112
201
232
/
/
pshufhw
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
112
201
232
/
/
pshufd
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
108
202
/
/
punpcklqdq
%
xmm10
%
xmm9
.
byte
102
65
15
113
241
15
/
/
psllw
0xf
%
xmm9
.
byte
102
65
15
113
225
15
/
/
psraw
0xf
%
xmm9
.
byte
102
69
15
219
200
/
/
pand
%
xmm8
%
xmm9
.
byte
243
68
15
127
8
/
/
movdqu
%
xmm9
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_y_sse2_lowp
.
globl
_sk_decal_y_sse2_lowp
FUNCTION
(
_sk_decal_y_sse2_lowp
)
_sk_decal_y_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
203
2
/
/
cmpleps
%
xmm3
%
xmm9
.
byte
242
69
15
112
201
232
/
/
pshuflw
0xe8
%
xmm9
%
xmm9
.
byte
243
69
15
112
201
232
/
/
pshufhw
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
112
201
232
/
/
pshufd
0xe8
%
xmm9
%
xmm9
.
byte
68
15
194
194
2
/
/
cmpleps
%
xmm2
%
xmm8
.
byte
242
69
15
112
192
232
/
/
pshuflw
0xe8
%
xmm8
%
xmm8
.
byte
243
69
15
112
192
232
/
/
pshufhw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
232
/
/
pshufd
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
108
193
/
/
punpcklqdq
%
xmm9
%
xmm8
.
byte
102
65
15
113
240
15
/
/
psllw
0xf
%
xmm8
.
byte
102
65
15
113
224
15
/
/
psraw
0xf
%
xmm8
.
byte
243
68
15
16
72
68
/
/
movss
0x44
(
%
rax
)
%
xmm9
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
68
15
40
211
/
/
movaps
%
xmm3
%
xmm10
.
byte
69
15
194
209
1
/
/
cmpltps
%
xmm9
%
xmm10
.
byte
242
69
15
112
210
232
/
/
pshuflw
0xe8
%
xmm10
%
xmm10
.
byte
243
69
15
112
210
232
/
/
pshufhw
0xe8
%
xmm10
%
xmm10
.
byte
102
69
15
112
210
232
/
/
pshufd
0xe8
%
xmm10
%
xmm10
.
byte
68
15
40
218
/
/
movaps
%
xmm2
%
xmm11
.
byte
69
15
194
217
1
/
/
cmpltps
%
xmm9
%
xmm11
.
byte
242
69
15
112
203
232
/
/
pshuflw
0xe8
%
xmm11
%
xmm9
.
byte
243
69
15
112
201
232
/
/
pshufhw
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
112
201
232
/
/
pshufd
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
108
202
/
/
punpcklqdq
%
xmm10
%
xmm9
.
byte
102
65
15
113
241
15
/
/
psllw
0xf
%
xmm9
.
byte
102
65
15
113
225
15
/
/
psraw
0xf
%
xmm9
.
byte
102
69
15
219
200
/
/
pand
%
xmm8
%
xmm9
.
byte
243
68
15
127
8
/
/
movdqu
%
xmm9
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_decal_x_and_y_sse2_lowp
.
globl
_sk_decal_x_and_y_sse2_lowp
FUNCTION
(
_sk_decal_x_and_y_sse2_lowp
)
_sk_decal_x_and_y_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
69
15
87
210
/
/
xorps
%
xmm10
%
xmm10
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
68
15
194
193
2
/
/
cmpleps
%
xmm1
%
xmm8
.
byte
242
69
15
112
192
232
/
/
pshuflw
0xe8
%
xmm8
%
xmm8
.
byte
243
69
15
112
192
232
/
/
pshufhw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
232
/
/
pshufd
0xe8
%
xmm8
%
xmm8
.
byte
69
15
87
201
/
/
xorps
%
xmm9
%
xmm9
.
byte
68
15
194
200
2
/
/
cmpleps
%
xmm0
%
xmm9
.
byte
242
69
15
112
201
232
/
/
pshuflw
0xe8
%
xmm9
%
xmm9
.
byte
243
69
15
112
201
232
/
/
pshufhw
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
112
201
232
/
/
pshufd
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
108
200
/
/
punpcklqdq
%
xmm8
%
xmm9
.
byte
102
65
15
113
241
15
/
/
psllw
0xf
%
xmm9
.
byte
102
65
15
113
225
15
/
/
psraw
0xf
%
xmm9
.
byte
243
68
15
16
88
64
/
/
movss
0x40
(
%
rax
)
%
xmm11
.
byte
243
68
15
16
64
68
/
/
movss
0x44
(
%
rax
)
%
xmm8
.
byte
69
15
198
219
0
/
/
shufps
0x0
%
xmm11
%
xmm11
.
byte
68
15
40
225
/
/
movaps
%
xmm1
%
xmm12
.
byte
69
15
194
227
1
/
/
cmpltps
%
xmm11
%
xmm12
.
byte
242
69
15
112
228
232
/
/
pshuflw
0xe8
%
xmm12
%
xmm12
.
byte
243
69
15
112
228
232
/
/
pshufhw
0xe8
%
xmm12
%
xmm12
.
byte
102
69
15
112
228
232
/
/
pshufd
0xe8
%
xmm12
%
xmm12
.
byte
68
15
40
232
/
/
movaps
%
xmm0
%
xmm13
.
byte
69
15
194
235
1
/
/
cmpltps
%
xmm11
%
xmm13
.
byte
242
69
15
112
221
232
/
/
pshuflw
0xe8
%
xmm13
%
xmm11
.
byte
243
69
15
112
219
232
/
/
pshufhw
0xe8
%
xmm11
%
xmm11
.
byte
102
69
15
112
219
232
/
/
pshufd
0xe8
%
xmm11
%
xmm11
.
byte
102
69
15
108
220
/
/
punpcklqdq
%
xmm12
%
xmm11
.
byte
102
65
15
113
243
15
/
/
psllw
0xf
%
xmm11
.
byte
102
65
15
113
227
15
/
/
psraw
0xf
%
xmm11
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
68
15
194
227
2
/
/
cmpleps
%
xmm3
%
xmm12
.
byte
242
69
15
112
228
232
/
/
pshuflw
0xe8
%
xmm12
%
xmm12
.
byte
243
69
15
112
228
232
/
/
pshufhw
0xe8
%
xmm12
%
xmm12
.
byte
102
69
15
112
228
232
/
/
pshufd
0xe8
%
xmm12
%
xmm12
.
byte
68
15
194
210
2
/
/
cmpleps
%
xmm2
%
xmm10
.
byte
242
69
15
112
210
232
/
/
pshuflw
0xe8
%
xmm10
%
xmm10
.
byte
243
69
15
112
210
232
/
/
pshufhw
0xe8
%
xmm10
%
xmm10
.
byte
102
69
15
112
210
232
/
/
pshufd
0xe8
%
xmm10
%
xmm10
.
byte
102
69
15
108
212
/
/
punpcklqdq
%
xmm12
%
xmm10
.
byte
102
65
15
113
242
15
/
/
psllw
0xf
%
xmm10
.
byte
102
65
15
113
226
15
/
/
psraw
0xf
%
xmm10
.
byte
102
69
15
219
209
/
/
pand
%
xmm9
%
xmm10
.
byte
102
69
15
219
211
/
/
pand
%
xmm11
%
xmm10
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
69
15
194
200
1
/
/
cmpltps
%
xmm8
%
xmm9
.
byte
242
69
15
112
201
232
/
/
pshuflw
0xe8
%
xmm9
%
xmm9
.
byte
243
69
15
112
201
232
/
/
pshufhw
0xe8
%
xmm9
%
xmm9
.
byte
102
69
15
112
201
232
/
/
pshufd
0xe8
%
xmm9
%
xmm9
.
byte
68
15
40
218
/
/
movaps
%
xmm2
%
xmm11
.
byte
69
15
194
216
1
/
/
cmpltps
%
xmm8
%
xmm11
.
byte
242
69
15
112
195
232
/
/
pshuflw
0xe8
%
xmm11
%
xmm8
.
byte
243
69
15
112
192
232
/
/
pshufhw
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
112
192
232
/
/
pshufd
0xe8
%
xmm8
%
xmm8
.
byte
102
69
15
108
193
/
/
punpcklqdq
%
xmm9
%
xmm8
.
byte
102
65
15
113
240
15
/
/
psllw
0xf
%
xmm8
.
byte
102
65
15
113
224
15
/
/
psraw
0xf
%
xmm8
.
byte
102
69
15
219
194
/
/
pand
%
xmm10
%
xmm8
.
byte
243
68
15
127
0
/
/
movdqu
%
xmm8
(
%
rax
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_check_decal_mask_sse2_lowp
.
globl
_sk_check_decal_mask_sse2_lowp
FUNCTION
(
_sk_check_decal_mask_sse2_lowp
)
_sk_check_decal_mask_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
68
15
16
0
/
/
movups
(
%
rax
)
%
xmm8
.
byte
65
15
84
192
/
/
andps
%
xmm8
%
xmm0
.
byte
65
15
84
200
/
/
andps
%
xmm8
%
xmm1
.
byte
65
15
84
208
/
/
andps
%
xmm8
%
xmm2
.
byte
65
15
84
216
/
/
andps
%
xmm8
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_gradient_sse2_lowp
.
globl
_sk_gradient_sse2_lowp
FUNCTION
(
_sk_gradient_sse2_lowp
)
_sk_gradient_sse2_lowp
:
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
15
41
124
36
240
/
/
movaps
%
xmm7
-
0x10
(
%
rsp
)
.
byte
15
41
116
36
224
/
/
movaps
%
xmm6
-
0x20
(
%
rsp
)
.
byte
15
41
108
36
208
/
/
movaps
%
xmm5
-
0x30
(
%
rsp
)
.
byte
15
41
100
36
192
/
/
movaps
%
xmm4
-
0x40
(
%
rsp
)
.
byte
68
15
40
249
/
/
movaps
%
xmm1
%
xmm15
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
139
0
/
/
mov
(
%
rax
)
%
r8
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
73
131
248
2
/
/
cmp
0x2
%
r8
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
114
56
/
/
jb
3b371
<
_sk_gradient_sse2_lowp
+
0x6c
>
.
byte
72
139
88
72
/
/
mov
0x48
(
%
rax
)
%
rbx
.
byte
73
255
200
/
/
dec
%
r8
.
byte
72
131
195
4
/
/
add
0x4
%
rbx
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
243
15
16
35
/
/
movss
(
%
rbx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
194
232
2
/
/
cmpleps
%
xmm0
%
xmm5
.
byte
102
15
250
213
/
/
psubd
%
xmm5
%
xmm2
.
byte
65
15
194
231
2
/
/
cmpleps
%
xmm15
%
xmm4
.
byte
102
15
250
220
/
/
psubd
%
xmm4
%
xmm3
.
byte
72
131
195
4
/
/
add
0x4
%
rbx
.
byte
73
255
200
/
/
dec
%
r8
.
byte
117
219
/
/
jne
3b34c
<
_sk_gradient_sse2_lowp
+
0x47
>
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
73
15
126
225
/
/
movq
%
xmm4
%
r9
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
102
73
15
126
219
/
/
movq
%
xmm3
%
r11
.
byte
69
137
218
/
/
mov
%
r11d
%
r10d
.
byte
73
193
235
32
/
/
shr
0x20
%
r11
.
byte
102
65
15
126
214
/
/
movd
%
xmm2
%
r14d
.
byte
102
68
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm8
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
65
15
126
223
/
/
movd
%
xmm3
%
r15d
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
65
15
126
212
/
/
movd
%
xmm2
%
r12d
.
byte
72
139
88
8
/
/
mov
0x8
(
%
rax
)
%
rbx
.
byte
243
70
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm10
.
byte
243
70
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm9
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
243
66
15
16
60
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm7
.
byte
243
66
15
16
36
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm4
.
byte
243
66
15
16
28
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm3
.
byte
102
69
15
126
197
/
/
movd
%
xmm8
%
r13d
.
byte
243
66
15
16
20
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm2
.
byte
69
15
20
202
/
/
unpcklps
%
xmm10
%
xmm9
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
68
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm9
.
byte
15
20
231
/
/
unpcklps
%
xmm7
%
xmm4
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
15
20
220
/
/
unpcklpd
%
xmm4
%
xmm3
.
byte
72
139
88
16
/
/
mov
0x10
(
%
rax
)
%
rbx
.
byte
243
70
15
16
4
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm8
.
byte
243
70
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm12
.
byte
243
70
15
16
20
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm10
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
243
66
15
16
60
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm7
.
byte
243
66
15
16
36
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm4
.
byte
243
66
15
16
20
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm2
.
byte
243
66
15
16
44
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm5
.
byte
69
15
20
224
/
/
unpcklps
%
xmm8
%
xmm12
.
byte
65
15
20
242
/
/
unpcklps
%
xmm10
%
xmm6
.
byte
102
68
15
20
230
/
/
unpcklpd
%
xmm6
%
xmm12
.
byte
15
20
231
/
/
unpcklps
%
xmm7
%
xmm4
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
102
15
20
212
/
/
unpcklpd
%
xmm4
%
xmm2
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
243
66
15
16
36
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm4
.
byte
243
70
15
16
28
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm11
.
byte
68
15
20
220
/
/
unpcklps
%
xmm4
%
xmm11
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
243
66
15
16
36
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm4
.
byte
243
66
15
16
52
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm6
.
byte
243
70
15
16
52
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm14
.
byte
243
66
15
16
60
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm7
.
byte
102
68
15
20
221
/
/
unpcklpd
%
xmm5
%
xmm11
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
68
15
20
247
/
/
unpcklps
%
xmm7
%
xmm14
.
byte
102
68
15
20
246
/
/
unpcklpd
%
xmm6
%
xmm14
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
243
66
15
16
36
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm4
.
byte
243
70
15
16
20
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm10
.
byte
68
15
20
212
/
/
unpcklps
%
xmm4
%
xmm10
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
68
15
20
213
/
/
unpcklpd
%
xmm5
%
xmm10
.
byte
243
66
15
16
36
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm4
.
byte
243
66
15
16
44
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
243
70
15
16
44
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm13
.
byte
243
66
15
16
36
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm4
.
byte
68
15
20
236
/
/
unpcklps
%
xmm4
%
xmm13
.
byte
102
68
15
20
237
/
/
unpcklpd
%
xmm5
%
xmm13
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
243
66
15
16
36
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm4
.
byte
243
66
15
16
52
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
243
66
15
16
44
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm5
.
byte
243
66
15
16
36
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm4
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
243
66
15
16
52
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm6
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
230
/
/
unpcklps
%
xmm6
%
xmm4
.
byte
243
66
15
16
52
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm6
.
byte
243
66
15
16
60
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
102
15
20
231
/
/
unpcklpd
%
xmm7
%
xmm4
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
243
66
15
16
60
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm7
.
byte
243
66
15
16
52
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm6
.
byte
15
20
247
/
/
unpcklps
%
xmm7
%
xmm6
.
byte
243
66
15
16
12
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm1
.
byte
243
66
15
16
60
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm7
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
41
76
36
144
/
/
movapd
%
xmm1
-
0x70
(
%
rsp
)
.
byte
243
66
15
16
60
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm7
.
byte
243
70
15
16
4
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm8
.
byte
68
15
20
199
/
/
unpcklps
%
xmm7
%
xmm8
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
15
20
247
/
/
unpcklps
%
xmm7
%
xmm6
.
byte
102
68
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm8
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
243
66
15
16
60
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm7
.
byte
243
66
15
16
52
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm6
.
byte
15
20
247
/
/
unpcklps
%
xmm7
%
xmm6
.
byte
243
66
15
16
12
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm1
.
byte
243
66
15
16
60
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm7
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
41
76
36
160
/
/
movapd
%
xmm1
-
0x60
(
%
rsp
)
.
byte
243
66
15
16
60
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm7
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
15
20
247
/
/
unpcklps
%
xmm7
%
xmm6
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
41
76
36
128
/
/
movapd
%
xmm1
-
0x80
(
%
rsp
)
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
66
15
16
52
160
/
/
movss
(
%
rax
%
r12
4
)
%
xmm6
.
byte
243
66
15
16
60
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
243
66
15
16
12
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm1
.
byte
243
66
15
16
52
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm6
.
byte
15
20
206
/
/
unpcklps
%
xmm6
%
xmm1
.
byte
102
15
20
207
/
/
unpcklpd
%
xmm7
%
xmm1
.
byte
102
15
41
76
36
176
/
/
movapd
%
xmm1
-
0x50
(
%
rsp
)
.
byte
243
66
15
16
52
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm6
.
byte
243
66
15
16
12
144
/
/
movss
(
%
rax
%
r10
4
)
%
xmm1
.
byte
15
20
206
/
/
unpcklps
%
xmm6
%
xmm1
.
byte
243
66
15
16
52
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm6
.
byte
243
66
15
16
60
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
102
15
20
207
/
/
unpcklpd
%
xmm7
%
xmm1
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
69
15
89
207
/
/
mulps
%
xmm15
%
xmm9
.
byte
68
15
88
204
/
/
addps
%
xmm4
%
xmm9
.
byte
15
40
53
194
25
0
0
/
/
movaps
0x19c2
(
%
rip
)
%
xmm6
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
15
40
37
196
24
0
0
/
/
movaps
0x18c4
(
%
rip
)
%
xmm4
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
204
/
/
addps
%
xmm4
%
xmm9
.
byte
15
88
220
/
/
addps
%
xmm4
%
xmm3
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
235
232
/
/
pshufd
0xe8
%
xmm3
%
xmm5
.
byte
243
65
15
91
217
/
/
cvttps2dq
%
xmm9
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
108
235
/
/
punpcklqdq
%
xmm3
%
xmm5
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
84
36
144
/
/
addps
-
0x70
(
%
rsp
)
%
xmm2
.
byte
69
15
89
231
/
/
mulps
%
xmm15
%
xmm12
.
byte
69
15
88
224
/
/
addps
%
xmm8
%
xmm12
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
68
15
89
230
/
/
mulps
%
xmm6
%
xmm12
.
byte
68
15
88
228
/
/
addps
%
xmm4
%
xmm12
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
68
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm9
.
byte
243
65
15
91
212
/
/
cvttps2dq
%
xmm12
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
68
15
108
202
/
/
punpcklqdq
%
xmm2
%
xmm9
.
byte
68
15
89
240
/
/
mulps
%
xmm0
%
xmm14
.
byte
68
15
88
116
36
160
/
/
addps
-
0x60
(
%
rsp
)
%
xmm14
.
byte
69
15
89
223
/
/
mulps
%
xmm15
%
xmm11
.
byte
68
15
88
92
36
128
/
/
addps
-
0x80
(
%
rsp
)
%
xmm11
.
byte
68
15
89
246
/
/
mulps
%
xmm6
%
xmm14
.
byte
68
15
89
222
/
/
mulps
%
xmm6
%
xmm11
.
byte
68
15
88
220
/
/
addps
%
xmm4
%
xmm11
.
byte
68
15
88
244
/
/
addps
%
xmm4
%
xmm14
.
byte
243
65
15
91
214
/
/
cvttps2dq
%
xmm14
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
243
65
15
91
219
/
/
cvttps2dq
%
xmm11
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
108
211
/
/
punpcklqdq
%
xmm3
%
xmm2
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
68
15
88
108
36
176
/
/
addps
-
0x50
(
%
rsp
)
%
xmm13
.
byte
69
15
89
215
/
/
mulps
%
xmm15
%
xmm10
.
byte
68
15
88
209
/
/
addps
%
xmm1
%
xmm10
.
byte
68
15
89
238
/
/
mulps
%
xmm6
%
xmm13
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
68
15
88
212
/
/
addps
%
xmm4
%
xmm10
.
byte
68
15
88
236
/
/
addps
%
xmm4
%
xmm13
.
byte
243
65
15
91
197
/
/
cvttps2dq
%
xmm13
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
216
232
/
/
pshufd
0xe8
%
xmm0
%
xmm3
.
byte
243
65
15
91
194
/
/
cvttps2dq
%
xmm10
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
15
40
100
36
192
/
/
movaps
-
0x40
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
208
/
/
movaps
-
0x30
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
224
/
/
movaps
-
0x20
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm7
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_gradient_sse2_lowp
.
globl
_sk_evenly_spaced_gradient_sse2_lowp
FUNCTION
(
_sk_evenly_spaced_gradient_sse2_lowp
)
_sk_evenly_spaced_gradient_sse2_lowp
:
.
byte
85
/
/
push
%
rbp
.
byte
65
87
/
/
push
%
r15
.
byte
65
86
/
/
push
%
r14
.
byte
65
85
/
/
push
%
r13
.
byte
65
84
/
/
push
%
r12
.
byte
83
/
/
push
%
rbx
.
byte
80
/
/
push
%
rax
.
byte
15
41
124
36
240
/
/
movaps
%
xmm7
-
0x10
(
%
rsp
)
.
byte
15
41
116
36
224
/
/
movaps
%
xmm6
-
0x20
(
%
rsp
)
.
byte
15
41
108
36
208
/
/
movaps
%
xmm5
-
0x30
(
%
rsp
)
.
byte
15
41
100
36
192
/
/
movaps
%
xmm4
-
0x40
(
%
rsp
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
72
139
24
/
/
mov
(
%
rax
)
%
rbx
.
byte
72
139
104
8
/
/
mov
0x8
(
%
rax
)
%
rbp
.
byte
72
255
203
/
/
dec
%
rbx
.
byte
120
7
/
/
js
3b7b5
<
_sk_evenly_spaced_gradient_sse2_lowp
+
0x34
>
.
byte
243
72
15
42
211
/
/
cvtsi2ss
%
rbx
%
xmm2
.
byte
235
21
/
/
jmp
3b7ca
<
_sk_evenly_spaced_gradient_sse2_lowp
+
0x49
>
.
byte
73
137
216
/
/
mov
%
rbx
%
r8
.
byte
73
209
232
/
/
shr
%
r8
.
byte
131
227
1
/
/
and
0x1
%
ebx
.
byte
76
9
195
/
/
or
%
r8
%
rbx
.
byte
243
72
15
42
211
/
/
cvtsi2ss
%
rbx
%
xmm2
.
byte
243
15
88
210
/
/
addss
%
xmm2
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
41
68
36
176
/
/
movaps
%
xmm0
-
0x50
(
%
rsp
)
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
243
15
91
218
/
/
cvttps2dq
%
xmm2
%
xmm3
.
byte
15
41
76
36
160
/
/
movaps
%
xmm1
-
0x60
(
%
rsp
)
.
byte
102
15
112
211
78
/
/
pshufd
0x4e
%
xmm3
%
xmm2
.
byte
102
73
15
126
214
/
/
movq
%
xmm2
%
r14
.
byte
69
137
242
/
/
mov
%
r14d
%
r10d
.
byte
73
193
238
32
/
/
shr
0x20
%
r14
.
byte
102
73
15
126
221
/
/
movq
%
xmm3
%
r13
.
byte
69
137
239
/
/
mov
%
r13d
%
r15d
.
byte
73
193
237
32
/
/
shr
0x20
%
r13
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
73
15
126
201
/
/
movq
%
xmm1
%
r9
.
byte
69
137
200
/
/
mov
%
r9d
%
r8d
.
byte
73
193
233
32
/
/
shr
0x20
%
r9
.
byte
102
73
15
126
196
/
/
movq
%
xmm0
%
r12
.
byte
69
137
227
/
/
mov
%
r12d
%
r11d
.
byte
73
193
236
32
/
/
shr
0x20
%
r12
.
byte
243
66
15
16
68
165
0
/
/
movss
0x0
(
%
rbp
%
r12
4
)
%
xmm0
.
byte
243
70
15
16
68
157
0
/
/
movss
0x0
(
%
rbp
%
r11
4
)
%
xmm8
.
byte
243
66
15
16
76
141
0
/
/
movss
0x0
(
%
rbp
%
r9
4
)
%
xmm1
.
byte
243
66
15
16
84
133
0
/
/
movss
0x0
(
%
rbp
%
r8
4
)
%
xmm2
.
byte
243
66
15
16
100
173
0
/
/
movss
0x0
(
%
rbp
%
r13
4
)
%
xmm4
.
byte
243
66
15
16
92
189
0
/
/
movss
0x0
(
%
rbp
%
r15
4
)
%
xmm3
.
byte
243
66
15
16
108
181
0
/
/
movss
0x0
(
%
rbp
%
r14
4
)
%
xmm5
.
byte
243
66
15
16
116
149
0
/
/
movss
0x0
(
%
rbp
%
r10
4
)
%
xmm6
.
byte
68
15
20
192
/
/
unpcklps
%
xmm0
%
xmm8
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
68
15
20
194
/
/
unpcklpd
%
xmm2
%
xmm8
.
byte
15
20
220
/
/
unpcklps
%
xmm4
%
xmm3
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
72
139
88
16
/
/
mov
0x10
(
%
rax
)
%
rbx
.
byte
243
66
15
16
4
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm0
.
byte
243
70
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm9
.
byte
243
66
15
16
12
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm1
.
byte
243
66
15
16
36
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm4
.
byte
243
66
15
16
44
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm5
.
byte
243
66
15
16
20
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm2
.
byte
243
66
15
16
52
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm6
.
byte
243
66
15
16
60
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm7
.
byte
68
15
20
200
/
/
unpcklps
%
xmm0
%
xmm9
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
102
68
15
20
204
/
/
unpcklpd
%
xmm4
%
xmm9
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
102
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm2
.
byte
72
139
88
24
/
/
mov
0x18
(
%
rax
)
%
rbx
.
byte
243
66
15
16
4
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm0
.
byte
243
70
15
16
28
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm11
.
byte
68
15
20
216
/
/
unpcklps
%
xmm0
%
xmm11
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
66
15
16
4
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm0
.
byte
243
70
15
16
44
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm13
.
byte
243
66
15
16
36
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm4
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
102
68
15
20
217
/
/
unpcklpd
%
xmm1
%
xmm11
.
byte
68
15
20
232
/
/
unpcklps
%
xmm0
%
xmm13
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
68
15
20
237
/
/
unpcklpd
%
xmm5
%
xmm13
.
byte
72
139
88
32
/
/
mov
0x20
(
%
rax
)
%
rbx
.
byte
243
66
15
16
4
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm0
.
byte
243
70
15
16
20
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm10
.
byte
68
15
20
208
/
/
unpcklps
%
xmm0
%
xmm10
.
byte
243
66
15
16
4
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm0
.
byte
243
66
15
16
12
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
68
15
20
209
/
/
unpcklpd
%
xmm1
%
xmm10
.
byte
243
66
15
16
4
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm0
.
byte
243
70
15
16
36
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm12
.
byte
68
15
20
224
/
/
unpcklps
%
xmm0
%
xmm12
.
byte
243
66
15
16
4
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm0
.
byte
243
66
15
16
12
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
68
15
20
225
/
/
unpcklpd
%
xmm1
%
xmm12
.
byte
72
139
88
40
/
/
mov
0x28
(
%
rax
)
%
rbx
.
byte
243
66
15
16
12
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm1
.
byte
243
66
15
16
4
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm0
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
243
66
15
16
12
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm1
.
byte
243
66
15
16
36
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
102
15
20
196
/
/
unpcklpd
%
xmm4
%
xmm0
.
byte
243
66
15
16
36
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm4
.
byte
243
66
15
16
12
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm1
.
byte
15
20
204
/
/
unpcklps
%
xmm4
%
xmm1
.
byte
243
66
15
16
36
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm4
.
byte
243
66
15
16
44
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
72
139
88
48
/
/
mov
0x30
(
%
rax
)
%
rbx
.
byte
243
66
15
16
36
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm4
.
byte
243
70
15
16
52
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm14
.
byte
68
15
20
244
/
/
unpcklps
%
xmm4
%
xmm14
.
byte
243
66
15
16
36
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm4
.
byte
243
66
15
16
44
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
68
15
20
245
/
/
unpcklpd
%
xmm5
%
xmm14
.
byte
243
66
15
16
44
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm5
.
byte
243
66
15
16
36
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm4
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
243
66
15
16
44
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm5
.
byte
243
66
15
16
60
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
102
15
20
231
/
/
unpcklpd
%
xmm7
%
xmm4
.
byte
72
139
88
56
/
/
mov
0x38
(
%
rax
)
%
rbx
.
byte
243
66
15
16
44
171
/
/
movss
(
%
rbx
%
r13
4
)
%
xmm5
.
byte
243
66
15
16
52
187
/
/
movss
(
%
rbx
%
r15
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
243
66
15
16
44
179
/
/
movss
(
%
rbx
%
r14
4
)
%
xmm5
.
byte
243
66
15
16
60
147
/
/
movss
(
%
rbx
%
r10
4
)
%
xmm7
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
102
15
20
247
/
/
unpcklpd
%
xmm7
%
xmm6
.
byte
102
15
41
116
36
128
/
/
movapd
%
xmm6
-
0x80
(
%
rsp
)
.
byte
243
66
15
16
60
163
/
/
movss
(
%
rbx
%
r12
4
)
%
xmm7
.
byte
243
66
15
16
44
155
/
/
movss
(
%
rbx
%
r11
4
)
%
xmm5
.
byte
15
20
239
/
/
unpcklps
%
xmm7
%
xmm5
.
byte
243
66
15
16
60
139
/
/
movss
(
%
rbx
%
r9
4
)
%
xmm7
.
byte
243
66
15
16
52
131
/
/
movss
(
%
rbx
%
r8
4
)
%
xmm6
.
byte
15
20
247
/
/
unpcklps
%
xmm7
%
xmm6
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
72
139
64
64
/
/
mov
0x40
(
%
rax
)
%
rax
.
byte
243
66
15
16
52
168
/
/
movss
(
%
rax
%
r13
4
)
%
xmm6
.
byte
243
66
15
16
60
184
/
/
movss
(
%
rax
%
r15
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
243
70
15
16
60
176
/
/
movss
(
%
rax
%
r14
4
)
%
xmm15
.
byte
243
66
15
16
52
144
/
/
movss
(
%
rax
%
r10
4
)
%
xmm6
.
byte
65
15
20
247
/
/
unpcklps
%
xmm15
%
xmm6
.
byte
102
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm7
.
byte
102
15
41
124
36
144
/
/
movapd
%
xmm7
-
0x70
(
%
rsp
)
.
byte
243
66
15
16
52
160
/
/
movss
(
%
rax
%
r12
4
)
%
xmm6
.
byte
243
66
15
16
60
152
/
/
movss
(
%
rax
%
r11
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
243
66
15
16
52
136
/
/
movss
(
%
rax
%
r9
4
)
%
xmm6
.
byte
243
70
15
16
60
128
/
/
movss
(
%
rax
%
r8
4
)
%
xmm15
.
byte
68
15
20
254
/
/
unpcklps
%
xmm6
%
xmm15
.
byte
102
65
15
20
255
/
/
unpcklpd
%
xmm15
%
xmm7
.
byte
68
15
40
124
36
160
/
/
movaps
-
0x60
(
%
rsp
)
%
xmm15
.
byte
65
15
89
223
/
/
mulps
%
xmm15
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
40
68
36
176
/
/
movaps
-
0x50
(
%
rsp
)
%
xmm0
.
byte
68
15
89
192
/
/
mulps
%
xmm0
%
xmm8
.
byte
68
15
88
193
/
/
addps
%
xmm1
%
xmm8
.
byte
15
40
13
78
21
0
0
/
/
movaps
0x154e
(
%
rip
)
%
xmm1
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
68
15
89
193
/
/
mulps
%
xmm1
%
xmm8
.
byte
15
40
13
80
20
0
0
/
/
movaps
0x1450
(
%
rip
)
%
xmm1
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
88
193
/
/
addps
%
xmm1
%
xmm8
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
243
65
15
91
240
/
/
cvttps2dq
%
xmm8
%
xmm6
.
byte
242
15
112
246
232
/
/
pshuflw
0xe8
%
xmm6
%
xmm6
.
byte
243
15
112
246
232
/
/
pshufhw
0xe8
%
xmm6
%
xmm6
.
byte
102
68
15
112
198
232
/
/
pshufd
0xe8
%
xmm6
%
xmm8
.
byte
102
68
15
108
195
/
/
punpcklqdq
%
xmm3
%
xmm8
.
byte
65
15
89
215
/
/
mulps
%
xmm15
%
xmm2
.
byte
65
15
88
214
/
/
addps
%
xmm14
%
xmm2
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
68
15
89
206
/
/
mulps
%
xmm6
%
xmm9
.
byte
68
15
88
204
/
/
addps
%
xmm4
%
xmm9
.
byte
15
40
5
242
20
0
0
/
/
movaps
0x14f2
(
%
rip
)
%
xmm0
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
243
65
15
91
217
/
/
cvttps2dq
%
xmm9
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
68
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm9
.
byte
102
68
15
108
202
/
/
punpcklqdq
%
xmm2
%
xmm9
.
byte
69
15
89
239
/
/
mulps
%
xmm15
%
xmm13
.
byte
68
15
88
108
36
128
/
/
addps
-
0x80
(
%
rsp
)
%
xmm13
.
byte
68
15
89
222
/
/
mulps
%
xmm6
%
xmm11
.
byte
68
15
88
221
/
/
addps
%
xmm5
%
xmm11
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
68
15
89
216
/
/
mulps
%
xmm0
%
xmm11
.
byte
68
15
88
217
/
/
addps
%
xmm1
%
xmm11
.
byte
68
15
88
233
/
/
addps
%
xmm1
%
xmm13
.
byte
243
65
15
91
213
/
/
cvttps2dq
%
xmm13
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
218
232
/
/
pshufd
0xe8
%
xmm2
%
xmm3
.
byte
243
65
15
91
211
/
/
cvttps2dq
%
xmm11
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
108
211
/
/
punpcklqdq
%
xmm3
%
xmm2
.
byte
69
15
89
231
/
/
mulps
%
xmm15
%
xmm12
.
byte
68
15
88
100
36
144
/
/
addps
-
0x70
(
%
rsp
)
%
xmm12
.
byte
68
15
89
214
/
/
mulps
%
xmm6
%
xmm10
.
byte
68
15
88
215
/
/
addps
%
xmm7
%
xmm10
.
byte
68
15
89
224
/
/
mulps
%
xmm0
%
xmm12
.
byte
68
15
89
208
/
/
mulps
%
xmm0
%
xmm10
.
byte
68
15
88
209
/
/
addps
%
xmm1
%
xmm10
.
byte
68
15
88
225
/
/
addps
%
xmm1
%
xmm12
.
byte
243
65
15
91
196
/
/
cvttps2dq
%
xmm12
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
243
65
15
91
202
/
/
cvttps2dq
%
xmm10
%
xmm1
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
15
40
100
36
192
/
/
movaps
-
0x40
(
%
rsp
)
%
xmm4
.
byte
15
40
108
36
208
/
/
movaps
-
0x30
(
%
rsp
)
%
xmm5
.
byte
15
40
116
36
224
/
/
movaps
-
0x20
(
%
rsp
)
%
xmm6
.
byte
15
40
124
36
240
/
/
movaps
-
0x10
(
%
rsp
)
%
xmm7
.
byte
72
131
196
8
/
/
add
0x8
%
rsp
.
byte
91
/
/
pop
%
rbx
.
byte
65
92
/
/
pop
%
r12
.
byte
65
93
/
/
pop
%
r13
.
byte
65
94
/
/
pop
%
r14
.
byte
65
95
/
/
pop
%
r15
.
byte
93
/
/
pop
%
rbp
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
.
globl
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
)
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
243
15
16
24
/
/
movss
(
%
rax
)
%
xmm3
.
byte
243
68
15
16
72
4
/
/
movss
0x4
(
%
rax
)
%
xmm9
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
64
16
/
/
movss
0x10
(
%
rax
)
%
xmm8
.
byte
69
15
198
192
0
/
/
shufps
0x0
%
xmm8
%
xmm8
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
216
/
/
addps
%
xmm8
%
xmm3
.
byte
65
15
88
208
/
/
addps
%
xmm8
%
xmm2
.
byte
68
15
40
21
183
19
0
0
/
/
movaps
0x13b7
(
%
rip
)
%
xmm10
#
3cff0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xda4
>
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
68
15
40
29
183
18
0
0
/
/
movaps
0x12b7
(
%
rip
)
%
xmm11
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
68
15
112
195
232
/
/
pshufd
0xe8
%
xmm3
%
xmm8
.
byte
102
68
15
108
194
/
/
punpcklqdq
%
xmm2
%
xmm8
.
byte
69
15
198
201
0
/
/
shufps
0x0
%
xmm9
%
xmm9
.
byte
243
15
16
80
20
/
/
movss
0x14
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
65
15
89
217
/
/
mulps
%
xmm9
%
xmm3
.
byte
68
15
89
200
/
/
mulps
%
xmm0
%
xmm9
.
byte
68
15
88
202
/
/
addps
%
xmm2
%
xmm9
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
69
15
89
202
/
/
mulps
%
xmm10
%
xmm9
.
byte
69
15
88
203
/
/
addps
%
xmm11
%
xmm9
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
243
15
91
211
/
/
cvttps2dq
%
xmm3
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
243
65
15
91
217
/
/
cvttps2dq
%
xmm9
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
68
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm9
.
byte
102
68
15
108
202
/
/
punpcklqdq
%
xmm2
%
xmm9
.
byte
243
15
16
80
8
/
/
movss
0x8
(
%
rax
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
68
15
16
96
24
/
/
movss
0x18
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
65
15
88
212
/
/
addps
%
xmm12
%
xmm2
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
65
15
89
210
/
/
mulps
%
xmm10
%
xmm2
.
byte
65
15
88
211
/
/
addps
%
xmm11
%
xmm2
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
108
211
/
/
punpcklqdq
%
xmm3
%
xmm2
.
byte
243
15
16
88
12
/
/
movss
0xc
(
%
rax
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
68
15
16
96
28
/
/
movss
0x1c
(
%
rax
)
%
xmm12
.
byte
69
15
198
228
0
/
/
shufps
0x0
%
xmm12
%
xmm12
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
65
15
88
220
/
/
addps
%
xmm12
%
xmm3
.
byte
65
15
88
204
/
/
addps
%
xmm12
%
xmm1
.
byte
65
15
89
202
/
/
mulps
%
xmm10
%
xmm1
.
byte
65
15
89
218
/
/
mulps
%
xmm10
%
xmm3
.
byte
65
15
88
219
/
/
addps
%
xmm11
%
xmm3
.
byte
65
15
88
203
/
/
addps
%
xmm11
%
xmm1
.
byte
243
15
91
193
/
/
cvttps2dq
%
xmm1
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
243
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm1
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
192
/
/
movdqa
%
xmm8
%
xmm0
.
byte
102
65
15
111
201
/
/
movdqa
%
xmm9
%
xmm1
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_unit_angle_sse2_lowp
.
globl
_sk_xy_to_unit_angle_sse2_lowp
FUNCTION
(
_sk_xy_to_unit_angle_sse2_lowp
)
_sk_xy_to_unit_angle_sse2_lowp
:
.
byte
15
41
124
36
232
/
/
movaps
%
xmm7
-
0x18
(
%
rsp
)
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
68
15
40
37
171
22
0
0
/
/
movaps
0x16ab
(
%
rip
)
%
xmm12
#
3d470
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1224
>
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
69
15
84
196
/
/
andps
%
xmm12
%
xmm8
.
byte
68
15
40
217
/
/
movaps
%
xmm1
%
xmm11
.
byte
69
15
84
220
/
/
andps
%
xmm12
%
xmm11
.
byte
68
15
40
236
/
/
movaps
%
xmm4
%
xmm13
.
byte
69
15
84
236
/
/
andps
%
xmm12
%
xmm13
.
byte
68
15
84
227
/
/
andps
%
xmm3
%
xmm12
.
byte
69
15
40
211
/
/
movaps
%
xmm11
%
xmm10
.
byte
69
15
194
212
1
/
/
cmpltps
%
xmm12
%
xmm10
.
byte
69
15
40
200
/
/
movaps
%
xmm8
%
xmm9
.
byte
69
15
194
205
1
/
/
cmpltps
%
xmm13
%
xmm9
.
byte
69
15
40
241
/
/
movaps
%
xmm9
%
xmm14
.
byte
69
15
85
245
/
/
andnps
%
xmm13
%
xmm14
.
byte
69
15
40
249
/
/
movaps
%
xmm9
%
xmm15
.
byte
69
15
85
248
/
/
andnps
%
xmm8
%
xmm15
.
byte
69
15
84
193
/
/
andps
%
xmm9
%
xmm8
.
byte
69
15
86
198
/
/
orps
%
xmm14
%
xmm8
.
byte
69
15
40
242
/
/
movaps
%
xmm10
%
xmm14
.
byte
69
15
85
244
/
/
andnps
%
xmm12
%
xmm14
.
byte
65
15
40
194
/
/
movaps
%
xmm10
%
xmm0
.
byte
65
15
85
195
/
/
andnps
%
xmm11
%
xmm0
.
byte
69
15
84
218
/
/
andps
%
xmm10
%
xmm11
.
byte
69
15
86
222
/
/
orps
%
xmm14
%
xmm11
.
byte
69
15
84
233
/
/
andps
%
xmm9
%
xmm13
.
byte
69
15
86
239
/
/
orps
%
xmm15
%
xmm13
.
byte
69
15
94
197
/
/
divps
%
xmm13
%
xmm8
.
byte
69
15
84
226
/
/
andps
%
xmm10
%
xmm12
.
byte
68
15
86
224
/
/
orps
%
xmm0
%
xmm12
.
byte
69
15
94
220
/
/
divps
%
xmm12
%
xmm11
.
byte
69
15
40
240
/
/
movaps
%
xmm8
%
xmm14
.
byte
69
15
89
246
/
/
mulps
%
xmm14
%
xmm14
.
byte
65
15
40
195
/
/
movaps
%
xmm11
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
68
15
40
37
142
22
0
0
/
/
movaps
0x168e
(
%
rip
)
%
xmm12
#
3d4e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1294
>
.
byte
68
15
40
232
/
/
movaps
%
xmm0
%
xmm13
.
byte
69
15
89
236
/
/
mulps
%
xmm12
%
xmm13
.
byte
69
15
89
230
/
/
mulps
%
xmm14
%
xmm12
.
byte
68
15
40
61
138
22
0
0
/
/
movaps
0x168a
(
%
rip
)
%
xmm15
#
3d4f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12a4
>
.
byte
69
15
88
231
/
/
addps
%
xmm15
%
xmm12
.
byte
69
15
88
239
/
/
addps
%
xmm15
%
xmm13
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
69
15
89
230
/
/
mulps
%
xmm14
%
xmm12
.
byte
68
15
40
61
130
22
0
0
/
/
movaps
0x1682
(
%
rip
)
%
xmm15
#
3d500
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12b4
>
.
byte
69
15
88
231
/
/
addps
%
xmm15
%
xmm12
.
byte
69
15
88
239
/
/
addps
%
xmm15
%
xmm13
.
byte
68
15
89
232
/
/
mulps
%
xmm0
%
xmm13
.
byte
69
15
89
230
/
/
mulps
%
xmm14
%
xmm12
.
byte
15
40
5
123
22
0
0
/
/
movaps
0x167b
(
%
rip
)
%
xmm0
#
3d510
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12c4
>
.
byte
68
15
88
224
/
/
addps
%
xmm0
%
xmm12
.
byte
68
15
88
232
/
/
addps
%
xmm0
%
xmm13
.
byte
69
15
89
235
/
/
mulps
%
xmm11
%
xmm13
.
byte
69
15
89
224
/
/
mulps
%
xmm8
%
xmm12
.
byte
15
40
5
116
22
0
0
/
/
movaps
0x1674
(
%
rip
)
%
xmm0
#
3d520
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12d4
>
.
byte
68
15
40
192
/
/
movaps
%
xmm0
%
xmm8
.
byte
65
15
92
197
/
/
subps
%
xmm13
%
xmm0
.
byte
65
15
84
194
/
/
andps
%
xmm10
%
xmm0
.
byte
69
15
85
213
/
/
andnps
%
xmm13
%
xmm10
.
byte
69
15
92
196
/
/
subps
%
xmm12
%
xmm8
.
byte
68
15
86
208
/
/
orps
%
xmm0
%
xmm10
.
byte
69
15
84
193
/
/
andps
%
xmm9
%
xmm8
.
byte
69
15
85
204
/
/
andnps
%
xmm12
%
xmm9
.
byte
69
15
86
200
/
/
orps
%
xmm8
%
xmm9
.
byte
69
15
87
192
/
/
xorps
%
xmm8
%
xmm8
.
byte
65
15
194
200
1
/
/
cmpltps
%
xmm8
%
xmm1
.
byte
15
40
5
32
16
0
0
/
/
movaps
0x1020
(
%
rip
)
%
xmm0
#
3cf00
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcb4
>
.
byte
68
15
40
216
/
/
movaps
%
xmm0
%
xmm11
.
byte
65
15
92
194
/
/
subps
%
xmm10
%
xmm0
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
65
15
85
202
/
/
andnps
%
xmm10
%
xmm1
.
byte
65
15
194
208
1
/
/
cmpltps
%
xmm8
%
xmm2
.
byte
69
15
92
217
/
/
subps
%
xmm9
%
xmm11
.
byte
15
86
200
/
/
orps
%
xmm0
%
xmm1
.
byte
68
15
84
218
/
/
andps
%
xmm2
%
xmm11
.
byte
65
15
85
209
/
/
andnps
%
xmm9
%
xmm2
.
byte
65
15
86
211
/
/
orps
%
xmm11
%
xmm2
.
byte
68
15
40
219
/
/
movaps
%
xmm3
%
xmm11
.
byte
69
15
194
216
1
/
/
cmpltps
%
xmm8
%
xmm11
.
byte
68
15
40
13
248
15
0
0
/
/
movaps
0xff8
(
%
rip
)
%
xmm9
#
3cf10
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xcc4
>
.
byte
69
15
40
209
/
/
movaps
%
xmm9
%
xmm10
.
byte
68
15
92
201
/
/
subps
%
xmm1
%
xmm9
.
byte
69
15
84
203
/
/
andps
%
xmm11
%
xmm9
.
byte
68
15
85
217
/
/
andnps
%
xmm1
%
xmm11
.
byte
69
15
86
217
/
/
orps
%
xmm9
%
xmm11
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
65
15
194
192
1
/
/
cmpltps
%
xmm8
%
xmm0
.
byte
68
15
92
210
/
/
subps
%
xmm2
%
xmm10
.
byte
68
15
84
208
/
/
andps
%
xmm0
%
xmm10
.
byte
15
85
194
/
/
andnps
%
xmm2
%
xmm0
.
byte
65
15
86
194
/
/
orps
%
xmm10
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
65
15
194
200
7
/
/
cmpordps
%
xmm8
%
xmm1
.
byte
69
15
194
195
7
/
/
cmpordps
%
xmm11
%
xmm8
.
byte
69
15
84
195
/
/
andps
%
xmm11
%
xmm8
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
65
15
40
192
/
/
movaps
%
xmm8
%
xmm0
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
15
40
124
36
232
/
/
movaps
-
0x18
(
%
rsp
)
%
xmm7
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_xy_to_radius_sse2_lowp
.
globl
_sk_xy_to_radius_sse2_lowp
FUNCTION
(
_sk_xy_to_radius_sse2_lowp
)
_sk_xy_to_radius_sse2_lowp
:
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
89
201
/
/
mulps
%
xmm1
%
xmm1
.
byte
68
15
40
194
/
/
movaps
%
xmm2
%
xmm8
.
byte
69
15
89
192
/
/
mulps
%
xmm8
%
xmm8
.
byte
68
15
88
192
/
/
addps
%
xmm0
%
xmm8
.
byte
68
15
40
203
/
/
movaps
%
xmm3
%
xmm9
.
byte
69
15
89
201
/
/
mulps
%
xmm9
%
xmm9
.
byte
68
15
88
201
/
/
addps
%
xmm1
%
xmm9
.
byte
65
15
81
192
/
/
sqrtps
%
xmm8
%
xmm0
.
byte
65
15
81
201
/
/
sqrtps
%
xmm9
%
xmm1
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
HIDDEN
_sk_srcover_rgba_8888_sse2_lowp
.
globl
_sk_srcover_rgba_8888_sse2_lowp
FUNCTION
(
_sk_srcover_rgba_8888_sse2_lowp
)
_sk_srcover_rgba_8888_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
65
137
250
/
/
mov
%
edi
%
r10d
.
byte
65
128
226
7
/
/
and
0x7
%
r10b
.
byte
65
254
202
/
/
dec
%
r10b
.
byte
69
15
182
202
/
/
movzbl
%
r10b
%
r9d
.
byte
65
128
249
6
/
/
cmp
0x6
%
r9b
.
byte
119
38
/
/
ja
3bfe8
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x4b
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
76
141
29
70
2
0
0
/
/
lea
0x246
(
%
rip
)
%
r11
#
3c214
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x277
>
.
byte
75
99
4
139
/
/
movslq
(
%
r11
%
r9
4
)
%
rax
.
byte
76
1
216
/
/
add
%
r11
%
rax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
243
65
15
16
60
144
/
/
movss
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
102
/
/
jmp
3c04e
<
_sk_srcover_rgba_8888_sse2_lowp
+
0xb1
>
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
87
/
/
jmp
3c04e
<
_sk_srcover_rgba_8888_sse2_lowp
+
0xb1
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
69
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
65
15
40
248
/
/
movapd
%
xmm8
%
xmm7
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
235
52
/
/
jmp
3c04e
<
_sk_srcover_rgba_8888_sse2_lowp
+
0xb1
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
20
/
/
movss
0x14
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
65
15
198
224
0
/
/
shufps
0x0
%
xmm8
%
xmm4
.
byte
65
15
198
224
226
/
/
shufps
0xe2
%
xmm8
%
xmm4
.
byte
68
15
40
196
/
/
movaps
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
16
/
/
movss
0x10
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
243
68
15
16
196
/
/
movss
%
xmm4
%
xmm8
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
15
40
239
/
/
movapd
%
xmm7
%
xmm5
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
114
229
16
/
/
psrad
0x10
%
xmm5
.
byte
102
65
15
111
224
/
/
movdqa
%
xmm8
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
236
/
/
packssdw
%
xmm4
%
xmm5
.
byte
102
68
15
111
37
56
14
0
0
/
/
movdqa
0xe38
(
%
rip
)
%
xmm12
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
65
15
219
228
/
/
pand
%
xmm12
%
xmm4
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
65
15
114
209
16
/
/
psrld
0x10
%
xmm9
.
byte
102
15
40
247
/
/
movapd
%
xmm7
%
xmm6
.
byte
102
15
114
214
16
/
/
psrld
0x10
%
xmm6
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
65
15
107
248
/
/
packssdw
%
xmm8
%
xmm7
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
102
65
15
114
225
16
/
/
psrad
0x10
%
xmm9
.
byte
102
65
15
107
241
/
/
packssdw
%
xmm9
%
xmm6
.
byte
102
65
15
219
244
/
/
pand
%
xmm12
%
xmm6
.
byte
102
69
15
111
196
/
/
movdqa
%
xmm12
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
68
15
111
204
/
/
movdqa
%
xmm4
%
xmm9
.
byte
102
69
15
213
200
/
/
pmullw
%
xmm8
%
xmm9
.
byte
102
69
15
253
204
/
/
paddw
%
xmm12
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
69
15
213
208
/
/
pmullw
%
xmm8
%
xmm10
.
byte
102
69
15
253
212
/
/
paddw
%
xmm12
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
68
15
253
209
/
/
paddw
%
xmm1
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
69
15
213
216
/
/
pmullw
%
xmm8
%
xmm11
.
byte
102
69
15
253
220
/
/
paddw
%
xmm12
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
68
15
253
218
/
/
paddw
%
xmm2
%
xmm11
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
69
15
253
196
/
/
paddw
%
xmm12
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
68
15
253
195
/
/
paddw
%
xmm3
%
xmm8
.
byte
102
65
15
111
210
/
/
movdqa
%
xmm10
%
xmm2
.
byte
102
15
113
242
8
/
/
psllw
0x8
%
xmm2
.
byte
102
65
15
235
209
/
/
por
%
xmm9
%
xmm2
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
97
216
/
/
punpcklwd
%
xmm0
%
xmm3
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
65
15
235
203
/
/
por
%
xmm11
%
xmm1
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
102
68
15
97
225
/
/
punpcklwd
%
xmm1
%
xmm12
.
byte
102
68
15
235
227
/
/
por
%
xmm3
%
xmm12
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
235
194
/
/
por
%
xmm2
%
xmm0
.
byte
65
128
250
6
/
/
cmp
0x6
%
r10b
.
byte
119
24
/
/
ja
3c1b0
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x213
>
.
byte
76
141
21
145
0
0
0
/
/
lea
0x91
(
%
rip
)
%
r10
#
3c230
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x293
>
.
byte
75
99
4
138
/
/
movslq
(
%
r10
%
r9
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
126
36
144
/
/
movd
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
235
73
/
/
jmp
3c1f9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x25c
>
.
byte
243
69
15
127
36
144
/
/
movdqu
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
243
65
15
127
68
144
16
/
/
movdqu
%
xmm0
0x10
(
%
r8
%
rdx
4
)
.
byte
235
58
/
/
jmp
3c1f9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x25c
>
.
byte
102
65
15
112
196
78
/
/
pshufd
0x4e
%
xmm12
%
xmm0
.
byte
102
65
15
126
68
144
8
/
/
movd
%
xmm0
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
36
144
/
/
movq
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
235
37
/
/
jmp
3c1f9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x25c
>
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
65
15
126
76
144
24
/
/
movd
%
xmm1
0x18
(
%
r8
%
rdx
4
)
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
65
15
126
76
144
20
/
/
movd
%
xmm1
0x14
(
%
r8
%
rdx
4
)
.
byte
102
65
15
126
68
144
16
/
/
movd
%
xmm0
0x10
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
36
144
/
/
movdqu
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
15
31
0
/
/
nopl
(
%
rax
)
.
byte
199
/
/
(
bad
)
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
244
/
/
push
%
rsp
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
227
/
/
jmpq
*
%
rbx
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
52
254
/
/
pushq
(
%
rsi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
40
/
/
ljmp
*
(
%
rax
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
19
/
/
callq
*
(
%
rbx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
120
255
/
/
js
3c231
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x294
>
.
byte
255
/
/
(
bad
)
.
byte
255
156
255
255
255
143
255
/
/
lcall
*
-
0x700001
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
255
195
/
/
inc
%
ebx
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
188
255
255
255
176
/
/
mov
0xb0ffffff
%
esp
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
164
/
/
movsb
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
HIDDEN
_sk_srcover_bgra_8888_sse2_lowp
.
globl
_sk_srcover_bgra_8888_sse2_lowp
FUNCTION
(
_sk_srcover_bgra_8888_sse2_lowp
)
_sk_srcover_bgra_8888_sse2_lowp
:
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
76
99
64
8
/
/
movslq
0x8
(
%
rax
)
%
r8
.
byte
76
15
175
193
/
/
imul
%
rcx
%
r8
.
byte
73
193
224
2
/
/
shl
0x2
%
r8
.
byte
76
3
0
/
/
add
(
%
rax
)
%
r8
.
byte
65
137
250
/
/
mov
%
edi
%
r10d
.
byte
65
128
226
7
/
/
and
0x7
%
r10b
.
byte
65
254
202
/
/
dec
%
r10b
.
byte
69
15
182
202
/
/
movzbl
%
r10b
%
r9d
.
byte
65
128
249
6
/
/
cmp
0x6
%
r9b
.
byte
119
38
/
/
ja
3c297
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x4b
>
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
76
141
29
67
2
0
0
/
/
lea
0x243
(
%
rip
)
%
r11
#
3c4c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x274
>
.
byte
75
99
4
139
/
/
movslq
(
%
r11
%
r9
4
)
%
rax
.
byte
76
1
216
/
/
add
%
r11
%
rax
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
243
65
15
16
60
144
/
/
movss
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
69
15
239
192
/
/
pxor
%
xmm8
%
xmm8
.
byte
235
102
/
/
jmp
3c2fd
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb1
>
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
243
69
15
111
68
144
16
/
/
movdqu
0x10
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
235
87
/
/
jmp
3c2fd
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb1
>
.
byte
102
65
15
110
100
144
8
/
/
movd
0x8
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
69
15
18
4
144
/
/
movlpd
(
%
r8
%
rdx
4
)
%
xmm8
.
byte
102
65
15
40
248
/
/
movapd
%
xmm8
%
xmm7
.
byte
102
68
15
111
196
/
/
movdqa
%
xmm4
%
xmm8
.
byte
235
52
/
/
jmp
3c2fd
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb1
>
.
byte
102
65
15
110
100
144
24
/
/
movd
0x18
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
102
68
15
112
196
69
/
/
pshufd
0x45
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
20
/
/
movss
0x14
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
65
15
198
224
0
/
/
shufps
0x0
%
xmm8
%
xmm4
.
byte
65
15
198
224
226
/
/
shufps
0xe2
%
xmm8
%
xmm4
.
byte
68
15
40
196
/
/
movaps
%
xmm4
%
xmm8
.
byte
243
65
15
16
100
144
16
/
/
movss
0x10
(
%
r8
%
rdx
4
)
%
xmm4
.
byte
243
68
15
16
196
/
/
movss
%
xmm4
%
xmm8
.
byte
102
65
15
16
60
144
/
/
movupd
(
%
r8
%
rdx
4
)
%
xmm7
.
byte
102
15
40
239
/
/
movapd
%
xmm7
%
xmm5
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
114
229
16
/
/
psrad
0x10
%
xmm5
.
byte
102
65
15
111
224
/
/
movdqa
%
xmm8
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
236
/
/
packssdw
%
xmm4
%
xmm5
.
byte
102
68
15
111
37
137
11
0
0
/
/
movdqa
0xb89
(
%
rip
)
%
xmm12
#
3ceb0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xc64
>
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
65
15
219
244
/
/
pand
%
xmm12
%
xmm6
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
69
15
111
200
/
/
movdqa
%
xmm8
%
xmm9
.
byte
102
65
15
114
209
16
/
/
psrld
0x10
%
xmm9
.
byte
102
15
40
231
/
/
movapd
%
xmm7
%
xmm4
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
65
15
114
208
24
/
/
psrld
0x18
%
xmm8
.
byte
102
65
15
114
240
16
/
/
pslld
0x10
%
xmm8
.
byte
102
65
15
114
224
16
/
/
psrad
0x10
%
xmm8
.
byte
102
65
15
107
248
/
/
packssdw
%
xmm8
%
xmm7
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
65
15
114
241
16
/
/
pslld
0x10
%
xmm9
.
byte
102
65
15
114
225
16
/
/
psrad
0x10
%
xmm9
.
byte
102
65
15
107
225
/
/
packssdw
%
xmm9
%
xmm4
.
byte
102
65
15
219
228
/
/
pand
%
xmm12
%
xmm4
.
byte
102
69
15
111
196
/
/
movdqa
%
xmm12
%
xmm8
.
byte
102
68
15
249
195
/
/
psubw
%
xmm3
%
xmm8
.
byte
102
68
15
111
204
/
/
movdqa
%
xmm4
%
xmm9
.
byte
102
69
15
213
200
/
/
pmullw
%
xmm8
%
xmm9
.
byte
102
69
15
253
204
/
/
paddw
%
xmm12
%
xmm9
.
byte
102
65
15
113
209
8
/
/
psrlw
0x8
%
xmm9
.
byte
102
68
15
253
200
/
/
paddw
%
xmm0
%
xmm9
.
byte
102
68
15
111
213
/
/
movdqa
%
xmm5
%
xmm10
.
byte
102
69
15
213
208
/
/
pmullw
%
xmm8
%
xmm10
.
byte
102
69
15
253
212
/
/
paddw
%
xmm12
%
xmm10
.
byte
102
65
15
113
210
8
/
/
psrlw
0x8
%
xmm10
.
byte
102
68
15
253
209
/
/
paddw
%
xmm1
%
xmm10
.
byte
102
68
15
111
222
/
/
movdqa
%
xmm6
%
xmm11
.
byte
102
69
15
213
216
/
/
pmullw
%
xmm8
%
xmm11
.
byte
102
69
15
253
220
/
/
paddw
%
xmm12
%
xmm11
.
byte
102
65
15
113
211
8
/
/
psrlw
0x8
%
xmm11
.
byte
102
68
15
253
218
/
/
paddw
%
xmm2
%
xmm11
.
byte
102
68
15
213
199
/
/
pmullw
%
xmm7
%
xmm8
.
byte
102
69
15
253
196
/
/
paddw
%
xmm12
%
xmm8
.
byte
102
65
15
113
208
8
/
/
psrlw
0x8
%
xmm8
.
byte
102
68
15
253
195
/
/
paddw
%
xmm3
%
xmm8
.
byte
102
65
15
111
210
/
/
movdqa
%
xmm10
%
xmm2
.
byte
102
15
113
242
8
/
/
psllw
0x8
%
xmm2
.
byte
102
65
15
235
211
/
/
por
%
xmm11
%
xmm2
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
97
216
/
/
punpcklwd
%
xmm0
%
xmm3
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
65
15
111
200
/
/
movdqa
%
xmm8
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
65
15
235
201
/
/
por
%
xmm9
%
xmm1
.
byte
102
69
15
239
228
/
/
pxor
%
xmm12
%
xmm12
.
byte
102
68
15
97
225
/
/
punpcklwd
%
xmm1
%
xmm12
.
byte
102
68
15
235
227
/
/
por
%
xmm3
%
xmm12
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
235
194
/
/
por
%
xmm2
%
xmm0
.
byte
65
128
250
6
/
/
cmp
0x6
%
r10b
.
byte
119
24
/
/
ja
3c45f
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x213
>
.
byte
76
141
21
142
0
0
0
/
/
lea
0x8e
(
%
rip
)
%
r10
#
3c4dc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x290
>
.
byte
75
99
4
138
/
/
movslq
(
%
r10
%
r9
4
)
%
rax
.
byte
76
1
208
/
/
add
%
r10
%
rax
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
102
69
15
126
36
144
/
/
movd
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
235
73
/
/
jmp
3c4a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x25c
>
.
byte
243
69
15
127
36
144
/
/
movdqu
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
243
65
15
127
68
144
16
/
/
movdqu
%
xmm0
0x10
(
%
r8
%
rdx
4
)
.
byte
235
58
/
/
jmp
3c4a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x25c
>
.
byte
102
65
15
112
196
78
/
/
pshufd
0x4e
%
xmm12
%
xmm0
.
byte
102
65
15
126
68
144
8
/
/
movd
%
xmm0
0x8
(
%
r8
%
rdx
4
)
.
byte
102
69
15
214
36
144
/
/
movq
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
235
37
/
/
jmp
3c4a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x25c
>
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
65
15
126
76
144
24
/
/
movd
%
xmm1
0x18
(
%
r8
%
rdx
4
)
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
65
15
126
76
144
20
/
/
movd
%
xmm1
0x14
(
%
r8
%
rdx
4
)
.
byte
102
65
15
126
68
144
16
/
/
movd
%
xmm0
0x10
(
%
r8
%
rdx
4
)
.
byte
243
69
15
127
36
144
/
/
movdqu
%
xmm12
(
%
r8
%
rdx
4
)
.
byte
72
173
/
/
lods
%
ds
:
(
%
rsi
)
%
rax
.
byte
102
65
15
111
193
/
/
movdqa
%
xmm9
%
xmm0
.
byte
102
65
15
111
202
/
/
movdqa
%
xmm10
%
xmm1
.
byte
102
65
15
111
211
/
/
movdqa
%
xmm11
%
xmm2
.
byte
102
65
15
111
216
/
/
movdqa
%
xmm8
%
xmm3
.
byte
255
224
/
/
jmpq
*
%
rax
.
byte
202
253
255
/
/
lret
0xfffd
.
byte
255
247
/
/
push
%
rdi
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
230
/
/
jmpq
*
%
rsi
.
byte
253
/
/
std
.
byte
255
/
/
(
bad
)
.
byte
255
55
/
/
pushq
(
%
rdi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
43
/
/
ljmp
*
(
%
rbx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
9
/
/
decl
(
%
rcx
)
.
byte
254
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
123
255
/
/
jnp
3c4dd
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x291
>
.
byte
255
/
/
(
bad
)
.
byte
255
159
255
255
255
146
/
/
lcall
*
-
0x6d000001
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
198
/
/
inc
%
esi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
191
255
255
255
179
/
/
mov
0xb3ffffff
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
.
byte
167
/
/
cmpsl
%
es
:
(
%
rdi
)
%
ds
:
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
.
byte
0xff
BALIGN4
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
63
1
/
/
cmpb
0x1
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
4
0
/
/
add
%
al
(
%
rax
%
rax
1
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
rax
%
rax
1
)
.
byte
252
/
/
cld
.
byte
190
0
0
128
191
/
/
mov
0xbf800000
%
esi
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
224
64
/
/
loopne
3c55c
<
.
literal4
+
0x64
>
.
byte
154
/
/
(
bad
)
.
byte
153
/
/
cltd
.
byte
153
/
/
cltd
.
byte
62
61
10
23
63
174
/
/
ds
cmp
0xae3f170a
%
eax
.
byte
71
225
61
/
/
rex
.
RXB
loope
3c565
<
.
literal4
+
0x6d
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
127
67
/
/
jg
3c56f
<
.
literal4
+
0x77
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
127
145
131
/
/
cmpb
0x83
-
0x6f
(
%
rdi
)
.
byte
158
/
/
sahf
.
byte
61
92
143
50
63
/
/
cmp
0x3f328f5c
%
eax
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
rbx
)
%
edi
.
byte
174
/
/
scas
%
es
:
(
%
rdi
)
%
al
.
byte
71
97
/
/
rex
.
RXB
(
bad
)
.
byte
61
82
184
78
65
/
/
cmp
0x414eb852
%
eax
.
byte
186
159
98
60
57
/
/
mov
0x393c629f
%
edx
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
32
187
180
164
144
63
/
/
and
%
bh
0x3f90a4b4
(
%
rbx
)
.
byte
252
/
/
cld
.
byte
199
/
/
(
bad
)
.
byte
16
62
/
/
adc
%
bh
(
%
rsi
)
.
byte
168
177
/
/
test
0xb1
%
al
.
byte
152
/
/
cwtl
.
byte
59
0
/
/
cmp
(
%
rax
)
%
eax
.
byte
0
192
/
/
add
%
al
%
al
.
byte
64
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
64
0
/
/
add
%
al
0x0
(
%
rax
)
.
byte
0
128
64
171
170
42
/
/
add
%
al
0x2aaaab40
(
%
rax
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
63
/
/
sub
(
%
rdi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
190
129
128
128
59
/
/
mov
0x3b808081
%
esi
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
8
33
/
/
or
%
ah
(
%
rcx
)
.
byte
132
55
/
/
test
%
dh
(
%
rdi
)
.
byte
224
7
/
/
loopne
3c589
<
.
literal4
+
0x91
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
33
8
/
/
and
%
ecx
(
%
rax
)
.
byte
2
58
/
/
add
(
%
rdx
)
%
bh
.
byte
31
/
/
(
bad
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
8
/
/
add
%
cl
(
%
rax
)
.
byte
33
4
61
128
0
128
55
/
/
and
%
eax
0x37800080
(
%
rdi
1
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
52
255
/
/
add
%
dh
(
%
rdi
%
rdi
8
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3c59c
<
.
literal4
+
0xa4
>
.
byte
119
115
/
/
ja
3c611
<
.
literal4
+
0x119
>
.
byte
248
/
/
clc
.
byte
194
117
191
/
/
retq
0xbf75
.
byte
191
63
249
68
180
/
/
mov
0xb444f93f
%
edi
.
byte
62
163
233
220
63
81
140
242
66
141
/
/
movabs
%
eax
%
ds
:
0x8d42f28c513fdce9
.
byte
188
190
63
248
245
/
/
mov
0xf5f83fbe
%
esp
.
byte
154
/
/
(
bad
)
.
byte
64
254
/
/
rex
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
0
0
/
/
add
%
al
(
%
r8
)
.
byte
0
75
0
/
/
add
%
cl
0x0
(
%
rbx
)
.
byte
0
200
/
/
add
%
cl
%
al
.
byte
66
0
0
/
/
rex
.
X
add
%
al
(
%
rax
)
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
65
203
61
/
/
addb
0x3d
-
0x35
(
%
rcx
)
.
byte
13
60
111
18
3
/
/
or
0x3126f3c
%
eax
.
byte
59
10
/
/
cmp
(
%
rdx
)
%
ecx
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
163
59
194
24
17
60
203
61
13
/
/
movabs
%
eax
0xd3dcb3c1118c23b
.
byte
190
80
128
3
62
/
/
mov
0x3e038050
%
esi
.
byte
31
/
/
(
bad
)
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
118
63
/
/
jbe
3c627
<
.
literal4
+
0x12f
>
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
248
/
/
clc
.
byte
65
0
0
/
/
add
%
al
(
%
r8
)
.
byte
124
66
/
/
jl
3c636
<
.
literal4
+
0x13e
>
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
137
136
136
55
0
15
/
/
mov
%
ecx
0xf003788
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
137
136
136
57
240
0
/
/
mov
%
ecx
0xf03988
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
137
136
136
59
15
0
/
/
mov
%
ecx
0xf3b88
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
137
136
136
61
0
0
/
/
mov
%
ecx
0x3d88
(
%
rax
)
.
byte
112
65
/
/
jo
3c659
<
.
literal4
+
0x161
>
.
byte
255
3
/
/
incl
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
8
32
/
/
or
%
ah
(
%
rax
)
.
byte
128
58
0
/
/
cmpb
0x0
(
%
rdx
)
.
byte
192
127
68
0
/
/
sarb
0x0
0x44
(
%
rdi
)
.
byte
0
64
64
/
/
add
%
al
0x40
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
71
/
/
jg
3c673
<
.
literal4
+
0x17b
>
.
byte
89
/
/
pop
%
rcx
.
byte
23
/
/
(
bad
)
.
byte
55
/
/
(
bad
)
.
byte
63
/
/
(
bad
)
.
byte
208
/
/
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
152
/
/
ds
cwtl
.
byte
221
147
61
111
43
231
/
/
fstl
-
0x18d490c3
(
%
rbx
)
.
byte
187
159
215
202
60
/
/
mov
0x3ccad79f
%
ebx
.
byte
212
/
/
(
bad
)
.
byte
100
84
/
/
fs
push
%
rsp
.
byte
189
169
240
34
62
/
/
mov
0x3e22f0a9
%
ebp
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
62
0
/
/
cmpb
0x0
(
%
rsi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
0
0
/
/
addb
0x0
(
%
rax
)
.
byte
0
191
0
0
192
191
/
/
add
%
bh
-
0x40400000
(
%
rdi
)
.
byte
114
28
/
/
jb
3c676
<
.
literal4
+
0x17e
>
.
byte
199
/
/
(
bad
)
.
byte
62
85
/
/
ds
push
%
rbp
.
byte
85
/
/
push
%
rbp
.
byte
149
/
/
xchg
%
eax
%
ebp
.
byte
191
0
0
192
63
/
/
mov
0x3fc00000
%
edi
.
byte
57
142
99
61
114
249
/
/
cmp
%
ecx
-
0x68dc29d
(
%
rsi
)
.
byte
127
63
/
/
jg
3c6ab
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x45f
>
.
byte
3
0
/
/
add
(
%
rax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
45
16
17
192
18
/
/
sub
0x12c01110
%
eax
.
byte
120
57
/
/
js
3c6b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x464
>
.
byte
64
32
148
90
62
4
157
30
/
/
and
%
dl
0x1e9d043e
(
%
rdx
%
rbx
2
)
.
byte
62
0
24
/
/
add
%
bl
%
ds
:
(
%
rax
)
.
byte
161
57
109
165
144
63
252
191
16
/
/
movabs
0x10bffc3f90a56d39
%
eax
.
byte
62
0
0
/
/
add
%
al
%
ds
:
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
4
0
/
/
add
%
al
(
%
rax
%
rax
1
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
56
255
/
/
cmp
%
bh
%
bh
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3c69d
<
.
literal4
+
0x1a5
>
.
byte
0
128
56
0
64
254
/
/
add
%
al
-
0x1bfffc8
(
%
rax
)
.
byte
255
/
/
.
byte
0xff
BALIGN32
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
0
/
/
add
%
eax
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
3
0
/
/
add
(
%
rax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
5
0
0
0
6
/
/
add
0x6000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
7
/
/
add
%
al
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
2
/
/
add
%
al
(
%
rdx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
3
/
/
add
%
al
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
4
0
/
/
add
%
al
(
%
rax
%
rax
1
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
5
0
0
0
6
/
/
add
0x6000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
7
/
/
add
%
al
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c728
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa0004dc
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c730
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x120004e4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c738
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a0004ec
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c740
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30004f4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c788
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa00053c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c790
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12000544
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c798
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a00054c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c7a0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3000554
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c7e8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa00059c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c7f0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x120005a4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c7f8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a0005ac
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c800
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30005b4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c848
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa0005fc
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c850
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12000604
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c858
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a00060c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c860
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3000614
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c8a8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa00065c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c8b0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12000664
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c8b8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a00066c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c8c0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3000674
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c908
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa0006bc
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c910
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x120006c4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c918
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a0006cc
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c920
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30006d4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c968
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa00071c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c970
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12000724
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c978
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a00072c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c980
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3000734
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03c9c8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa00077c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203c9d0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12000784
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03c9d8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a00078c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303c9e0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3000794
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03ca28
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa0007dc
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203ca30
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x120007e4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03ca38
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a0007ec
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303ca40
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x30007f4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03ca88
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa00083c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
17
/
/
decl
0x11ffffff
(
%
rip
)
#
1203ca90
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x12000844
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
21
255
255
255
25
/
/
callq
*
0x19ffffff
(
%
rip
)
#
1a03ca98
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1a00084c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
29
255
255
255
2
/
/
lcall
*
0x2ffffff
(
%
rip
)
#
303caa0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3000854
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
18
/
/
callq
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
22
/
/
callq
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
26
/
/
lcall
*
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
30
/
/
lcall
*
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
255
0
/
/
cmp
0x0
%
bh
.
byte
0
128
0
0
0
0
/
/
add
%
al
0x0
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
248
/
/
clc
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
248
/
/
clc
.
byte
224
255
/
/
loopne
3cd53
<
.
const
+
0x693
>
.
byte
248
/
/
clc
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
240
0
0
/
/
lock
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
240
255
15
/
/
lock
decl
(
%
rdi
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
183
0
19
0
255
/
/
add
%
dh
-
0xffed00
(
%
rdi
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
255
0
/
/
cmp
0x0
%
bh
.
byte
248
/
/
clc
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
255
0
/
/
cmp
0x0
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
4
5
/
/
add
0x5
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
12
13
/
/
or
0xd
%
al
.
byte
14
/
/
(
bad
)
.
byte
15
16
17
/
/
movups
(
%
rcx
)
%
xmm2
.
byte
20
21
/
/
adc
0x15
%
al
.
byte
24
25
/
/
sbb
%
bl
(
%
rcx
)
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
24
25
/
/
sbb
%
bl
(
%
rcx
)
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
30
/
/
(
bad
)
.
byte
31
/
/
(
bad
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
4
5
/
/
add
0x5
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
12
13
/
/
or
0xd
%
al
.
byte
14
/
/
(
bad
)
.
byte
15
16
17
/
/
movups
(
%
rcx
)
%
xmm2
.
byte
20
21
/
/
adc
0x15
%
al
.
byte
24
25
/
/
sbb
%
bl
(
%
rcx
)
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
24
25
/
/
sbb
%
bl
(
%
rcx
)
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
30
/
/
(
bad
)
.
byte
31
/
/
(
bad
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
4
5
/
/
add
0x5
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
12
13
/
/
or
0xd
%
al
.
byte
14
/
/
(
bad
)
.
byte
15
16
17
/
/
movups
(
%
rcx
)
%
xmm2
.
byte
20
21
/
/
adc
0x15
%
al
.
byte
24
25
/
/
sbb
%
bl
(
%
rcx
)
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
24
25
/
/
sbb
%
bl
(
%
rcx
)
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
28
29
/
/
sbb
0x1d
%
al
.
byte
30
/
/
(
bad
)
.
byte
31
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
BALIGN16
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
2
/
/
add
%
al
(
%
rdx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
2
/
/
add
%
al
(
%
rdx
)
.
byte
4
6
/
/
add
0x6
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
0
/
/
add
%
eax
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
3
0
/
/
add
(
%
rax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
5
0
0
0
6
/
/
add
0x6000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
7
/
/
add
%
al
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
63
/
/
(
bad
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
63
0
/
/
cmpb
0x0
(
%
rdi
)
.
byte
0
128
63
0
0
128
/
/
add
%
al
-
0x7fffffc1
(
%
rax
)
.
byte
63
/
/
(
bad
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
63
1
/
/
cmpb
0x1
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
1
/
/
add
%
al
(
%
rcx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
4
0
/
/
add
%
al
(
%
rax
%
rax
1
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
rax
%
rax
1
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
rax
%
rax
1
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
rax
%
rax
1
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
rax
%
rax
1
)
.
byte
252
/
/
cld
.
byte
190
0
0
252
190
/
/
mov
0xbefc0000
%
esi
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
252
/
/
cld
.
byte
190
0
0
252
190
/
/
mov
0xbefc0000
%
esi
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
191
0
0
128
191
0
/
/
cmpb
0x0
-
0x40800000
(
%
rdi
)
.
byte
0
128
191
0
0
128
/
/
add
%
al
-
0x7fffff41
(
%
rax
)
.
byte
191
0
0
224
64
/
/
mov
0x40e00000
%
edi
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
224
64
/
/
loopne
3cfc8
<
.
literal16
+
0x118
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
224
64
/
/
loopne
3cfcc
<
.
literal16
+
0x11c
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
224
64
/
/
loopne
3cfd0
<
.
literal16
+
0x120
>
.
byte
154
/
/
(
bad
)
.
byte
153
/
/
cltd
.
byte
153
/
/
cltd
.
byte
62
154
/
/
ds
(
bad
)
.
byte
153
/
/
cltd
.
byte
153
/
/
cltd
.
byte
62
154
/
/
ds
(
bad
)
.
byte
153
/
/
cltd
.
byte
153
/
/
cltd
.
byte
62
154
/
/
ds
(
bad
)
.
byte
153
/
/
cltd
.
byte
153
/
/
cltd
.
byte
62
61
10
23
63
61
/
/
ds
cmp
0x3d3f170a
%
eax
.
byte
10
23
/
/
or
(
%
rdi
)
%
dl
.
byte
63
/
/
(
bad
)
.
byte
61
10
23
63
61
/
/
cmp
0x3d3f170a
%
eax
.
byte
10
23
/
/
or
(
%
rdi
)
%
dl
.
byte
63
/
/
(
bad
)
.
byte
174
/
/
scas
%
es
:
(
%
rdi
)
%
al
.
byte
71
225
61
/
/
rex
.
RXB
loope
3cff1
<
.
literal16
+
0x141
>
.
byte
174
/
/
scas
%
es
:
(
%
rdi
)
%
al
.
byte
71
225
61
/
/
rex
.
RXB
loope
3cff5
<
.
literal16
+
0x145
>
.
byte
174
/
/
scas
%
es
:
(
%
rdi
)
%
al
.
byte
71
225
61
/
/
rex
.
RXB
loope
3cff9
<
.
literal16
+
0x149
>
.
byte
174
/
/
scas
%
es
:
(
%
rdi
)
%
al
.
byte
71
225
61
/
/
rex
.
RXB
loope
3cffd
<
.
literal16
+
0x14d
>
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
255
/
/
(
bad
)
.
byte
255
5
255
255
255
9
/
/
incl
0x9ffffff
(
%
rip
)
#
a03cfd8
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xa000d8c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
13
255
255
255
2
/
/
decl
0x2ffffff
(
%
rip
)
#
303cfe0
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3000d94
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
6
/
/
incl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
10
/
/
decl
(
%
rdx
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
14
/
/
decl
(
%
rsi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
127
67
/
/
add
%
bh
0x43
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
127
67
/
/
jg
3d03b
<
.
literal16
+
0x18b
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
127
67
/
/
jg
3d03f
<
.
literal16
+
0x18f
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
127
67
/
/
jg
3d043
<
.
literal16
+
0x193
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
127
0
0
/
/
cmpb
0x0
0x0
(
%
rdi
)
.
byte
128
127
0
0
/
/
cmpb
0x0
0x0
(
%
rdi
)
.
byte
128
127
0
0
/
/
cmpb
0x0
0x0
(
%
rdi
)
.
byte
128
127
145
131
/
/
cmpb
0x83
-
0x6f
(
%
rdi
)
.
byte
158
/
/
sahf
.
byte
61
145
131
158
61
/
/
cmp
0x3d9e8391
%
eax
.
byte
145
/
/
xchg
%
eax
%
ecx
.
byte
131
158
61
145
131
158
61
/
/
sbbl
0x3d
-
0x617c6ec3
(
%
rsi
)
.
byte
92
/
/
pop
%
rsp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
rdi
)
%
bh
.
byte
92
/
/
pop
%
rsp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
rdi
)
%
bh
.
byte
92
/
/
pop
%
rsp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
rdi
)
%
bh
.
byte
92
/
/
pop
%
rsp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
rdi
)
%
bh
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
rbx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
rbx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
rbx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
rbx
)
%
edi
.
byte
174
/
/
scas
%
es
:
(
%
rdi
)
%
al
.
byte
71
97
/
/
rex
.
RXB
(
bad
)
.
byte
61
174
71
97
61
/
/
cmp
0x3d6147ae
%
eax
.
byte
174
/
/
scas
%
es
:
(
%
rdi
)
%
al
.
byte
71
97
/
/
rex
.
RXB
(
bad
)
.
byte
61
174
71
97
61
/
/
cmp
0x3d6147ae
%
eax
.
byte
82
/
/
push
%
rdx
.
byte
184
78
65
82
184
/
/
mov
0xb852414e
%
eax
.
byte
78
/
/
rex
.
WRX
.
byte
65
82
/
/
push
%
r10
.
byte
184
78
65
82
184
/
/
mov
0xb852414e
%
eax
.
byte
78
/
/
rex
.
WRX
.
byte
65
57
215
/
/
cmp
%
edx
%
r15d
.
byte
32
187
57
215
32
187
/
/
and
%
bh
-
0x44df28c7
(
%
rbx
)
.
byte
57
215
/
/
cmp
%
edx
%
edi
.
byte
32
187
57
215
32
187
/
/
and
%
bh
-
0x44df28c7
(
%
rbx
)
.
byte
186
159
98
60
186
/
/
mov
0xba3c629f
%
edx
.
byte
159
/
/
lahf
.
byte
98
/
/
(
bad
)
.
byte
60
186
/
/
cmp
0xba
%
al
.
byte
159
/
/
lahf
.
byte
98
/
/
(
bad
)
.
byte
60
186
/
/
cmp
0xba
%
al
.
byte
159
/
/
lahf
.
byte
98
/
/
(
bad
)
.
byte
60
109
/
/
cmp
0x6d
%
al
.
byte
165
/
/
movsl
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
144
/
/
nop
.
byte
63
/
/
(
bad
)
.
byte
109
/
/
insl
(
%
dx
)
%
es
:
(
%
rdi
)
.
byte
165
/
/
movsl
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
144
/
/
nop
.
byte
63
/
/
(
bad
)
.
byte
109
/
/
insl
(
%
dx
)
%
es
:
(
%
rdi
)
.
byte
165
/
/
movsl
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
144
/
/
nop
.
byte
63
/
/
(
bad
)
.
byte
109
/
/
insl
(
%
dx
)
%
es
:
(
%
rdi
)
.
byte
165
/
/
movsl
%
ds
:
(
%
rsi
)
%
es
:
(
%
rdi
)
.
byte
144
/
/
nop
.
byte
63
/
/
(
bad
)
.
byte
252
/
/
cld
.
byte
191
16
62
252
191
/
/
mov
0xbffc3e10
%
edi
.
byte
16
62
/
/
adc
%
bh
(
%
rsi
)
.
byte
252
/
/
cld
.
byte
191
16
62
252
191
/
/
mov
0xbffc3e10
%
edi
.
byte
16
62
/
/
adc
%
bh
(
%
rsi
)
.
byte
168
177
/
/
test
0xb1
%
al
.
byte
152
/
/
cwtl
.
byte
59
168
177
152
59
168
/
/
cmp
-
0x57c4674f
(
%
rax
)
%
ebp
.
byte
177
152
/
/
mov
0x98
%
cl
.
byte
59
168
177
152
59
0
/
/
cmp
0x3b98b1
(
%
rax
)
%
ebp
.
byte
0
192
/
/
add
%
al
%
al
.
byte
64
0
0
/
/
add
%
al
(
%
rax
)
.
byte
192
64
0
0
/
/
rolb
0x0
0x0
(
%
rax
)
.
byte
192
64
0
0
/
/
rolb
0x0
0x0
(
%
rax
)
.
byte
192
64
0
0
/
/
rolb
0x0
0x0
(
%
rax
)
.
byte
0
64
0
/
/
add
%
al
0x0
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
64
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
64
0
/
/
add
%
al
0x0
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
64
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
64
0
0
/
/
addb
0x0
0x0
(
%
rax
)
.
byte
128
64
0
0
/
/
addb
0x0
0x0
(
%
rax
)
.
byte
128
64
0
0
/
/
addb
0x0
0x0
(
%
rax
)
.
byte
128
64
171
170
/
/
addb
0xaa
-
0x55
(
%
rax
)
.
byte
42
62
/
/
sub
(
%
rsi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
62
/
/
sub
(
%
rsi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
62
/
/
sub
(
%
rsi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
62
/
/
sub
(
%
rsi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
63
/
/
sub
(
%
rdi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
63
/
/
sub
(
%
rdi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
63
/
/
sub
(
%
rdi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
42
63
/
/
sub
(
%
rdi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
190
171
170
170
190
/
/
mov
0xbeaaaaab
%
esi
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
rdi
)
.
byte
190
171
170
170
190
/
/
mov
0xbeaaaaab
%
esi
.
byte
129
128
128
59
129
128
128
59
129
128
/
/
addl
0x80813b80
-
0x7f7ec480
(
%
rax
)
.
byte
128
59
129
/
/
cmpb
0x81
(
%
rbx
)
.
byte
128
128
59
0
248
0
0
/
/
addb
0x0
0xf8003b
(
%
rax
)
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
8
33
/
/
or
%
ah
(
%
rcx
)
.
byte
132
55
/
/
test
%
dh
(
%
rdi
)
.
byte
8
33
/
/
or
%
ah
(
%
rcx
)
.
byte
132
55
/
/
test
%
dh
(
%
rdi
)
.
byte
8
33
/
/
or
%
ah
(
%
rcx
)
.
byte
132
55
/
/
test
%
dh
(
%
rdi
)
.
byte
8
33
/
/
or
%
ah
(
%
rcx
)
.
byte
132
55
/
/
test
%
dh
(
%
rdi
)
.
byte
224
7
/
/
loopne
3d159
<
.
literal16
+
0x2a9
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
224
7
/
/
loopne
3d15d
<
.
literal16
+
0x2ad
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
224
7
/
/
loopne
3d161
<
.
literal16
+
0x2b1
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
224
7
/
/
loopne
3d165
<
.
literal16
+
0x2b5
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
33
8
/
/
and
%
ecx
(
%
rax
)
.
byte
2
58
/
/
add
(
%
rdx
)
%
bh
.
byte
33
8
/
/
and
%
ecx
(
%
rax
)
.
byte
2
58
/
/
add
(
%
rdx
)
%
bh
.
byte
33
8
/
/
and
%
ecx
(
%
rax
)
.
byte
2
58
/
/
add
(
%
rdx
)
%
bh
.
byte
33
8
/
/
and
%
ecx
(
%
rax
)
.
byte
2
58
/
/
add
(
%
rdx
)
%
bh
.
byte
31
/
/
(
bad
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
8
/
/
add
%
cl
(
%
rax
)
.
byte
33
4
61
8
33
4
61
/
/
and
%
eax
0x3d042108
(
%
rdi
1
)
.
byte
8
33
/
/
or
%
ah
(
%
rcx
)
.
byte
4
61
/
/
add
0x3d
%
al
.
byte
8
33
/
/
or
%
ah
(
%
rcx
)
.
byte
4
61
/
/
add
0x3d
%
al
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
8
255
/
/
or
%
bh
%
bh
.
byte
10
255
/
/
or
%
bh
%
bh
.
byte
12
255
/
/
or
0xff
%
al
.
byte
14
/
/
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
128
0
128
55
128
/
/
add
%
al
-
0x7fc88000
(
%
rax
)
.
byte
0
128
55
128
0
128
/
/
add
%
al
-
0x7fff7fc9
(
%
rax
)
.
byte
55
/
/
(
bad
)
.
byte
128
0
128
/
/
addb
0x80
(
%
rax
)
.
byte
55
/
/
(
bad
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
52
0
/
/
add
%
dh
(
%
rax
%
rax
1
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
52
255
/
/
xor
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3d1d4
<
.
literal16
+
0x324
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3d1d8
<
.
literal16
+
0x328
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3d1dc
<
.
literal16
+
0x32c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3d1e0
<
.
literal16
+
0x330
>
.
byte
119
115
/
/
ja
3d255
<
.
literal16
+
0x3a5
>
.
byte
248
/
/
clc
.
byte
194
119
115
/
/
retq
0x7377
.
byte
248
/
/
clc
.
byte
194
119
115
/
/
retq
0x7377
.
byte
248
/
/
clc
.
byte
194
119
115
/
/
retq
0x7377
.
byte
248
/
/
clc
.
byte
194
117
191
/
/
retq
0xbf75
.
byte
191
63
117
191
191
/
/
mov
0xbfbf753f
%
edi
.
byte
63
/
/
(
bad
)
.
byte
117
191
/
/
jne
3d1b9
<
.
literal16
+
0x309
>
.
byte
191
63
117
191
191
/
/
mov
0xbfbf753f
%
edi
.
byte
63
/
/
(
bad
)
.
byte
249
/
/
stc
.
byte
68
180
62
/
/
rex
.
R
mov
0x3e
%
spl
.
byte
249
/
/
stc
.
byte
68
180
62
/
/
rex
.
R
mov
0x3e
%
spl
.
byte
249
/
/
stc
.
byte
68
180
62
/
/
rex
.
R
mov
0x3e
%
spl
.
byte
249
/
/
stc
.
byte
68
180
62
/
/
rex
.
R
mov
0x3e
%
spl
.
byte
163
233
220
63
163
233
220
63
163
/
/
movabs
%
eax
0xa33fdce9a33fdce9
.
byte
233
220
63
163
233
/
/
jmpq
ffffffffe9a711fa
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xffffffffe9a34fae
>
.
byte
220
63
/
/
fdivrl
(
%
rdi
)
.
byte
81
/
/
push
%
rcx
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
81
/
/
rex
.
X
push
%
rcx
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
81
/
/
rex
.
X
push
%
rcx
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
81
/
/
rex
.
X
push
%
rcx
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
141
188
190
63
141
188
190
/
/
lea
-
0x414372c1
(
%
rsi
%
r15
4
)
%
edi
.
byte
63
/
/
(
bad
)
.
byte
141
188
190
63
141
188
190
/
/
lea
-
0x414372c1
(
%
rsi
%
rdi
4
)
%
edi
.
byte
63
/
/
(
bad
)
.
byte
248
/
/
clc
.
byte
245
/
/
cmc
.
byte
154
/
/
(
bad
)
.
byte
64
248
/
/
rex
clc
.
byte
245
/
/
cmc
.
byte
154
/
/
(
bad
)
.
byte
64
248
/
/
rex
clc
.
byte
245
/
/
cmc
.
byte
154
/
/
(
bad
)
.
byte
64
248
/
/
rex
clc
.
byte
245
/
/
cmc
.
byte
154
/
/
(
bad
)
.
byte
64
254
/
/
rex
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
254
/
/
rex
.
B
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
254
/
/
rex
.
B
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
254
/
/
rex
.
B
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
0
0
/
/
add
%
al
(
%
r8
)
.
byte
0
75
0
/
/
add
%
cl
0x0
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
75
0
0
/
/
rex
.
WXB
add
%
al
(
%
r8
)
.
byte
0
75
0
/
/
add
%
cl
0x0
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
75
0
0
/
/
rex
.
WXB
add
%
al
(
%
r8
)
.
byte
200
66
0
0
/
/
enterq
0x42
0x0
.
byte
200
66
0
0
/
/
enterq
0x42
0x0
.
byte
200
66
0
0
/
/
enterq
0x42
0x0
.
byte
200
66
0
0
/
/
enterq
0x42
0x0
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
65
0
0
/
/
addb
0x0
0x0
(
%
rcx
)
.
byte
128
65
0
0
/
/
addb
0x0
0x0
(
%
rcx
)
.
byte
128
65
0
0
/
/
addb
0x0
0x0
(
%
rcx
)
.
byte
128
65
203
61
/
/
addb
0x3d
-
0x35
(
%
rcx
)
.
byte
13
60
203
61
13
/
/
or
0xd3dcb3c
%
eax
.
byte
60
203
/
/
cmp
0xcb
%
al
.
byte
61
13
60
203
61
/
/
cmp
0x3dcb3c0d
%
eax
.
byte
13
60
111
18
3
/
/
or
0x3126f3c
%
eax
.
byte
59
111
18
/
/
cmp
0x12
(
%
rdi
)
%
ebp
.
byte
3
59
/
/
add
(
%
rbx
)
%
edi
.
byte
111
/
/
outsl
%
ds
:
(
%
rsi
)
(
%
dx
)
.
byte
18
3
/
/
adc
(
%
rbx
)
%
al
.
byte
59
111
18
/
/
cmp
0x12
(
%
rdi
)
%
ebp
.
byte
3
59
/
/
add
(
%
rbx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
163
59
10
215
163
59
10
215
163
/
/
movabs
%
eax
0xa3d70a3ba3d70a3b
.
byte
59
10
/
/
cmp
(
%
rdx
)
%
ecx
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
163
59
194
24
17
60
194
24
17
/
/
movabs
%
eax
0x1118c23c1118c23b
.
byte
60
194
/
/
cmp
0xc2
%
al
.
byte
24
17
/
/
sbb
%
dl
(
%
rcx
)
.
byte
60
194
/
/
cmp
0xc2
%
al
.
byte
24
17
/
/
sbb
%
dl
(
%
rcx
)
.
byte
60
203
/
/
cmp
0xcb
%
al
.
byte
61
13
190
203
61
/
/
cmp
0x3dcbbe0d
%
eax
.
byte
13
190
203
61
13
/
/
or
0xd3dcbbe
%
eax
.
byte
190
203
61
13
190
/
/
mov
0xbe0d3dcb
%
esi
.
byte
80
/
/
push
%
rax
.
byte
128
3
62
/
/
addb
0x3e
(
%
rbx
)
.
byte
80
/
/
push
%
rax
.
byte
128
3
62
/
/
addb
0x3e
(
%
rbx
)
.
byte
80
/
/
push
%
rax
.
byte
128
3
62
/
/
addb
0x3e
(
%
rbx
)
.
byte
80
/
/
push
%
rax
.
byte
128
3
62
/
/
addb
0x3e
(
%
rbx
)
.
byte
31
/
/
(
bad
)
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
118
63
/
/
jbe
3d343
<
.
literal16
+
0x493
>
.
byte
31
/
/
(
bad
)
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
118
63
/
/
jbe
3d347
<
.
literal16
+
0x497
>
.
byte
31
/
/
(
bad
)
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
118
63
/
/
jbe
3d34b
<
.
literal16
+
0x49b
>
.
byte
31
/
/
(
bad
)
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
118
63
/
/
jbe
3d34f
<
.
literal16
+
0x49f
>
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
rax
)
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
rax
)
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
rax
)
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
rax
)
.
byte
0
4
0
/
/
add
%
al
(
%
rax
%
rax
1
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
65
0
0
/
/
add
%
al
(
%
r8
)
.
byte
248
/
/
clc
.
byte
65
0
0
/
/
add
%
al
(
%
r8
)
.
byte
248
/
/
clc
.
byte
65
0
0
/
/
add
%
al
(
%
r8
)
.
byte
248
/
/
clc
.
byte
65
0
0
/
/
add
%
al
(
%
r8
)
.
byte
124
66
/
/
jl
3d386
<
.
literal16
+
0x4d6
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
124
66
/
/
jl
3d38a
<
.
literal16
+
0x4da
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
124
66
/
/
jl
3d38e
<
.
literal16
+
0x4de
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
124
66
/
/
jl
3d392
<
.
literal16
+
0x4e2
>
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
137
136
136
55
137
136
/
/
mov
%
ecx
-
0x7776c878
(
%
rax
)
.
byte
136
55
/
/
mov
%
dh
(
%
rdi
)
.
byte
137
136
136
55
137
136
/
/
mov
%
ecx
-
0x7776c878
(
%
rax
)
.
byte
136
55
/
/
mov
%
dh
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
137
136
136
57
137
136
/
/
mov
%
ecx
-
0x7776c678
(
%
rax
)
.
byte
136
57
/
/
mov
%
bh
(
%
rcx
)
.
byte
137
136
136
57
137
136
/
/
mov
%
ecx
-
0x7776c678
(
%
rax
)
.
byte
136
57
/
/
mov
%
bh
(
%
rcx
)
.
byte
240
0
0
/
/
lock
add
%
al
(
%
rax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
137
136
136
59
137
/
/
add
%
cl
-
0x76c47778
(
%
rcx
)
.
byte
136
136
59
137
136
136
/
/
mov
%
cl
-
0x777776c5
(
%
rax
)
.
byte
59
137
136
136
59
15
/
/
cmp
0xf3b8888
(
%
rcx
)
%
ecx
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
137
136
136
61
137
/
/
add
%
cl
-
0x76c27778
(
%
rcx
)
.
byte
136
136
61
137
136
136
/
/
mov
%
cl
-
0x777776c3
(
%
rax
)
.
byte
61
137
136
136
61
/
/
cmp
0x3d888889
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
112
65
/
/
jo
3d415
<
.
literal16
+
0x565
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
112
65
/
/
jo
3d419
<
.
literal16
+
0x569
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
112
65
/
/
jo
3d41d
<
.
literal16
+
0x56d
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
112
65
/
/
jo
3d421
<
.
literal16
+
0x571
>
.
byte
255
3
/
/
incl
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
3
/
/
incl
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
3
/
/
incl
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
3
/
/
incl
(
%
rbx
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
8
32
/
/
or
%
ah
(
%
rax
)
.
byte
128
58
8
/
/
cmpb
0x8
(
%
rdx
)
.
byte
32
128
58
8
32
128
/
/
and
%
al
-
0x7fdff7c6
(
%
rax
)
.
byte
58
8
/
/
cmp
(
%
rax
)
%
cl
.
byte
32
128
58
0
192
127
/
/
and
%
al
0x7fc0003a
(
%
rax
)
.
byte
68
0
192
/
/
add
%
r8b
%
al
.
byte
127
68
/
/
jg
3d44c
<
.
literal16
+
0x59c
>
.
byte
0
192
/
/
add
%
al
%
al
.
byte
127
68
/
/
jg
3d450
<
.
literal16
+
0x5a0
>
.
byte
0
192
/
/
add
%
al
%
al
.
byte
127
68
/
/
jg
3d454
<
.
literal16
+
0x5a4
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
64
/
/
rex
.
byte
64
0
0
/
/
add
%
al
(
%
rax
)
.
byte
64
/
/
rex
.
byte
64
0
0
/
/
add
%
al
(
%
rax
)
.
byte
64
/
/
rex
.
byte
64
0
0
/
/
add
%
al
(
%
rax
)
.
byte
64
/
/
rex
.
byte
64
0
128
0
0
0
128
/
/
add
%
al
-
0x80000000
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
128
0
0
0
128
/
/
add
%
al
-
0x80000000
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3d433
<
.
literal16
+
0x583
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
0
/
/
jg
3d437
<
.
literal16
+
0x587
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
0
/
/
jg
3d43b
<
.
literal16
+
0x58b
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
0
/
/
jg
3d43f
<
.
literal16
+
0x58f
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
56
0
/
/
cmp
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
56
0
/
/
cmp
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
56
0
/
/
cmp
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
56
0
/
/
cmp
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
0
0
/
/
addb
0x0
(
%
rax
)
.
byte
0
128
0
0
0
128
/
/
add
%
al
-
0x80000000
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
128
255
255
255
127
/
/
add
%
al
0x7fffffff
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
255
/
/
jg
3d478
<
.
literal16
+
0x5c8
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
255
/
/
jg
3d47c
<
.
literal16
+
0x5cc
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3d481
<
.
literal16
+
0x5d1
>
.
byte
0
128
56
0
0
128
/
/
add
%
al
-
0x7fffffc8
(
%
rax
)
.
byte
56
0
/
/
cmp
%
al
(
%
rax
)
.
byte
0
128
56
0
0
128
/
/
add
%
al
-
0x7fffffc8
(
%
rax
)
.
byte
56
0
/
/
cmp
%
al
(
%
rax
)
.
byte
64
254
/
/
rex
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
64
254
/
/
rex
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
64
254
/
/
rex
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
64
254
/
/
rex
(
bad
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
127
71
/
/
jg
3d4eb
<
.
literal16
+
0x63b
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
71
/
/
jg
3d4ef
<
.
literal16
+
0x63f
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
71
/
/
jg
3d4f3
<
.
literal16
+
0x643
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
71
/
/
jg
3d4f7
<
.
literal16
+
0x647
>
.
byte
208
/
/
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
208
/
/
ds
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
208
/
/
ds
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
208
/
/
ds
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
89
/
/
ds
pop
%
rcx
.
byte
23
/
/
(
bad
)
.
byte
55
/
/
(
bad
)
.
byte
63
/
/
(
bad
)
.
byte
89
/
/
pop
%
rcx
.
byte
23
/
/
(
bad
)
.
byte
55
/
/
(
bad
)
.
byte
63
/
/
(
bad
)
.
byte
89
/
/
pop
%
rcx
.
byte
23
/
/
(
bad
)
.
byte
55
/
/
(
bad
)
.
byte
63
/
/
(
bad
)
.
byte
89
/
/
pop
%
rcx
.
byte
23
/
/
(
bad
)
.
byte
55
/
/
(
bad
)
.
byte
63
/
/
(
bad
)
.
byte
152
/
/
cwtl
.
byte
221
147
61
152
221
147
/
/
fstl
-
0x6c2267c3
(
%
rbx
)
.
byte
61
152
221
147
61
/
/
cmp
0x3d93dd98
%
eax
.
byte
152
/
/
cwtl
.
byte
221
147
61
111
43
231
/
/
fstl
-
0x18d490c3
(
%
rbx
)
.
byte
187
111
43
231
187
/
/
mov
0xbbe72b6f
%
ebx
.
byte
111
/
/
outsl
%
ds
:
(
%
rsi
)
(
%
dx
)
.
byte
43
231
/
/
sub
%
edi
%
esp
.
byte
187
111
43
231
187
/
/
mov
0xbbe72b6f
%
ebx
.
byte
159
/
/
lahf
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
202
60
159
/
/
lret
0x9f3c
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
202
60
159
/
/
lret
0x9f3c
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
202
60
159
/
/
lret
0x9f3c
.
byte
215
/
/
xlat
%
ds
:
(
%
rbx
)
.
byte
202
60
212
/
/
lret
0xd43c
.
byte
100
84
/
/
fs
push
%
rsp
.
byte
189
212
100
84
189
/
/
mov
0xbd5464d4
%
ebp
.
byte
212
/
/
(
bad
)
.
byte
100
84
/
/
fs
push
%
rsp
.
byte
189
212
100
84
189
/
/
mov
0xbd5464d4
%
ebp
.
byte
169
240
34
62
169
/
/
test
0xa93e22f0
%
eax
.
byte
240
34
62
/
/
lock
and
(
%
rsi
)
%
bh
.
byte
169
240
34
62
169
/
/
test
0xa93e22f0
%
eax
.
byte
240
34
62
/
/
lock
and
(
%
rsi
)
%
bh
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
62
0
/
/
cmpb
0x0
(
%
rsi
)
.
byte
0
128
62
0
0
128
/
/
add
%
al
-
0x7fffffc2
(
%
rax
)
.
byte
62
0
0
/
/
add
%
al
%
ds
:
(
%
rax
)
.
byte
128
62
0
/
/
cmpb
0x0
(
%
rsi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
191
0
0
0
191
/
/
mov
0xbf000000
%
edi
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
191
0
0
0
191
/
/
add
%
bh
-
0x41000000
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
192
191
0
0
192
191
0
/
/
sarb
0x0
-
0x40400000
(
%
rdi
)
.
byte
0
192
/
/
add
%
al
%
al
.
byte
191
0
0
192
191
/
/
mov
0xbfc00000
%
edi
.
byte
114
28
/
/
jb
3d56e
<
.
literal16
+
0x6be
>
.
byte
199
/
/
(
bad
)
.
byte
62
114
28
/
/
jb
pt
3d572
<
.
literal16
+
0x6c2
>
.
byte
199
/
/
(
bad
)
.
byte
62
114
28
/
/
jb
pt
3d576
<
.
literal16
+
0x6c6
>
.
byte
199
/
/
(
bad
)
.
byte
62
114
28
/
/
jb
pt
3d57a
<
.
literal16
+
0x6ca
>
.
byte
199
/
/
(
bad
)
.
byte
62
85
/
/
ds
push
%
rbp
.
byte
85
/
/
push
%
rbp
.
byte
149
/
/
xchg
%
eax
%
ebp
.
byte
191
85
85
149
191
/
/
mov
0xbf955555
%
edi
.
byte
85
/
/
push
%
rbp
.
byte
85
/
/
push
%
rbp
.
byte
149
/
/
xchg
%
eax
%
ebp
.
byte
191
85
85
149
191
/
/
mov
0xbf955555
%
edi
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
192
63
0
/
/
sarb
0x0
(
%
rdi
)
.
byte
0
192
/
/
add
%
al
%
al
.
byte
63
/
/
(
bad
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
192
63
0
/
/
sarb
0x0
(
%
rdi
)
.
byte
0
192
/
/
add
%
al
%
al
.
byte
63
/
/
(
bad
)
.
byte
57
142
99
61
57
142
/
/
cmp
%
ecx
-
0x71c6c29d
(
%
rsi
)
.
byte
99
61
57
142
99
61
/
/
movslq
0x3d638e39
(
%
rip
)
%
edi
#
3d6763c5
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x3d63a179
>
.
byte
57
142
99
61
114
249
/
/
cmp
%
ecx
-
0x68dc29d
(
%
rsi
)
.
byte
127
63
/
/
jg
3d5d3
<
.
literal16
+
0x723
>
.
byte
114
249
/
/
jb
3d58f
<
.
literal16
+
0x6df
>
.
byte
127
63
/
/
jg
3d5d7
<
.
literal16
+
0x727
>
.
byte
114
249
/
/
jb
3d593
<
.
literal16
+
0x6e3
>
.
byte
127
63
/
/
jg
3d5db
<
.
literal16
+
0x72b
>
.
byte
114
249
/
/
jb
3d597
<
.
literal16
+
0x6e7
>
.
byte
127
63
/
/
jg
3d5df
<
.
literal16
+
0x72f
>
.
byte
3
0
/
/
add
(
%
rax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
3
0
/
/
add
(
%
rax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
3
0
/
/
add
(
%
rax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
3
0
/
/
add
(
%
rax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
45
16
17
192
45
/
/
ljmp
*
0x2dc01110
(
%
rip
)
#
2dc3e6d5
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc02489
>
.
byte
16
17
/
/
adc
%
dl
(
%
rcx
)
.
byte
192
45
16
17
192
45
16
/
/
shrb
0x10
0x2dc01110
(
%
rip
)
#
2dc3e6de
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x2dc02492
>
.
byte
17
192
/
/
adc
%
eax
%
eax
.
byte
18
120
57
/
/
adc
0x39
(
%
rax
)
%
bh
.
byte
64
18
120
57
/
/
adc
0x39
(
%
rax
)
%
dil
.
byte
64
18
120
57
/
/
adc
0x39
(
%
rax
)
%
dil
.
byte
64
18
120
57
/
/
adc
0x39
(
%
rax
)
%
dil
.
byte
64
32
148
90
62
32
148
90
/
/
and
%
dl
0x5a94203e
(
%
rdx
%
rbx
2
)
.
byte
62
32
148
90
62
32
148
90
/
/
and
%
dl
%
ds
:
0x5a94203e
(
%
rdx
%
rbx
2
)
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
(
bad
)
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
(
bad
)
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
(
bad
)
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
(
bad
)
.
byte
62
0
24
/
/
add
%
bl
%
ds
:
(
%
rax
)
.
byte
161
57
0
24
161
57
0
24
161
/
/
movabs
0xa1180039a1180039
%
eax
.
byte
57
0
/
/
cmp
%
eax
(
%
rax
)
.
byte
24
161
57
0
0
255
/
/
sbb
%
ah
-
0xffffc7
(
%
rcx
)
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
rax
)
.
byte
2
4
6
/
/
add
(
%
rsi
%
rax
1
)
%
al
.
byte
8
10
/
/
or
%
cl
(
%
rdx
)
.
byte
12
14
/
/
or
0xe
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
254
1
/
/
incb
(
%
rcx
)
.
byte
0
128
0
128
0
128
/
/
add
%
al
-
0x7fff8000
(
%
rax
)
.
byte
0
128
0
128
0
128
/
/
add
%
al
-
0x7fff8000
(
%
rax
)
.
byte
0
128
0
128
0
1
/
/
add
%
al
0x1008000
(
%
rax
)
.
byte
4
5
/
/
add
0x5
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
8
9
/
/
or
%
cl
(
%
rcx
)
.
byte
12
13
/
/
or
0xd
%
al
.
byte
12
13
/
/
or
0xd
%
al
.
byte
14
/
/
(
bad
)
.
byte
15
3
255
/
/
lsl
%
di
%
edi
.
byte
7
/
/
(
bad
)
.
byte
255
11
/
/
decl
(
%
rbx
)
.
byte
255
15
/
/
decl
(
%
rdi
)
.
byte
255
11
/
/
decl
(
%
rbx
)
.
byte
255
15
/
/
decl
(
%
rdi
)
.
byte
255
15
/
/
decl
(
%
rdi
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
2
/
/
incl
(
%
rdx
)
.
byte
3
6
/
/
add
(
%
rsi
)
%
eax
.
byte
7
/
/
(
bad
)
.
byte
10
11
/
/
or
(
%
rbx
)
%
cl
.
byte
14
/
/
(
bad
)
.
byte
15
10
/
/
(
bad
)
.
byte
11
14
/
/
or
(
%
rsi
)
%
ecx
.
byte
15
14
/
/
femms
.
byte
15
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
248
/
/
clc
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
63
/
/
add
%
bh
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
31
/
/
add
%
bl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
248
/
/
clc
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
224
255
/
/
loopne
3d6c1
<
.
literal16
+
0x811
>
.
byte
224
255
/
/
loopne
3d6c3
<
.
literal16
+
0x813
>
.
byte
224
255
/
/
loopne
3d6c5
<
.
literal16
+
0x815
>
.
byte
224
255
/
/
loopne
3d6c7
<
.
literal16
+
0x817
>
.
byte
224
255
/
/
loopne
3d6c9
<
.
literal16
+
0x819
>
.
byte
224
255
/
/
loopne
3d6cb
<
.
literal16
+
0x81b
>
.
byte
224
255
/
/
loopne
3d6cd
<
.
literal16
+
0x81d
>
.
byte
224
255
/
/
loopne
3d6cf
<
.
literal16
+
0x81f
>
.
byte
15
0
15
/
/
str
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
15
/
/
add
%
cl
(
%
rdi
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
240
0
240
/
/
lock
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
240
255
240
/
/
lock
push
%
rax
.
byte
255
240
/
/
push
%
rax
.
byte
255
240
/
/
push
%
rax
.
byte
255
240
/
/
push
%
rax
.
byte
255
240
/
/
push
%
rax
.
byte
255
240
/
/
push
%
rax
.
byte
255
240
/
/
push
%
rax
.
byte
255
54
/
/
pushq
(
%
rsi
)
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
54
/
/
add
%
dh
(
%
rsi
)
.
byte
0
183
0
183
0
183
/
/
add
%
dh
-
0x48ff4900
(
%
rdi
)
.
byte
0
183
0
183
0
183
/
/
add
%
dh
-
0x48ff4900
(
%
rdi
)
.
byte
0
183
0
183
0
19
/
/
add
%
dh
0x1300b700
(
%
rdi
)
.
byte
0
19
/
/
add
%
dl
(
%
rbx
)
.
byte
0
19
/
/
add
%
dl
(
%
rbx
)
.
byte
0
19
/
/
add
%
dl
(
%
rbx
)
.
byte
0
19
/
/
add
%
dl
(
%
rbx
)
.
byte
0
19
/
/
add
%
dl
(
%
rbx
)
.
byte
0
19
/
/
add
%
dl
(
%
rbx
)
.
byte
0
19
/
/
add
%
dl
(
%
rbx
)
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
254
/
/
add
%
bh
%
dh
.
byte
0
/
/
.
byte
0x0
BALIGN8
.
byte
1
0
/
/
add
%
eax
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
1
0
/
/
add
%
eax
(
%
rax
)
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
2
0
/
/
add
(
%
rax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
3d76b
<
.
literal8
+
0x1b
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
0
/
/
jg
3d76f
<
.
literal8
+
0x1f
>
.
byte
0
0
/
/
add
%
al
(
%
rax
)
.
byte
128
0
0
/
/
addb
0x0
(
%
rax
)
.
byte
0
/
/
.
byte
0x0
.
byte
128
0
0
/
/
addb
0x0
(
%
rax
)
#
elif
defined
(
__i386__
)
BALIGN32
HIDDEN
_sk_start_pipeline_sse2
.
globl
_sk_start_pipeline_sse2
FUNCTION
(
_sk_start_pipeline_sse2
)
_sk_start_pipeline_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
59
69
20
/
/
cmp
0x14
(
%
ebp
)
%
eax
.
byte
15
131
161
0
0
0
/
/
jae
b6
<
_sk_start_pipeline_sse2
+
0xb6
>
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
117
24
/
/
mov
0x18
(
%
ebp
)
%
esi
.
byte
139
62
/
/
mov
(
%
esi
)
%
edi
.
byte
131
198
4
/
/
add
0x4
%
esi
.
byte
141
73
4
/
/
lea
0x4
(
%
ecx
)
%
ecx
.
byte
137
77
240
/
/
mov
%
ecx
-
0x10
(
%
ebp
)
.
byte
57
85
240
/
/
cmp
%
edx
-
0x10
(
%
ebp
)
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
137
77
152
/
/
mov
%
ecx
-
0x68
(
%
ebp
)
.
byte
137
69
236
/
/
mov
%
eax
-
0x14
(
%
ebp
)
.
byte
137
69
156
/
/
mov
%
eax
-
0x64
(
%
ebp
)
.
byte
199
69
160
0
0
0
0
/
/
movl
0x0
-
0x60
(
%
ebp
)
.
byte
141
69
168
/
/
lea
-
0x58
(
%
ebp
)
%
eax
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
41
64
48
/
/
movaps
%
xmm0
0x30
(
%
eax
)
.
byte
15
41
64
32
/
/
movaps
%
xmm0
0x20
(
%
eax
)
.
byte
15
41
64
16
/
/
movaps
%
xmm0
0x10
(
%
eax
)
.
byte
15
41
0
/
/
movaps
%
xmm0
(
%
eax
)
.
byte
137
200
/
/
mov
%
ecx
%
eax
.
byte
141
93
152
/
/
lea
-
0x68
(
%
ebp
)
%
ebx
.
byte
119
41
/
/
ja
84
<
_sk_start_pipeline_sse2
+
0x84
>
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
86
/
/
push
%
esi
.
byte
83
/
/
push
%
ebx
.
byte
255
215
/
/
call
*
%
edi
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
131
196
16
/
/
add
0x10
%
esp
.
byte
139
77
152
/
/
mov
-
0x68
(
%
ebp
)
%
ecx
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
137
69
152
/
/
mov
%
eax
-
0x68
(
%
ebp
)
.
byte
131
193
8
/
/
add
0x8
%
ecx
.
byte
57
209
/
/
cmp
%
edx
%
ecx
.
byte
118
215
/
/
jbe
5b
<
_sk_start_pipeline_sse2
+
0x5b
>
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
41
193
/
/
sub
%
eax
%
ecx
.
byte
116
31
/
/
je
a9
<
_sk_start_pipeline_sse2
+
0xa9
>
.
byte
137
77
160
/
/
mov
%
ecx
-
0x60
(
%
ebp
)
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
86
/
/
push
%
esi
.
byte
141
69
152
/
/
lea
-
0x68
(
%
ebp
)
%
eax
.
byte
80
/
/
push
%
eax
.
byte
255
215
/
/
call
*
%
edi
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
131
196
16
/
/
add
0x10
%
esp
.
byte
139
69
236
/
/
mov
-
0x14
(
%
ebp
)
%
eax
.
byte
64
/
/
inc
%
eax
.
byte
59
69
20
/
/
cmp
0x14
(
%
ebp
)
%
eax
.
byte
15
133
115
255
255
255
/
/
jne
29
<
_sk_start_pipeline_sse2
+
0x29
>
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_just_return_sse2
.
globl
_sk_just_return_sse2
FUNCTION
(
_sk_just_return_sse2
)
_sk_just_return_sse2
:
.
byte
195
/
/
ret
HIDDEN
_sk_seed_shader_sse2
.
globl
_sk_seed_shader_sse2
FUNCTION
(
_sk_seed_shader_sse2
)
_sk_seed_shader_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
c9
<
_sk_seed_shader_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
102
15
110
1
/
/
movd
(
%
ecx
)
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm1
.
byte
15
16
6
/
/
movups
(
%
esi
)
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
88
136
135
6
1
0
/
/
addps
0x10687
(
%
eax
)
%
xmm1
.
byte
141
114
8
/
/
lea
0x8
(
%
edx
)
%
esi
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
41
81
64
/
/
movaps
%
xmm2
0x40
(
%
ecx
)
.
byte
15
41
81
48
/
/
movaps
%
xmm2
0x30
(
%
ecx
)
.
byte
15
41
81
32
/
/
movaps
%
xmm2
0x20
(
%
ecx
)
.
byte
15
41
81
16
/
/
movaps
%
xmm2
0x10
(
%
ecx
)
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
144
151
6
1
0
/
/
movaps
0x10697
(
%
eax
)
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
86
/
/
push
%
esi
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dither_sse2
.
globl
_sk_dither_sse2
FUNCTION
(
_sk_dither_sse2
)
_sk_dither_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
131
236
36
/
/
sub
0x24
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
232
0
0
0
0
/
/
call
140
<
_sk_dither_sse2
+
0x1a
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
102
15
110
32
/
/
movd
(
%
eax
)
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
102
15
254
161
48
6
1
0
/
/
paddd
0x10630
(
%
ecx
)
%
xmm4
.
byte
102
15
110
104
4
/
/
movd
0x4
(
%
eax
)
%
xmm5
.
byte
102
15
112
253
0
/
/
pshufd
0x0
%
xmm5
%
xmm7
.
byte
102
15
239
252
/
/
pxor
%
xmm4
%
xmm7
.
byte
102
15
111
177
64
6
1
0
/
/
movdqa
0x10640
(
%
ecx
)
%
xmm6
.
byte
102
15
111
207
/
/
movdqa
%
xmm7
%
xmm1
.
byte
102
15
219
206
/
/
pand
%
xmm6
%
xmm1
.
byte
102
15
114
241
5
/
/
pslld
0x5
%
xmm1
.
byte
102
15
111
129
96
6
1
0
/
/
movdqa
0x10660
(
%
ecx
)
%
xmm0
.
byte
102
15
111
239
/
/
movdqa
%
xmm7
%
xmm5
.
byte
102
15
219
232
/
/
pand
%
xmm0
%
xmm5
.
byte
102
15
114
245
2
/
/
pslld
0x2
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
219
244
/
/
pand
%
xmm4
%
xmm6
.
byte
102
15
114
246
4
/
/
pslld
0x4
%
xmm6
.
byte
102
15
219
196
/
/
pand
%
xmm4
%
xmm0
.
byte
102
15
254
192
/
/
paddd
%
xmm0
%
xmm0
.
byte
102
15
235
198
/
/
por
%
xmm6
%
xmm0
.
byte
102
15
111
137
80
6
1
0
/
/
movdqa
0x10650
(
%
ecx
)
%
xmm1
.
byte
102
15
219
249
/
/
pand
%
xmm1
%
xmm7
.
byte
102
15
219
225
/
/
pand
%
xmm1
%
xmm4
.
byte
102
15
114
212
2
/
/
psrld
0x2
%
xmm4
.
byte
102
15
235
224
/
/
por
%
xmm0
%
xmm4
.
byte
102
15
235
229
/
/
por
%
xmm5
%
xmm4
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
102
15
114
215
1
/
/
psrld
0x1
%
xmm7
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
15
91
196
/
/
cvtdq2ps
%
xmm4
%
xmm0
.
byte
15
89
129
112
6
1
0
/
/
mulps
0x10670
(
%
ecx
)
%
xmm0
.
byte
15
88
129
128
6
1
0
/
/
addps
0x10680
(
%
ecx
)
%
xmm0
.
byte
243
15
16
14
/
/
movss
(
%
esi
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
88
77
232
/
/
addps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
15
95
234
/
/
maxps
%
xmm2
%
xmm5
.
byte
15
93
216
/
/
minps
%
xmm0
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
227
/
/
maxps
%
xmm3
%
xmm4
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
93
200
/
/
minps
%
xmm0
%
xmm1
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
141
74
8
/
/
lea
0x8
(
%
edx
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
81
/
/
push
%
ecx
.
byte
80
/
/
push
%
eax
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
52
/
/
add
0x34
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_uniform_color_sse2
.
globl
_sk_uniform_color_sse2
FUNCTION
(
_sk_uniform_color_sse2
)
_sk_uniform_color_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
243
15
16
73
4
/
/
movss
0x4
(
%
ecx
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
89
12
/
/
movss
0xc
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_black_color_sse2
.
globl
_sk_black_color_sse2
FUNCTION
(
_sk_black_color_sse2
)
_sk_black_color_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
27c
<
_sk_black_color_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
152
228
4
1
0
/
/
movaps
0x104e4
(
%
eax
)
%
xmm3
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_white_color_sse2
.
globl
_sk_white_color_sse2
FUNCTION
(
_sk_white_color_sse2
)
_sk_white_color_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
2ac
<
_sk_white_color_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
128
180
4
1
0
/
/
movaps
0x104b4
(
%
eax
)
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_rgba_sse2
.
globl
_sk_load_rgba_sse2
FUNCTION
(
_sk_load_rgba_sse2
)
_sk_load_rgba_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
15
16
1
/
/
movups
(
%
ecx
)
%
xmm0
.
byte
15
16
73
16
/
/
movups
0x10
(
%
ecx
)
%
xmm1
.
byte
15
16
81
32
/
/
movups
0x20
(
%
ecx
)
%
xmm2
.
byte
15
16
89
48
/
/
movups
0x30
(
%
ecx
)
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_rgba_sse2
.
globl
_sk_store_rgba_sse2
FUNCTION
(
_sk_store_rgba_sse2
)
_sk_store_rgba_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
15
17
2
/
/
movups
%
xmm0
(
%
edx
)
.
byte
15
17
74
16
/
/
movups
%
xmm1
0x10
(
%
edx
)
.
byte
15
17
82
32
/
/
movups
%
xmm2
0x20
(
%
edx
)
.
byte
15
17
90
48
/
/
movups
%
xmm3
0x30
(
%
edx
)
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clear_sse2
.
globl
_sk_clear_sse2
FUNCTION
(
_sk_clear_sse2
)
_sk_clear_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcatop_sse2
.
globl
_sk_srcatop_sse2
FUNCTION
(
_sk_srcatop_sse2
)
_sk_srcatop_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
35b
<
_sk_srcatop_sse2
+
0xb
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
97
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
170
5
4
1
0
/
/
movaps
0x10405
(
%
edx
)
%
xmm5
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
40
113
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
40
113
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
40
113
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstatop_sse2
.
globl
_sk_dstatop_sse2
FUNCTION
(
_sk_dstatop_sse2
)
_sk_dstatop_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
3ba
<
_sk_dstatop_sse2
+
0xb
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
105
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm5
.
byte
15
40
113
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm6
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
40
162
166
3
1
0
/
/
movaps
0x103a6
(
%
edx
)
%
xmm4
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
40
113
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm6
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
40
113
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm6
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcin_sse2
.
globl
_sk_srcin_sse2
FUNCTION
(
_sk_srcin_sse2
)
_sk_srcin_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
97
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstin_sse2
.
globl
_sk_dstin_sse2
FUNCTION
(
_sk_dstin_sse2
)
_sk_dstin_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
65
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
40
73
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
40
81
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
89
89
64
/
/
mulps
0x40
(
%
ecx
)
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcout_sse2
.
globl
_sk_srcout_sse2
FUNCTION
(
_sk_srcout_sse2
)
_sk_srcout_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
47b
<
_sk_srcout_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
40
160
229
2
1
0
/
/
movaps
0x102e5
(
%
eax
)
%
xmm4
.
byte
15
92
98
64
/
/
subps
0x40
(
%
edx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
82
/
/
push
%
edx
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstout_sse2
.
globl
_sk_dstout_sse2
FUNCTION
(
_sk_dstout_sse2
)
_sk_dstout_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
4b3
<
_sk_dstout_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
40
160
173
2
1
0
/
/
movaps
0x102ad
(
%
eax
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
40
66
16
/
/
movaps
0x10
(
%
edx
)
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
74
32
/
/
movaps
0x20
(
%
edx
)
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
40
82
48
/
/
movaps
0x30
(
%
edx
)
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
89
98
64
/
/
mulps
0x40
(
%
edx
)
%
xmm4
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
82
/
/
push
%
edx
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcover_sse2
.
globl
_sk_srcover_sse2
FUNCTION
(
_sk_srcover_sse2
)
_sk_srcover_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
4fa
<
_sk_srcover_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
40
160
102
2
1
0
/
/
movaps
0x10266
(
%
eax
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
40
106
16
/
/
movaps
0x10
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
40
106
32
/
/
movaps
0x20
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
40
106
48
/
/
movaps
0x30
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
89
98
64
/
/
mulps
0x40
(
%
edx
)
%
xmm4
.
byte
15
88
220
/
/
addps
%
xmm4
%
xmm3
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
82
/
/
push
%
edx
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstover_sse2
.
globl
_sk_dstover_sse2
FUNCTION
(
_sk_dstover_sse2
)
_sk_dstover_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
54a
<
_sk_dstover_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
40
106
64
/
/
movaps
0x40
(
%
edx
)
%
xmm5
.
byte
15
40
160
22
2
1
0
/
/
movaps
0x10216
(
%
eax
)
%
xmm4
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
66
16
/
/
addps
0x10
(
%
edx
)
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
74
32
/
/
addps
0x20
(
%
edx
)
%
xmm1
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
82
48
/
/
addps
0x30
(
%
edx
)
%
xmm2
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
82
/
/
push
%
edx
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_modulate_sse2
.
globl
_sk_modulate_sse2
FUNCTION
(
_sk_modulate_sse2
)
_sk_modulate_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
89
65
16
/
/
mulps
0x10
(
%
ecx
)
%
xmm0
.
byte
15
89
73
32
/
/
mulps
0x20
(
%
ecx
)
%
xmm1
.
byte
15
89
81
48
/
/
mulps
0x30
(
%
ecx
)
%
xmm2
.
byte
15
89
89
64
/
/
mulps
0x40
(
%
ecx
)
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_multiply_sse2
.
globl
_sk_multiply_sse2
FUNCTION
(
_sk_multiply_sse2
)
_sk_multiply_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
56
/
/
sub
0x38
%
esp
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
232
0
0
0
0
/
/
call
5c9
<
_sk_multiply_sse2
+
0x12
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
80
64
/
/
movaps
0x40
(
%
eax
)
%
xmm2
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
40
129
151
1
1
0
/
/
movaps
0x10197
(
%
ecx
)
%
xmm0
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
92
242
/
/
subps
%
xmm2
%
xmm6
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
89
253
/
/
mulps
%
xmm5
%
xmm7
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
40
80
16
/
/
movaps
0x10
(
%
eax
)
%
xmm2
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
89
226
/
/
mulps
%
xmm2
%
xmm4
.
byte
15
88
231
/
/
addps
%
xmm7
%
xmm4
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
15
88
236
/
/
addps
%
xmm4
%
xmm5
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
40
96
32
/
/
movaps
0x20
(
%
eax
)
%
xmm4
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
88
250
/
/
addps
%
xmm2
%
xmm7
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
40
85
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
40
96
48
/
/
movaps
0x30
(
%
eax
)
%
xmm4
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
88
249
/
/
addps
%
xmm1
%
xmm7
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
40
77
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm1
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
72
/
/
add
0x48
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_plus__sse2
.
globl
_sk_plus__sse2
FUNCTION
(
_sk_plus__sse2
)
_sk_plus__sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
66f
<
_sk_plus__sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
88
66
16
/
/
addps
0x10
(
%
edx
)
%
xmm0
.
byte
15
40
160
241
0
1
0
/
/
movaps
0x100f1
(
%
eax
)
%
xmm4
.
byte
15
93
196
/
/
minps
%
xmm4
%
xmm0
.
byte
15
88
74
32
/
/
addps
0x20
(
%
edx
)
%
xmm1
.
byte
15
93
204
/
/
minps
%
xmm4
%
xmm1
.
byte
15
88
82
48
/
/
addps
0x30
(
%
edx
)
%
xmm2
.
byte
15
93
212
/
/
minps
%
xmm4
%
xmm2
.
byte
15
88
90
64
/
/
addps
0x40
(
%
edx
)
%
xmm3
.
byte
15
93
220
/
/
minps
%
xmm4
%
xmm3
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
82
/
/
push
%
edx
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_screen_sse2
.
globl
_sk_screen_sse2
FUNCTION
(
_sk_screen_sse2
)
_sk_screen_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
105
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm5
.
byte
15
40
113
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm6
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
88
233
/
/
addps
%
xmm1
%
xmm5
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
92
238
/
/
subps
%
xmm6
%
xmm5
.
byte
15
40
65
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm0
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
15
40
65
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm0
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
92
248
/
/
subps
%
xmm0
%
xmm7
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xor__sse2
.
globl
_sk_xor__sse2
FUNCTION
(
_sk_xor__sse2
)
_sk_xor__sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
232
0
0
0
0
/
/
call
71d
<
_sk_xor__sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
105
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm5
.
byte
15
40
154
67
0
1
0
/
/
movaps
0x10043
(
%
edx
)
%
xmm3
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
92
245
/
/
subps
%
xmm5
%
xmm6
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
92
220
/
/
subps
%
xmm4
%
xmm3
.
byte
15
40
121
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
40
121
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
40
121
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_darken_sse2
.
globl
_sk_darken_sse2
FUNCTION
(
_sk_darken_sse2
)
_sk_darken_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
232
0
0
0
0
/
/
call
785
<
_sk_darken_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
113
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm6
.
byte
15
40
105
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm5
.
byte
15
40
121
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm7
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
95
229
/
/
maxps
%
xmm5
%
xmm4
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
40
231
/
/
movaps
%
xmm7
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
95
207
/
/
maxps
%
xmm7
%
xmm1
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
40
73
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm1
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
15
40
138
219
255
0
0
/
/
movaps
0xffdb
(
%
edx
)
%
xmm1
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_lighten_sse2
.
globl
_sk_lighten_sse2
FUNCTION
(
_sk_lighten_sse2
)
_sk_lighten_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
232
0
0
0
0
/
/
call
805
<
_sk_lighten_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
113
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm6
.
byte
15
40
105
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm5
.
byte
15
40
121
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm7
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
40
231
/
/
movaps
%
xmm7
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
93
207
/
/
minps
%
xmm7
%
xmm1
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
40
73
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm1
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
93
209
/
/
minps
%
xmm1
%
xmm2
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
15
40
138
91
255
0
0
/
/
movaps
0xff5b
(
%
edx
)
%
xmm1
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_difference_sse2
.
globl
_sk_difference_sse2
FUNCTION
(
_sk_difference_sse2
)
_sk_difference_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
888
<
_sk_difference_sse2
+
0x11
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
105
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm5
.
byte
15
40
113
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm6
.
byte
15
40
121
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm7
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
93
206
/
/
minps
%
xmm6
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
93
231
/
/
minps
%
xmm7
%
xmm4
.
byte
15
88
228
/
/
addps
%
xmm4
%
xmm4
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
15
40
113
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm6
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
88
226
/
/
addps
%
xmm2
%
xmm4
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
93
214
/
/
minps
%
xmm6
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
40
146
216
254
0
0
/
/
movaps
0xfed8
(
%
edx
)
%
xmm2
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_exclusion_sse2
.
globl
_sk_exclusion_sse2
FUNCTION
(
_sk_exclusion_sse2
)
_sk_exclusion_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
232
0
0
0
0
/
/
call
90e
<
_sk_exclusion_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
105
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm5
.
byte
15
40
113
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm6
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
237
/
/
addps
%
xmm5
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
88
246
/
/
addps
%
xmm6
%
xmm6
.
byte
15
92
230
/
/
subps
%
xmm6
%
xmm4
.
byte
15
40
73
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm1
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
40
138
82
254
0
0
/
/
movaps
0xfe52
(
%
edx
)
%
xmm1
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
89
73
64
/
/
mulps
0x40
(
%
ecx
)
%
xmm1
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_colorburn_sse2
.
globl
_sk_colorburn_sse2
FUNCTION
(
_sk_colorburn_sse2
)
_sk_colorburn_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
88
/
/
sub
0x58
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
41
85
168
/
/
movaps
%
xmm2
-
0x58
(
%
ebp
)
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
232
0
0
0
0
/
/
call
98d
<
_sk_colorburn_sse2
+
0x19
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
72
64
/
/
movaps
0x40
(
%
eax
)
%
xmm1
.
byte
15
40
104
16
/
/
movaps
0x10
(
%
eax
)
%
xmm5
.
byte
15
41
109
232
/
/
movaps
%
xmm5
-
0x18
(
%
ebp
)
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
83
244
/
/
rcpps
%
xmm4
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
93
198
/
/
minps
%
xmm6
%
xmm0
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
92
232
/
/
subps
%
xmm0
%
xmm5
.
byte
15
40
177
211
253
0
0
/
/
movaps
0xfdd3
(
%
ecx
)
%
xmm6
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
41
69
200
/
/
movaps
%
xmm0
-
0x38
(
%
ebp
)
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
88
239
/
/
addps
%
xmm7
%
xmm5
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
15
41
117
184
/
/
movaps
%
xmm6
-
0x48
(
%
ebp
)
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
40
117
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm6
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
194
231
0
/
/
cmpeqps
%
xmm7
%
xmm4
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
15
85
229
/
/
andnps
%
xmm5
%
xmm4
.
byte
15
86
224
/
/
orps
%
xmm0
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
194
193
0
/
/
cmpeqps
%
xmm1
%
xmm0
.
byte
15
84
216
/
/
andps
%
xmm0
%
xmm3
.
byte
15
85
196
/
/
andnps
%
xmm4
%
xmm0
.
byte
15
86
195
/
/
orps
%
xmm3
%
xmm0
.
byte
15
41
69
232
/
/
movaps
%
xmm0
-
0x18
(
%
ebp
)
.
byte
15
40
96
32
/
/
movaps
0x20
(
%
eax
)
%
xmm4
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
40
109
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
83
218
/
/
rcpps
%
xmm2
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
93
195
/
/
minps
%
xmm3
%
xmm0
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
40
117
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm6
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
194
215
0
/
/
cmpeqps
%
xmm7
%
xmm2
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
84
194
/
/
andps
%
xmm2
%
xmm0
.
byte
15
85
211
/
/
andnps
%
xmm3
%
xmm2
.
byte
15
86
208
/
/
orps
%
xmm0
%
xmm2
.
byte
15
88
244
/
/
addps
%
xmm4
%
xmm6
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
194
233
0
/
/
cmpeqps
%
xmm1
%
xmm5
.
byte
15
84
245
/
/
andps
%
xmm5
%
xmm6
.
byte
15
85
234
/
/
andnps
%
xmm2
%
xmm5
.
byte
15
86
238
/
/
orps
%
xmm6
%
xmm5
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
15
40
117
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
41
117
200
/
/
movaps
%
xmm6
-
0x38
(
%
ebp
)
.
byte
15
194
248
0
/
/
cmpeqps
%
xmm0
%
xmm7
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
40
80
48
/
/
movaps
0x30
(
%
eax
)
%
xmm2
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
89
69
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
83
219
/
/
rcpps
%
xmm3
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
93
227
/
/
minps
%
xmm3
%
xmm4
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
15
194
209
0
/
/
cmpeqps
%
xmm1
%
xmm2
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
41
69
184
/
/
movaps
%
xmm0
-
0x48
(
%
ebp
)
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
77
200
/
/
addps
-
0x38
(
%
ebp
)
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
84
247
/
/
andps
%
xmm7
%
xmm6
.
byte
15
85
249
/
/
andnps
%
xmm1
%
xmm7
.
byte
15
86
254
/
/
orps
%
xmm6
%
xmm7
.
byte
15
84
218
/
/
andps
%
xmm2
%
xmm3
.
byte
15
85
215
/
/
andnps
%
xmm7
%
xmm2
.
byte
15
86
211
/
/
orps
%
xmm3
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
88
93
184
/
/
addps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
69
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
104
/
/
add
0x68
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_colordodge_sse2
.
globl
_sk_colordodge_sse2
FUNCTION
(
_sk_colordodge_sse2
)
_sk_colordodge_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
88
/
/
sub
0x58
%
esp
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
41
85
168
/
/
movaps
%
xmm2
-
0x58
(
%
ebp
)
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
b16
<
_sk_colordodge_sse2
+
0x16
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
72
16
/
/
movaps
0x10
(
%
eax
)
%
xmm1
.
byte
15
40
145
74
252
0
0
/
/
movaps
0xfc4a
(
%
ecx
)
%
xmm2
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
41
85
184
/
/
movaps
%
xmm2
-
0x48
(
%
ebp
)
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
83
219
/
/
rcpps
%
xmm3
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
40
104
64
/
/
movaps
0x40
(
%
eax
)
%
xmm5
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
93
203
/
/
minps
%
xmm3
%
xmm1
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
41
101
200
/
/
movaps
%
xmm4
-
0x38
(
%
ebp
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
194
230
0
/
/
cmpeqps
%
xmm6
%
xmm4
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
15
85
225
/
/
andnps
%
xmm1
%
xmm4
.
byte
15
86
224
/
/
orps
%
xmm0
%
xmm4
.
byte
15
88
231
/
/
addps
%
xmm7
%
xmm4
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
40
125
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm7
.
byte
15
194
248
0
/
/
cmpeqps
%
xmm0
%
xmm7
.
byte
15
84
223
/
/
andps
%
xmm7
%
xmm3
.
byte
15
85
252
/
/
andnps
%
xmm4
%
xmm7
.
byte
15
86
251
/
/
orps
%
xmm3
%
xmm7
.
byte
15
41
125
216
/
/
movaps
%
xmm7
-
0x28
(
%
ebp
)
.
byte
15
40
64
32
/
/
movaps
0x20
(
%
eax
)
%
xmm0
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
83
228
/
/
rcpps
%
xmm4
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
93
204
/
/
minps
%
xmm4
%
xmm1
.
byte
15
40
101
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
89
226
/
/
mulps
%
xmm2
%
xmm4
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
194
198
0
/
/
cmpeqps
%
xmm6
%
xmm0
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
84
208
/
/
andps
%
xmm0
%
xmm2
.
byte
15
85
193
/
/
andnps
%
xmm1
%
xmm0
.
byte
15
86
194
/
/
orps
%
xmm2
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
194
217
0
/
/
cmpeqps
%
xmm1
%
xmm3
.
byte
15
84
227
/
/
andps
%
xmm3
%
xmm4
.
byte
15
85
216
/
/
andnps
%
xmm0
%
xmm3
.
byte
15
86
220
/
/
orps
%
xmm4
%
xmm3
.
byte
15
40
64
48
/
/
movaps
0x30
(
%
eax
)
%
xmm0
.
byte
15
194
200
0
/
/
cmpeqps
%
xmm0
%
xmm1
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
77
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm1
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
40
125
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm7
.
byte
15
92
231
/
/
subps
%
xmm7
%
xmm4
.
byte
15
83
228
/
/
rcpps
%
xmm4
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
93
236
/
/
minps
%
xmm4
%
xmm5
.
byte
15
40
231
/
/
movaps
%
xmm7
%
xmm4
.
byte
15
40
125
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
194
198
0
/
/
cmpeqps
%
xmm6
%
xmm0
.
byte
15
89
238
/
/
mulps
%
xmm6
%
xmm5
.
byte
15
88
239
/
/
addps
%
xmm7
%
xmm5
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
85
197
/
/
andnps
%
xmm5
%
xmm0
.
byte
15
86
196
/
/
orps
%
xmm4
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
84
250
/
/
andps
%
xmm2
%
xmm7
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
15
86
215
/
/
orps
%
xmm7
%
xmm2
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
104
/
/
add
0x68
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_hardlight_sse2
.
globl
_sk_hardlight_sse2
FUNCTION
(
_sk_hardlight_sse2
)
_sk_hardlight_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
88
/
/
sub
0x58
%
esp
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
41
85
168
/
/
movaps
%
xmm2
-
0x58
(
%
ebp
)
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
c77
<
_sk_hardlight_sse2
+
0x16
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
72
64
/
/
movaps
0x40
(
%
eax
)
%
xmm1
.
byte
15
40
145
233
250
0
0
/
/
movaps
0xfae9
(
%
ecx
)
%
xmm2
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
40
88
16
/
/
movaps
0x10
(
%
eax
)
%
xmm3
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
92
251
/
/
subps
%
xmm3
%
xmm7
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
41
125
184
/
/
movaps
%
xmm7
-
0x48
(
%
ebp
)
.
byte
15
88
228
/
/
addps
%
xmm4
%
xmm4
.
byte
15
92
252
/
/
subps
%
xmm4
%
xmm7
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
88
228
/
/
addps
%
xmm4
%
xmm4
.
byte
15
194
230
2
/
/
cmpleps
%
xmm6
%
xmm4
.
byte
15
88
219
/
/
addps
%
xmm3
%
xmm3
.
byte
15
84
220
/
/
andps
%
xmm4
%
xmm3
.
byte
15
85
231
/
/
andnps
%
xmm7
%
xmm4
.
byte
15
86
227
/
/
orps
%
xmm3
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
40
88
32
/
/
movaps
0x20
(
%
eax
)
%
xmm3
.
byte
15
40
69
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
40
77
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
88
255
/
/
addps
%
xmm7
%
xmm7
.
byte
15
40
85
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm2
.
byte
15
92
215
/
/
subps
%
xmm7
%
xmm2
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
194
206
2
/
/
cmpleps
%
xmm6
%
xmm1
.
byte
15
88
219
/
/
addps
%
xmm3
%
xmm3
.
byte
15
84
217
/
/
andps
%
xmm1
%
xmm3
.
byte
15
85
202
/
/
andnps
%
xmm2
%
xmm1
.
byte
15
86
203
/
/
orps
%
xmm3
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
40
125
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm7
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
40
80
48
/
/
movaps
0x30
(
%
eax
)
%
xmm2
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
41
69
232
/
/
movaps
%
xmm0
-
0x18
(
%
ebp
)
.
byte
15
40
239
/
/
movaps
%
xmm7
%
xmm5
.
byte
15
88
237
/
/
addps
%
xmm5
%
xmm5
.
byte
15
194
238
2
/
/
cmpleps
%
xmm6
%
xmm5
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
88
246
/
/
addps
%
xmm6
%
xmm6
.
byte
15
40
125
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm7
.
byte
15
92
254
/
/
subps
%
xmm6
%
xmm7
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
15
84
213
/
/
andps
%
xmm5
%
xmm2
.
byte
15
85
239
/
/
andnps
%
xmm7
%
xmm5
.
byte
15
86
234
/
/
orps
%
xmm2
%
xmm5
.
byte
15
88
109
232
/
/
addps
-
0x18
(
%
ebp
)
%
xmm5
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
104
/
/
add
0x68
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_overlay_sse2
.
globl
_sk_overlay_sse2
FUNCTION
(
_sk_overlay_sse2
)
_sk_overlay_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
104
/
/
sub
0x68
%
esp
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
41
85
152
/
/
movaps
%
xmm2
-
0x68
(
%
ebp
)
.
byte
15
41
77
168
/
/
movaps
%
xmm1
-
0x58
(
%
ebp
)
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
232
0
0
0
0
/
/
call
dc2
<
_sk_overlay_sse2
+
0x1a
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
80
64
/
/
movaps
0x40
(
%
eax
)
%
xmm2
.
byte
15
40
161
158
249
0
0
/
/
movaps
0xf99e
(
%
ecx
)
%
xmm4
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
92
242
/
/
subps
%
xmm2
%
xmm6
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
41
101
200
/
/
movaps
%
xmm4
-
0x38
(
%
ebp
)
.
byte
15
40
64
16
/
/
movaps
0x10
(
%
eax
)
%
xmm0
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
92
248
/
/
subps
%
xmm0
%
xmm7
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
41
109
184
/
/
movaps
%
xmm5
-
0x48
(
%
ebp
)
.
byte
15
88
228
/
/
addps
%
xmm4
%
xmm4
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
40
77
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
194
202
2
/
/
cmpleps
%
xmm2
%
xmm1
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
15
85
205
/
/
andnps
%
xmm5
%
xmm1
.
byte
15
86
200
/
/
orps
%
xmm0
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
40
88
32
/
/
movaps
0x20
(
%
eax
)
%
xmm3
.
byte
15
40
101
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
125
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm7
.
byte
15
92
248
/
/
subps
%
xmm0
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
88
255
/
/
addps
%
xmm7
%
xmm7
.
byte
15
40
77
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm1
.
byte
15
92
207
/
/
subps
%
xmm7
%
xmm1
.
byte
15
88
237
/
/
addps
%
xmm5
%
xmm5
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
194
239
2
/
/
cmpleps
%
xmm7
%
xmm5
.
byte
15
88
219
/
/
addps
%
xmm3
%
xmm3
.
byte
15
84
221
/
/
andps
%
xmm5
%
xmm3
.
byte
15
85
233
/
/
andnps
%
xmm1
%
xmm5
.
byte
15
86
235
/
/
orps
%
xmm3
%
xmm5
.
byte
15
88
236
/
/
addps
%
xmm4
%
xmm5
.
byte
15
40
93
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm3
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
40
72
48
/
/
movaps
0x30
(
%
eax
)
%
xmm1
.
byte
15
40
101
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
88
246
/
/
addps
%
xmm6
%
xmm6
.
byte
15
194
247
2
/
/
cmpleps
%
xmm7
%
xmm6
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
92
249
/
/
subps
%
xmm1
%
xmm7
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
88
227
/
/
addps
%
xmm3
%
xmm4
.
byte
15
92
223
/
/
subps
%
xmm7
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
219
/
/
addps
%
xmm3
%
xmm3
.
byte
15
40
125
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm7
.
byte
15
92
251
/
/
subps
%
xmm3
%
xmm7
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
84
206
/
/
andps
%
xmm6
%
xmm1
.
byte
15
85
247
/
/
andnps
%
xmm7
%
xmm6
.
byte
15
86
241
/
/
orps
%
xmm1
%
xmm6
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
120
/
/
add
0x78
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_softlight_sse2
.
globl
_sk_softlight_sse2
FUNCTION
(
_sk_softlight_sse2
)
_sk_softlight_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
129
236
184
0
0
0
/
/
sub
0xb8
%
esp
.
byte
15
41
93
168
/
/
movaps
%
xmm3
-
0x58
(
%
ebp
)
.
byte
15
41
149
88
255
255
255
/
/
movaps
%
xmm2
-
0xa8
(
%
ebp
)
.
byte
15
41
77
136
/
/
movaps
%
xmm1
-
0x78
(
%
ebp
)
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
88
64
/
/
movaps
0x40
(
%
eax
)
%
xmm3
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
194
195
1
/
/
cmpltps
%
xmm3
%
xmm0
.
byte
15
41
69
200
/
/
movaps
%
xmm0
-
0x38
(
%
ebp
)
.
byte
15
40
104
16
/
/
movaps
0x10
(
%
eax
)
%
xmm5
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
15
94
211
/
/
divps
%
xmm3
%
xmm2
.
byte
15
84
208
/
/
andps
%
xmm0
%
xmm2
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
89
246
/
/
mulps
%
xmm6
%
xmm6
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
232
0
0
0
0
/
/
call
f52
<
_sk_softlight_sse2
+
0x52
>
.
byte
89
/
/
pop
%
ecx
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
40
137
126
248
0
0
/
/
movaps
0xf87e
(
%
ecx
)
%
xmm1
.
byte
15
41
141
104
255
255
255
/
/
movaps
%
xmm1
-
0x98
(
%
ebp
)
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
40
185
14
248
0
0
/
/
movaps
0xf80e
(
%
ecx
)
%
xmm7
.
byte
15
41
125
232
/
/
movaps
%
xmm7
-
0x18
(
%
ebp
)
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
15
82
242
/
/
rsqrtps
%
xmm2
%
xmm6
.
byte
15
83
206
/
/
rcpps
%
xmm6
%
xmm1
.
byte
15
92
202
/
/
subps
%
xmm2
%
xmm1
.
byte
15
40
161
142
248
0
0
/
/
movaps
0xf88e
(
%
ecx
)
%
xmm4
.
byte
15
41
101
152
/
/
movaps
%
xmm4
-
0x68
(
%
ebp
)
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
41
117
184
/
/
movaps
%
xmm6
-
0x48
(
%
ebp
)
.
byte
15
194
198
2
/
/
cmpleps
%
xmm6
%
xmm0
.
byte
15
84
208
/
/
andps
%
xmm0
%
xmm2
.
byte
15
85
193
/
/
andnps
%
xmm1
%
xmm0
.
byte
15
86
194
/
/
orps
%
xmm2
%
xmm0
.
byte
15
40
85
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm2
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
40
93
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm3
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
15
89
253
/
/
mulps
%
xmm5
%
xmm7
.
byte
15
40
101
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
41
141
120
255
255
255
/
/
movaps
%
xmm1
-
0x88
(
%
ebp
)
.
byte
15
40
117
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm6
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
41
101
216
/
/
movaps
%
xmm4
-
0x28
(
%
ebp
)
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
245
/
/
addps
%
xmm5
%
xmm6
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
194
211
2
/
/
cmpleps
%
xmm3
%
xmm2
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
84
250
/
/
andps
%
xmm2
%
xmm7
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
15
86
215
/
/
orps
%
xmm7
%
xmm2
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
41
149
72
255
255
255
/
/
movaps
%
xmm2
-
0xb8
(
%
ebp
)
.
byte
15
40
64
32
/
/
movaps
0x20
(
%
eax
)
%
xmm0
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
94
101
184
/
/
divps
-
0x48
(
%
ebp
)
%
xmm4
.
byte
15
84
101
200
/
/
andps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
88
201
/
/
addps
%
xmm1
%
xmm1
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
89
219
/
/
mulps
%
xmm3
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
88
141
104
255
255
255
/
/
addps
-
0x98
(
%
ebp
)
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
40
125
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm7
.
byte
15
92
252
/
/
subps
%
xmm4
%
xmm7
.
byte
15
82
220
/
/
rsqrtps
%
xmm4
%
xmm3
.
byte
15
83
235
/
/
rcpps
%
xmm3
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
89
101
152
/
/
mulps
-
0x68
(
%
ebp
)
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
93
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm3
.
byte
15
88
219
/
/
addps
%
xmm3
%
xmm3
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
40
117
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
194
69
184
2
/
/
cmpleps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
85
197
/
/
andnps
%
xmm5
%
xmm0
.
byte
15
86
196
/
/
orps
%
xmm4
%
xmm0
.
byte
15
40
101
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm4
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
109
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm5
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
40
77
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm1
.
byte
15
89
141
120
255
255
255
/
/
mulps
-
0x88
(
%
ebp
)
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
194
221
2
/
/
cmpleps
%
xmm5
%
xmm3
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
84
251
/
/
andps
%
xmm3
%
xmm7
.
byte
15
85
216
/
/
andnps
%
xmm0
%
xmm3
.
byte
15
86
223
/
/
orps
%
xmm7
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
40
64
48
/
/
movaps
0x30
(
%
eax
)
%
xmm0
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
94
252
/
/
divps
%
xmm4
%
xmm7
.
byte
15
84
125
200
/
/
andps
-
0x38
(
%
ebp
)
%
xmm7
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
92
207
/
/
subps
%
xmm7
%
xmm1
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
173
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm5
.
byte
15
88
239
/
/
addps
%
xmm7
%
xmm5
.
byte
15
40
101
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
82
207
/
/
rsqrtps
%
xmm7
%
xmm1
.
byte
15
83
201
/
/
rcpps
%
xmm1
%
xmm1
.
byte
15
92
207
/
/
subps
%
xmm7
%
xmm1
.
byte
15
41
77
200
/
/
movaps
%
xmm1
-
0x38
(
%
ebp
)
.
byte
15
88
255
/
/
addps
%
xmm7
%
xmm7
.
byte
15
88
255
/
/
addps
%
xmm7
%
xmm7
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
89
210
/
/
mulps
%
xmm2
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
40
173
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm5
.
byte
15
88
237
/
/
addps
%
xmm5
%
xmm5
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
15
40
101
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
89
226
/
/
mulps
%
xmm2
%
xmm4
.
byte
15
88
230
/
/
addps
%
xmm6
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
41
101
232
/
/
movaps
%
xmm4
-
0x18
(
%
ebp
)
.
byte
15
40
101
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
40
125
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm7
.
byte
15
194
199
2
/
/
cmpleps
%
xmm7
%
xmm0
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
15
85
69
200
/
/
andnps
-
0x38
(
%
ebp
)
%
xmm0
.
byte
15
86
193
/
/
orps
%
xmm1
%
xmm0
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
40
85
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm2
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
40
141
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm1
.
byte
15
89
141
120
255
255
255
/
/
mulps
-
0x88
(
%
ebp
)
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
194
234
2
/
/
cmpleps
%
xmm2
%
xmm5
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
84
213
/
/
andps
%
xmm5
%
xmm2
.
byte
15
85
232
/
/
andnps
%
xmm0
%
xmm5
.
byte
15
86
234
/
/
orps
%
xmm2
%
xmm5
.
byte
15
88
233
/
/
addps
%
xmm1
%
xmm5
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
133
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
129
196
200
0
0
0
/
/
add
0xc8
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_hue_sse2
.
globl
_sk_hue_sse2
FUNCTION
(
_sk_hue_sse2
)
_sk_hue_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
129
236
184
0
0
0
/
/
sub
0xb8
%
esp
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
72
16
/
/
movaps
0x10
(
%
eax
)
%
xmm1
.
byte
15
40
64
32
/
/
movaps
0x20
(
%
eax
)
%
xmm0
.
byte
15
41
69
136
/
/
movaps
%
xmm0
-
0x78
(
%
ebp
)
.
byte
15
40
88
48
/
/
movaps
0x30
(
%
eax
)
%
xmm3
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
95
211
/
/
maxps
%
xmm3
%
xmm2
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
41
77
152
/
/
movaps
%
xmm1
-
0x68
(
%
ebp
)
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
15
93
195
/
/
minps
%
xmm3
%
xmm0
.
byte
15
93
200
/
/
minps
%
xmm0
%
xmm1
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
41
101
168
/
/
movaps
%
xmm4
-
0x58
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
1213
<
_sk_hue_sse2
+
0x4f
>
.
byte
89
/
/
pop
%
ecx
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
40
72
64
/
/
movaps
0x40
(
%
eax
)
%
xmm1
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
15
40
169
77
245
0
0
/
/
movaps
0xf54d
(
%
ecx
)
%
xmm5
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
41
133
72
255
255
255
/
/
movaps
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
41
181
104
255
255
255
/
/
movaps
%
xmm6
-
0x98
(
%
ebp
)
.
byte
15
40
69
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
41
173
88
255
255
255
/
/
movaps
%
xmm5
-
0xa8
(
%
ebp
)
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
41
85
184
/
/
movaps
%
xmm2
-
0x48
(
%
ebp
)
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
93
200
/
/
minps
%
xmm0
%
xmm1
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
93
249
/
/
minps
%
xmm1
%
xmm7
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
95
200
/
/
maxps
%
xmm0
%
xmm1
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
92
231
/
/
subps
%
xmm7
%
xmm4
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
92
239
/
/
subps
%
xmm7
%
xmm5
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
15
40
77
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm1
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
41
173
120
255
255
255
/
/
movaps
%
xmm5
-
0x88
(
%
ebp
)
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
153
221
245
0
0
/
/
movaps
0xf5dd
(
%
ecx
)
%
xmm3
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
117
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm6
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
40
153
237
245
0
0
/
/
movaps
0xf5ed
(
%
ecx
)
%
xmm3
.
byte
15
40
109
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm5
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
88
249
/
/
addps
%
xmm1
%
xmm7
.
byte
15
40
137
77
245
0
0
/
/
movaps
0xf54d
(
%
ecx
)
%
xmm1
.
byte
15
92
77
184
/
/
subps
-
0x48
(
%
ebp
)
%
xmm1
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
41
117
152
/
/
movaps
%
xmm6
-
0x68
(
%
ebp
)
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
41
109
136
/
/
movaps
%
xmm5
-
0x78
(
%
ebp
)
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
40
77
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm1
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
41
117
168
/
/
movaps
%
xmm6
-
0x58
(
%
ebp
)
.
byte
15
40
169
253
245
0
0
/
/
movaps
0xf5fd
(
%
ecx
)
%
xmm5
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
94
212
/
/
divps
%
xmm4
%
xmm2
.
byte
15
40
181
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm6
.
byte
15
94
244
/
/
divps
%
xmm4
%
xmm6
.
byte
15
94
196
/
/
divps
%
xmm4
%
xmm0
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
194
231
4
/
/
cmpneqps
%
xmm7
%
xmm4
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
84
244
/
/
andps
%
xmm4
%
xmm6
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
89
69
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
89
77
184
/
/
mulps
-
0x48
(
%
ebp
)
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
93
193
/
/
minps
%
xmm1
%
xmm0
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
93
224
/
/
minps
%
xmm0
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
95
193
/
/
maxps
%
xmm1
%
xmm0
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
95
248
/
/
maxps
%
xmm0
%
xmm7
.
byte
15
40
69
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
88
235
/
/
addps
%
xmm3
%
xmm5
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
194
196
2
/
/
cmpleps
%
xmm4
%
xmm0
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
15
92
220
/
/
subps
%
xmm4
%
xmm3
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
94
227
/
/
divps
%
xmm3
%
xmm4
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
85
220
/
/
andnps
%
xmm4
%
xmm3
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
86
227
/
/
orps
%
xmm3
%
xmm4
.
byte
15
40
85
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm2
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
93
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
92
218
/
/
subps
%
xmm2
%
xmm3
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
194
215
1
/
/
cmpltps
%
xmm7
%
xmm2
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
85
220
/
/
andnps
%
xmm4
%
xmm3
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
92
253
/
/
subps
%
xmm5
%
xmm7
.
byte
15
94
231
/
/
divps
%
xmm7
%
xmm4
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
15
84
226
/
/
andps
%
xmm2
%
xmm4
.
byte
15
86
101
200
/
/
orps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
41
101
200
/
/
movaps
%
xmm4
-
0x38
(
%
ebp
)
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
94
93
232
/
/
divps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
85
227
/
/
andnps
%
xmm3
%
xmm4
.
byte
15
84
240
/
/
andps
%
xmm0
%
xmm6
.
byte
15
86
244
/
/
orps
%
xmm4
%
xmm6
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
85
220
/
/
andnps
%
xmm4
%
xmm3
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
40
117
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
94
231
/
/
divps
%
xmm7
%
xmm4
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
15
84
226
/
/
andps
%
xmm2
%
xmm4
.
byte
15
86
227
/
/
orps
%
xmm3
%
xmm4
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
94
93
232
/
/
divps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
15
85
195
/
/
andnps
%
xmm3
%
xmm0
.
byte
15
86
193
/
/
orps
%
xmm1
%
xmm0
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
85
200
/
/
andnps
%
xmm0
%
xmm1
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
94
199
/
/
divps
%
xmm7
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
84
194
/
/
andps
%
xmm2
%
xmm0
.
byte
15
86
193
/
/
orps
%
xmm1
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
95
193
/
/
maxps
%
xmm1
%
xmm0
.
byte
15
40
157
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm3
.
byte
15
88
93
152
/
/
addps
-
0x68
(
%
ebp
)
%
xmm3
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
40
141
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm1
.
byte
15
88
77
136
/
/
addps
-
0x78
(
%
ebp
)
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
40
85
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm2
.
byte
15
88
149
88
255
255
255
/
/
addps
-
0xa8
(
%
ebp
)
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
129
196
200
0
0
0
/
/
add
0xc8
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_saturation_sse2
.
globl
_sk_saturation_sse2
FUNCTION
(
_sk_saturation_sse2
)
_sk_saturation_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
129
236
216
0
0
0
/
/
sub
0xd8
%
esp
.
byte
15
41
149
40
255
255
255
/
/
movaps
%
xmm2
-
0xd8
(
%
ebp
)
.
byte
15
41
141
56
255
255
255
/
/
movaps
%
xmm1
-
0xc8
(
%
ebp
)
.
byte
15
41
133
72
255
255
255
/
/
movaps
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
112
16
/
/
movaps
0x10
(
%
eax
)
%
xmm6
.
byte
15
41
117
184
/
/
movaps
%
xmm6
-
0x48
(
%
ebp
)
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
95
236
/
/
maxps
%
xmm4
%
xmm5
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
93
226
/
/
minps
%
xmm2
%
xmm4
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
93
212
/
/
minps
%
xmm4
%
xmm2
.
byte
15
40
64
32
/
/
movaps
0x20
(
%
eax
)
%
xmm0
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
40
120
48
/
/
movaps
0x30
(
%
eax
)
%
xmm7
.
byte
15
41
125
200
/
/
movaps
%
xmm7
-
0x38
(
%
ebp
)
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
93
207
/
/
minps
%
xmm7
%
xmm1
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
93
209
/
/
minps
%
xmm1
%
xmm2
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
95
207
/
/
maxps
%
xmm7
%
xmm1
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
92
218
/
/
subps
%
xmm2
%
xmm3
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
15
40
64
64
/
/
movaps
0x40
(
%
eax
)
%
xmm0
.
byte
15
41
69
152
/
/
movaps
%
xmm0
-
0x68
(
%
ebp
)
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
41
77
168
/
/
movaps
%
xmm1
-
0x58
(
%
ebp
)
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
89
253
/
/
mulps
%
xmm5
%
xmm7
.
byte
232
0
0
0
0
/
/
call
1567
<
_sk_saturation_sse2
+
0xb0
>
.
byte
89
/
/
pop
%
ecx
.
byte
15
40
153
137
242
0
0
/
/
movaps
0xf289
(
%
ecx
)
%
xmm3
.
byte
15
41
157
88
255
255
255
/
/
movaps
%
xmm3
-
0xa8
(
%
ebp
)
.
byte
15
40
77
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm1
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
40
153
153
242
0
0
/
/
movaps
0xf299
(
%
ecx
)
%
xmm3
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
40
129
249
241
0
0
/
/
movaps
0xf1f9
(
%
ecx
)
%
xmm0
.
byte
15
41
133
104
255
255
255
/
/
movaps
%
xmm0
-
0x98
(
%
ebp
)
.
byte
15
92
69
216
/
/
subps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
41
77
184
/
/
movaps
%
xmm1
-
0x48
(
%
ebp
)
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
41
117
136
/
/
movaps
%
xmm6
-
0x78
(
%
ebp
)
.
byte
15
40
69
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
41
141
120
255
255
255
/
/
movaps
%
xmm1
-
0x88
(
%
ebp
)
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
129
169
242
0
0
/
/
movaps
0xf2a9
(
%
ecx
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
40
109
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm5
.
byte
15
94
236
/
/
divps
%
xmm4
%
xmm5
.
byte
15
94
212
/
/
divps
%
xmm4
%
xmm2
.
byte
15
94
252
/
/
divps
%
xmm4
%
xmm7
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
194
227
4
/
/
cmpneqps
%
xmm3
%
xmm4
.
byte
15
84
236
/
/
andps
%
xmm4
%
xmm5
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
84
231
/
/
andps
%
xmm7
%
xmm4
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
40
165
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm4
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
89
125
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
89
77
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
88
233
/
/
addps
%
xmm1
%
xmm5
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
93
217
/
/
minps
%
xmm1
%
xmm3
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
93
251
/
/
minps
%
xmm3
%
xmm7
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
95
217
/
/
maxps
%
xmm1
%
xmm3
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
95
243
/
/
maxps
%
xmm3
%
xmm6
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
88
220
/
/
addps
%
xmm4
%
xmm3
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
194
223
2
/
/
cmpleps
%
xmm7
%
xmm3
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
92
215
/
/
subps
%
xmm7
%
xmm2
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
92
248
/
/
subps
%
xmm0
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
94
250
/
/
divps
%
xmm2
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
85
215
/
/
andnps
%
xmm7
%
xmm2
.
byte
15
84
235
/
/
andps
%
xmm3
%
xmm5
.
byte
15
86
234
/
/
orps
%
xmm2
%
xmm5
.
byte
15
40
125
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm7
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
40
101
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm4
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
231
/
/
addps
%
xmm7
%
xmm4
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
41
101
216
/
/
movaps
%
xmm4
-
0x28
(
%
ebp
)
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
194
214
1
/
/
cmpltps
%
xmm6
%
xmm2
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
85
229
/
/
andnps
%
xmm5
%
xmm4
.
byte
15
92
232
/
/
subps
%
xmm0
%
xmm5
.
byte
15
92
248
/
/
subps
%
xmm0
%
xmm7
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
15
94
238
/
/
divps
%
xmm6
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
84
234
/
/
andps
%
xmm2
%
xmm5
.
byte
15
86
236
/
/
orps
%
xmm4
%
xmm5
.
byte
15
41
109
168
/
/
movaps
%
xmm5
-
0x58
(
%
ebp
)
.
byte
15
40
101
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
94
101
232
/
/
divps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
85
236
/
/
andnps
%
xmm4
%
xmm5
.
byte
15
40
101
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
84
227
/
/
andps
%
xmm3
%
xmm4
.
byte
15
86
229
/
/
orps
%
xmm5
%
xmm4
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
85
236
/
/
andnps
%
xmm4
%
xmm5
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
94
230
/
/
divps
%
xmm6
%
xmm4
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
15
84
226
/
/
andps
%
xmm2
%
xmm4
.
byte
15
86
229
/
/
orps
%
xmm5
%
xmm4
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
94
101
232
/
/
divps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
15
84
203
/
/
andps
%
xmm3
%
xmm1
.
byte
15
85
220
/
/
andnps
%
xmm4
%
xmm3
.
byte
15
86
217
/
/
orps
%
xmm1
%
xmm3
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
85
203
/
/
andnps
%
xmm3
%
xmm1
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
15
94
222
/
/
divps
%
xmm6
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
84
218
/
/
andps
%
xmm2
%
xmm3
.
byte
15
86
217
/
/
orps
%
xmm1
%
xmm3
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
40
77
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm1
.
byte
15
95
200
/
/
maxps
%
xmm0
%
xmm1
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
95
216
/
/
maxps
%
xmm0
%
xmm3
.
byte
15
40
165
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm4
.
byte
15
92
101
152
/
/
subps
-
0x68
(
%
ebp
)
%
xmm4
.
byte
15
40
133
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
69
184
/
/
addps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
40
141
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
77
136
/
/
addps
-
0x78
(
%
ebp
)
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
89
165
40
255
255
255
/
/
mulps
-
0xd8
(
%
ebp
)
%
xmm4
.
byte
15
40
149
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
129
196
232
0
0
0
/
/
add
0xe8
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_color_sse2
.
globl
_sk_color_sse2
FUNCTION
(
_sk_color_sse2
)
_sk_color_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
129
236
200
0
0
0
/
/
sub
0xc8
%
esp
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
41
85
152
/
/
movaps
%
xmm2
-
0x68
(
%
ebp
)
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
41
165
104
255
255
255
/
/
movaps
%
xmm4
-
0x98
(
%
ebp
)
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
41
173
120
255
255
255
/
/
movaps
%
xmm5
-
0x88
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
17c2
<
_sk_color_sse2
+
0x2a
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
80
16
/
/
movaps
0x10
(
%
eax
)
%
xmm2
.
byte
15
40
72
32
/
/
movaps
0x20
(
%
eax
)
%
xmm1
.
byte
15
40
177
46
240
0
0
/
/
movaps
0xf02e
(
%
ecx
)
%
xmm6
.
byte
15
41
117
216
/
/
movaps
%
xmm6
-
0x28
(
%
ebp
)
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
40
153
62
240
0
0
/
/
movaps
0xf03e
(
%
ecx
)
%
xmm3
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
88
240
/
/
addps
%
xmm0
%
xmm6
.
byte
15
40
129
158
239
0
0
/
/
movaps
0xef9e
(
%
ecx
)
%
xmm0
.
byte
15
41
69
168
/
/
movaps
%
xmm0
-
0x58
(
%
ebp
)
.
byte
15
92
69
200
/
/
subps
-
0x38
(
%
ebp
)
%
xmm0
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
41
149
56
255
255
255
/
/
movaps
%
xmm2
-
0xc8
(
%
ebp
)
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
41
141
88
255
255
255
/
/
movaps
%
xmm1
-
0xa8
(
%
ebp
)
.
byte
15
40
80
48
/
/
movaps
0x30
(
%
eax
)
%
xmm2
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
41
133
72
255
255
255
/
/
movaps
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
15
40
137
78
240
0
0
/
/
movaps
0xf04e
(
%
ecx
)
%
xmm1
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
40
88
64
/
/
movaps
0x40
(
%
eax
)
%
xmm3
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
89
109
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
89
125
184
/
/
mulps
-
0x48
(
%
ebp
)
%
xmm7
.
byte
15
88
253
/
/
addps
%
xmm5
%
xmm7
.
byte
15
40
109
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm5
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
41
109
168
/
/
movaps
%
xmm5
-
0x58
(
%
ebp
)
.
byte
15
40
109
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm5
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
101
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
41
101
232
/
/
movaps
%
xmm4
-
0x18
(
%
ebp
)
.
byte
15
88
235
/
/
addps
%
xmm3
%
xmm5
.
byte
15
41
109
200
/
/
movaps
%
xmm5
-
0x38
(
%
ebp
)
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
89
109
152
/
/
mulps
-
0x68
(
%
ebp
)
%
xmm5
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
93
218
/
/
minps
%
xmm2
%
xmm3
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
93
227
/
/
minps
%
xmm3
%
xmm4
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
95
218
/
/
maxps
%
xmm2
%
xmm3
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
95
235
/
/
maxps
%
xmm3
%
xmm5
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
40
125
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
194
252
2
/
/
cmpleps
%
xmm4
%
xmm7
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
92
220
/
/
subps
%
xmm4
%
xmm3
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
94
227
/
/
divps
%
xmm3
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
15
85
220
/
/
andnps
%
xmm4
%
xmm3
.
byte
15
84
199
/
/
andps
%
xmm7
%
xmm0
.
byte
15
86
195
/
/
orps
%
xmm3
%
xmm0
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
15
40
101
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
92
220
/
/
subps
%
xmm4
%
xmm3
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
41
101
216
/
/
movaps
%
xmm4
-
0x28
(
%
ebp
)
.
byte
15
194
229
1
/
/
cmpltps
%
xmm5
%
xmm4
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
85
216
/
/
andnps
%
xmm0
%
xmm3
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
41
109
136
/
/
movaps
%
xmm5
-
0x78
(
%
ebp
)
.
byte
15
94
197
/
/
divps
%
xmm5
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
15
86
69
232
/
/
orps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
94
93
184
/
/
divps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
41
125
232
/
/
movaps
%
xmm7
-
0x18
(
%
ebp
)
.
byte
15
40
109
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm5
.
byte
15
85
235
/
/
andnps
%
xmm3
%
xmm5
.
byte
15
41
109
232
/
/
movaps
%
xmm5
-
0x18
(
%
ebp
)
.
byte
15
84
247
/
/
andps
%
xmm7
%
xmm6
.
byte
15
86
117
232
/
/
orps
-
0x18
(
%
ebp
)
%
xmm6
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
85
222
/
/
andnps
%
xmm6
%
xmm3
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
15
89
117
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm6
.
byte
15
40
109
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm5
.
byte
15
94
245
/
/
divps
%
xmm5
%
xmm6
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
15
84
244
/
/
andps
%
xmm4
%
xmm6
.
byte
15
86
243
/
/
orps
%
xmm3
%
xmm6
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
94
93
184
/
/
divps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
84
215
/
/
andps
%
xmm7
%
xmm2
.
byte
15
85
251
/
/
andnps
%
xmm3
%
xmm7
.
byte
15
86
250
/
/
orps
%
xmm2
%
xmm7
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
15
85
215
/
/
andnps
%
xmm7
%
xmm2
.
byte
15
92
249
/
/
subps
%
xmm1
%
xmm7
.
byte
15
89
125
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm7
.
byte
15
94
253
/
/
divps
%
xmm5
%
xmm7
.
byte
15
88
249
/
/
addps
%
xmm1
%
xmm7
.
byte
15
84
252
/
/
andps
%
xmm4
%
xmm7
.
byte
15
86
250
/
/
orps
%
xmm2
%
xmm7
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
95
193
/
/
maxps
%
xmm1
%
xmm0
.
byte
15
95
241
/
/
maxps
%
xmm1
%
xmm6
.
byte
15
95
249
/
/
maxps
%
xmm1
%
xmm7
.
byte
15
40
141
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm1
.
byte
15
40
85
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
88
141
56
255
255
255
/
/
addps
-
0xc8
(
%
ebp
)
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
141
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm1
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
88
141
88
255
255
255
/
/
addps
-
0xa8
(
%
ebp
)
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
40
85
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
149
72
255
255
255
/
/
addps
-
0xb8
(
%
ebp
)
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
129
196
216
0
0
0
/
/
add
0xd8
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_luminosity_sse2
.
globl
_sk_luminosity_sse2
FUNCTION
(
_sk_luminosity_sse2
)
_sk_luminosity_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
129
236
184
0
0
0
/
/
sub
0xb8
%
esp
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
232
0
0
0
0
/
/
call
1a25
<
_sk_luminosity_sse2
+
0x11
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
161
203
237
0
0
/
/
movaps
0xedcb
(
%
ecx
)
%
xmm4
.
byte
15
41
101
232
/
/
movaps
%
xmm4
-
0x18
(
%
ebp
)
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
40
161
219
237
0
0
/
/
movaps
0xeddb
(
%
ecx
)
%
xmm4
.
byte
15
41
101
216
/
/
movaps
%
xmm4
-
0x28
(
%
ebp
)
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
40
80
64
/
/
movaps
0x40
(
%
eax
)
%
xmm2
.
byte
15
41
85
184
/
/
movaps
%
xmm2
-
0x48
(
%
ebp
)
.
byte
15
40
161
59
237
0
0
/
/
movaps
0xed3b
(
%
ecx
)
%
xmm4
.
byte
15
40
252
/
/
movaps
%
xmm4
%
xmm7
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
41
133
72
255
255
255
/
/
movaps
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
15
41
141
88
255
255
255
/
/
movaps
%
xmm1
-
0xa8
(
%
ebp
)
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
41
189
104
255
255
255
/
/
movaps
%
xmm7
-
0x98
(
%
ebp
)
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
137
235
237
0
0
/
/
movaps
0xedeb
(
%
ecx
)
%
xmm1
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
104
16
/
/
movaps
0x10
(
%
eax
)
%
xmm5
.
byte
15
41
109
136
/
/
movaps
%
xmm5
-
0x78
(
%
ebp
)
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
40
112
32
/
/
movaps
0x20
(
%
eax
)
%
xmm6
.
byte
15
41
117
152
/
/
movaps
%
xmm6
-
0x68
(
%
ebp
)
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
89
69
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
89
125
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
41
93
168
/
/
movaps
%
xmm3
-
0x58
(
%
ebp
)
.
byte
15
40
69
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
41
69
136
/
/
movaps
%
xmm0
-
0x78
(
%
ebp
)
.
byte
15
40
69
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
41
69
152
/
/
movaps
%
xmm0
-
0x68
(
%
ebp
)
.
byte
15
40
64
48
/
/
movaps
0x30
(
%
eax
)
%
xmm0
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
41
165
120
255
255
255
/
/
movaps
%
xmm4
-
0x88
(
%
ebp
)
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
93
194
/
/
minps
%
xmm2
%
xmm0
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
93
224
/
/
minps
%
xmm0
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
95
194
/
/
maxps
%
xmm2
%
xmm0
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
95
248
/
/
maxps
%
xmm0
%
xmm7
.
byte
15
41
125
200
/
/
movaps
%
xmm7
-
0x38
(
%
ebp
)
.
byte
15
40
69
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
40
125
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
194
196
2
/
/
cmpleps
%
xmm4
%
xmm0
.
byte
15
40
249
/
/
movaps
%
xmm1
%
xmm7
.
byte
15
92
252
/
/
subps
%
xmm4
%
xmm7
.
byte
15
41
125
216
/
/
movaps
%
xmm7
-
0x28
(
%
ebp
)
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
94
231
/
/
divps
%
xmm7
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
85
252
/
/
andnps
%
xmm4
%
xmm7
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
15
86
239
/
/
orps
%
xmm7
%
xmm5
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
15
40
231
/
/
movaps
%
xmm7
%
xmm4
.
byte
15
40
93
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm3
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
92
220
/
/
subps
%
xmm4
%
xmm3
.
byte
15
41
93
168
/
/
movaps
%
xmm3
-
0x58
(
%
ebp
)
.
byte
15
41
101
232
/
/
movaps
%
xmm4
-
0x18
(
%
ebp
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
40
125
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm7
.
byte
15
194
223
1
/
/
cmpltps
%
xmm7
%
xmm3
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
85
229
/
/
andnps
%
xmm5
%
xmm4
.
byte
15
41
101
184
/
/
movaps
%
xmm4
-
0x48
(
%
ebp
)
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
40
101
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
41
101
232
/
/
movaps
%
xmm4
-
0x18
(
%
ebp
)
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
92
249
/
/
subps
%
xmm1
%
xmm7
.
byte
15
41
125
200
/
/
movaps
%
xmm7
-
0x38
(
%
ebp
)
.
byte
15
94
239
/
/
divps
%
xmm7
%
xmm5
.
byte
15
88
233
/
/
addps
%
xmm1
%
xmm5
.
byte
15
84
235
/
/
andps
%
xmm3
%
xmm5
.
byte
15
86
109
184
/
/
orps
-
0x48
(
%
ebp
)
%
xmm5
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
94
101
216
/
/
divps
-
0x28
(
%
ebp
)
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
85
252
/
/
andnps
%
xmm4
%
xmm7
.
byte
15
84
240
/
/
andps
%
xmm0
%
xmm6
.
byte
15
86
247
/
/
orps
%
xmm7
%
xmm6
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
85
230
/
/
andnps
%
xmm6
%
xmm4
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
15
40
125
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm7
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
94
117
200
/
/
divps
-
0x38
(
%
ebp
)
%
xmm6
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
15
84
243
/
/
andps
%
xmm3
%
xmm6
.
byte
15
86
244
/
/
orps
%
xmm4
%
xmm6
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
94
101
216
/
/
divps
-
0x28
(
%
ebp
)
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
84
208
/
/
andps
%
xmm0
%
xmm2
.
byte
15
85
196
/
/
andnps
%
xmm4
%
xmm0
.
byte
15
86
194
/
/
orps
%
xmm2
%
xmm0
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
94
69
200
/
/
divps
-
0x38
(
%
ebp
)
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
84
195
/
/
andps
%
xmm3
%
xmm0
.
byte
15
86
194
/
/
orps
%
xmm2
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
95
233
/
/
maxps
%
xmm1
%
xmm5
.
byte
15
95
241
/
/
maxps
%
xmm1
%
xmm6
.
byte
15
95
193
/
/
maxps
%
xmm1
%
xmm0
.
byte
15
40
157
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm3
.
byte
15
88
93
136
/
/
addps
-
0x78
(
%
ebp
)
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
40
141
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm1
.
byte
15
88
77
152
/
/
addps
-
0x68
(
%
ebp
)
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
40
149
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm2
.
byte
15
88
149
104
255
255
255
/
/
addps
-
0x98
(
%
ebp
)
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
40
93
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
129
196
200
0
0
0
/
/
add
0xc8
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcover_rgba_8888_sse2
.
globl
_sk_srcover_rgba_8888_sse2
FUNCTION
(
_sk_srcover_rgba_8888_sse2
)
_sk_srcover_rgba_8888_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
102
15
127
77
216
/
/
movdqa
%
xmm1
-
0x28
(
%
ebp
)
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
232
0
0
0
0
/
/
call
1c94
<
_sk_srcover_rgba_8888_sse2
+
0x1b
>
.
byte
95
/
/
pop
%
edi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
48
/
/
mov
(
%
eax
)
%
esi
.
byte
139
86
4
/
/
mov
0x4
(
%
esi
)
%
edx
.
byte
15
175
81
4
/
/
imul
0x4
(
%
ecx
)
%
edx
.
byte
193
226
2
/
/
shl
0x2
%
edx
.
byte
3
22
/
/
add
(
%
esi
)
%
edx
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
49
/
/
mov
(
%
ecx
)
%
esi
.
byte
15
133
2
1
0
0
/
/
jne
1db8
<
_sk_srcover_rgba_8888_sse2
+
0x13f
>
.
byte
243
15
111
12
178
/
/
movdqu
(
%
edx
%
esi
4
)
%
xmm1
.
byte
102
15
111
135
140
235
0
0
/
/
movdqa
0xeb8c
(
%
edi
)
%
xmm0
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
114
214
8
/
/
psrld
0x8
%
xmm6
.
byte
102
15
219
240
/
/
pand
%
xmm0
%
xmm6
.
byte
102
15
111
249
/
/
movdqa
%
xmm1
%
xmm7
.
byte
102
15
114
215
16
/
/
psrld
0x10
%
xmm7
.
byte
102
15
219
248
/
/
pand
%
xmm0
%
xmm7
.
byte
15
91
194
/
/
cvtdq2ps
%
xmm2
%
xmm0
.
byte
15
41
65
16
/
/
movaps
%
xmm0
0x10
(
%
ecx
)
.
byte
15
40
167
204
234
0
0
/
/
movaps
0xeacc
(
%
edi
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
40
151
156
235
0
0
/
/
movaps
0xeb9c
(
%
edi
)
%
xmm2
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
91
238
/
/
cvtdq2ps
%
xmm6
%
xmm5
.
byte
15
41
105
32
/
/
movaps
%
xmm5
0x20
(
%
ecx
)
.
byte
15
40
117
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm6
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
91
247
/
/
cvtdq2ps
%
xmm7
%
xmm6
.
byte
15
41
113
48
/
/
movaps
%
xmm6
0x30
(
%
ecx
)
.
byte
15
40
125
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm7
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
102
15
114
209
24
/
/
psrld
0x18
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
41
73
64
/
/
movaps
%
xmm1
0x40
(
%
ecx
)
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
88
227
/
/
addps
%
xmm3
%
xmm4
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
95
200
/
/
maxps
%
xmm0
%
xmm1
.
byte
15
93
202
/
/
minps
%
xmm2
%
xmm1
.
byte
102
15
91
201
/
/
cvtps2dq
%
xmm1
%
xmm1
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
221
/
/
maxps
%
xmm5
%
xmm3
.
byte
15
93
218
/
/
minps
%
xmm2
%
xmm3
.
byte
102
15
91
219
/
/
cvtps2dq
%
xmm3
%
xmm3
.
byte
102
15
114
243
8
/
/
pslld
0x8
%
xmm3
.
byte
102
15
235
217
/
/
por
%
xmm1
%
xmm3
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
15
95
206
/
/
maxps
%
xmm6
%
xmm1
.
byte
15
93
202
/
/
minps
%
xmm2
%
xmm1
.
byte
15
95
252
/
/
maxps
%
xmm4
%
xmm7
.
byte
15
93
250
/
/
minps
%
xmm2
%
xmm7
.
byte
102
15
91
209
/
/
cvtps2dq
%
xmm1
%
xmm2
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
91
207
/
/
cvtps2dq
%
xmm7
%
xmm1
.
byte
102
15
114
241
24
/
/
pslld
0x18
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
235
203
/
/
por
%
xmm3
%
xmm1
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
117
92
/
/
jne
1df3
<
_sk_srcover_rgba_8888_sse2
+
0x17a
>
.
byte
243
15
127
12
178
/
/
movdqu
%
xmm1
(
%
edx
%
esi
4
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
137
216
/
/
mov
%
ebx
%
eax
.
byte
136
69
243
/
/
mov
%
al
-
0xd
(
%
ebp
)
.
byte
128
101
243
3
/
/
andb
0x3
-
0xd
(
%
ebp
)
.
byte
128
125
243
1
/
/
cmpb
0x1
-
0xd
(
%
ebp
)
.
byte
116
80
/
/
je
1e17
<
_sk_srcover_rgba_8888_sse2
+
0x19e
>
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
128
125
243
2
/
/
cmpb
0x2
-
0xd
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
116
21
/
/
je
1de9
<
_sk_srcover_rgba_8888_sse2
+
0x170
>
.
byte
128
125
243
3
/
/
cmpb
0x3
-
0xd
(
%
ebp
)
.
byte
15
133
221
254
255
255
/
/
jne
1cbb
<
_sk_srcover_rgba_8888_sse2
+
0x42
>
.
byte
102
15
110
68
178
8
/
/
movd
0x8
(
%
edx
%
esi
4
)
%
xmm0
.
byte
102
15
112
200
69
/
/
pshufd
0x45
%
xmm0
%
xmm1
.
byte
102
15
18
12
178
/
/
movlpd
(
%
edx
%
esi
4
)
%
xmm1
.
byte
233
200
254
255
255
/
/
jmp
1cbb
<
_sk_srcover_rgba_8888_sse2
+
0x42
>
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
41
/
/
je
1e24
<
_sk_srcover_rgba_8888_sse2
+
0x1ab
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
1e10
<
_sk_srcover_rgba_8888_sse2
+
0x197
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
151
/
/
jne
1d9c
<
_sk_srcover_rgba_8888_sse2
+
0x123
>
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
15
126
84
178
8
/
/
movd
%
xmm2
0x8
(
%
edx
%
esi
4
)
.
byte
102
15
214
12
178
/
/
movq
%
xmm1
(
%
edx
%
esi
4
)
.
byte
235
133
/
/
jmp
1d9c
<
_sk_srcover_rgba_8888_sse2
+
0x123
>
.
byte
102
15
110
12
178
/
/
movd
(
%
edx
%
esi
4
)
%
xmm1
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
233
151
254
255
255
/
/
jmp
1cbb
<
_sk_srcover_rgba_8888_sse2
+
0x42
>
.
byte
102
15
126
12
178
/
/
movd
%
xmm1
(
%
edx
%
esi
4
)
.
byte
233
110
255
255
255
/
/
jmp
1d9c
<
_sk_srcover_rgba_8888_sse2
+
0x123
>
HIDDEN
_sk_srcover_bgra_8888_sse2
.
globl
_sk_srcover_bgra_8888_sse2
FUNCTION
(
_sk_srcover_bgra_8888_sse2
)
_sk_srcover_bgra_8888_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
232
0
0
0
0
/
/
call
1e48
<
_sk_srcover_bgra_8888_sse2
+
0x1a
>
.
byte
95
/
/
pop
%
edi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
48
/
/
mov
(
%
eax
)
%
esi
.
byte
139
86
4
/
/
mov
0x4
(
%
esi
)
%
edx
.
byte
15
175
81
4
/
/
imul
0x4
(
%
ecx
)
%
edx
.
byte
193
226
2
/
/
shl
0x2
%
edx
.
byte
3
22
/
/
add
(
%
esi
)
%
edx
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
49
/
/
mov
(
%
ecx
)
%
esi
.
byte
15
133
252
0
0
0
/
/
jne
1f66
<
_sk_srcover_bgra_8888_sse2
+
0x138
>
.
byte
243
15
111
36
178
/
/
movdqu
(
%
edx
%
esi
4
)
%
xmm4
.
byte
102
15
111
135
216
233
0
0
/
/
movdqa
0xe9d8
(
%
edi
)
%
xmm0
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
114
213
8
/
/
psrld
0x8
%
xmm5
.
byte
102
15
219
232
/
/
pand
%
xmm0
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
114
214
16
/
/
psrld
0x10
%
xmm6
.
byte
102
15
219
240
/
/
pand
%
xmm0
%
xmm6
.
byte
15
91
209
/
/
cvtdq2ps
%
xmm1
%
xmm2
.
byte
15
41
81
48
/
/
movaps
%
xmm2
0x30
(
%
ecx
)
.
byte
15
91
205
/
/
cvtdq2ps
%
xmm5
%
xmm1
.
byte
15
41
73
32
/
/
movaps
%
xmm1
0x20
(
%
ecx
)
.
byte
15
91
198
/
/
cvtdq2ps
%
xmm6
%
xmm0
.
byte
15
41
65
16
/
/
movaps
%
xmm0
0x10
(
%
ecx
)
.
byte
15
40
175
24
233
0
0
/
/
movaps
0xe918
(
%
edi
)
%
xmm5
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
40
183
232
233
0
0
/
/
movaps
0xe9e8
(
%
edi
)
%
xmm6
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
40
125
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
40
125
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
102
15
114
212
24
/
/
psrld
0x18
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
235
/
/
addps
%
xmm3
%
xmm5
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
218
/
/
maxps
%
xmm2
%
xmm3
.
byte
15
93
222
/
/
minps
%
xmm6
%
xmm3
.
byte
102
15
91
219
/
/
cvtps2dq
%
xmm3
%
xmm3
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
93
230
/
/
minps
%
xmm6
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
102
15
114
244
8
/
/
pslld
0x8
%
xmm4
.
byte
102
15
235
227
/
/
por
%
xmm3
%
xmm4
.
byte
102
15
87
219
/
/
xorpd
%
xmm3
%
xmm3
.
byte
15
95
216
/
/
maxps
%
xmm0
%
xmm3
.
byte
15
93
222
/
/
minps
%
xmm6
%
xmm3
.
byte
15
95
253
/
/
maxps
%
xmm5
%
xmm7
.
byte
15
93
254
/
/
minps
%
xmm6
%
xmm7
.
byte
102
15
91
243
/
/
cvtps2dq
%
xmm3
%
xmm6
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
91
223
/
/
cvtps2dq
%
xmm7
%
xmm3
.
byte
102
15
114
243
24
/
/
pslld
0x18
%
xmm3
.
byte
102
15
235
222
/
/
por
%
xmm6
%
xmm3
.
byte
102
15
235
220
/
/
por
%
xmm4
%
xmm3
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
117
86
/
/
jne
1fa1
<
_sk_srcover_bgra_8888_sse2
+
0x173
>
.
byte
243
15
127
28
178
/
/
movdqu
%
xmm3
(
%
edx
%
esi
4
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
137
216
/
/
mov
%
ebx
%
eax
.
byte
136
69
243
/
/
mov
%
al
-
0xd
(
%
ebp
)
.
byte
128
101
243
3
/
/
andb
0x3
-
0xd
(
%
ebp
)
.
byte
128
125
243
1
/
/
cmpb
0x1
-
0xd
(
%
ebp
)
.
byte
116
80
/
/
je
1fc5
<
_sk_srcover_bgra_8888_sse2
+
0x197
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
125
243
2
/
/
cmpb
0x2
-
0xd
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
116
21
/
/
je
1f97
<
_sk_srcover_bgra_8888_sse2
+
0x169
>
.
byte
128
125
243
3
/
/
cmpb
0x3
-
0xd
(
%
ebp
)
.
byte
15
133
227
254
255
255
/
/
jne
1e6f
<
_sk_srcover_bgra_8888_sse2
+
0x41
>
.
byte
102
15
110
68
178
8
/
/
movd
0x8
(
%
edx
%
esi
4
)
%
xmm0
.
byte
102
15
112
224
69
/
/
pshufd
0x45
%
xmm0
%
xmm4
.
byte
102
15
18
36
178
/
/
movlpd
(
%
edx
%
esi
4
)
%
xmm4
.
byte
233
206
254
255
255
/
/
jmp
1e6f
<
_sk_srcover_bgra_8888_sse2
+
0x41
>
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
41
/
/
je
1fd2
<
_sk_srcover_bgra_8888_sse2
+
0x1a4
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
1fbe
<
_sk_srcover_bgra_8888_sse2
+
0x190
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
157
/
/
jne
1f50
<
_sk_srcover_bgra_8888_sse2
+
0x122
>
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
15
126
100
178
8
/
/
movd
%
xmm4
0x8
(
%
edx
%
esi
4
)
.
byte
102
15
214
28
178
/
/
movq
%
xmm3
(
%
edx
%
esi
4
)
.
byte
235
139
/
/
jmp
1f50
<
_sk_srcover_bgra_8888_sse2
+
0x122
>
.
byte
102
15
110
36
178
/
/
movd
(
%
edx
%
esi
4
)
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
233
157
254
255
255
/
/
jmp
1e6f
<
_sk_srcover_bgra_8888_sse2
+
0x41
>
.
byte
102
15
126
28
178
/
/
movd
%
xmm3
(
%
edx
%
esi
4
)
.
byte
233
116
255
255
255
/
/
jmp
1f50
<
_sk_srcover_bgra_8888_sse2
+
0x122
>
HIDDEN
_sk_clamp_0_sse2
.
globl
_sk_clamp_0_sse2
FUNCTION
(
_sk_clamp_0_sse2
)
_sk_clamp_0_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
196
/
/
maxps
%
xmm4
%
xmm0
.
byte
15
95
204
/
/
maxps
%
xmm4
%
xmm1
.
byte
15
95
212
/
/
maxps
%
xmm4
%
xmm2
.
byte
15
95
220
/
/
maxps
%
xmm4
%
xmm3
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clamp_1_sse2
.
globl
_sk_clamp_1_sse2
FUNCTION
(
_sk_clamp_1_sse2
)
_sk_clamp_1_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
2010
<
_sk_clamp_1_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
40
160
80
231
0
0
/
/
movaps
0xe750
(
%
eax
)
%
xmm4
.
byte
15
93
196
/
/
minps
%
xmm4
%
xmm0
.
byte
15
93
204
/
/
minps
%
xmm4
%
xmm1
.
byte
15
93
212
/
/
minps
%
xmm4
%
xmm2
.
byte
15
93
220
/
/
minps
%
xmm4
%
xmm3
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clamp_a_sse2
.
globl
_sk_clamp_a_sse2
FUNCTION
(
_sk_clamp_a_sse2
)
_sk_clamp_a_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
2043
<
_sk_clamp_a_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
93
152
29
231
0
0
/
/
minps
0xe71d
(
%
eax
)
%
xmm3
.
byte
15
93
195
/
/
minps
%
xmm3
%
xmm0
.
byte
15
93
203
/
/
minps
%
xmm3
%
xmm1
.
byte
15
93
211
/
/
minps
%
xmm3
%
xmm2
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clamp_a_dst_sse2
.
globl
_sk_clamp_a_dst_sse2
FUNCTION
(
_sk_clamp_a_dst_sse2
)
_sk_clamp_a_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
2073
<
_sk_clamp_a_dst_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
40
98
64
/
/
movaps
0x40
(
%
edx
)
%
xmm4
.
byte
15
93
160
237
230
0
0
/
/
minps
0xe6ed
(
%
eax
)
%
xmm4
.
byte
15
41
98
64
/
/
movaps
%
xmm4
0x40
(
%
edx
)
.
byte
15
40
106
16
/
/
movaps
0x10
(
%
edx
)
%
xmm5
.
byte
15
40
114
32
/
/
movaps
0x20
(
%
edx
)
%
xmm6
.
byte
15
93
236
/
/
minps
%
xmm4
%
xmm5
.
byte
15
41
106
16
/
/
movaps
%
xmm5
0x10
(
%
edx
)
.
byte
15
93
244
/
/
minps
%
xmm4
%
xmm6
.
byte
15
41
114
32
/
/
movaps
%
xmm6
0x20
(
%
edx
)
.
byte
15
40
106
48
/
/
movaps
0x30
(
%
edx
)
%
xmm5
.
byte
15
93
236
/
/
minps
%
xmm4
%
xmm5
.
byte
15
41
106
48
/
/
movaps
%
xmm5
0x30
(
%
edx
)
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
82
/
/
push
%
edx
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_set_rgb_sse2
.
globl
_sk_set_rgb_sse2
FUNCTION
(
_sk_set_rgb_sse2
)
_sk_set_rgb_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
243
15
16
73
4
/
/
movss
0x4
(
%
ecx
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_swap_rb_sse2
.
globl
_sk_swap_rb_sse2
FUNCTION
(
_sk_swap_rb_sse2
)
_sk_swap_rb_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_invert_sse2
.
globl
_sk_invert_sse2
FUNCTION
(
_sk_invert_sse2
)
_sk_invert_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
211e
<
_sk_invert_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
40
160
66
230
0
0
/
/
movaps
0xe642
(
%
eax
)
%
xmm4
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
92
232
/
/
subps
%
xmm0
%
xmm5
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
15
40
252
/
/
movaps
%
xmm4
%
xmm7
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_move_src_dst_sse2
.
globl
_sk_move_src_dst_sse2
FUNCTION
(
_sk_move_src_dst_sse2
)
_sk_move_src_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
41
65
16
/
/
movaps
%
xmm0
0x10
(
%
ecx
)
.
byte
15
41
73
32
/
/
movaps
%
xmm1
0x20
(
%
ecx
)
.
byte
15
41
81
48
/
/
movaps
%
xmm2
0x30
(
%
ecx
)
.
byte
15
41
89
64
/
/
movaps
%
xmm3
0x40
(
%
ecx
)
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_move_dst_src_sse2
.
globl
_sk_move_dst_src_sse2
FUNCTION
(
_sk_move_dst_src_sse2
)
_sk_move_dst_src_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
65
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm0
.
byte
15
40
73
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm1
.
byte
15
40
81
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm2
.
byte
15
40
89
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm3
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_premul_sse2
.
globl
_sk_premul_sse2
FUNCTION
(
_sk_premul_sse2
)
_sk_premul_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_premul_dst_sse2
.
globl
_sk_premul_dst_sse2
FUNCTION
(
_sk_premul_dst_sse2
)
_sk_premul_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
15
40
97
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm4
.
byte
15
40
105
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
41
105
16
/
/
movaps
%
xmm5
0x10
(
%
ecx
)
.
byte
15
40
105
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
41
105
32
/
/
movaps
%
xmm5
0x20
(
%
ecx
)
.
byte
15
89
97
48
/
/
mulps
0x30
(
%
ecx
)
%
xmm4
.
byte
15
41
97
48
/
/
movaps
%
xmm4
0x30
(
%
ecx
)
.
byte
141
80
4
/
/
lea
0x4
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_unpremul_sse2
.
globl
_sk_unpremul_sse2
FUNCTION
(
_sk_unpremul_sse2
)
_sk_unpremul_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
221c
<
_sk_unpremul_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
40
160
68
229
0
0
/
/
movaps
0xe544
(
%
eax
)
%
xmm4
.
byte
15
94
227
/
/
divps
%
xmm3
%
xmm4
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
194
168
36
230
0
0
1
/
/
cmpltps
0xe624
(
%
eax
)
%
xmm5
.
byte
15
84
236
/
/
andps
%
xmm4
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_force_opaque_sse2
.
globl
_sk_force_opaque_sse2
FUNCTION
(
_sk_force_opaque_sse2
)
_sk_force_opaque_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
225d
<
_sk_force_opaque_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
152
3
229
0
0
/
/
movaps
0xe503
(
%
eax
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_force_opaque_dst_sse2
.
globl
_sk_force_opaque_dst_sse2
FUNCTION
(
_sk_force_opaque_dst_sse2
)
_sk_force_opaque_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
2284
<
_sk_force_opaque_dst_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
40
160
220
228
0
0
/
/
movaps
0xe4dc
(
%
eax
)
%
xmm4
.
byte
15
41
98
64
/
/
movaps
%
xmm4
0x40
(
%
edx
)
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
82
/
/
push
%
edx
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_from_srgb_sse2
.
globl
_sk_from_srgb_sse2
FUNCTION
(
_sk_from_srgb_sse2
)
_sk_from_srgb_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
56
/
/
sub
0x38
%
esp
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
232
0
0
0
0
/
/
call
22bd
<
_sk_from_srgb_sse2
+
0x18
>
.
byte
88
/
/
pop
%
eax
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
89
201
/
/
mulps
%
xmm1
%
xmm1
.
byte
15
40
160
51
229
0
0
/
/
movaps
0xe533
(
%
eax
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
176
163
229
0
0
/
/
movaps
0xe5a3
(
%
eax
)
%
xmm6
.
byte
15
41
117
216
/
/
movaps
%
xmm6
-
0x28
(
%
ebp
)
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
176
147
229
0
0
/
/
movaps
0xe593
(
%
eax
)
%
xmm6
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
40
184
179
229
0
0
/
/
movaps
0xe5b3
(
%
eax
)
%
xmm7
.
byte
15
41
125
232
/
/
movaps
%
xmm7
-
0x18
(
%
ebp
)
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
40
184
195
229
0
0
/
/
movaps
0xe5c3
(
%
eax
)
%
xmm7
.
byte
15
194
215
1
/
/
cmpltps
%
xmm7
%
xmm2
.
byte
15
84
202
/
/
andps
%
xmm2
%
xmm1
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
15
86
209
/
/
orps
%
xmm1
%
xmm2
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
77
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
88
77
232
/
/
addps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
194
223
1
/
/
cmpltps
%
xmm7
%
xmm3
.
byte
15
84
195
/
/
andps
%
xmm3
%
xmm0
.
byte
15
85
217
/
/
andnps
%
xmm1
%
xmm3
.
byte
15
86
216
/
/
orps
%
xmm0
%
xmm3
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
101
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm4
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
88
101
232
/
/
addps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
194
239
1
/
/
cmpltps
%
xmm7
%
xmm5
.
byte
15
84
245
/
/
andps
%
xmm5
%
xmm6
.
byte
15
85
236
/
/
andnps
%
xmm4
%
xmm5
.
byte
15
86
238
/
/
orps
%
xmm6
%
xmm5
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
40
213
/
/
movaps
%
xmm5
%
xmm2
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
72
/
/
add
0x48
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_from_srgb_dst_sse2
.
globl
_sk_from_srgb_dst_sse2
FUNCTION
(
_sk_from_srgb_dst_sse2
)
_sk_from_srgb_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
72
/
/
sub
0x48
%
esp
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
15
41
69
232
/
/
movaps
%
xmm0
-
0x18
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
2398
<
_sk_from_srgb_dst_sse2
+
0x1b
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
72
16
/
/
movaps
0x10
(
%
eax
)
%
xmm1
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
237
/
/
mulps
%
xmm5
%
xmm5
.
byte
15
40
161
88
228
0
0
/
/
movaps
0xe458
(
%
ecx
)
%
xmm4
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
40
177
200
228
0
0
/
/
movaps
0xe4c8
(
%
ecx
)
%
xmm6
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
40
169
184
228
0
0
/
/
movaps
0xe4b8
(
%
ecx
)
%
xmm5
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
40
185
216
228
0
0
/
/
movaps
0xe4d8
(
%
ecx
)
%
xmm7
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
40
153
232
228
0
0
/
/
movaps
0xe4e8
(
%
ecx
)
%
xmm3
.
byte
15
194
203
1
/
/
cmpltps
%
xmm3
%
xmm1
.
byte
15
84
209
/
/
andps
%
xmm1
%
xmm2
.
byte
15
85
200
/
/
andnps
%
xmm0
%
xmm1
.
byte
15
86
202
/
/
orps
%
xmm2
%
xmm1
.
byte
15
40
64
32
/
/
movaps
0x20
(
%
eax
)
%
xmm0
.
byte
15
41
72
16
/
/
movaps
%
xmm1
0x10
(
%
eax
)
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
89
201
/
/
mulps
%
xmm1
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
194
195
1
/
/
cmpltps
%
xmm3
%
xmm0
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
15
85
194
/
/
andnps
%
xmm2
%
xmm0
.
byte
15
86
193
/
/
orps
%
xmm1
%
xmm0
.
byte
15
41
64
32
/
/
movaps
%
xmm0
0x20
(
%
eax
)
.
byte
15
40
64
48
/
/
movaps
0x30
(
%
eax
)
%
xmm0
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
88
230
/
/
addps
%
xmm6
%
xmm4
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
89
201
/
/
mulps
%
xmm1
%
xmm1
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
88
231
/
/
addps
%
xmm7
%
xmm4
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
194
195
1
/
/
cmpltps
%
xmm3
%
xmm0
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
15
85
196
/
/
andnps
%
xmm4
%
xmm0
.
byte
15
86
197
/
/
orps
%
xmm5
%
xmm0
.
byte
15
41
64
48
/
/
movaps
%
xmm0
0x30
(
%
eax
)
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
81
4
/
/
lea
0x4
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
69
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
77
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
88
/
/
add
0x58
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_to_srgb_sse2
.
globl
_sk_to_srgb_sse2
FUNCTION
(
_sk_to_srgb_sse2
)
_sk_to_srgb_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
72
/
/
sub
0x48
%
esp
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
232
0
0
0
0
/
/
call
2483
<
_sk_to_srgb_sse2
+
0x18
>
.
byte
88
/
/
pop
%
eax
.
byte
15
82
218
/
/
rsqrtps
%
xmm2
%
xmm3
.
byte
15
40
160
29
228
0
0
/
/
movaps
0xe41d
(
%
eax
)
%
xmm4
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
40
128
45
228
0
0
/
/
movaps
0xe42d
(
%
eax
)
%
xmm0
.
byte
15
41
69
232
/
/
movaps
%
xmm0
-
0x18
(
%
ebp
)
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
40
128
61
228
0
0
/
/
movaps
0xe43d
(
%
eax
)
%
xmm0
.
byte
15
41
69
200
/
/
movaps
%
xmm0
-
0x38
(
%
ebp
)
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
40
128
77
228
0
0
/
/
movaps
0xe44d
(
%
eax
)
%
xmm0
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
83
195
/
/
rcpps
%
xmm3
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
40
168
13
228
0
0
/
/
movaps
0xe40d
(
%
eax
)
%
xmm5
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
40
152
93
228
0
0
/
/
movaps
0xe45d
(
%
eax
)
%
xmm3
.
byte
15
194
211
1
/
/
cmpltps
%
xmm3
%
xmm2
.
byte
15
84
202
/
/
andps
%
xmm2
%
xmm1
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
15
86
209
/
/
orps
%
xmm1
%
xmm2
.
byte
15
82
198
/
/
rsqrtps
%
xmm6
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
77
232
/
/
addps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
77
200
/
/
addps
-
0x38
(
%
ebp
)
%
xmm1
.
byte
15
88
69
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
83
192
/
/
rcpps
%
xmm0
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
194
243
1
/
/
cmpltps
%
xmm3
%
xmm6
.
byte
15
84
206
/
/
andps
%
xmm6
%
xmm1
.
byte
15
85
240
/
/
andnps
%
xmm0
%
xmm6
.
byte
15
86
241
/
/
orps
%
xmm1
%
xmm6
.
byte
15
82
199
/
/
rsqrtps
%
xmm7
%
xmm0
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
88
101
232
/
/
addps
-
0x18
(
%
ebp
)
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
88
101
200
/
/
addps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
88
69
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
83
192
/
/
rcpps
%
xmm0
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
194
251
1
/
/
cmpltps
%
xmm3
%
xmm7
.
byte
15
84
239
/
/
andps
%
xmm7
%
xmm5
.
byte
15
85
248
/
/
andnps
%
xmm0
%
xmm7
.
byte
15
86
253
/
/
orps
%
xmm5
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
88
/
/
add
0x58
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_rgb_to_hsl_sse2
.
globl
_sk_rgb_to_hsl_sse2
FUNCTION
(
_sk_rgb_to_hsl_sse2
)
_sk_rgb_to_hsl_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
56
/
/
sub
0x38
%
esp
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
40
249
/
/
movaps
%
xmm1
%
xmm7
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
232
0
0
0
0
/
/
call
257a
<
_sk_rgb_to_hsl_sse2
+
0x15
>
.
byte
88
/
/
pop
%
eax
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
95
194
/
/
maxps
%
xmm2
%
xmm0
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
95
216
/
/
maxps
%
xmm0
%
xmm3
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
93
194
/
/
minps
%
xmm2
%
xmm0
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
93
200
/
/
minps
%
xmm0
%
xmm1
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
15
40
136
230
225
0
0
/
/
movaps
0xe1e6
(
%
eax
)
%
xmm1
.
byte
15
94
200
/
/
divps
%
xmm0
%
xmm1
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
239
/
/
movaps
%
xmm7
%
xmm5
.
byte
15
194
234
1
/
/
cmpltps
%
xmm2
%
xmm5
.
byte
15
84
168
118
227
0
0
/
/
andps
0xe376
(
%
eax
)
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
194
196
0
/
/
cmpeqps
%
xmm4
%
xmm0
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
194
247
0
/
/
cmpeqps
%
xmm7
%
xmm6
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
92
231
/
/
subps
%
xmm7
%
xmm4
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
40
136
134
227
0
0
/
/
movaps
0xe386
(
%
eax
)
%
xmm1
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
88
160
150
227
0
0
/
/
addps
0xe396
(
%
eax
)
%
xmm4
.
byte
15
84
214
/
/
andps
%
xmm6
%
xmm2
.
byte
15
85
244
/
/
andnps
%
xmm4
%
xmm6
.
byte
15
86
242
/
/
orps
%
xmm2
%
xmm6
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
15
85
198
/
/
andnps
%
xmm6
%
xmm0
.
byte
15
86
197
/
/
orps
%
xmm5
%
xmm0
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
194
226
4
/
/
cmpneqps
%
xmm2
%
xmm4
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
40
168
214
225
0
0
/
/
movaps
0xe1d6
(
%
eax
)
%
xmm5
.
byte
15
92
202
/
/
subps
%
xmm2
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
194
234
1
/
/
cmpltps
%
xmm2
%
xmm5
.
byte
15
84
205
/
/
andps
%
xmm5
%
xmm1
.
byte
15
85
235
/
/
andnps
%
xmm3
%
xmm5
.
byte
15
86
233
/
/
orps
%
xmm1
%
xmm5
.
byte
15
40
77
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
94
205
/
/
divps
%
xmm5
%
xmm1
.
byte
15
89
128
166
227
0
0
/
/
mulps
0xe3a6
(
%
eax
)
%
xmm0
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
15
84
204
/
/
andps
%
xmm4
%
xmm1
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
72
/
/
add
0x48
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_hsl_to_rgb_sse2
.
globl
_sk_hsl_to_rgb_sse2
FUNCTION
(
_sk_hsl_to_rgb_sse2
)
_sk_hsl_to_rgb_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
129
236
200
0
0
0
/
/
sub
0xc8
%
esp
.
byte
15
41
157
56
255
255
255
/
/
movaps
%
xmm3
-
0xc8
(
%
ebp
)
.
byte
15
41
69
184
/
/
movaps
%
xmm0
-
0x48
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
2675
<
_sk_hsl_to_rgb_sse2
+
0x19
>
.
byte
88
/
/
pop
%
eax
.
byte
15
40
160
219
224
0
0
/
/
movaps
0xe0db
(
%
eax
)
%
xmm4
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
41
165
104
255
255
255
/
/
movaps
%
xmm4
-
0x98
(
%
ebp
)
.
byte
15
194
218
2
/
/
cmpleps
%
xmm2
%
xmm3
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
194
233
0
/
/
cmpeqps
%
xmm1
%
xmm5
.
byte
15
41
109
232
/
/
movaps
%
xmm5
-
0x18
(
%
ebp
)
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
92
241
/
/
subps
%
xmm1
%
xmm6
.
byte
15
84
243
/
/
andps
%
xmm3
%
xmm6
.
byte
15
85
217
/
/
andnps
%
xmm1
%
xmm3
.
byte
15
86
222
/
/
orps
%
xmm6
%
xmm3
.
byte
15
40
168
187
226
0
0
/
/
movaps
0xe2bb
(
%
eax
)
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
243
15
91
205
/
/
cvttps2dq
%
xmm5
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
194
193
1
/
/
cmpltps
%
xmm1
%
xmm0
.
byte
15
40
176
235
224
0
0
/
/
movaps
0xe0eb
(
%
eax
)
%
xmm6
.
byte
15
41
117
136
/
/
movaps
%
xmm6
-
0x78
(
%
ebp
)
.
byte
15
84
198
/
/
andps
%
xmm6
%
xmm0
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
41
149
72
255
255
255
/
/
movaps
%
xmm2
-
0xb8
(
%
ebp
)
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
88
210
/
/
addps
%
xmm2
%
xmm2
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
40
184
203
226
0
0
/
/
movaps
0xe2cb
(
%
eax
)
%
xmm7
.
byte
15
41
189
88
255
255
255
/
/
movaps
%
xmm7
-
0xa8
(
%
ebp
)
.
byte
15
194
253
2
/
/
cmpleps
%
xmm5
%
xmm7
.
byte
15
92
218
/
/
subps
%
xmm2
%
xmm3
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
15
194
205
2
/
/
cmpleps
%
xmm5
%
xmm1
.
byte
15
40
160
171
226
0
0
/
/
movaps
0xe2ab
(
%
eax
)
%
xmm4
.
byte
15
41
165
120
255
255
255
/
/
movaps
%
xmm4
-
0x88
(
%
ebp
)
.
byte
15
194
229
2
/
/
cmpleps
%
xmm5
%
xmm4
.
byte
15
40
176
123
226
0
0
/
/
movaps
0xe27b
(
%
eax
)
%
xmm6
.
byte
15
89
238
/
/
mulps
%
xmm6
%
xmm5
.
byte
15
40
128
155
226
0
0
/
/
movaps
0xe29b
(
%
eax
)
%
xmm0
.
byte
15
41
69
168
/
/
movaps
%
xmm0
-
0x58
(
%
ebp
)
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
84
223
/
/
andps
%
xmm7
%
xmm3
.
byte
15
85
248
/
/
andnps
%
xmm0
%
xmm7
.
byte
15
86
251
/
/
orps
%
xmm3
%
xmm7
.
byte
15
84
249
/
/
andps
%
xmm1
%
xmm7
.
byte
15
85
77
216
/
/
andnps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
86
207
/
/
orps
%
xmm7
%
xmm1
.
byte
15
40
125
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm7
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
84
204
/
/
andps
%
xmm4
%
xmm1
.
byte
15
85
229
/
/
andnps
%
xmm5
%
xmm4
.
byte
15
86
225
/
/
orps
%
xmm1
%
xmm4
.
byte
15
40
69
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
85
196
/
/
andnps
%
xmm4
%
xmm0
.
byte
15
41
69
152
/
/
movaps
%
xmm0
-
0x68
(
%
ebp
)
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm1
.
byte
15
84
136
235
224
0
0
/
/
andps
0xe0eb
(
%
eax
)
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
40
128
203
226
0
0
/
/
movaps
0xe2cb
(
%
eax
)
%
xmm0
.
byte
15
194
193
2
/
/
cmpleps
%
xmm1
%
xmm0
.
byte
15
40
152
219
224
0
0
/
/
movaps
0xe0db
(
%
eax
)
%
xmm3
.
byte
15
194
217
2
/
/
cmpleps
%
xmm1
%
xmm3
.
byte
15
40
160
171
226
0
0
/
/
movaps
0xe2ab
(
%
eax
)
%
xmm4
.
byte
15
194
225
2
/
/
cmpleps
%
xmm1
%
xmm4
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
40
168
155
226
0
0
/
/
movaps
0xe29b
(
%
eax
)
%
xmm5
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
84
248
/
/
andps
%
xmm0
%
xmm7
.
byte
15
85
197
/
/
andnps
%
xmm5
%
xmm0
.
byte
15
86
199
/
/
orps
%
xmm7
%
xmm0
.
byte
15
84
195
/
/
andps
%
xmm3
%
xmm0
.
byte
15
85
93
216
/
/
andnps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
86
216
/
/
orps
%
xmm0
%
xmm3
.
byte
15
40
109
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm5
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
15
84
220
/
/
andps
%
xmm4
%
xmm3
.
byte
15
85
225
/
/
andnps
%
xmm1
%
xmm4
.
byte
15
86
227
/
/
orps
%
xmm3
%
xmm4
.
byte
15
40
125
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm7
.
byte
15
85
252
/
/
andnps
%
xmm4
%
xmm7
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
15
88
152
219
226
0
0
/
/
addps
0xe2db
(
%
eax
)
%
xmm3
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm1
.
byte
15
84
77
136
/
/
andps
-
0x78
(
%
ebp
)
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
40
77
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm1
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
226
/
/
addps
%
xmm2
%
xmm4
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
15
40
141
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm1
.
byte
15
194
203
2
/
/
cmpleps
%
xmm3
%
xmm1
.
byte
15
84
209
/
/
andps
%
xmm1
%
xmm2
.
byte
15
85
204
/
/
andnps
%
xmm4
%
xmm1
.
byte
15
86
202
/
/
orps
%
xmm2
%
xmm1
.
byte
15
40
133
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm0
.
byte
15
194
195
2
/
/
cmpleps
%
xmm3
%
xmm0
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
15
85
69
216
/
/
andnps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
86
193
/
/
orps
%
xmm1
%
xmm0
.
byte
15
40
141
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm1
.
byte
15
194
203
2
/
/
cmpleps
%
xmm3
%
xmm1
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
15
85
206
/
/
andnps
%
xmm6
%
xmm1
.
byte
15
86
200
/
/
orps
%
xmm0
%
xmm1
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
40
157
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm3
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
84
217
/
/
andps
%
xmm1
%
xmm3
.
byte
15
85
202
/
/
andnps
%
xmm2
%
xmm1
.
byte
15
40
69
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm0
.
byte
15
86
195
/
/
orps
%
xmm3
%
xmm0
.
byte
15
86
251
/
/
orps
%
xmm3
%
xmm7
.
byte
15
86
217
/
/
orps
%
xmm1
%
xmm3
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
40
157
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
129
196
216
0
0
0
/
/
add
0xd8
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_scale_1_float_sse2
.
globl
_sk_scale_1_float_sse2
FUNCTION
(
_sk_scale_1_float_sse2
)
_sk_scale_1_float_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_scale_u8_sse2
.
globl
_sk_scale_u8_sse2
FUNCTION
(
_sk_scale_u8_sse2
)
_sk_scale_u8_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
28ef
<
_sk_scale_u8_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
65
/
/
jne
294b
<
_sk_scale_u8_sse2
+
0x6a
>
.
byte
102
15
110
36
62
/
/
movd
(
%
esi
%
edi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
219
162
49
223
0
0
/
/
pand
0xdf31
(
%
edx
)
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
162
113
224
0
0
/
/
mulps
0xe071
(
%
edx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
50
/
/
je
2985
<
_sk_scale_u8_sse2
+
0xa4
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
296f
<
_sk_scale_u8_sse2
+
0x8e
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
182
/
/
jne
2917
<
_sk_scale_u8_sse2
+
0x36
>
.
byte
15
182
92
62
2
/
/
movzbl
0x2
(
%
esi
%
edi
1
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
15
183
52
62
/
/
movzwl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
238
/
/
movd
%
esi
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
102
15
97
232
/
/
punpcklwd
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
235
146
/
/
jmp
2917
<
_sk_scale_u8_sse2
+
0x36
>
.
byte
15
182
52
62
/
/
movzbl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
235
136
/
/
jmp
2917
<
_sk_scale_u8_sse2
+
0x36
>
HIDDEN
_sk_scale_565_sse2
.
globl
_sk_scale_565_sse2
FUNCTION
(
_sk_scale_565_sse2
)
_sk_scale_565_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
29a9
<
_sk_scale_565_sse2
+
0x1a
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
133
157
0
0
0
/
/
jne
2a67
<
_sk_scale_565_sse2
+
0xd8
>
.
byte
243
15
126
52
126
/
/
movq
(
%
esi
%
edi
2
)
%
xmm6
.
byte
102
15
97
240
/
/
punpcklwd
%
xmm0
%
xmm6
.
byte
102
15
111
162
199
223
0
0
/
/
movdqa
0xdfc7
(
%
edx
)
%
xmm4
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
15
91
212
/
/
cvtdq2ps
%
xmm4
%
xmm2
.
byte
15
89
146
215
223
0
0
/
/
mulps
0xdfd7
(
%
edx
)
%
xmm2
.
byte
102
15
111
170
231
223
0
0
/
/
movdqa
0xdfe7
(
%
edx
)
%
xmm5
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
170
247
223
0
0
/
/
mulps
0xdff7
(
%
edx
)
%
xmm5
.
byte
102
15
219
178
7
224
0
0
/
/
pand
0xe007
(
%
edx
)
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
178
23
224
0
0
/
/
mulps
0xe017
(
%
edx
)
%
xmm6
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
93
254
/
/
minps
%
xmm6
%
xmm7
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
93
231
/
/
minps
%
xmm7
%
xmm4
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
95
254
/
/
maxps
%
xmm6
%
xmm7
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
95
199
/
/
maxps
%
xmm7
%
xmm0
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
15
194
121
64
1
/
/
cmpltps
0x40
(
%
ecx
)
%
xmm7
.
byte
15
84
231
/
/
andps
%
xmm7
%
xmm4
.
byte
15
85
248
/
/
andnps
%
xmm0
%
xmm7
.
byte
15
86
252
/
/
orps
%
xmm4
%
xmm7
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
89
109
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm5
.
byte
15
89
117
200
/
/
mulps
-
0x38
(
%
ebp
)
%
xmm6
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
51
/
/
je
2aa2
<
_sk_scale_565_sse2
+
0x113
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
23
/
/
je
2a8f
<
_sk_scale_565_sse2
+
0x100
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
15
133
82
255
255
255
/
/
jne
29d3
<
_sk_scale_565_sse2
+
0x44
>
.
byte
15
183
92
126
4
/
/
movzwl
0x4
(
%
esi
%
edi
2
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
244
69
/
/
pshufd
0x45
%
xmm4
%
xmm6
.
byte
102
15
110
36
126
/
/
movd
(
%
esi
%
edi
2
)
%
xmm4
.
byte
242
15
112
228
212
/
/
pshuflw
0xd4
%
xmm4
%
xmm4
.
byte
242
15
16
244
/
/
movsd
%
xmm4
%
xmm6
.
byte
233
49
255
255
255
/
/
jmp
29d3
<
_sk_scale_565_sse2
+
0x44
>
.
byte
15
183
52
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
esi
.
byte
102
15
110
246
/
/
movd
%
esi
%
xmm6
.
byte
233
36
255
255
255
/
/
jmp
29d3
<
_sk_scale_565_sse2
+
0x44
>
HIDDEN
_sk_lerp_1_float_sse2
.
globl
_sk_lerp_1_float_sse2
FUNCTION
(
_sk_lerp_1_float_sse2
)
_sk_lerp_1_float_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
16
34
/
/
movss
(
%
edx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
40
104
16
/
/
movaps
0x10
(
%
eax
)
%
xmm5
.
byte
15
40
112
32
/
/
movaps
0x20
(
%
eax
)
%
xmm6
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
40
104
48
/
/
movaps
0x30
(
%
eax
)
%
xmm5
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
104
64
/
/
movaps
0x40
(
%
eax
)
%
xmm5
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_lerp_u8_sse2
.
globl
_sk_lerp_u8_sse2
FUNCTION
(
_sk_lerp_u8_sse2
)
_sk_lerp_u8_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
2b17
<
_sk_lerp_u8_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
102
/
/
jne
2b98
<
_sk_lerp_u8_sse2
+
0x8f
>
.
byte
102
15
110
36
62
/
/
movd
(
%
esi
%
edi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
219
162
9
221
0
0
/
/
pand
0xdd09
(
%
edx
)
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
162
73
222
0
0
/
/
mulps
0xde49
(
%
edx
)
%
xmm4
.
byte
15
40
105
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm5
.
byte
15
40
113
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm6
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
40
105
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm5
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
105
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm5
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
53
/
/
je
2bd5
<
_sk_lerp_u8_sse2
+
0xcc
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
2bbc
<
_sk_lerp_u8_sse2
+
0xb3
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
145
/
/
jne
2b3f
<
_sk_lerp_u8_sse2
+
0x36
>
.
byte
15
182
92
62
2
/
/
movzbl
0x2
(
%
esi
%
edi
1
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
15
183
52
62
/
/
movzwl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
238
/
/
movd
%
esi
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
102
15
97
232
/
/
punpcklwd
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
233
106
255
255
255
/
/
jmp
2b3f
<
_sk_lerp_u8_sse2
+
0x36
>
.
byte
15
182
52
62
/
/
movzbl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
233
93
255
255
255
/
/
jmp
2b3f
<
_sk_lerp_u8_sse2
+
0x36
>
HIDDEN
_sk_lerp_565_sse2
.
globl
_sk_lerp_565_sse2
FUNCTION
(
_sk_lerp_565_sse2
)
_sk_lerp_565_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
108
/
/
sub
0x6c
%
esp
.
byte
15
41
93
168
/
/
movaps
%
xmm3
-
0x58
(
%
ebp
)
.
byte
15
41
85
136
/
/
movaps
%
xmm2
-
0x78
(
%
ebp
)
.
byte
15
41
77
152
/
/
movaps
%
xmm1
-
0x68
(
%
ebp
)
.
byte
102
15
127
69
184
/
/
movdqa
%
xmm0
-
0x48
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
2c01
<
_sk_lerp_565_sse2
+
0x1f
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
133
211
0
0
0
/
/
jne
2cf5
<
_sk_lerp_565_sse2
+
0x113
>
.
byte
243
15
126
44
126
/
/
movq
(
%
esi
%
edi
2
)
%
xmm5
.
byte
102
15
97
232
/
/
punpcklwd
%
xmm0
%
xmm5
.
byte
102
15
111
162
111
221
0
0
/
/
movdqa
0xdd6f
(
%
edx
)
%
xmm4
.
byte
102
15
219
229
/
/
pand
%
xmm5
%
xmm4
.
byte
15
91
204
/
/
cvtdq2ps
%
xmm4
%
xmm1
.
byte
15
89
138
127
221
0
0
/
/
mulps
0xdd7f
(
%
edx
)
%
xmm1
.
byte
102
15
111
178
143
221
0
0
/
/
movdqa
0xdd8f
(
%
edx
)
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
178
159
221
0
0
/
/
mulps
0xdd9f
(
%
edx
)
%
xmm6
.
byte
102
15
219
170
175
221
0
0
/
/
pand
0xddaf
(
%
edx
)
%
xmm5
.
byte
15
91
213
/
/
cvtdq2ps
%
xmm5
%
xmm2
.
byte
15
89
146
191
221
0
0
/
/
mulps
0xddbf
(
%
edx
)
%
xmm2
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
40
65
32
/
/
movaps
0x20
(
%
ecx
)
%
xmm0
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
15
40
101
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
93
243
/
/
minps
%
xmm3
%
xmm6
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
40
65
16
/
/
movaps
0x10
(
%
ecx
)
%
xmm0
.
byte
15
40
109
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm5
.
byte
15
92
232
/
/
subps
%
xmm0
%
xmm5
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
93
206
/
/
minps
%
xmm6
%
xmm1
.
byte
15
40
113
64
/
/
movaps
0x40
(
%
ecx
)
%
xmm6
.
byte
15
95
251
/
/
maxps
%
xmm3
%
xmm7
.
byte
15
95
215
/
/
maxps
%
xmm7
%
xmm2
.
byte
15
40
93
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm3
.
byte
15
40
251
/
/
movaps
%
xmm3
%
xmm7
.
byte
15
194
254
1
/
/
cmpltps
%
xmm6
%
xmm7
.
byte
15
84
207
/
/
andps
%
xmm7
%
xmm1
.
byte
15
85
250
/
/
andnps
%
xmm2
%
xmm7
.
byte
15
86
249
/
/
orps
%
xmm1
%
xmm7
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
88
101
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm4
.
byte
15
40
65
48
/
/
movaps
0x30
(
%
ecx
)
%
xmm0
.
byte
15
40
85
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm2
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
15
89
85
200
/
/
mulps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
92
222
/
/
subps
%
xmm6
%
xmm3
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
124
/
/
add
0x7c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
51
/
/
je
2d30
<
_sk_lerp_565_sse2
+
0x14e
>
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
23
/
/
je
2d1d
<
_sk_lerp_565_sse2
+
0x13b
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
15
133
28
255
255
255
/
/
jne
2c2b
<
_sk_lerp_565_sse2
+
0x49
>
.
byte
15
183
92
126
4
/
/
movzwl
0x4
(
%
esi
%
edi
2
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
236
69
/
/
pshufd
0x45
%
xmm4
%
xmm5
.
byte
102
15
110
36
126
/
/
movd
(
%
esi
%
edi
2
)
%
xmm4
.
byte
242
15
112
228
212
/
/
pshuflw
0xd4
%
xmm4
%
xmm4
.
byte
242
15
16
236
/
/
movsd
%
xmm4
%
xmm5
.
byte
233
251
254
255
255
/
/
jmp
2c2b
<
_sk_lerp_565_sse2
+
0x49
>
.
byte
15
183
52
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
esi
.
byte
102
15
110
238
/
/
movd
%
esi
%
xmm5
.
byte
233
238
254
255
255
/
/
jmp
2c2b
<
_sk_lerp_565_sse2
+
0x49
>
HIDDEN
_sk_load_tables_sse2
.
globl
_sk_load_tables_sse2
FUNCTION
(
_sk_load_tables_sse2
)
_sk_load_tables_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
2d4b
<
_sk_load_tables_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
48
/
/
mov
(
%
eax
)
%
esi
.
byte
139
65
8
/
/
mov
0x8
(
%
ecx
)
%
eax
.
byte
133
192
/
/
test
%
eax
%
eax
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
139
30
/
/
mov
(
%
esi
)
%
ebx
.
byte
15
133
22
1
0
0
/
/
jne
2e79
<
_sk_load_tables_sse2
+
0x13c
>
.
byte
243
15
111
28
187
/
/
movdqu
(
%
ebx
%
edi
4
)
%
xmm3
.
byte
102
15
111
146
213
218
0
0
/
/
movdqa
0xdad5
(
%
edx
)
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
112
224
231
/
/
pshufd
0xe7
%
xmm0
%
xmm4
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
139
126
4
/
/
mov
0x4
(
%
esi
)
%
edi
.
byte
243
15
16
36
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm4
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
44
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm5
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
243
15
16
4
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm0
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
243
15
16
12
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm1
.
byte
139
70
8
/
/
mov
0x8
(
%
esi
)
%
eax
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
15
112
241
231
/
/
pshufd
0xe7
%
xmm1
%
xmm6
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
243
15
16
36
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm4
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
249
229
/
/
pshufd
0xe5
%
xmm1
%
xmm7
.
byte
243
15
16
12
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm1
.
byte
102
15
126
255
/
/
movd
%
xmm7
%
edi
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
15
20
230
/
/
unpcklps
%
xmm6
%
xmm4
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
139
70
12
/
/
mov
0xc
(
%
esi
)
%
eax
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
114
213
16
/
/
psrld
0x10
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
102
15
112
213
78
/
/
pshufd
0x4e
%
xmm5
%
xmm2
.
byte
102
15
112
245
231
/
/
pshufd
0xe7
%
xmm5
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
243
15
16
52
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm6
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
243
15
16
60
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm7
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
102
15
112
237
229
/
/
pshufd
0xe5
%
xmm5
%
xmm5
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
243
15
16
44
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm5
.
byte
102
15
20
204
/
/
unpcklpd
%
xmm4
%
xmm1
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
102
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
154
21
220
0
0
/
/
mulps
0xdc15
(
%
edx
)
%
xmm3
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
137
194
/
/
mov
%
eax
%
edx
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
36
3
/
/
and
0x3
%
al
.
byte
60
1
/
/
cmp
0x1
%
al
.
byte
116
37
/
/
je
2ea4
<
_sk_load_tables_sse2
+
0x167
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
60
2
/
/
cmp
0x2
%
al
.
byte
116
19
/
/
je
2e9a
<
_sk_load_tables_sse2
+
0x15d
>
.
byte
60
3
/
/
cmp
0x3
%
al
.
byte
15
133
217
254
255
255
/
/
jne
2d68
<
_sk_load_tables_sse2
+
0x2b
>
.
byte
102
15
110
68
187
8
/
/
movd
0x8
(
%
ebx
%
edi
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
15
18
28
187
/
/
movlpd
(
%
ebx
%
edi
4
)
%
xmm3
.
byte
233
196
254
255
255
/
/
jmp
2d68
<
_sk_load_tables_sse2
+
0x2b
>
.
byte
102
15
110
28
187
/
/
movd
(
%
ebx
%
edi
4
)
%
xmm3
.
byte
233
186
254
255
255
/
/
jmp
2d68
<
_sk_load_tables_sse2
+
0x2b
>
HIDDEN
_sk_load_tables_u16_be_sse2
.
globl
_sk_load_tables_u16_be_sse2
FUNCTION
(
_sk_load_tables_u16_be_sse2
)
_sk_load_tables_u16_be_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
232
0
0
0
0
/
/
call
2ebc
<
_sk_load_tables_u16_be_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
139
65
8
/
/
mov
0x8
(
%
ecx
)
%
eax
.
byte
193
231
2
/
/
shl
0x2
%
edi
.
byte
133
192
/
/
test
%
eax
%
eax
.
byte
139
54
/
/
mov
(
%
esi
)
%
esi
.
byte
139
30
/
/
mov
(
%
esi
)
%
ebx
.
byte
15
133
210
1
0
0
/
/
jne
30a9
<
_sk_load_tables_u16_be_sse2
+
0x1fb
>
.
byte
102
15
16
4
123
/
/
movupd
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
243
15
111
76
123
16
/
/
movdqu
0x10
(
%
ebx
%
edi
2
)
%
xmm1
.
byte
102
15
40
216
/
/
movapd
%
xmm0
%
xmm3
.
byte
102
15
97
217
/
/
punpcklwd
%
xmm1
%
xmm3
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
105
216
/
/
punpckhwd
%
xmm0
%
xmm3
.
byte
102
15
126
77
208
/
/
movd
%
xmm1
-
0x30
(
%
ebp
)
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
126
69
212
/
/
movd
%
xmm0
-
0x2c
(
%
ebp
)
.
byte
243
15
126
69
208
/
/
movq
-
0x30
(
%
ebp
)
%
xmm0
.
byte
102
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm0
.
byte
102
15
112
209
231
/
/
pshufd
0xe7
%
xmm1
%
xmm2
.
byte
102
15
126
85
220
/
/
movd
%
xmm2
-
0x24
(
%
ebp
)
.
byte
102
15
112
201
78
/
/
pshufd
0x4e
%
xmm1
%
xmm1
.
byte
102
15
126
77
216
/
/
movd
%
xmm1
-
0x28
(
%
ebp
)
.
byte
243
15
126
77
216
/
/
movq
-
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
126
93
224
/
/
movd
%
xmm3
-
0x20
(
%
ebp
)
.
byte
102
15
112
211
229
/
/
pshufd
0xe5
%
xmm3
%
xmm2
.
byte
102
15
126
85
228
/
/
movd
%
xmm2
-
0x1c
(
%
ebp
)
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
111
146
20
219
0
0
/
/
movdqa
0xdb14
(
%
edx
)
%
xmm2
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
97
196
/
/
punpcklwd
%
xmm4
%
xmm0
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
15
112
240
231
/
/
pshufd
0xe7
%
xmm0
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
139
126
4
/
/
mov
0x4
(
%
esi
)
%
edi
.
byte
243
15
16
52
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm6
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
243
15
16
44
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm5
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
248
229
/
/
pshufd
0xe5
%
xmm0
%
xmm7
.
byte
243
15
16
4
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm0
.
byte
102
15
126
248
/
/
movd
%
xmm7
%
eax
.
byte
243
15
16
60
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm7
.
byte
139
70
8
/
/
mov
0x8
(
%
esi
)
%
eax
.
byte
15
20
238
/
/
unpcklps
%
xmm6
%
xmm5
.
byte
15
20
199
/
/
unpcklps
%
xmm7
%
xmm0
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
15
97
204
/
/
punpcklwd
%
xmm4
%
xmm1
.
byte
102
15
112
233
78
/
/
pshufd
0x4e
%
xmm1
%
xmm5
.
byte
102
15
112
241
231
/
/
pshufd
0xe7
%
xmm1
%
xmm6
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
102
15
126
239
/
/
movd
%
xmm5
%
edi
.
byte
243
15
16
44
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm5
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
249
229
/
/
pshufd
0xe5
%
xmm1
%
xmm7
.
byte
243
15
16
12
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm1
.
byte
102
15
126
255
/
/
movd
%
xmm7
%
edi
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
15
20
238
/
/
unpcklps
%
xmm6
%
xmm5
.
byte
243
15
126
117
224
/
/
movq
-
0x20
(
%
ebp
)
%
xmm6
.
byte
102
15
97
240
/
/
punpcklwd
%
xmm0
%
xmm6
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
139
70
12
/
/
mov
0xc
(
%
esi
)
%
eax
.
byte
242
15
112
238
232
/
/
pshuflw
0xe8
%
xmm6
%
xmm5
.
byte
243
15
112
237
232
/
/
pshufhw
0xe8
%
xmm5
%
xmm5
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
102
15
97
236
/
/
punpcklwd
%
xmm4
%
xmm5
.
byte
102
15
112
213
78
/
/
pshufd
0x4e
%
xmm5
%
xmm2
.
byte
102
15
112
245
231
/
/
pshufd
0xe7
%
xmm5
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
243
15
16
52
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm6
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
243
15
16
60
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm7
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
102
15
112
237
229
/
/
pshufd
0xe5
%
xmm5
%
xmm5
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
243
15
16
44
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm5
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
102
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm2
.
byte
102
15
112
219
78
/
/
pshufd
0x4e
%
xmm3
%
xmm3
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
113
245
8
/
/
psllw
0x8
%
xmm5
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
235
221
/
/
por
%
xmm5
%
xmm3
.
byte
102
15
126
93
232
/
/
movd
%
xmm3
-
0x18
(
%
ebp
)
.
byte
102
15
112
219
229
/
/
pshufd
0xe5
%
xmm3
%
xmm3
.
byte
102
15
126
93
236
/
/
movd
%
xmm3
-
0x14
(
%
ebp
)
.
byte
243
15
126
93
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm3
.
byte
102
15
97
220
/
/
punpcklwd
%
xmm4
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
154
36
219
0
0
/
/
mulps
0xdb24
(
%
edx
)
%
xmm3
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
137
194
/
/
mov
%
eax
%
edx
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
242
15
16
4
123
/
/
movsd
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
131
248
1
/
/
cmp
0x1
%
eax
.
byte
15
132
39
254
255
255
/
/
je
2ee2
<
_sk_load_tables_u16_be_sse2
+
0x34
>
.
byte
102
15
22
68
123
8
/
/
movhpd
0x8
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
131
248
3
/
/
cmp
0x3
%
eax
.
byte
15
130
24
254
255
255
/
/
jb
2ee2
<
_sk_load_tables_u16_be_sse2
+
0x34
>
.
byte
243
15
126
76
123
16
/
/
movq
0x10
(
%
ebx
%
edi
2
)
%
xmm1
.
byte
233
13
254
255
255
/
/
jmp
2ee2
<
_sk_load_tables_u16_be_sse2
+
0x34
>
HIDDEN
_sk_load_tables_rgb_u16_be_sse2
.
globl
_sk_load_tables_rgb_u16_be_sse2
FUNCTION
(
_sk_load_tables_rgb_u16_be_sse2
)
_sk_load_tables_rgb_u16_be_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
28
/
/
sub
0x1c
%
esp
.
byte
232
0
0
0
0
/
/
call
30e3
<
_sk_load_tables_rgb_u16_be_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
65
8
/
/
mov
0x8
(
%
ecx
)
%
eax
.
byte
107
57
3
/
/
imul
0x3
(
%
ecx
)
%
edi
.
byte
133
192
/
/
test
%
eax
%
eax
.
byte
139
54
/
/
mov
(
%
esi
)
%
esi
.
byte
139
30
/
/
mov
(
%
esi
)
%
ebx
.
byte
15
133
179
1
0
0
/
/
jne
32af
<
_sk_load_tables_rgb_u16_be_sse2
+
0x1da
>
.
byte
243
15
111
4
123
/
/
movdqu
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
243
15
111
92
123
8
/
/
movdqu
0x8
(
%
ebx
%
edi
2
)
%
xmm3
.
byte
102
15
115
219
4
/
/
psrldq
0x4
%
xmm3
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
115
217
6
/
/
psrldq
0x6
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
115
218
6
/
/
psrldq
0x6
%
xmm2
.
byte
102
15
97
202
/
/
punpcklwd
%
xmm2
%
xmm1
.
byte
102
15
97
195
/
/
punpcklwd
%
xmm3
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
126
85
216
/
/
movd
%
xmm2
-
0x28
(
%
ebp
)
.
byte
102
15
112
202
229
/
/
pshufd
0xe5
%
xmm2
%
xmm1
.
byte
102
15
126
77
220
/
/
movd
%
xmm1
-
0x24
(
%
ebp
)
.
byte
243
15
126
85
216
/
/
movq
-
0x28
(
%
ebp
)
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
15
126
93
224
/
/
movd
%
xmm3
-
0x20
(
%
ebp
)
.
byte
102
15
112
203
229
/
/
pshufd
0xe5
%
xmm3
%
xmm1
.
byte
102
15
126
77
228
/
/
movd
%
xmm1
-
0x1c
(
%
ebp
)
.
byte
243
15
126
77
224
/
/
movq
-
0x20
(
%
ebp
)
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
126
69
232
/
/
movd
%
xmm0
-
0x18
(
%
ebp
)
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
102
15
126
69
236
/
/
movd
%
xmm0
-
0x14
(
%
ebp
)
.
byte
242
15
112
194
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
111
146
237
216
0
0
/
/
movdqa
0xd8ed
(
%
edx
)
%
xmm2
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
97
195
/
/
punpcklwd
%
xmm3
%
xmm0
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
15
112
232
231
/
/
pshufd
0xe7
%
xmm0
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
139
126
4
/
/
mov
0x4
(
%
esi
)
%
edi
.
byte
243
15
16
44
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm5
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
243
15
16
36
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm4
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
240
229
/
/
pshufd
0xe5
%
xmm0
%
xmm6
.
byte
243
15
16
4
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm0
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
243
15
16
52
135
/
/
movss
(
%
edi
%
eax
4
)
%
xmm6
.
byte
139
70
8
/
/
mov
0x8
(
%
esi
)
%
eax
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
15
20
198
/
/
unpcklps
%
xmm6
%
xmm0
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
15
97
203
/
/
punpcklwd
%
xmm3
%
xmm1
.
byte
102
15
112
233
78
/
/
pshufd
0x4e
%
xmm1
%
xmm5
.
byte
102
15
112
241
231
/
/
pshufd
0xe7
%
xmm1
%
xmm6
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
102
15
126
239
/
/
movd
%
xmm5
%
edi
.
byte
243
15
16
44
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm5
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
249
229
/
/
pshufd
0xe5
%
xmm1
%
xmm7
.
byte
243
15
16
12
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm1
.
byte
102
15
126
255
/
/
movd
%
xmm7
%
edi
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
102
15
20
196
/
/
unpcklpd
%
xmm4
%
xmm0
.
byte
243
15
126
101
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
15
20
238
/
/
unpcklps
%
xmm6
%
xmm5
.
byte
139
70
12
/
/
mov
0xc
(
%
esi
)
%
eax
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
243
15
112
228
232
/
/
pshufhw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
219
226
/
/
pand
%
xmm2
%
xmm4
.
byte
102
15
97
227
/
/
punpcklwd
%
xmm3
%
xmm4
.
byte
102
15
112
212
78
/
/
pshufd
0x4e
%
xmm4
%
xmm2
.
byte
102
15
112
220
231
/
/
pshufd
0xe7
%
xmm4
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
243
15
16
52
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm6
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
112
228
229
/
/
pshufd
0xe5
%
xmm4
%
xmm4
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
243
15
16
36
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm4
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
15
20
212
/
/
unpcklps
%
xmm4
%
xmm2
.
byte
102
15
20
214
/
/
unpcklpd
%
xmm6
%
xmm2
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
137
198
/
/
mov
%
eax
%
esi
.
byte
141
70
8
/
/
lea
0x8
(
%
esi
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
154
125
214
0
0
/
/
movaps
0xd67d
(
%
edx
)
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
86
4
/
/
call
*
0x4
(
%
esi
)
.
byte
131
196
44
/
/
add
0x2c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
15
110
4
123
/
/
movd
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
102
15
196
68
123
4
2
/
/
pinsrw
0x2
0x4
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
131
248
1
/
/
cmp
0x1
%
eax
.
byte
117
13
/
/
jne
32d1
<
_sk_load_tables_rgb_u16_be_sse2
+
0x1fc
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
233
77
254
255
255
/
/
jmp
311e
<
_sk_load_tables_rgb_u16_be_sse2
+
0x49
>
.
byte
102
15
110
76
123
6
/
/
movd
0x6
(
%
ebx
%
edi
2
)
%
xmm1
.
byte
102
15
196
76
123
10
2
/
/
pinsrw
0x2
0xa
(
%
ebx
%
edi
2
)
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
131
248
3
/
/
cmp
0x3
%
eax
.
byte
114
18
/
/
jb
32f9
<
_sk_load_tables_rgb_u16_be_sse2
+
0x224
>
.
byte
102
15
110
92
123
12
/
/
movd
0xc
(
%
ebx
%
edi
2
)
%
xmm3
.
byte
102
15
196
92
123
16
2
/
/
pinsrw
0x2
0x10
(
%
ebx
%
edi
2
)
%
xmm3
.
byte
233
37
254
255
255
/
/
jmp
311e
<
_sk_load_tables_rgb_u16_be_sse2
+
0x49
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
233
28
254
255
255
/
/
jmp
311e
<
_sk_load_tables_rgb_u16_be_sse2
+
0x49
>
HIDDEN
_sk_byte_tables_sse2
.
globl
_sk_byte_tables_sse2
FUNCTION
(
_sk_byte_tables_sse2
)
_sk_byte_tables_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
3310
<
_sk_byte_tables_sse2
+
0xe
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
40
129
80
212
0
0
/
/
movaps
0xd450
(
%
ecx
)
%
xmm0
.
byte
15
93
232
/
/
minps
%
xmm0
%
xmm5
.
byte
15
40
161
32
213
0
0
/
/
movaps
0xd520
(
%
ecx
)
%
xmm4
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
102
15
91
237
/
/
cvtps2dq
%
xmm5
%
xmm5
.
byte
102
15
112
245
78
/
/
pshufd
0x4e
%
xmm5
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
112
245
231
/
/
pshufd
0xe7
%
xmm5
%
xmm6
.
byte
139
58
/
/
mov
(
%
edx
)
%
edi
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
102
15
126
243
/
/
movd
%
xmm6
%
ebx
.
byte
15
182
28
31
/
/
movzbl
(
%
edi
%
ebx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
9
243
/
/
or
%
esi
%
ebx
.
byte
102
15
112
245
229
/
/
pshufd
0xe5
%
xmm5
%
xmm6
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
15
182
4
7
/
/
movzbl
(
%
edi
%
eax
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
240
/
/
or
%
esi
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
139
66
4
/
/
mov
0x4
(
%
edx
)
%
eax
.
byte
102
15
196
235
1
/
/
pinsrw
0x1
%
ebx
%
xmm5
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
15
95
241
/
/
maxps
%
xmm1
%
xmm6
.
byte
15
93
240
/
/
minps
%
xmm0
%
xmm6
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
102
15
91
206
/
/
cvtps2dq
%
xmm6
%
xmm1
.
byte
102
15
112
241
78
/
/
pshufd
0x4e
%
xmm1
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
112
241
231
/
/
pshufd
0xe7
%
xmm1
%
xmm6
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
15
182
60
56
/
/
movzbl
(
%
eax
%
edi
1
)
%
edi
.
byte
193
231
8
/
/
shl
0x8
%
edi
.
byte
9
247
/
/
or
%
esi
%
edi
.
byte
102
15
112
241
229
/
/
pshufd
0xe5
%
xmm1
%
xmm6
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
102
15
126
243
/
/
movd
%
xmm6
%
ebx
.
byte
15
182
4
24
/
/
movzbl
(
%
eax
%
ebx
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
240
/
/
or
%
esi
%
eax
.
byte
102
15
110
200
/
/
movd
%
eax
%
xmm1
.
byte
102
15
196
207
1
/
/
pinsrw
0x1
%
edi
%
xmm1
.
byte
139
66
8
/
/
mov
0x8
(
%
edx
)
%
eax
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
15
95
242
/
/
maxps
%
xmm2
%
xmm6
.
byte
15
93
240
/
/
minps
%
xmm0
%
xmm6
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
102
15
91
214
/
/
cvtps2dq
%
xmm6
%
xmm2
.
byte
102
15
112
242
78
/
/
pshufd
0x4e
%
xmm2
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
112
242
231
/
/
pshufd
0xe7
%
xmm2
%
xmm6
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
15
182
60
56
/
/
movzbl
(
%
eax
%
edi
1
)
%
edi
.
byte
193
231
8
/
/
shl
0x8
%
edi
.
byte
9
247
/
/
or
%
esi
%
edi
.
byte
102
15
112
242
229
/
/
pshufd
0xe5
%
xmm2
%
xmm6
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
102
15
126
243
/
/
movd
%
xmm6
%
ebx
.
byte
15
182
4
24
/
/
movzbl
(
%
eax
%
ebx
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
240
/
/
or
%
esi
%
eax
.
byte
102
15
87
210
/
/
xorpd
%
xmm2
%
xmm2
.
byte
15
95
211
/
/
maxps
%
xmm3
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
102
15
110
216
/
/
movd
%
eax
%
xmm3
.
byte
102
15
196
223
1
/
/
pinsrw
0x1
%
edi
%
xmm3
.
byte
139
66
12
/
/
mov
0xc
(
%
edx
)
%
eax
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
91
194
/
/
cvtps2dq
%
xmm2
%
xmm0
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
15
126
210
/
/
movd
%
xmm2
%
edx
.
byte
102
15
112
208
231
/
/
pshufd
0xe7
%
xmm0
%
xmm2
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
15
182
20
16
/
/
movzbl
(
%
eax
%
edx
1
)
%
edx
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
193
230
8
/
/
shl
0x8
%
esi
.
byte
9
214
/
/
or
%
edx
%
esi
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
15
182
20
16
/
/
movzbl
(
%
eax
%
edx
1
)
%
edx
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
15
182
4
56
/
/
movzbl
(
%
eax
%
edi
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
208
/
/
or
%
edx
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
196
230
1
/
/
pinsrw
0x1
%
esi
%
xmm4
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
15
96
234
/
/
punpcklbw
%
xmm2
%
xmm5
.
byte
102
15
97
234
/
/
punpcklwd
%
xmm2
%
xmm5
.
byte
15
91
197
/
/
cvtdq2ps
%
xmm5
%
xmm0
.
byte
102
15
96
202
/
/
punpcklbw
%
xmm2
%
xmm1
.
byte
102
15
97
202
/
/
punpcklwd
%
xmm2
%
xmm1
.
byte
102
15
96
218
/
/
punpcklbw
%
xmm2
%
xmm3
.
byte
102
15
97
218
/
/
punpcklwd
%
xmm2
%
xmm3
.
byte
102
15
96
226
/
/
punpcklbw
%
xmm2
%
xmm4
.
byte
102
15
97
226
/
/
punpcklwd
%
xmm2
%
xmm4
.
byte
15
40
169
80
214
0
0
/
/
movaps
0xd650
(
%
ecx
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
91
211
/
/
cvtdq2ps
%
xmm3
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
91
220
/
/
cvtdq2ps
%
xmm4
%
xmm3
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_byte_tables_rgb_sse2
.
globl
_sk_byte_tables_rgb_sse2
FUNCTION
(
_sk_byte_tables_rgb_sse2
)
_sk_byte_tables_rgb_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
34e3
<
_sk_byte_tables_rgb_sse2
+
0xe
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
139
114
12
/
/
mov
0xc
(
%
edx
)
%
esi
.
byte
78
/
/
dec
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
40
129
125
210
0
0
/
/
movaps
0xd27d
(
%
ecx
)
%
xmm0
.
byte
15
93
232
/
/
minps
%
xmm0
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
102
15
91
237
/
/
cvtps2dq
%
xmm5
%
xmm5
.
byte
102
15
112
245
78
/
/
pshufd
0x4e
%
xmm5
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
112
245
231
/
/
pshufd
0xe7
%
xmm5
%
xmm6
.
byte
139
58
/
/
mov
(
%
edx
)
%
edi
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
102
15
126
243
/
/
movd
%
xmm6
%
ebx
.
byte
15
182
28
31
/
/
movzbl
(
%
edi
%
ebx
1
)
%
ebx
.
byte
193
227
8
/
/
shl
0x8
%
ebx
.
byte
9
243
/
/
or
%
esi
%
ebx
.
byte
102
15
112
245
229
/
/
pshufd
0xe5
%
xmm5
%
xmm6
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
15
182
4
7
/
/
movzbl
(
%
edi
%
eax
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
240
/
/
or
%
esi
%
eax
.
byte
102
15
110
232
/
/
movd
%
eax
%
xmm5
.
byte
139
66
4
/
/
mov
0x4
(
%
edx
)
%
eax
.
byte
102
15
196
235
1
/
/
pinsrw
0x1
%
ebx
%
xmm5
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
15
95
241
/
/
maxps
%
xmm1
%
xmm6
.
byte
15
93
240
/
/
minps
%
xmm0
%
xmm6
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
102
15
91
206
/
/
cvtps2dq
%
xmm6
%
xmm1
.
byte
102
15
112
241
78
/
/
pshufd
0x4e
%
xmm1
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
112
241
231
/
/
pshufd
0xe7
%
xmm1
%
xmm6
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
15
182
60
56
/
/
movzbl
(
%
eax
%
edi
1
)
%
edi
.
byte
193
231
8
/
/
shl
0x8
%
edi
.
byte
9
247
/
/
or
%
esi
%
edi
.
byte
102
15
112
241
229
/
/
pshufd
0xe5
%
xmm1
%
xmm6
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
102
15
126
243
/
/
movd
%
xmm6
%
ebx
.
byte
15
182
4
24
/
/
movzbl
(
%
eax
%
ebx
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
240
/
/
or
%
esi
%
eax
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
15
95
202
/
/
maxps
%
xmm2
%
xmm1
.
byte
15
93
200
/
/
minps
%
xmm0
%
xmm1
.
byte
102
15
110
208
/
/
movd
%
eax
%
xmm2
.
byte
102
15
196
215
1
/
/
pinsrw
0x1
%
edi
%
xmm2
.
byte
139
66
8
/
/
mov
0x8
(
%
edx
)
%
eax
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
91
193
/
/
cvtps2dq
%
xmm1
%
xmm0
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
15
182
20
16
/
/
movzbl
(
%
eax
%
edx
1
)
%
edx
.
byte
15
182
52
48
/
/
movzbl
(
%
eax
%
esi
1
)
%
esi
.
byte
193
230
8
/
/
shl
0x8
%
esi
.
byte
9
214
/
/
or
%
edx
%
esi
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
15
182
20
16
/
/
movzbl
(
%
eax
%
edx
1
)
%
edx
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
15
182
4
56
/
/
movzbl
(
%
eax
%
edi
1
)
%
eax
.
byte
193
224
8
/
/
shl
0x8
%
eax
.
byte
9
208
/
/
or
%
edx
%
eax
.
byte
102
15
110
224
/
/
movd
%
eax
%
xmm4
.
byte
102
15
196
230
1
/
/
pinsrw
0x1
%
esi
%
xmm4
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
96
233
/
/
punpcklbw
%
xmm1
%
xmm5
.
byte
102
15
97
233
/
/
punpcklwd
%
xmm1
%
xmm5
.
byte
15
91
197
/
/
cvtdq2ps
%
xmm5
%
xmm0
.
byte
102
15
96
209
/
/
punpcklbw
%
xmm1
%
xmm2
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
96
225
/
/
punpcklbw
%
xmm1
%
xmm4
.
byte
102
15
97
225
/
/
punpcklwd
%
xmm1
%
xmm4
.
byte
15
40
169
125
212
0
0
/
/
movaps
0xd47d
(
%
ecx
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
91
202
/
/
cvtdq2ps
%
xmm2
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
91
212
/
/
cvtdq2ps
%
xmm4
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_table_r_sse2
.
globl
_sk_table_r_sse2
FUNCTION
(
_sk_table_r_sse2
)
_sk_table_r_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
81
4
/
/
mov
0x4
(
%
ecx
)
%
edx
.
byte
74
/
/
dec
%
edx
.
byte
102
15
110
226
/
/
movd
%
edx
%
xmm4
.
byte
232
0
0
0
0
/
/
call
3665
<
_sk_table_r_sse2
+
0x18
>
.
byte
90
/
/
pop
%
edx
.
byte
139
9
/
/
mov
(
%
ecx
)
%
ecx
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
93
170
251
208
0
0
/
/
minps
0xd0fb
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
102
15
91
197
/
/
cvtps2dq
%
xmm5
%
xmm0
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
15
112
232
231
/
/
pshufd
0xe7
%
xmm0
%
xmm5
.
byte
102
15
126
234
/
/
movd
%
xmm5
%
edx
.
byte
243
15
16
44
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm5
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
240
229
/
/
pshufd
0xe5
%
xmm0
%
xmm6
.
byte
243
15
16
4
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm0
.
byte
102
15
126
242
/
/
movd
%
xmm6
%
edx
.
byte
243
15
16
52
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm6
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
15
20
198
/
/
unpcklps
%
xmm6
%
xmm0
.
byte
102
15
20
196
/
/
unpcklpd
%
xmm4
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_table_g_sse2
.
globl
_sk_table_g_sse2
FUNCTION
(
_sk_table_g_sse2
)
_sk_table_g_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
81
4
/
/
mov
0x4
(
%
ecx
)
%
edx
.
byte
74
/
/
dec
%
edx
.
byte
102
15
110
226
/
/
movd
%
edx
%
xmm4
.
byte
232
0
0
0
0
/
/
call
36eb
<
_sk_table_g_sse2
+
0x18
>
.
byte
90
/
/
pop
%
edx
.
byte
139
9
/
/
mov
(
%
ecx
)
%
ecx
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
233
/
/
maxps
%
xmm1
%
xmm5
.
byte
15
93
170
117
208
0
0
/
/
minps
0xd075
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
102
15
91
205
/
/
cvtps2dq
%
xmm5
%
xmm1
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
15
112
233
231
/
/
pshufd
0xe7
%
xmm1
%
xmm5
.
byte
102
15
126
234
/
/
movd
%
xmm5
%
edx
.
byte
243
15
16
44
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm5
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
112
241
229
/
/
pshufd
0xe5
%
xmm1
%
xmm6
.
byte
243
15
16
12
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm1
.
byte
102
15
126
242
/
/
movd
%
xmm6
%
edx
.
byte
243
15
16
52
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm6
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
15
20
206
/
/
unpcklps
%
xmm6
%
xmm1
.
byte
102
15
20
204
/
/
unpcklpd
%
xmm4
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_table_b_sse2
.
globl
_sk_table_b_sse2
FUNCTION
(
_sk_table_b_sse2
)
_sk_table_b_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
81
4
/
/
mov
0x4
(
%
ecx
)
%
edx
.
byte
74
/
/
dec
%
edx
.
byte
102
15
110
226
/
/
movd
%
edx
%
xmm4
.
byte
232
0
0
0
0
/
/
call
3771
<
_sk_table_b_sse2
+
0x18
>
.
byte
90
/
/
pop
%
edx
.
byte
139
9
/
/
mov
(
%
ecx
)
%
ecx
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
234
/
/
maxps
%
xmm2
%
xmm5
.
byte
15
93
170
239
207
0
0
/
/
minps
0xcfef
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
102
15
91
213
/
/
cvtps2dq
%
xmm5
%
xmm2
.
byte
102
15
112
226
78
/
/
pshufd
0x4e
%
xmm2
%
xmm4
.
byte
102
15
112
234
231
/
/
pshufd
0xe7
%
xmm2
%
xmm5
.
byte
102
15
126
234
/
/
movd
%
xmm5
%
edx
.
byte
243
15
16
44
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm5
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
102
15
126
210
/
/
movd
%
xmm2
%
edx
.
byte
102
15
112
242
229
/
/
pshufd
0xe5
%
xmm2
%
xmm6
.
byte
243
15
16
20
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm2
.
byte
102
15
126
242
/
/
movd
%
xmm6
%
edx
.
byte
243
15
16
52
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm6
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
15
20
214
/
/
unpcklps
%
xmm6
%
xmm2
.
byte
102
15
20
212
/
/
unpcklpd
%
xmm4
%
xmm2
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_table_a_sse2
.
globl
_sk_table_a_sse2
FUNCTION
(
_sk_table_a_sse2
)
_sk_table_a_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
81
4
/
/
mov
0x4
(
%
ecx
)
%
edx
.
byte
74
/
/
dec
%
edx
.
byte
102
15
110
226
/
/
movd
%
edx
%
xmm4
.
byte
232
0
0
0
0
/
/
call
37f7
<
_sk_table_a_sse2
+
0x18
>
.
byte
90
/
/
pop
%
edx
.
byte
139
9
/
/
mov
(
%
ecx
)
%
ecx
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
235
/
/
maxps
%
xmm3
%
xmm5
.
byte
15
93
170
105
207
0
0
/
/
minps
0xcf69
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
102
15
91
221
/
/
cvtps2dq
%
xmm5
%
xmm3
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
15
112
235
231
/
/
pshufd
0xe7
%
xmm3
%
xmm5
.
byte
102
15
126
234
/
/
movd
%
xmm5
%
edx
.
byte
243
15
16
44
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm5
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
102
15
126
218
/
/
movd
%
xmm3
%
edx
.
byte
102
15
112
243
229
/
/
pshufd
0xe5
%
xmm3
%
xmm6
.
byte
243
15
16
28
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm3
.
byte
102
15
126
242
/
/
movd
%
xmm6
%
edx
.
byte
243
15
16
52
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm6
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
15
20
222
/
/
unpcklps
%
xmm6
%
xmm3
.
byte
102
15
20
220
/
/
unpcklpd
%
xmm4
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_parametric_r_sse2
.
globl
_sk_parametric_r_sse2
FUNCTION
(
_sk_parametric_r_sse2
)
_sk_parametric_r_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
40
/
/
sub
0x28
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
387e
<
_sk_parametric_r_sse2
+
0x19
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
81
12
/
/
movss
0xc
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
194
206
2
/
/
cmpleps
%
xmm6
%
xmm1
.
byte
243
15
16
113
24
/
/
movss
0x18
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
243
15
16
113
8
/
/
movss
0x8
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
91
245
/
/
cvtdq2ps
%
xmm5
%
xmm6
.
byte
15
89
178
114
209
0
0
/
/
mulps
0xd172
(
%
edx
)
%
xmm6
.
byte
15
40
186
130
209
0
0
/
/
movaps
0xd182
(
%
edx
)
%
xmm7
.
byte
15
84
253
/
/
andps
%
xmm5
%
xmm7
.
byte
15
86
186
210
206
0
0
/
/
orps
0xced2
(
%
edx
)
%
xmm7
.
byte
15
88
178
146
209
0
0
/
/
addps
0xd192
(
%
edx
)
%
xmm6
.
byte
15
40
162
162
209
0
0
/
/
movaps
0xd1a2
(
%
edx
)
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
88
186
178
209
0
0
/
/
addps
0xd1b2
(
%
edx
)
%
xmm7
.
byte
15
40
162
194
209
0
0
/
/
movaps
0xd1c2
(
%
edx
)
%
xmm4
.
byte
15
94
231
/
/
divps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
243
15
91
230
/
/
cvttps2dq
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
194
196
1
/
/
cmpltps
%
xmm4
%
xmm0
.
byte
15
40
186
226
206
0
0
/
/
movaps
0xcee2
(
%
edx
)
%
xmm7
.
byte
15
84
199
/
/
andps
%
xmm7
%
xmm0
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
88
178
210
209
0
0
/
/
addps
0xd1d2
(
%
edx
)
%
xmm6
.
byte
15
40
162
226
209
0
0
/
/
movaps
0xd1e2
(
%
edx
)
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
40
162
242
209
0
0
/
/
movaps
0xd1f2
(
%
edx
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
130
2
210
0
0
/
/
movaps
0xd202
(
%
edx
)
%
xmm0
.
byte
15
94
196
/
/
divps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
130
18
210
0
0
/
/
mulps
0xd212
(
%
edx
)
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
236
4
/
/
cmpneqps
%
xmm4
%
xmm5
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
243
15
16
65
20
/
/
movss
0x14
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
84
209
/
/
andps
%
xmm1
%
xmm2
.
byte
15
85
200
/
/
andnps
%
xmm0
%
xmm1
.
byte
15
86
202
/
/
orps
%
xmm2
%
xmm1
.
byte
15
95
204
/
/
maxps
%
xmm4
%
xmm1
.
byte
15
93
207
/
/
minps
%
xmm7
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
56
/
/
add
0x38
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_parametric_g_sse2
.
globl
_sk_parametric_g_sse2
FUNCTION
(
_sk_parametric_g_sse2
)
_sk_parametric_g_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
40
/
/
sub
0x28
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
232
0
0
0
0
/
/
call
39c1
<
_sk_parametric_g_sse2
+
0x16
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
81
12
/
/
movss
0xc
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
194
206
2
/
/
cmpleps
%
xmm6
%
xmm1
.
byte
243
15
16
113
24
/
/
movss
0x18
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
243
15
16
113
8
/
/
movss
0x8
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
91
245
/
/
cvtdq2ps
%
xmm5
%
xmm6
.
byte
15
89
178
47
208
0
0
/
/
mulps
0xd02f
(
%
edx
)
%
xmm6
.
byte
15
40
186
63
208
0
0
/
/
movaps
0xd03f
(
%
edx
)
%
xmm7
.
byte
15
84
253
/
/
andps
%
xmm5
%
xmm7
.
byte
15
86
186
143
205
0
0
/
/
orps
0xcd8f
(
%
edx
)
%
xmm7
.
byte
15
88
178
79
208
0
0
/
/
addps
0xd04f
(
%
edx
)
%
xmm6
.
byte
15
40
162
95
208
0
0
/
/
movaps
0xd05f
(
%
edx
)
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
88
186
111
208
0
0
/
/
addps
0xd06f
(
%
edx
)
%
xmm7
.
byte
15
40
162
127
208
0
0
/
/
movaps
0xd07f
(
%
edx
)
%
xmm4
.
byte
15
94
231
/
/
divps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
243
15
91
230
/
/
cvttps2dq
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
194
196
1
/
/
cmpltps
%
xmm4
%
xmm0
.
byte
15
40
186
159
205
0
0
/
/
movaps
0xcd9f
(
%
edx
)
%
xmm7
.
byte
15
84
199
/
/
andps
%
xmm7
%
xmm0
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
88
178
143
208
0
0
/
/
addps
0xd08f
(
%
edx
)
%
xmm6
.
byte
15
40
162
159
208
0
0
/
/
movaps
0xd09f
(
%
edx
)
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
40
162
175
208
0
0
/
/
movaps
0xd0af
(
%
edx
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
130
191
208
0
0
/
/
movaps
0xd0bf
(
%
edx
)
%
xmm0
.
byte
15
94
196
/
/
divps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
130
207
208
0
0
/
/
mulps
0xd0cf
(
%
edx
)
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
236
4
/
/
cmpneqps
%
xmm4
%
xmm5
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
243
15
16
65
20
/
/
movss
0x14
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
84
209
/
/
andps
%
xmm1
%
xmm2
.
byte
15
85
200
/
/
andnps
%
xmm0
%
xmm1
.
byte
15
86
202
/
/
orps
%
xmm2
%
xmm1
.
byte
15
95
204
/
/
maxps
%
xmm4
%
xmm1
.
byte
15
93
207
/
/
minps
%
xmm7
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
56
/
/
add
0x38
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_parametric_b_sse2
.
globl
_sk_parametric_b_sse2
FUNCTION
(
_sk_parametric_b_sse2
)
_sk_parametric_b_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
40
/
/
sub
0x28
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
232
0
0
0
0
/
/
call
3b01
<
_sk_parametric_b_sse2
+
0x16
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
73
12
/
/
movss
0xc
(
%
ecx
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
15
194
214
2
/
/
cmpleps
%
xmm6
%
xmm2
.
byte
243
15
16
113
24
/
/
movss
0x18
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
243
15
16
113
8
/
/
movss
0x8
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
91
245
/
/
cvtdq2ps
%
xmm5
%
xmm6
.
byte
15
89
178
239
206
0
0
/
/
mulps
0xceef
(
%
edx
)
%
xmm6
.
byte
15
40
186
255
206
0
0
/
/
movaps
0xceff
(
%
edx
)
%
xmm7
.
byte
15
84
253
/
/
andps
%
xmm5
%
xmm7
.
byte
15
86
186
79
204
0
0
/
/
orps
0xcc4f
(
%
edx
)
%
xmm7
.
byte
15
88
178
15
207
0
0
/
/
addps
0xcf0f
(
%
edx
)
%
xmm6
.
byte
15
40
162
31
207
0
0
/
/
movaps
0xcf1f
(
%
edx
)
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
88
186
47
207
0
0
/
/
addps
0xcf2f
(
%
edx
)
%
xmm7
.
byte
15
40
162
63
207
0
0
/
/
movaps
0xcf3f
(
%
edx
)
%
xmm4
.
byte
15
94
231
/
/
divps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
243
15
91
230
/
/
cvttps2dq
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
194
196
1
/
/
cmpltps
%
xmm4
%
xmm0
.
byte
15
40
186
95
204
0
0
/
/
movaps
0xcc5f
(
%
edx
)
%
xmm7
.
byte
15
84
199
/
/
andps
%
xmm7
%
xmm0
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
88
178
79
207
0
0
/
/
addps
0xcf4f
(
%
edx
)
%
xmm6
.
byte
15
40
162
95
207
0
0
/
/
movaps
0xcf5f
(
%
edx
)
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
40
162
111
207
0
0
/
/
movaps
0xcf6f
(
%
edx
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
130
127
207
0
0
/
/
movaps
0xcf7f
(
%
edx
)
%
xmm0
.
byte
15
94
196
/
/
divps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
130
143
207
0
0
/
/
mulps
0xcf8f
(
%
edx
)
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
236
4
/
/
cmpneqps
%
xmm4
%
xmm5
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
243
15
16
65
20
/
/
movss
0x14
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
84
202
/
/
andps
%
xmm2
%
xmm1
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
15
86
209
/
/
orps
%
xmm1
%
xmm2
.
byte
15
95
212
/
/
maxps
%
xmm4
%
xmm2
.
byte
15
93
215
/
/
minps
%
xmm7
%
xmm2
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
56
/
/
add
0x38
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_parametric_a_sse2
.
globl
_sk_parametric_a_sse2
FUNCTION
(
_sk_parametric_a_sse2
)
_sk_parametric_a_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
40
/
/
sub
0x28
%
esp
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
232
0
0
0
0
/
/
call
3c41
<
_sk_parametric_a_sse2
+
0x16
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
73
12
/
/
movss
0xc
(
%
ecx
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
194
222
2
/
/
cmpleps
%
xmm6
%
xmm3
.
byte
243
15
16
113
24
/
/
movss
0x18
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
243
15
16
113
8
/
/
movss
0x8
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
91
245
/
/
cvtdq2ps
%
xmm5
%
xmm6
.
byte
15
89
178
175
205
0
0
/
/
mulps
0xcdaf
(
%
edx
)
%
xmm6
.
byte
15
40
186
191
205
0
0
/
/
movaps
0xcdbf
(
%
edx
)
%
xmm7
.
byte
15
84
253
/
/
andps
%
xmm5
%
xmm7
.
byte
15
86
186
15
203
0
0
/
/
orps
0xcb0f
(
%
edx
)
%
xmm7
.
byte
15
88
178
207
205
0
0
/
/
addps
0xcdcf
(
%
edx
)
%
xmm6
.
byte
15
40
162
223
205
0
0
/
/
movaps
0xcddf
(
%
edx
)
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
88
186
239
205
0
0
/
/
addps
0xcdef
(
%
edx
)
%
xmm7
.
byte
15
40
162
255
205
0
0
/
/
movaps
0xcdff
(
%
edx
)
%
xmm4
.
byte
15
94
231
/
/
divps
%
xmm7
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
243
15
91
230
/
/
cvttps2dq
%
xmm6
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
194
196
1
/
/
cmpltps
%
xmm4
%
xmm0
.
byte
15
40
186
31
203
0
0
/
/
movaps
0xcb1f
(
%
edx
)
%
xmm7
.
byte
15
84
199
/
/
andps
%
xmm7
%
xmm0
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
88
178
15
206
0
0
/
/
addps
0xce0f
(
%
edx
)
%
xmm6
.
byte
15
40
162
31
206
0
0
/
/
movaps
0xce1f
(
%
edx
)
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
92
244
/
/
subps
%
xmm4
%
xmm6
.
byte
15
40
162
47
206
0
0
/
/
movaps
0xce2f
(
%
edx
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
40
130
63
206
0
0
/
/
movaps
0xce3f
(
%
edx
)
%
xmm0
.
byte
15
94
196
/
/
divps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
89
130
79
206
0
0
/
/
mulps
0xce4f
(
%
edx
)
%
xmm0
.
byte
102
15
91
192
/
/
cvtps2dq
%
xmm0
%
xmm0
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
236
4
/
/
cmpneqps
%
xmm4
%
xmm5
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
243
15
16
65
20
/
/
movss
0x14
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
84
203
/
/
andps
%
xmm3
%
xmm1
.
byte
15
85
216
/
/
andnps
%
xmm0
%
xmm3
.
byte
15
86
217
/
/
orps
%
xmm1
%
xmm3
.
byte
15
95
220
/
/
maxps
%
xmm4
%
xmm3
.
byte
15
93
223
/
/
minps
%
xmm7
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
40
85
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm2
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
56
/
/
add
0x38
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_gamma_sse2
.
globl
_sk_gamma_sse2
FUNCTION
(
_sk_gamma_sse2
)
_sk_gamma_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
88
/
/
sub
0x58
%
esp
.
byte
15
41
93
168
/
/
movaps
%
xmm3
-
0x58
(
%
ebp
)
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
41
69
184
/
/
movaps
%
xmm0
-
0x48
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
3d86
<
_sk_gamma_sse2
+
0x1b
>
.
byte
88
/
/
pop
%
eax
.
byte
15
91
232
/
/
cvtdq2ps
%
xmm0
%
xmm5
.
byte
15
89
168
106
204
0
0
/
/
mulps
0xcc6a
(
%
eax
)
%
xmm5
.
byte
15
40
160
122
204
0
0
/
/
movaps
0xcc7a
(
%
eax
)
%
xmm4
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
86
144
202
201
0
0
/
/
orps
0xc9ca
(
%
eax
)
%
xmm2
.
byte
15
88
168
138
204
0
0
/
/
addps
0xcc8a
(
%
eax
)
%
xmm5
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
89
136
154
204
0
0
/
/
mulps
0xcc9a
(
%
eax
)
%
xmm1
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
88
144
170
204
0
0
/
/
addps
0xccaa
(
%
eax
)
%
xmm2
.
byte
15
40
136
186
204
0
0
/
/
movaps
0xccba
(
%
eax
)
%
xmm1
.
byte
15
94
202
/
/
divps
%
xmm2
%
xmm1
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
16
2
/
/
movss
(
%
edx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
243
15
91
197
/
/
cvttps2dq
%
xmm5
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm1
.
byte
15
84
136
218
201
0
0
/
/
andps
0xc9da
(
%
eax
)
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
88
168
202
204
0
0
/
/
addps
0xccca
(
%
eax
)
%
xmm5
.
byte
15
40
184
234
204
0
0
/
/
movaps
0xccea
(
%
eax
)
%
xmm7
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
40
176
218
204
0
0
/
/
movaps
0xccda
(
%
eax
)
%
xmm6
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
40
144
250
204
0
0
/
/
movaps
0xccfa
(
%
eax
)
%
xmm2
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
94
216
/
/
divps
%
xmm0
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
91
193
/
/
cvtdq2ps
%
xmm1
%
xmm0
.
byte
15
89
128
106
204
0
0
/
/
mulps
0xcc6a
(
%
eax
)
%
xmm0
.
byte
15
84
204
/
/
andps
%
xmm4
%
xmm1
.
byte
15
86
136
202
201
0
0
/
/
orps
0xc9ca
(
%
eax
)
%
xmm1
.
byte
15
88
128
138
204
0
0
/
/
addps
0xcc8a
(
%
eax
)
%
xmm0
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
168
154
204
0
0
/
/
mulps
0xcc9a
(
%
eax
)
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
88
136
170
204
0
0
/
/
addps
0xccaa
(
%
eax
)
%
xmm1
.
byte
15
40
168
186
204
0
0
/
/
movaps
0xccba
(
%
eax
)
%
xmm5
.
byte
15
94
233
/
/
divps
%
xmm1
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
89
69
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
243
15
91
200
/
/
cvttps2dq
%
xmm0
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
194
233
1
/
/
cmpltps
%
xmm1
%
xmm5
.
byte
15
84
168
218
201
0
0
/
/
andps
0xc9da
(
%
eax
)
%
xmm5
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
88
128
202
204
0
0
/
/
addps
0xccca
(
%
eax
)
%
xmm0
.
byte
15
40
207
/
/
movaps
%
xmm7
%
xmm1
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
15
89
238
/
/
mulps
%
xmm6
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
94
233
/
/
divps
%
xmm1
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
40
77
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm1
.
byte
15
91
193
/
/
cvtdq2ps
%
xmm1
%
xmm0
.
byte
15
89
128
106
204
0
0
/
/
mulps
0xcc6a
(
%
eax
)
%
xmm0
.
byte
15
84
225
/
/
andps
%
xmm1
%
xmm4
.
byte
15
86
160
202
201
0
0
/
/
orps
0xc9ca
(
%
eax
)
%
xmm4
.
byte
15
88
128
138
204
0
0
/
/
addps
0xcc8a
(
%
eax
)
%
xmm0
.
byte
15
40
136
154
204
0
0
/
/
movaps
0xcc9a
(
%
eax
)
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
88
160
170
204
0
0
/
/
addps
0xccaa
(
%
eax
)
%
xmm4
.
byte
15
40
136
186
204
0
0
/
/
movaps
0xccba
(
%
eax
)
%
xmm1
.
byte
15
94
204
/
/
divps
%
xmm4
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
89
69
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
243
15
91
200
/
/
cvttps2dq
%
xmm0
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
194
225
1
/
/
cmpltps
%
xmm1
%
xmm4
.
byte
15
84
160
218
201
0
0
/
/
andps
0xc9da
(
%
eax
)
%
xmm4
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
88
128
202
204
0
0
/
/
addps
0xccca
(
%
eax
)
%
xmm0
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
92
252
/
/
subps
%
xmm4
%
xmm7
.
byte
15
94
215
/
/
divps
%
xmm7
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
40
128
10
205
0
0
/
/
movaps
0xcd0a
(
%
eax
)
%
xmm0
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
102
15
91
203
/
/
cvtps2dq
%
xmm3
%
xmm1
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
194
196
4
/
/
cmpneqps
%
xmm4
%
xmm0
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
102
15
91
205
/
/
cvtps2dq
%
xmm5
%
xmm1
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
194
220
4
/
/
cmpneqps
%
xmm4
%
xmm3
.
byte
15
84
217
/
/
andps
%
xmm1
%
xmm3
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
15
194
220
4
/
/
cmpneqps
%
xmm4
%
xmm3
.
byte
102
15
91
210
/
/
cvtps2dq
%
xmm2
%
xmm2
.
byte
15
84
218
/
/
andps
%
xmm2
%
xmm3
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
93
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
104
/
/
add
0x68
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_gamma_dst_sse2
.
globl
_sk_gamma_dst_sse2
FUNCTION
(
_sk_gamma_dst_sse2
)
_sk_gamma_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
129
236
212
0
0
0
/
/
sub
0xd4
%
esp
.
byte
15
41
157
40
255
255
255
/
/
movaps
%
xmm3
-
0xd8
(
%
ebp
)
.
byte
15
41
149
56
255
255
255
/
/
movaps
%
xmm2
-
0xc8
(
%
ebp
)
.
byte
15
41
141
72
255
255
255
/
/
movaps
%
xmm1
-
0xb8
(
%
ebp
)
.
byte
15
41
133
88
255
255
255
/
/
movaps
%
xmm0
-
0xa8
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
3fad
<
_sk_gamma_dst_sse2
+
0x2b
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
72
16
/
/
movaps
0x10
(
%
eax
)
%
xmm1
.
byte
15
91
193
/
/
cvtdq2ps
%
xmm1
%
xmm0
.
byte
15
40
150
67
202
0
0
/
/
movaps
0xca43
(
%
esi
)
%
xmm2
.
byte
15
41
149
104
255
255
255
/
/
movaps
%
xmm2
-
0x98
(
%
ebp
)
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
40
166
83
202
0
0
/
/
movaps
0xca53
(
%
esi
)
%
xmm4
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
40
158
163
199
0
0
/
/
movaps
0xc7a3
(
%
esi
)
%
xmm3
.
byte
15
41
157
120
255
255
255
/
/
movaps
%
xmm3
-
0x88
(
%
ebp
)
.
byte
15
86
211
/
/
orps
%
xmm3
%
xmm2
.
byte
15
40
158
99
202
0
0
/
/
movaps
0xca63
(
%
esi
)
%
xmm3
.
byte
15
41
93
136
/
/
movaps
%
xmm3
-
0x78
(
%
ebp
)
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
40
174
115
202
0
0
/
/
movaps
0xca73
(
%
esi
)
%
xmm5
.
byte
15
41
109
152
/
/
movaps
%
xmm5
-
0x68
(
%
ebp
)
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
40
158
131
202
0
0
/
/
movaps
0xca83
(
%
esi
)
%
xmm3
.
byte
15
41
93
168
/
/
movaps
%
xmm3
-
0x58
(
%
ebp
)
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
15
40
158
147
202
0
0
/
/
movaps
0xca93
(
%
esi
)
%
xmm3
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
94
218
/
/
divps
%
xmm2
%
xmm3
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
16
18
/
/
movss
(
%
edx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
243
15
91
208
/
/
cvttps2dq
%
xmm0
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
194
218
1
/
/
cmpltps
%
xmm2
%
xmm3
.
byte
15
40
174
179
199
0
0
/
/
movaps
0xc7b3
(
%
esi
)
%
xmm5
.
byte
15
41
109
200
/
/
movaps
%
xmm5
-
0x38
(
%
ebp
)
.
byte
15
84
221
/
/
andps
%
xmm5
%
xmm3
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
92
218
/
/
subps
%
xmm2
%
xmm3
.
byte
15
40
150
163
202
0
0
/
/
movaps
0xcaa3
(
%
esi
)
%
xmm2
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
40
190
195
202
0
0
/
/
movaps
0xcac3
(
%
esi
)
%
xmm7
.
byte
15
40
239
/
/
movaps
%
xmm7
%
xmm5
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
182
179
202
0
0
/
/
movaps
0xcab3
(
%
esi
)
%
xmm6
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
40
158
211
202
0
0
/
/
movaps
0xcad3
(
%
esi
)
%
xmm3
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
94
213
/
/
divps
%
xmm5
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
40
134
227
202
0
0
/
/
movaps
0xcae3
(
%
esi
)
%
xmm0
.
byte
15
41
69
232
/
/
movaps
%
xmm0
-
0x18
(
%
ebp
)
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
102
15
91
194
/
/
cvtps2dq
%
xmm2
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
194
202
4
/
/
cmpneqps
%
xmm2
%
xmm1
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
15
40
64
32
/
/
movaps
0x20
(
%
eax
)
%
xmm0
.
byte
15
41
72
16
/
/
movaps
%
xmm1
0x10
(
%
eax
)
.
byte
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm1
.
byte
15
89
141
104
255
255
255
/
/
mulps
-
0x98
(
%
ebp
)
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
86
149
120
255
255
255
/
/
orps
-
0x88
(
%
ebp
)
%
xmm2
.
byte
15
88
77
136
/
/
addps
-
0x78
(
%
ebp
)
%
xmm1
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
89
109
152
/
/
mulps
-
0x68
(
%
ebp
)
%
xmm5
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
15
88
85
168
/
/
addps
-
0x58
(
%
ebp
)
%
xmm2
.
byte
15
40
109
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm5
.
byte
15
94
234
/
/
divps
%
xmm2
%
xmm5
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
243
15
16
18
/
/
movss
(
%
edx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
243
15
91
209
/
/
cvttps2dq
%
xmm1
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
194
234
1
/
/
cmpltps
%
xmm2
%
xmm5
.
byte
15
84
109
200
/
/
andps
-
0x38
(
%
ebp
)
%
xmm5
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
15
88
77
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
238
/
/
mulps
%
xmm6
%
xmm5
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
94
234
/
/
divps
%
xmm2
%
xmm5
.
byte
15
88
233
/
/
addps
%
xmm1
%
xmm5
.
byte
15
89
109
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
91
205
/
/
cvtps2dq
%
xmm5
%
xmm1
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
194
197
4
/
/
cmpneqps
%
xmm5
%
xmm0
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
15
41
64
32
/
/
movaps
%
xmm0
0x20
(
%
eax
)
.
byte
15
40
64
48
/
/
movaps
0x30
(
%
eax
)
%
xmm0
.
byte
15
91
200
/
/
cvtdq2ps
%
xmm0
%
xmm1
.
byte
15
89
141
104
255
255
255
/
/
mulps
-
0x98
(
%
ebp
)
%
xmm1
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
86
165
120
255
255
255
/
/
orps
-
0x88
(
%
ebp
)
%
xmm4
.
byte
15
88
77
136
/
/
addps
-
0x78
(
%
ebp
)
%
xmm1
.
byte
15
40
85
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
92
202
/
/
subps
%
xmm2
%
xmm1
.
byte
15
88
101
168
/
/
addps
-
0x58
(
%
ebp
)
%
xmm4
.
byte
15
40
85
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm2
.
byte
15
94
212
/
/
divps
%
xmm4
%
xmm2
.
byte
15
92
202
/
/
subps
%
xmm2
%
xmm1
.
byte
243
15
16
18
/
/
movss
(
%
edx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
243
15
91
209
/
/
cvttps2dq
%
xmm1
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
194
226
1
/
/
cmpltps
%
xmm2
%
xmm4
.
byte
15
84
101
200
/
/
andps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
88
77
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
92
252
/
/
subps
%
xmm4
%
xmm7
.
byte
15
94
223
/
/
divps
%
xmm7
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
89
93
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
194
197
4
/
/
cmpneqps
%
xmm5
%
xmm0
.
byte
102
15
91
203
/
/
cvtps2dq
%
xmm3
%
xmm1
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
15
41
64
48
/
/
movaps
%
xmm0
0x30
(
%
eax
)
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
133
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm0
.
byte
15
40
141
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm1
.
byte
15
40
149
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm2
.
byte
15
40
157
40
255
255
255
/
/
movaps
-
0xd8
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
129
196
228
0
0
0
/
/
add
0xe4
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_lab_to_xyz_sse2
.
globl
_sk_lab_to_xyz_sse2
FUNCTION
(
_sk_lab_to_xyz_sse2
)
_sk_lab_to_xyz_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
24
/
/
sub
0x18
%
esp
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
232
0
0
0
0
/
/
call
4209
<
_sk_lab_to_xyz_sse2
+
0x15
>
.
byte
88
/
/
pop
%
eax
.
byte
15
89
144
151
200
0
0
/
/
mulps
0xc897
(
%
eax
)
%
xmm2
.
byte
15
40
160
39
198
0
0
/
/
movaps
0xc627
(
%
eax
)
%
xmm4
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
40
168
167
200
0
0
/
/
movaps
0xc8a7
(
%
eax
)
%
xmm5
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
88
144
183
200
0
0
/
/
addps
0xc8b7
(
%
eax
)
%
xmm2
.
byte
15
89
144
199
200
0
0
/
/
mulps
0xc8c7
(
%
eax
)
%
xmm2
.
byte
15
89
136
215
200
0
0
/
/
mulps
0xc8d7
(
%
eax
)
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
15
89
152
231
200
0
0
/
/
mulps
0xc8e7
(
%
eax
)
%
xmm3
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
152
247
200
0
0
/
/
movaps
0xc8f7
(
%
eax
)
%
xmm3
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
194
224
1
/
/
cmpltps
%
xmm0
%
xmm4
.
byte
15
40
176
7
201
0
0
/
/
movaps
0xc907
(
%
eax
)
%
xmm6
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
40
184
23
201
0
0
/
/
movaps
0xc917
(
%
eax
)
%
xmm7
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
15
85
225
/
/
andnps
%
xmm1
%
xmm4
.
byte
15
86
224
/
/
orps
%
xmm0
%
xmm4
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm1
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
15
85
202
/
/
andnps
%
xmm2
%
xmm1
.
byte
15
86
200
/
/
orps
%
xmm0
%
xmm1
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
194
216
1
/
/
cmpltps
%
xmm0
%
xmm3
.
byte
15
84
195
/
/
andps
%
xmm3
%
xmm0
.
byte
15
85
221
/
/
andnps
%
xmm5
%
xmm3
.
byte
15
86
216
/
/
orps
%
xmm0
%
xmm3
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
89
160
39
201
0
0
/
/
mulps
0xc927
(
%
eax
)
%
xmm4
.
byte
15
89
152
55
201
0
0
/
/
mulps
0xc937
(
%
eax
)
%
xmm3
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
40
/
/
add
0x28
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_a8_sse2
.
globl
_sk_load_a8_sse2
FUNCTION
(
_sk_load_a8_sse2
)
_sk_load_a8_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
42f9
<
_sk_load_a8_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
60
/
/
jne
4350
<
_sk_load_a8_sse2
+
0x65
>
.
byte
102
15
110
4
62
/
/
movd
(
%
esi
%
edi
1
)
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
102
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm0
.
byte
102
15
219
130
39
197
0
0
/
/
pand
0xc527
(
%
edx
)
%
xmm0
.
byte
15
91
216
/
/
cvtdq2ps
%
xmm0
%
xmm3
.
byte
15
89
154
103
198
0
0
/
/
mulps
0xc667
(
%
edx
)
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
102
15
87
201
/
/
xorpd
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
50
/
/
je
438a
<
_sk_load_a8_sse2
+
0x9f
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
4374
<
_sk_load_a8_sse2
+
0x89
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
187
/
/
jne
4321
<
_sk_load_a8_sse2
+
0x36
>
.
byte
15
182
92
62
2
/
/
movzbl
0x2
(
%
esi
%
edi
1
)
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
15
183
52
62
/
/
movzwl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
206
/
/
movd
%
esi
%
xmm1
.
byte
102
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
242
15
16
193
/
/
movsd
%
xmm1
%
xmm0
.
byte
235
151
/
/
jmp
4321
<
_sk_load_a8_sse2
+
0x36
>
.
byte
15
182
52
62
/
/
movzbl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
198
/
/
movd
%
esi
%
xmm0
.
byte
235
141
/
/
jmp
4321
<
_sk_load_a8_sse2
+
0x36
>
HIDDEN
_sk_load_a8_dst_sse2
.
globl
_sk_load_a8_dst_sse2
FUNCTION
(
_sk_load_a8_dst_sse2
)
_sk_load_a8_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
139
49
/
/
mov
(
%
ecx
)
%
esi
.
byte
139
122
4
/
/
mov
0x4
(
%
edx
)
%
edi
.
byte
15
175
121
4
/
/
imul
0x4
(
%
ecx
)
%
edi
.
byte
3
58
/
/
add
(
%
edx
)
%
edi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
127
97
48
/
/
movdqa
%
xmm4
0x30
(
%
ecx
)
.
byte
102
15
127
97
32
/
/
movdqa
%
xmm4
0x20
(
%
ecx
)
.
byte
102
15
127
97
16
/
/
movdqa
%
xmm4
0x10
(
%
ecx
)
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
232
0
0
0
0
/
/
call
43cd
<
_sk_load_a8_dst_sse2
+
0x39
>
.
byte
90
/
/
pop
%
edx
.
byte
117
54
/
/
jne
4406
<
_sk_load_a8_dst_sse2
+
0x72
>
.
byte
102
15
110
36
55
/
/
movd
(
%
edi
%
esi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
219
162
83
196
0
0
/
/
pand
0xc453
(
%
edx
)
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
162
147
197
0
0
/
/
mulps
0xc593
(
%
edx
)
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
46
/
/
je
443c
<
_sk_load_a8_dst_sse2
+
0xa8
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
4426
<
_sk_load_a8_dst_sse2
+
0x92
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
197
/
/
jne
43dd
<
_sk_load_a8_dst_sse2
+
0x49
>
.
byte
15
182
92
55
2
/
/
movzbl
0x2
(
%
edi
%
esi
1
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
15
183
52
55
/
/
movzwl
(
%
edi
%
esi
1
)
%
esi
.
byte
102
15
110
238
/
/
movd
%
esi
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
102
15
97
232
/
/
punpcklwd
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
235
161
/
/
jmp
43dd
<
_sk_load_a8_dst_sse2
+
0x49
>
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
235
151
/
/
jmp
43dd
<
_sk_load_a8_dst_sse2
+
0x49
>
HIDDEN
_sk_gather_a8_sse2
.
globl
_sk_gather_a8_sse2
FUNCTION
(
_sk_gather_a8_sse2
)
_sk_gather_a8_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
226
/
/
minps
%
xmm2
%
xmm4
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
204
/
/
cvttps2dq
%
xmm4
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
15
182
52
50
/
/
movzbl
(
%
edx
%
esi
1
)
%
esi
.
byte
15
182
60
58
/
/
movzbl
(
%
edx
%
edi
1
)
%
edi
.
byte
193
231
8
/
/
shl
0x8
%
edi
.
byte
9
247
/
/
or
%
esi
%
edi
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
15
182
12
10
/
/
movzbl
(
%
edx
%
ecx
1
)
%
ecx
.
byte
15
182
20
50
/
/
movzbl
(
%
edx
%
esi
1
)
%
edx
.
byte
193
226
8
/
/
shl
0x8
%
edx
.
byte
9
202
/
/
or
%
ecx
%
edx
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
196
199
1
/
/
pinsrw
0x1
%
edi
%
xmm0
.
byte
232
0
0
0
0
/
/
call
44fa
<
_sk_gather_a8_sse2
+
0xb4
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
96
193
/
/
punpcklbw
%
xmm1
%
xmm0
.
byte
102
15
97
193
/
/
punpcklwd
%
xmm1
%
xmm0
.
byte
15
91
216
/
/
cvtdq2ps
%
xmm0
%
xmm3
.
byte
15
89
153
102
196
0
0
/
/
mulps
0xc466
(
%
ecx
)
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
16
/
/
add
0x10
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_a8_sse2
.
globl
_sk_store_a8_sse2
FUNCTION
(
_sk_store_a8_sse2
)
_sk_store_a8_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
453e
<
_sk_store_a8_sse2
+
0xe
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
87
4
/
/
mov
0x4
(
%
edi
)
%
edx
.
byte
15
175
81
4
/
/
imul
0x4
(
%
ecx
)
%
edx
.
byte
3
23
/
/
add
(
%
edi
)
%
edx
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
227
/
/
maxps
%
xmm3
%
xmm4
.
byte
15
93
166
34
194
0
0
/
/
minps
0xc222
(
%
esi
)
%
xmm4
.
byte
15
89
166
242
194
0
0
/
/
mulps
0xc2f2
(
%
esi
)
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
224
/
/
packssdw
%
xmm0
%
xmm4
.
byte
102
15
103
224
/
/
packuswb
%
xmm0
%
xmm4
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
26
/
/
jne
459d
<
_sk_store_a8_sse2
+
0x6d
>
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
137
52
58
/
/
mov
%
esi
(
%
edx
%
edi
1
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
54
/
/
je
45e3
<
_sk_store_a8_sse2
+
0xb3
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
14
/
/
je
45c0
<
_sk_store_a8_sse2
+
0x90
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
211
/
/
jne
458a
<
_sk_store_a8_sse2
+
0x5a
>
.
byte
102
15
197
220
4
/
/
pextrw
0x4
%
xmm4
%
ebx
.
byte
136
92
58
2
/
/
mov
%
bl
0x2
(
%
edx
%
edi
1
)
.
byte
102
15
112
228
212
/
/
pshufd
0xd4
%
xmm4
%
xmm4
.
byte
102
15
219
166
18
198
0
0
/
/
pand
0xc612
(
%
esi
)
%
xmm4
.
byte
102
15
103
228
/
/
packuswb
%
xmm4
%
xmm4
.
byte
102
15
103
228
/
/
packuswb
%
xmm4
%
xmm4
.
byte
102
15
103
228
/
/
packuswb
%
xmm4
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
137
52
58
/
/
mov
%
si
(
%
edx
%
edi
1
)
.
byte
235
167
/
/
jmp
458a
<
_sk_store_a8_sse2
+
0x5a
>
.
byte
102
15
126
227
/
/
movd
%
xmm4
%
ebx
.
byte
136
28
58
/
/
mov
%
bl
(
%
edx
%
edi
1
)
.
byte
235
158
/
/
jmp
458a
<
_sk_store_a8_sse2
+
0x5a
>
HIDDEN
_sk_load_g8_sse2
.
globl
_sk_load_g8_sse2
FUNCTION
(
_sk_load_g8_sse2
)
_sk_load_g8_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
45fa
<
_sk_load_g8_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
63
/
/
jne
4654
<
_sk_load_g8_sse2
+
0x68
>
.
byte
102
15
110
4
62
/
/
movd
(
%
esi
%
edi
1
)
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
102
15
97
192
/
/
punpcklwd
%
xmm0
%
xmm0
.
byte
102
15
219
130
38
194
0
0
/
/
pand
0xc226
(
%
edx
)
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
130
102
195
0
0
/
/
mulps
0xc366
(
%
edx
)
%
xmm0
.
byte
141
112
8
/
/
lea
0x8
(
%
eax
)
%
esi
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
154
102
193
0
0
/
/
movaps
0xc166
(
%
edx
)
%
xmm3
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
86
/
/
push
%
esi
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
50
/
/
je
468e
<
_sk_load_g8_sse2
+
0xa2
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
4678
<
_sk_load_g8_sse2
+
0x8c
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
184
/
/
jne
4622
<
_sk_load_g8_sse2
+
0x36
>
.
byte
15
182
92
62
2
/
/
movzbl
0x2
(
%
esi
%
edi
1
)
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
102
15
112
192
69
/
/
pshufd
0x45
%
xmm0
%
xmm0
.
byte
15
183
52
62
/
/
movzwl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
206
/
/
movd
%
esi
%
xmm1
.
byte
102
15
96
200
/
/
punpcklbw
%
xmm0
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
242
15
16
193
/
/
movsd
%
xmm1
%
xmm0
.
byte
235
148
/
/
jmp
4622
<
_sk_load_g8_sse2
+
0x36
>
.
byte
15
182
52
62
/
/
movzbl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
198
/
/
movd
%
esi
%
xmm0
.
byte
235
138
/
/
jmp
4622
<
_sk_load_g8_sse2
+
0x36
>
HIDDEN
_sk_load_g8_dst_sse2
.
globl
_sk_load_g8_dst_sse2
FUNCTION
(
_sk_load_g8_dst_sse2
)
_sk_load_g8_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
46a6
<
_sk_load_g8_dst_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
73
/
/
jne
470a
<
_sk_load_g8_dst_sse2
+
0x72
>
.
byte
102
15
110
36
62
/
/
movd
(
%
esi
%
edi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
219
162
122
193
0
0
/
/
pand
0xc17a
(
%
edx
)
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
162
186
194
0
0
/
/
mulps
0xc2ba
(
%
edx
)
%
xmm4
.
byte
15
41
97
48
/
/
movaps
%
xmm4
0x30
(
%
ecx
)
.
byte
15
41
97
32
/
/
movaps
%
xmm4
0x20
(
%
ecx
)
.
byte
15
41
97
16
/
/
movaps
%
xmm4
0x10
(
%
ecx
)
.
byte
15
40
162
186
192
0
0
/
/
movaps
0xc0ba
(
%
edx
)
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
50
/
/
je
4744
<
_sk_load_g8_dst_sse2
+
0xac
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
472e
<
_sk_load_g8_dst_sse2
+
0x96
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
174
/
/
jne
46ce
<
_sk_load_g8_dst_sse2
+
0x36
>
.
byte
15
182
92
62
2
/
/
movzbl
0x2
(
%
esi
%
edi
1
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
15
183
52
62
/
/
movzwl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
238
/
/
movd
%
esi
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
102
15
97
232
/
/
punpcklwd
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
235
138
/
/
jmp
46ce
<
_sk_load_g8_dst_sse2
+
0x36
>
.
byte
15
182
52
62
/
/
movzbl
(
%
esi
%
edi
1
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
235
128
/
/
jmp
46ce
<
_sk_load_g8_dst_sse2
+
0x36
>
HIDDEN
_sk_gather_g8_sse2
.
globl
_sk_gather_g8_sse2
FUNCTION
(
_sk_gather_g8_sse2
)
_sk_gather_g8_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
226
/
/
minps
%
xmm2
%
xmm4
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
204
/
/
cvttps2dq
%
xmm4
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
15
182
52
50
/
/
movzbl
(
%
edx
%
esi
1
)
%
esi
.
byte
15
182
60
58
/
/
movzbl
(
%
edx
%
edi
1
)
%
edi
.
byte
193
231
8
/
/
shl
0x8
%
edi
.
byte
9
247
/
/
or
%
esi
%
edi
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
15
182
12
10
/
/
movzbl
(
%
edx
%
ecx
1
)
%
ecx
.
byte
15
182
20
50
/
/
movzbl
(
%
edx
%
esi
1
)
%
edx
.
byte
193
226
8
/
/
shl
0x8
%
edx
.
byte
9
202
/
/
or
%
ecx
%
edx
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
232
0
0
0
0
/
/
call
47fd
<
_sk_gather_g8_sse2
+
0xaf
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
102
15
196
199
1
/
/
pinsrw
0x1
%
edi
%
xmm0
.
byte
102
15
96
193
/
/
punpcklbw
%
xmm1
%
xmm0
.
byte
102
15
97
193
/
/
punpcklwd
%
xmm1
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
129
99
193
0
0
/
/
mulps
0xc163
(
%
ecx
)
%
xmm0
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
153
99
191
0
0
/
/
movaps
0xbf63
(
%
ecx
)
%
xmm3
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
82
/
/
push
%
edx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
16
/
/
add
0x10
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_565_sse2
.
globl
_sk_load_565_sse2
FUNCTION
(
_sk_load_565_sse2
)
_sk_load_565_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
4848
<
_sk_load_565_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
97
/
/
jne
48c6
<
_sk_load_565_sse2
+
0x8c
>
.
byte
243
15
126
20
126
/
/
movq
(
%
esi
%
edi
2
)
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
15
111
130
40
193
0
0
/
/
movdqa
0xc128
(
%
edx
)
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
130
56
193
0
0
/
/
mulps
0xc138
(
%
edx
)
%
xmm0
.
byte
102
15
111
138
72
193
0
0
/
/
movdqa
0xc148
(
%
edx
)
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
138
88
193
0
0
/
/
mulps
0xc158
(
%
edx
)
%
xmm1
.
byte
102
15
219
146
104
193
0
0
/
/
pand
0xc168
(
%
edx
)
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
146
120
193
0
0
/
/
mulps
0xc178
(
%
edx
)
%
xmm2
.
byte
141
112
8
/
/
lea
0x8
(
%
eax
)
%
esi
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
154
24
191
0
0
/
/
movaps
0xbf18
(
%
edx
)
%
xmm3
.
byte
86
/
/
push
%
esi
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
47
/
/
je
48fd
<
_sk_load_565_sse2
+
0xc3
>
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
48ea
<
_sk_load_565_sse2
+
0xb0
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
146
/
/
jne
486e
<
_sk_load_565_sse2
+
0x34
>
.
byte
15
183
92
126
4
/
/
movzwl
0x4
(
%
esi
%
edi
2
)
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
102
15
112
208
69
/
/
pshufd
0x45
%
xmm0
%
xmm2
.
byte
102
15
110
4
126
/
/
movd
(
%
esi
%
edi
2
)
%
xmm0
.
byte
242
15
112
192
212
/
/
pshuflw
0xd4
%
xmm0
%
xmm0
.
byte
242
15
16
208
/
/
movsd
%
xmm0
%
xmm2
.
byte
233
113
255
255
255
/
/
jmp
486e
<
_sk_load_565_sse2
+
0x34
>
.
byte
15
183
52
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
esi
.
byte
102
15
110
214
/
/
movd
%
esi
%
xmm2
.
byte
233
100
255
255
255
/
/
jmp
486e
<
_sk_load_565_sse2
+
0x34
>
HIDDEN
_sk_load_565_dst_sse2
.
globl
_sk_load_565_dst_sse2
FUNCTION
(
_sk_load_565_dst_sse2
)
_sk_load_565_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
4918
<
_sk_load_565_dst_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
113
/
/
jne
49a6
<
_sk_load_565_dst_sse2
+
0x9c
>
.
byte
243
15
126
36
126
/
/
movq
(
%
esi
%
edi
2
)
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
111
170
88
192
0
0
/
/
movdqa
0xc058
(
%
edx
)
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
170
104
192
0
0
/
/
mulps
0xc068
(
%
edx
)
%
xmm5
.
byte
15
41
105
16
/
/
movaps
%
xmm5
0x10
(
%
ecx
)
.
byte
102
15
111
170
120
192
0
0
/
/
movdqa
0xc078
(
%
edx
)
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
170
136
192
0
0
/
/
mulps
0xc088
(
%
edx
)
%
xmm5
.
byte
15
41
105
32
/
/
movaps
%
xmm5
0x20
(
%
ecx
)
.
byte
102
15
219
162
152
192
0
0
/
/
pand
0xc098
(
%
edx
)
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
162
168
192
0
0
/
/
mulps
0xc0a8
(
%
edx
)
%
xmm4
.
byte
15
41
97
48
/
/
movaps
%
xmm4
0x30
(
%
ecx
)
.
byte
15
40
162
72
190
0
0
/
/
movaps
0xbe48
(
%
edx
)
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
47
/
/
je
49dd
<
_sk_load_565_dst_sse2
+
0xd3
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
49ca
<
_sk_load_565_dst_sse2
+
0xc0
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
130
/
/
jne
493e
<
_sk_load_565_dst_sse2
+
0x34
>
.
byte
15
183
92
126
4
/
/
movzwl
0x4
(
%
esi
%
edi
2
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
102
15
110
44
126
/
/
movd
(
%
esi
%
edi
2
)
%
xmm5
.
byte
242
15
112
237
212
/
/
pshuflw
0xd4
%
xmm5
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
233
97
255
255
255
/
/
jmp
493e
<
_sk_load_565_dst_sse2
+
0x34
>
.
byte
15
183
52
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
233
84
255
255
255
/
/
jmp
493e
<
_sk_load_565_dst_sse2
+
0x34
>
HIDDEN
_sk_gather_565_sse2
.
globl
_sk_gather_565_sse2
FUNCTION
(
_sk_gather_565_sse2
)
_sk_gather_565_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
226
/
/
minps
%
xmm2
%
xmm4
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
204
/
/
cvttps2dq
%
xmm4
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
102
15
110
209
/
/
movd
%
ecx
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
102
15
196
209
1
/
/
pinsrw
0x1
%
ecx
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
112
193
231
/
/
pshufd
0xe7
%
xmm1
%
xmm0
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
102
15
196
209
2
/
/
pinsrw
0x2
%
ecx
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
232
0
0
0
0
/
/
call
4a9a
<
_sk_gather_565_sse2
+
0xb0
>
.
byte
90
/
/
pop
%
edx
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
209
3
/
/
pinsrw
0x3
%
ecx
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
15
111
130
214
190
0
0
/
/
movdqa
0xbed6
(
%
edx
)
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
130
230
190
0
0
/
/
mulps
0xbee6
(
%
edx
)
%
xmm0
.
byte
102
15
111
138
246
190
0
0
/
/
movdqa
0xbef6
(
%
edx
)
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
138
6
191
0
0
/
/
mulps
0xbf06
(
%
edx
)
%
xmm1
.
byte
102
15
219
146
22
191
0
0
/
/
pand
0xbf16
(
%
edx
)
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
146
38
191
0
0
/
/
mulps
0xbf26
(
%
edx
)
%
xmm2
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
154
198
188
0
0
/
/
movaps
0xbcc6
(
%
edx
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_565_sse2
.
globl
_sk_store_565_sse2
FUNCTION
(
_sk_store_565_sse2
)
_sk_store_565_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
4b0d
<
_sk_store_565_sse2
+
0xe
>
.
byte
94
/
/
pop
%
esi
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
40
182
83
188
0
0
/
/
movaps
0xbc53
(
%
esi
)
%
xmm6
.
byte
15
93
238
/
/
minps
%
xmm6
%
xmm5
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
95
249
/
/
maxps
%
xmm1
%
xmm7
.
byte
15
93
254
/
/
minps
%
xmm6
%
xmm7
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
15
93
230
/
/
minps
%
xmm6
%
xmm4
.
byte
15
40
182
83
192
0
0
/
/
movaps
0xc053
(
%
esi
)
%
xmm6
.
byte
15
89
238
/
/
mulps
%
xmm6
%
xmm5
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
102
15
91
237
/
/
cvtps2dq
%
xmm5
%
xmm5
.
byte
102
15
114
245
11
/
/
pslld
0xb
%
xmm5
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
102
15
86
229
/
/
orpd
%
xmm5
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
87
4
/
/
mov
0x4
(
%
edi
)
%
edx
.
byte
15
175
81
4
/
/
imul
0x4
(
%
ecx
)
%
edx
.
byte
1
210
/
/
add
%
edx
%
edx
.
byte
3
23
/
/
add
(
%
edi
)
%
edx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
89
190
99
192
0
0
/
/
mulps
0xc063
(
%
esi
)
%
xmm7
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
102
15
91
239
/
/
cvtps2dq
%
xmm7
%
xmm5
.
byte
102
15
114
245
5
/
/
pslld
0x5
%
xmm5
.
byte
102
15
86
229
/
/
orpd
%
xmm5
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
224
/
/
packssdw
%
xmm0
%
xmm4
.
byte
102
15
126
101
232
/
/
movd
%
xmm4
-
0x18
(
%
ebp
)
.
byte
102
15
112
236
229
/
/
pshufd
0xe5
%
xmm4
%
xmm5
.
byte
102
15
126
109
236
/
/
movd
%
xmm5
-
0x14
(
%
ebp
)
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
117
34
/
/
jne
4bbd
<
_sk_store_565_sse2
+
0xbe
>
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
126
235
/
/
movd
%
xmm5
%
ebx
.
byte
137
52
122
/
/
mov
%
esi
(
%
edx
%
edi
2
)
.
byte
137
92
122
4
/
/
mov
%
ebx
0x4
(
%
edx
%
edi
2
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
243
15
126
101
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
37
/
/
je
4bf3
<
_sk_store_565_sse2
+
0xf4
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
15
/
/
je
4be2
<
_sk_store_565_sse2
+
0xe3
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
210
/
/
jne
4baa
<
_sk_store_565_sse2
+
0xab
>
.
byte
102
15
197
244
4
/
/
pextrw
0x4
%
xmm4
%
esi
.
byte
102
137
116
122
4
/
/
mov
%
si
0x4
(
%
edx
%
edi
2
)
.
byte
102
15
112
228
212
/
/
pshufd
0xd4
%
xmm4
%
xmm4
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
126
36
122
/
/
movd
%
xmm4
(
%
edx
%
edi
2
)
.
byte
235
183
/
/
jmp
4baa
<
_sk_store_565_sse2
+
0xab
>
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
137
52
122
/
/
mov
%
si
(
%
edx
%
edi
2
)
.
byte
235
173
/
/
jmp
4baa
<
_sk_store_565_sse2
+
0xab
>
HIDDEN
_sk_load_4444_sse2
.
globl
_sk_load_4444_sse2
FUNCTION
(
_sk_load_4444_sse2
)
_sk_load_4444_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
4c0b
<
_sk_load_4444_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
112
/
/
jne
4c98
<
_sk_load_4444_sse2
+
0x9b
>
.
byte
243
15
126
28
126
/
/
movq
(
%
esi
%
edi
2
)
%
xmm3
.
byte
102
15
97
216
/
/
punpcklwd
%
xmm0
%
xmm3
.
byte
102
15
111
130
117
191
0
0
/
/
movdqa
0xbf75
(
%
edx
)
%
xmm0
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
130
133
191
0
0
/
/
mulps
0xbf85
(
%
edx
)
%
xmm0
.
byte
102
15
111
138
149
191
0
0
/
/
movdqa
0xbf95
(
%
edx
)
%
xmm1
.
byte
102
15
219
203
/
/
pand
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
138
165
191
0
0
/
/
mulps
0xbfa5
(
%
edx
)
%
xmm1
.
byte
102
15
111
146
181
191
0
0
/
/
movdqa
0xbfb5
(
%
edx
)
%
xmm2
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
146
197
191
0
0
/
/
mulps
0xbfc5
(
%
edx
)
%
xmm2
.
byte
102
15
219
154
213
191
0
0
/
/
pand
0xbfd5
(
%
edx
)
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
154
229
191
0
0
/
/
mulps
0xbfe5
(
%
edx
)
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
47
/
/
je
4ccf
<
_sk_load_4444_sse2
+
0xd2
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
19
/
/
je
4cbc
<
_sk_load_4444_sse2
+
0xbf
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
131
/
/
jne
4c31
<
_sk_load_4444_sse2
+
0x34
>
.
byte
15
183
92
126
4
/
/
movzwl
0x4
(
%
esi
%
edi
2
)
%
ebx
.
byte
102
15
110
195
/
/
movd
%
ebx
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
15
110
4
126
/
/
movd
(
%
esi
%
edi
2
)
%
xmm0
.
byte
242
15
112
192
212
/
/
pshuflw
0xd4
%
xmm0
%
xmm0
.
byte
242
15
16
216
/
/
movsd
%
xmm0
%
xmm3
.
byte
233
98
255
255
255
/
/
jmp
4c31
<
_sk_load_4444_sse2
+
0x34
>
.
byte
15
183
52
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
esi
.
byte
102
15
110
222
/
/
movd
%
esi
%
xmm3
.
byte
233
85
255
255
255
/
/
jmp
4c31
<
_sk_load_4444_sse2
+
0x34
>
HIDDEN
_sk_load_4444_dst_sse2
.
globl
_sk_load_4444_dst_sse2
FUNCTION
(
_sk_load_4444_dst_sse2
)
_sk_load_4444_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
4cea
<
_sk_load_4444_dst_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
133
128
0
0
0
/
/
jne
4d8b
<
_sk_load_4444_dst_sse2
+
0xaf
>
.
byte
243
15
126
36
126
/
/
movq
(
%
esi
%
edi
2
)
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
111
170
150
190
0
0
/
/
movdqa
0xbe96
(
%
edx
)
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
170
166
190
0
0
/
/
mulps
0xbea6
(
%
edx
)
%
xmm5
.
byte
15
41
105
16
/
/
movaps
%
xmm5
0x10
(
%
ecx
)
.
byte
102
15
111
170
182
190
0
0
/
/
movdqa
0xbeb6
(
%
edx
)
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
170
198
190
0
0
/
/
mulps
0xbec6
(
%
edx
)
%
xmm5
.
byte
15
41
105
32
/
/
movaps
%
xmm5
0x20
(
%
ecx
)
.
byte
102
15
111
170
214
190
0
0
/
/
movdqa
0xbed6
(
%
edx
)
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
89
170
230
190
0
0
/
/
mulps
0xbee6
(
%
edx
)
%
xmm5
.
byte
15
41
105
48
/
/
movaps
%
xmm5
0x30
(
%
ecx
)
.
byte
102
15
219
162
246
190
0
0
/
/
pand
0xbef6
(
%
edx
)
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
162
6
191
0
0
/
/
mulps
0xbf06
(
%
edx
)
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
51
/
/
je
4dc6
<
_sk_load_4444_dst_sse2
+
0xea
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
23
/
/
je
4db3
<
_sk_load_4444_dst_sse2
+
0xd7
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
15
133
111
255
255
255
/
/
jne
4d14
<
_sk_load_4444_dst_sse2
+
0x38
>
.
byte
15
183
92
126
4
/
/
movzwl
0x4
(
%
esi
%
edi
2
)
%
ebx
.
byte
102
15
110
227
/
/
movd
%
ebx
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
102
15
110
44
126
/
/
movd
(
%
esi
%
edi
2
)
%
xmm5
.
byte
242
15
112
237
212
/
/
pshuflw
0xd4
%
xmm5
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
233
78
255
255
255
/
/
jmp
4d14
<
_sk_load_4444_dst_sse2
+
0x38
>
.
byte
15
183
52
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
233
65
255
255
255
/
/
jmp
4d14
<
_sk_load_4444_dst_sse2
+
0x38
>
HIDDEN
_sk_gather_4444_sse2
.
globl
_sk_gather_4444_sse2
FUNCTION
(
_sk_gather_4444_sse2
)
_sk_gather_4444_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
226
/
/
minps
%
xmm2
%
xmm4
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
204
/
/
cvttps2dq
%
xmm4
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
102
15
110
217
/
/
movd
%
ecx
%
xmm3
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
102
15
196
217
1
/
/
pinsrw
0x1
%
ecx
%
xmm3
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
112
193
231
/
/
pshufd
0xe7
%
xmm1
%
xmm0
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
102
15
196
217
2
/
/
pinsrw
0x2
%
ecx
%
xmm3
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
15
183
12
74
/
/
movzwl
(
%
edx
%
ecx
2
)
%
ecx
.
byte
102
15
196
217
3
/
/
pinsrw
0x3
%
ecx
%
xmm3
.
byte
232
0
0
0
0
/
/
call
4e88
<
_sk_gather_4444_sse2
+
0xb5
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
97
216
/
/
punpcklwd
%
xmm0
%
xmm3
.
byte
102
15
111
129
248
188
0
0
/
/
movdqa
0xbcf8
(
%
ecx
)
%
xmm0
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
129
8
189
0
0
/
/
mulps
0xbd08
(
%
ecx
)
%
xmm0
.
byte
102
15
111
137
24
189
0
0
/
/
movdqa
0xbd18
(
%
ecx
)
%
xmm1
.
byte
102
15
219
203
/
/
pand
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
137
40
189
0
0
/
/
mulps
0xbd28
(
%
ecx
)
%
xmm1
.
byte
102
15
111
145
56
189
0
0
/
/
movdqa
0xbd38
(
%
ecx
)
%
xmm2
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
145
72
189
0
0
/
/
mulps
0xbd48
(
%
ecx
)
%
xmm2
.
byte
102
15
219
153
88
189
0
0
/
/
pand
0xbd58
(
%
ecx
)
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
153
104
189
0
0
/
/
mulps
0xbd68
(
%
ecx
)
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_4444_sse2
.
globl
_sk_store_4444_sse2
FUNCTION
(
_sk_store_4444_sse2
)
_sk_store_4444_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
4f09
<
_sk_store_4444_sse2
+
0x12
>
.
byte
88
/
/
pop
%
eax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
40
168
87
184
0
0
/
/
movaps
0xb857
(
%
eax
)
%
xmm5
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
40
176
247
188
0
0
/
/
movaps
0xbcf7
(
%
eax
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
102
15
114
244
12
/
/
pslld
0xc
%
xmm4
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
95
249
/
/
maxps
%
xmm1
%
xmm7
.
byte
15
93
253
/
/
minps
%
xmm5
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
102
15
91
255
/
/
cvtps2dq
%
xmm7
%
xmm7
.
byte
102
15
114
247
8
/
/
pslld
0x8
%
xmm7
.
byte
102
15
235
252
/
/
por
%
xmm4
%
xmm7
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
93
216
/
/
maxps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
93
221
/
/
minps
%
xmm5
%
xmm3
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
102
15
91
236
/
/
cvtps2dq
%
xmm4
%
xmm5
.
byte
102
15
114
245
4
/
/
pslld
0x4
%
xmm5
.
byte
102
15
91
227
/
/
cvtps2dq
%
xmm3
%
xmm4
.
byte
102
15
86
229
/
/
orpd
%
xmm5
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
102
15
86
231
/
/
orpd
%
xmm7
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
224
/
/
packssdw
%
xmm0
%
xmm4
.
byte
102
15
126
101
232
/
/
movd
%
xmm4
-
0x18
(
%
ebp
)
.
byte
102
15
112
236
229
/
/
pshufd
0xe5
%
xmm4
%
xmm5
.
byte
102
15
126
109
236
/
/
movd
%
xmm5
-
0x14
(
%
ebp
)
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
117
42
/
/
jne
4fdc
<
_sk_store_4444_sse2
+
0xe5
>
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
102
15
126
235
/
/
movd
%
xmm5
%
ebx
.
byte
137
60
86
/
/
mov
%
edi
(
%
esi
%
edx
2
)
.
byte
137
92
86
4
/
/
mov
%
ebx
0x4
(
%
esi
%
edx
2
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
243
15
126
101
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
37
/
/
je
5012
<
_sk_store_4444_sse2
+
0x11b
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
15
/
/
je
5001
<
_sk_store_4444_sse2
+
0x10a
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
202
/
/
jne
4fc1
<
_sk_store_4444_sse2
+
0xca
>
.
byte
102
15
197
252
4
/
/
pextrw
0x4
%
xmm4
%
edi
.
byte
102
137
124
86
4
/
/
mov
%
di
0x4
(
%
esi
%
edx
2
)
.
byte
102
15
112
220
212
/
/
pshufd
0xd4
%
xmm4
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
126
28
86
/
/
movd
%
xmm3
(
%
esi
%
edx
2
)
.
byte
235
175
/
/
jmp
4fc1
<
_sk_store_4444_sse2
+
0xca
>
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
102
137
60
86
/
/
mov
%
di
(
%
esi
%
edx
2
)
.
byte
235
165
/
/
jmp
4fc1
<
_sk_store_4444_sse2
+
0xca
>
HIDDEN
_sk_load_8888_sse2
.
globl
_sk_load_8888_sse2
FUNCTION
(
_sk_load_8888_sse2
)
_sk_load_8888_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
502a
<
_sk_load_8888_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
102
/
/
jne
50ae
<
_sk_load_8888_sse2
+
0x92
>
.
byte
243
15
111
28
190
/
/
movdqu
(
%
esi
%
edi
4
)
%
xmm3
.
byte
102
15
111
146
246
183
0
0
/
/
movdqa
0xb7f6
(
%
edx
)
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
162
54
185
0
0
/
/
movaps
0xb936
(
%
edx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
114
213
16
/
/
psrld
0x10
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
15
91
213
/
/
cvtdq2ps
%
xmm5
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
35
/
/
je
50d9
<
_sk_load_8888_sse2
+
0xbd
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
50cf
<
_sk_load_8888_sse2
+
0xb3
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
137
/
/
jne
504d
<
_sk_load_8888_sse2
+
0x31
>
.
byte
102
15
110
68
190
8
/
/
movd
0x8
(
%
esi
%
edi
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
15
18
28
190
/
/
movlpd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
233
116
255
255
255
/
/
jmp
504d
<
_sk_load_8888_sse2
+
0x31
>
.
byte
102
15
110
28
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
233
106
255
255
255
/
/
jmp
504d
<
_sk_load_8888_sse2
+
0x31
>
HIDDEN
_sk_load_8888_dst_sse2
.
globl
_sk_load_8888_dst_sse2
FUNCTION
(
_sk_load_8888_dst_sse2
)
_sk_load_8888_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
50f1
<
_sk_load_8888_dst_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
118
/
/
jne
5185
<
_sk_load_8888_dst_sse2
+
0xa2
>
.
byte
243
15
111
36
190
/
/
movdqu
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
111
170
47
183
0
0
/
/
movdqa
0xb72f
(
%
edx
)
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
40
186
111
184
0
0
/
/
movaps
0xb86f
(
%
edx
)
%
xmm7
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
41
113
16
/
/
movaps
%
xmm6
0x10
(
%
ecx
)
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
114
214
8
/
/
psrld
0x8
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
41
113
32
/
/
movaps
%
xmm6
0x20
(
%
ecx
)
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
114
214
16
/
/
psrld
0x10
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
238
/
/
cvtdq2ps
%
xmm6
%
xmm5
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
41
105
48
/
/
movaps
%
xmm5
0x30
(
%
ecx
)
.
byte
102
15
114
212
24
/
/
psrld
0x18
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
39
/
/
je
51b4
<
_sk_load_8888_dst_sse2
+
0xd1
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
20
/
/
je
51aa
<
_sk_load_8888_dst_sse2
+
0xc7
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
15
133
117
255
255
255
/
/
jne
5114
<
_sk_load_8888_dst_sse2
+
0x31
>
.
byte
102
15
110
100
190
8
/
/
movd
0x8
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
102
15
18
36
190
/
/
movlpd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
233
96
255
255
255
/
/
jmp
5114
<
_sk_load_8888_dst_sse2
+
0x31
>
.
byte
102
15
110
36
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
233
86
255
255
255
/
/
jmp
5114
<
_sk_load_8888_dst_sse2
+
0x31
>
HIDDEN
_sk_gather_8888_sse2
.
globl
_sk_gather_8888_sse2
FUNCTION
(
_sk_gather_8888_sse2
)
_sk_gather_8888_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
93
234
/
/
minps
%
xmm2
%
xmm5
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
93
224
/
/
minps
%
xmm0
%
xmm4
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
205
/
/
cvttps2dq
%
xmm5
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
112
209
231
/
/
pshufd
0xe7
%
xmm1
%
xmm2
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
102
15
110
20
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
110
4
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm0
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
102
15
110
28
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm3
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
110
12
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm1
.
byte
232
0
0
0
0
/
/
call
5264
<
_sk_gather_8888_sse2
+
0xa6
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
98
194
/
/
punpckldq
%
xmm2
%
xmm0
.
byte
102
15
98
217
/
/
punpckldq
%
xmm1
%
xmm3
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
102
15
111
145
188
181
0
0
/
/
movdqa
0xb5bc
(
%
ecx
)
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
161
252
182
0
0
/
/
movaps
0xb6fc
(
%
ecx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
114
213
16
/
/
psrld
0x10
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
15
91
213
/
/
cvtdq2ps
%
xmm5
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_8888_sse2
.
globl
_sk_store_8888_sse2
FUNCTION
(
_sk_store_8888_sse2
)
_sk_store_8888_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
52e3
<
_sk_store_8888_sse2
+
0x12
>
.
byte
88
/
/
pop
%
eax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
40
168
125
180
0
0
/
/
movaps
0xb47d
(
%
eax
)
%
xmm5
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
40
176
77
181
0
0
/
/
movaps
0xb54d
(
%
eax
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
95
249
/
/
maxps
%
xmm1
%
xmm7
.
byte
15
93
253
/
/
minps
%
xmm5
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
102
15
91
255
/
/
cvtps2dq
%
xmm7
%
xmm7
.
byte
102
15
114
247
8
/
/
pslld
0x8
%
xmm7
.
byte
102
15
235
252
/
/
por
%
xmm4
%
xmm7
.
byte
102
15
87
228
/
/
xorpd
%
xmm4
%
xmm4
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
93
216
/
/
maxps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
93
221
/
/
minps
%
xmm5
%
xmm3
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
102
15
91
236
/
/
cvtps2dq
%
xmm4
%
xmm5
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
91
227
/
/
cvtps2dq
%
xmm3
%
xmm4
.
byte
102
15
114
244
24
/
/
pslld
0x18
%
xmm4
.
byte
102
15
235
229
/
/
por
%
xmm5
%
xmm4
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
117
32
/
/
jne
5390
<
_sk_store_8888_sse2
+
0xbf
>
.
byte
243
15
127
36
150
/
/
movdqu
%
xmm4
(
%
esi
%
edx
4
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
28
/
/
je
53b4
<
_sk_store_8888_sse2
+
0xe3
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
53ad
<
_sk_store_8888_sse2
+
0xdc
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
211
/
/
jne
5375
<
_sk_store_8888_sse2
+
0xa4
>
.
byte
102
15
112
220
78
/
/
pshufd
0x4e
%
xmm4
%
xmm3
.
byte
102
15
126
92
150
8
/
/
movd
%
xmm3
0x8
(
%
esi
%
edx
4
)
.
byte
102
15
214
36
150
/
/
movq
%
xmm4
(
%
esi
%
edx
4
)
.
byte
235
193
/
/
jmp
5375
<
_sk_store_8888_sse2
+
0xa4
>
.
byte
102
15
126
36
150
/
/
movd
%
xmm4
(
%
esi
%
edx
4
)
.
byte
235
186
/
/
jmp
5375
<
_sk_store_8888_sse2
+
0xa4
>
HIDDEN
_sk_load_bgra_sse2
.
globl
_sk_load_bgra_sse2
FUNCTION
(
_sk_load_bgra_sse2
)
_sk_load_bgra_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
53c9
<
_sk_load_bgra_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
102
/
/
jne
544d
<
_sk_load_bgra_sse2
+
0x92
>
.
byte
243
15
111
28
190
/
/
movdqu
(
%
esi
%
edi
4
)
%
xmm3
.
byte
102
15
111
130
87
180
0
0
/
/
movdqa
0xb457
(
%
edx
)
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
209
/
/
cvtdq2ps
%
xmm1
%
xmm2
.
byte
15
40
162
151
181
0
0
/
/
movaps
0xb597
(
%
edx
)
%
xmm4
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
114
213
16
/
/
psrld
0x10
%
xmm5
.
byte
102
15
219
232
/
/
pand
%
xmm0
%
xmm5
.
byte
15
91
197
/
/
cvtdq2ps
%
xmm5
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
35
/
/
je
5478
<
_sk_load_bgra_sse2
+
0xbd
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
546e
<
_sk_load_bgra_sse2
+
0xb3
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
137
/
/
jne
53ec
<
_sk_load_bgra_sse2
+
0x31
>
.
byte
102
15
110
68
190
8
/
/
movd
0x8
(
%
esi
%
edi
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
15
18
28
190
/
/
movlpd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
233
116
255
255
255
/
/
jmp
53ec
<
_sk_load_bgra_sse2
+
0x31
>
.
byte
102
15
110
28
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
233
106
255
255
255
/
/
jmp
53ec
<
_sk_load_bgra_sse2
+
0x31
>
HIDDEN
_sk_load_bgra_dst_sse2
.
globl
_sk_load_bgra_dst_sse2
FUNCTION
(
_sk_load_bgra_dst_sse2
)
_sk_load_bgra_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
5490
<
_sk_load_bgra_dst_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
118
/
/
jne
5524
<
_sk_load_bgra_dst_sse2
+
0xa2
>
.
byte
243
15
111
36
190
/
/
movdqu
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
111
170
144
179
0
0
/
/
movdqa
0xb390
(
%
edx
)
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
40
186
208
180
0
0
/
/
movaps
0xb4d0
(
%
edx
)
%
xmm7
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
41
113
48
/
/
movaps
%
xmm6
0x30
(
%
ecx
)
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
114
214
8
/
/
psrld
0x8
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
41
113
32
/
/
movaps
%
xmm6
0x20
(
%
ecx
)
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
114
214
16
/
/
psrld
0x10
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
238
/
/
cvtdq2ps
%
xmm6
%
xmm5
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
41
105
16
/
/
movaps
%
xmm5
0x10
(
%
ecx
)
.
byte
102
15
114
212
24
/
/
psrld
0x18
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
39
/
/
je
5553
<
_sk_load_bgra_dst_sse2
+
0xd1
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
20
/
/
je
5549
<
_sk_load_bgra_dst_sse2
+
0xc7
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
15
133
117
255
255
255
/
/
jne
54b3
<
_sk_load_bgra_dst_sse2
+
0x31
>
.
byte
102
15
110
100
190
8
/
/
movd
0x8
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
102
15
18
36
190
/
/
movlpd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
233
96
255
255
255
/
/
jmp
54b3
<
_sk_load_bgra_dst_sse2
+
0x31
>
.
byte
102
15
110
36
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
233
86
255
255
255
/
/
jmp
54b3
<
_sk_load_bgra_dst_sse2
+
0x31
>
HIDDEN
_sk_gather_bgra_sse2
.
globl
_sk_gather_bgra_sse2
FUNCTION
(
_sk_gather_bgra_sse2
)
_sk_gather_bgra_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
93
234
/
/
minps
%
xmm2
%
xmm5
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
93
224
/
/
minps
%
xmm0
%
xmm4
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
205
/
/
cvttps2dq
%
xmm5
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
112
209
231
/
/
pshufd
0xe7
%
xmm1
%
xmm2
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
102
15
110
20
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
110
4
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm0
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
102
15
110
28
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm3
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
110
12
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm1
.
byte
232
0
0
0
0
/
/
call
5603
<
_sk_gather_bgra_sse2
+
0xa6
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
98
194
/
/
punpckldq
%
xmm2
%
xmm0
.
byte
102
15
98
217
/
/
punpckldq
%
xmm1
%
xmm3
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
102
15
111
129
29
178
0
0
/
/
movdqa
0xb21d
(
%
ecx
)
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
209
/
/
cvtdq2ps
%
xmm1
%
xmm2
.
byte
15
40
161
93
179
0
0
/
/
movaps
0xb35d
(
%
ecx
)
%
xmm4
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
114
213
16
/
/
psrld
0x10
%
xmm5
.
byte
102
15
219
232
/
/
pand
%
xmm0
%
xmm5
.
byte
15
91
197
/
/
cvtdq2ps
%
xmm5
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_bgra_sse2
.
globl
_sk_store_bgra_sse2
FUNCTION
(
_sk_store_bgra_sse2
)
_sk_store_bgra_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
5682
<
_sk_store_bgra_sse2
+
0x12
>
.
byte
88
/
/
pop
%
eax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
15
40
168
222
176
0
0
/
/
movaps
0xb0de
(
%
eax
)
%
xmm5
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
40
176
174
177
0
0
/
/
movaps
0xb1ae
(
%
eax
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
95
249
/
/
maxps
%
xmm1
%
xmm7
.
byte
15
93
253
/
/
minps
%
xmm5
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
102
15
91
255
/
/
cvtps2dq
%
xmm7
%
xmm7
.
byte
102
15
114
247
8
/
/
pslld
0x8
%
xmm7
.
byte
102
15
235
252
/
/
por
%
xmm4
%
xmm7
.
byte
102
15
87
228
/
/
xorpd
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
95
93
216
/
/
maxps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
93
221
/
/
minps
%
xmm5
%
xmm3
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
102
15
91
236
/
/
cvtps2dq
%
xmm4
%
xmm5
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
91
227
/
/
cvtps2dq
%
xmm3
%
xmm4
.
byte
102
15
114
244
24
/
/
pslld
0x18
%
xmm4
.
byte
102
15
235
229
/
/
por
%
xmm5
%
xmm4
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
117
32
/
/
jne
572f
<
_sk_store_bgra_sse2
+
0xbf
>
.
byte
243
15
127
36
150
/
/
movdqu
%
xmm4
(
%
esi
%
edx
4
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
28
/
/
je
5753
<
_sk_store_bgra_sse2
+
0xe3
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
574c
<
_sk_store_bgra_sse2
+
0xdc
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
211
/
/
jne
5714
<
_sk_store_bgra_sse2
+
0xa4
>
.
byte
102
15
112
220
78
/
/
pshufd
0x4e
%
xmm4
%
xmm3
.
byte
102
15
126
92
150
8
/
/
movd
%
xmm3
0x8
(
%
esi
%
edx
4
)
.
byte
102
15
214
36
150
/
/
movq
%
xmm4
(
%
esi
%
edx
4
)
.
byte
235
193
/
/
jmp
5714
<
_sk_store_bgra_sse2
+
0xa4
>
.
byte
102
15
126
36
150
/
/
movd
%
xmm4
(
%
esi
%
edx
4
)
.
byte
235
186
/
/
jmp
5714
<
_sk_store_bgra_sse2
+
0xa4
>
HIDDEN
_sk_load_1010102_sse2
.
globl
_sk_load_1010102_sse2
FUNCTION
(
_sk_load_1010102_sse2
)
_sk_load_1010102_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
5768
<
_sk_load_1010102_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
106
/
/
jne
57f0
<
_sk_load_1010102_sse2
+
0x96
>
.
byte
243
15
111
28
190
/
/
movdqu
(
%
esi
%
edi
4
)
%
xmm3
.
byte
102
15
111
146
168
180
0
0
/
/
movdqa
0xb4a8
(
%
edx
)
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
162
184
180
0
0
/
/
movaps
0xb4b8
(
%
edx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
209
10
/
/
psrld
0xa
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
114
213
20
/
/
psrld
0x14
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
15
91
213
/
/
cvtdq2ps
%
xmm5
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
114
211
30
/
/
psrld
0x1e
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
154
200
177
0
0
/
/
mulps
0xb1c8
(
%
edx
)
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
35
/
/
je
581b
<
_sk_load_1010102_sse2
+
0xc1
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
5811
<
_sk_load_1010102_sse2
+
0xb7
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
133
/
/
jne
578b
<
_sk_load_1010102_sse2
+
0x31
>
.
byte
102
15
110
68
190
8
/
/
movd
0x8
(
%
esi
%
edi
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
15
18
28
190
/
/
movlpd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
233
112
255
255
255
/
/
jmp
578b
<
_sk_load_1010102_sse2
+
0x31
>
.
byte
102
15
110
28
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
233
102
255
255
255
/
/
jmp
578b
<
_sk_load_1010102_sse2
+
0x31
>
HIDDEN
_sk_load_1010102_dst_sse2
.
globl
_sk_load_1010102_dst_sse2
FUNCTION
(
_sk_load_1010102_dst_sse2
)
_sk_load_1010102_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
5833
<
_sk_load_1010102_dst_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
117
122
/
/
jne
58cb
<
_sk_load_1010102_dst_sse2
+
0xa6
>
.
byte
243
15
111
36
190
/
/
movdqu
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
111
170
221
179
0
0
/
/
movdqa
0xb3dd
(
%
edx
)
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
40
186
237
179
0
0
/
/
movaps
0xb3ed
(
%
edx
)
%
xmm7
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
41
113
16
/
/
movaps
%
xmm6
0x10
(
%
ecx
)
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
114
214
10
/
/
psrld
0xa
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
41
113
32
/
/
movaps
%
xmm6
0x20
(
%
ecx
)
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
114
214
20
/
/
psrld
0x14
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
15
91
238
/
/
cvtdq2ps
%
xmm6
%
xmm5
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
41
105
48
/
/
movaps
%
xmm5
0x30
(
%
ecx
)
.
byte
102
15
114
212
30
/
/
psrld
0x1e
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
162
253
176
0
0
/
/
mulps
0xb0fd
(
%
edx
)
%
xmm4
.
byte
15
41
97
64
/
/
movaps
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
39
/
/
je
58fa
<
_sk_load_1010102_dst_sse2
+
0xd5
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
20
/
/
je
58f0
<
_sk_load_1010102_dst_sse2
+
0xcb
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
15
133
113
255
255
255
/
/
jne
5856
<
_sk_load_1010102_dst_sse2
+
0x31
>
.
byte
102
15
110
100
190
8
/
/
movd
0x8
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
112
228
69
/
/
pshufd
0x45
%
xmm4
%
xmm4
.
byte
102
15
18
36
190
/
/
movlpd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
233
92
255
255
255
/
/
jmp
5856
<
_sk_load_1010102_dst_sse2
+
0x31
>
.
byte
102
15
110
36
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
233
82
255
255
255
/
/
jmp
5856
<
_sk_load_1010102_dst_sse2
+
0x31
>
HIDDEN
_sk_gather_1010102_sse2
.
globl
_sk_gather_1010102_sse2
FUNCTION
(
_sk_gather_1010102_sse2
)
_sk_gather_1010102_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
95
232
/
/
maxps
%
xmm0
%
xmm5
.
byte
15
93
234
/
/
minps
%
xmm2
%
xmm5
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
93
224
/
/
minps
%
xmm0
%
xmm4
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
205
/
/
cvttps2dq
%
xmm5
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
112
209
231
/
/
pshufd
0xe7
%
xmm1
%
xmm2
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
102
15
110
20
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
110
4
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm0
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
102
15
110
28
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm3
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
110
12
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm1
.
byte
232
0
0
0
0
/
/
call
59aa
<
_sk_gather_1010102_sse2
+
0xa6
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
98
194
/
/
punpckldq
%
xmm2
%
xmm0
.
byte
102
15
98
217
/
/
punpckldq
%
xmm1
%
xmm3
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
102
15
111
145
102
178
0
0
/
/
movdqa
0xb266
(
%
ecx
)
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
219
194
/
/
pand
%
xmm2
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
161
118
178
0
0
/
/
movaps
0xb276
(
%
ecx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
209
10
/
/
psrld
0xa
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
114
213
20
/
/
psrld
0x14
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
15
91
213
/
/
cvtdq2ps
%
xmm5
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
114
211
30
/
/
psrld
0x1e
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
153
134
175
0
0
/
/
mulps
0xaf86
(
%
ecx
)
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_1010102_sse2
.
globl
_sk_store_1010102_sse2
FUNCTION
(
_sk_store_1010102_sse2
)
_sk_store_1010102_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
232
0
0
0
0
/
/
call
5a29
<
_sk_store_1010102_sse2
+
0xe
>
.
byte
88
/
/
pop
%
eax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
40
168
55
173
0
0
/
/
movaps
0xad37
(
%
eax
)
%
xmm5
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
40
176
7
178
0
0
/
/
movaps
0xb207
(
%
eax
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
95
249
/
/
maxps
%
xmm1
%
xmm7
.
byte
15
93
253
/
/
minps
%
xmm5
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
102
15
91
255
/
/
cvtps2dq
%
xmm7
%
xmm7
.
byte
102
15
114
247
10
/
/
pslld
0xa
%
xmm7
.
byte
102
15
235
252
/
/
por
%
xmm4
%
xmm7
.
byte
102
15
87
228
/
/
xorpd
%
xmm4
%
xmm4
.
byte
15
95
226
/
/
maxps
%
xmm2
%
xmm4
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
102
15
91
244
/
/
cvtps2dq
%
xmm4
%
xmm6
.
byte
102
15
114
246
20
/
/
pslld
0x14
%
xmm6
.
byte
102
15
235
247
/
/
por
%
xmm7
%
xmm6
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
227
/
/
maxps
%
xmm3
%
xmm4
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
89
160
23
178
0
0
/
/
mulps
0xb217
(
%
eax
)
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
102
15
114
244
30
/
/
pslld
0x1e
%
xmm4
.
byte
102
15
235
230
/
/
por
%
xmm6
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
117
24
/
/
jne
5acd
<
_sk_store_1010102_sse2
+
0xb2
>
.
byte
243
15
127
36
150
/
/
movdqu
%
xmm4
(
%
esi
%
edx
4
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
128
227
3
/
/
and
0x3
%
bl
.
byte
128
251
1
/
/
cmp
0x1
%
bl
.
byte
116
28
/
/
je
5af1
<
_sk_store_1010102_sse2
+
0xd6
>
.
byte
128
251
2
/
/
cmp
0x2
%
bl
.
byte
116
16
/
/
je
5aea
<
_sk_store_1010102_sse2
+
0xcf
>
.
byte
128
251
3
/
/
cmp
0x3
%
bl
.
byte
117
219
/
/
jne
5aba
<
_sk_store_1010102_sse2
+
0x9f
>
.
byte
102
15
112
236
78
/
/
pshufd
0x4e
%
xmm4
%
xmm5
.
byte
102
15
126
108
150
8
/
/
movd
%
xmm5
0x8
(
%
esi
%
edx
4
)
.
byte
102
15
214
36
150
/
/
movq
%
xmm4
(
%
esi
%
edx
4
)
.
byte
235
201
/
/
jmp
5aba
<
_sk_store_1010102_sse2
+
0x9f
>
.
byte
102
15
126
36
150
/
/
movd
%
xmm4
(
%
esi
%
edx
4
)
.
byte
235
194
/
/
jmp
5aba
<
_sk_store_1010102_sse2
+
0x9f
>
HIDDEN
_sk_load_f16_sse2
.
globl
_sk_load_f16_sse2
FUNCTION
(
_sk_load_f16_sse2
)
_sk_load_f16_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
108
/
/
sub
0x6c
%
esp
.
byte
232
0
0
0
0
/
/
call
5b06
<
_sk_load_f16_sse2
+
0xe
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
3
/
/
shl
0x3
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
133
173
1
0
0
/
/
jne
5cd5
<
_sk_load_f16_sse2
+
0x1dd
>
.
byte
102
15
16
4
254
/
/
movupd
(
%
esi
%
edi
8
)
%
xmm0
.
byte
243
15
111
76
254
16
/
/
movdqu
0x10
(
%
esi
%
edi
8
)
%
xmm1
.
byte
102
15
40
216
/
/
movapd
%
xmm0
%
xmm3
.
byte
102
15
97
217
/
/
punpcklwd
%
xmm1
%
xmm3
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
97
208
/
/
punpcklwd
%
xmm0
%
xmm2
.
byte
102
15
105
216
/
/
punpckhwd
%
xmm0
%
xmm3
.
byte
102
15
126
85
208
/
/
movd
%
xmm2
-
0x30
(
%
ebp
)
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
15
126
69
212
/
/
movd
%
xmm0
-
0x2c
(
%
ebp
)
.
byte
243
15
126
77
208
/
/
movq
-
0x30
(
%
ebp
)
%
xmm1
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
69
220
/
/
movd
%
xmm0
-
0x24
(
%
ebp
)
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
15
126
69
216
/
/
movd
%
xmm0
-
0x28
(
%
ebp
)
.
byte
243
15
126
101
216
/
/
movq
-
0x28
(
%
ebp
)
%
xmm4
.
byte
102
15
126
93
224
/
/
movd
%
xmm3
-
0x20
(
%
ebp
)
.
byte
102
15
112
195
229
/
/
pshufd
0xe5
%
xmm3
%
xmm0
.
byte
102
15
126
69
228
/
/
movd
%
xmm0
-
0x1c
(
%
ebp
)
.
byte
242
15
16
69
224
/
/
movsd
-
0x20
(
%
ebp
)
%
xmm0
.
byte
15
41
69
152
/
/
movaps
%
xmm0
-
0x68
(
%
ebp
)
.
byte
102
15
112
195
231
/
/
pshufd
0xe7
%
xmm3
%
xmm0
.
byte
102
15
126
69
236
/
/
movd
%
xmm0
-
0x14
(
%
ebp
)
.
byte
102
15
112
195
78
/
/
pshufd
0x4e
%
xmm3
%
xmm0
.
byte
102
15
126
69
232
/
/
movd
%
xmm0
-
0x18
(
%
ebp
)
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
15
111
170
74
177
0
0
/
/
movdqa
0xb14a
(
%
edx
)
%
xmm5
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
219
245
/
/
pand
%
xmm5
%
xmm6
.
byte
102
15
219
138
90
177
0
0
/
/
pand
0xb15a
(
%
edx
)
%
xmm1
.
byte
102
15
111
130
106
177
0
0
/
/
movdqa
0xb16a
(
%
edx
)
%
xmm0
.
byte
102
15
127
69
184
/
/
movdqa
%
xmm0
-
0x48
(
%
ebp
)
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
102
217
/
/
pcmpgtd
%
xmm1
%
xmm3
.
byte
102
15
127
93
136
/
/
movdqa
%
xmm3
-
0x78
(
%
ebp
)
.
byte
102
15
114
241
13
/
/
pslld
0xd
%
xmm1
.
byte
102
15
235
206
/
/
por
%
xmm6
%
xmm1
.
byte
102
15
97
226
/
/
punpcklwd
%
xmm2
%
xmm4
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
219
213
/
/
pand
%
xmm5
%
xmm2
.
byte
102
15
111
154
90
177
0
0
/
/
movdqa
0xb15a
(
%
edx
)
%
xmm3
.
byte
102
15
219
227
/
/
pand
%
xmm3
%
xmm4
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
102
196
/
/
pcmpgtd
%
xmm4
%
xmm0
.
byte
102
15
127
69
168
/
/
movdqa
%
xmm0
-
0x58
(
%
ebp
)
.
byte
102
15
114
244
13
/
/
pslld
0xd
%
xmm4
.
byte
102
15
235
226
/
/
por
%
xmm2
%
xmm4
.
byte
102
15
111
69
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm0
.
byte
102
15
97
199
/
/
punpcklwd
%
xmm7
%
xmm0
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
219
253
/
/
pand
%
xmm5
%
xmm7
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
111
146
106
177
0
0
/
/
movdqa
0xb16a
(
%
edx
)
%
xmm2
.
byte
102
15
102
208
/
/
pcmpgtd
%
xmm0
%
xmm2
.
byte
102
15
114
240
13
/
/
pslld
0xd
%
xmm0
.
byte
102
15
235
199
/
/
por
%
xmm7
%
xmm0
.
byte
243
15
126
125
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm7
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
102
15
97
254
/
/
punpcklwd
%
xmm6
%
xmm7
.
byte
102
15
219
239
/
/
pand
%
xmm7
%
xmm5
.
byte
102
15
219
251
/
/
pand
%
xmm3
%
xmm7
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
111
93
184
/
/
movdqa
-
0x48
(
%
ebp
)
%
xmm3
.
byte
102
15
102
223
/
/
pcmpgtd
%
xmm7
%
xmm3
.
byte
102
15
127
93
184
/
/
movdqa
%
xmm3
-
0x48
(
%
ebp
)
.
byte
102
15
114
247
13
/
/
pslld
0xd
%
xmm7
.
byte
102
15
235
253
/
/
por
%
xmm5
%
xmm7
.
byte
102
15
111
154
122
177
0
0
/
/
movdqa
0xb17a
(
%
edx
)
%
xmm3
.
byte
102
15
254
203
/
/
paddd
%
xmm3
%
xmm1
.
byte
102
15
254
227
/
/
paddd
%
xmm3
%
xmm4
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
102
15
254
251
/
/
paddd
%
xmm3
%
xmm7
.
byte
102
15
111
93
136
/
/
movdqa
-
0x78
(
%
ebp
)
%
xmm3
.
byte
102
15
223
217
/
/
pandn
%
xmm1
%
xmm3
.
byte
102
15
111
77
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm1
.
byte
102
15
223
204
/
/
pandn
%
xmm4
%
xmm1
.
byte
102
15
223
208
/
/
pandn
%
xmm0
%
xmm2
.
byte
102
15
111
101
184
/
/
movdqa
-
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
223
231
/
/
pandn
%
xmm7
%
xmm4
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
124
/
/
add
0x7c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
242
15
16
4
254
/
/
movsd
(
%
esi
%
edi
8
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
131
251
1
/
/
cmp
0x1
%
ebx
.
byte
15
132
76
254
255
255
/
/
je
5b33
<
_sk_load_f16_sse2
+
0x3b
>
.
byte
102
15
22
68
254
8
/
/
movhpd
0x8
(
%
esi
%
edi
8
)
%
xmm0
.
byte
131
251
3
/
/
cmp
0x3
%
ebx
.
byte
15
130
61
254
255
255
/
/
jb
5b33
<
_sk_load_f16_sse2
+
0x3b
>
.
byte
243
15
126
76
254
16
/
/
movq
0x10
(
%
esi
%
edi
8
)
%
xmm1
.
byte
233
50
254
255
255
/
/
jmp
5b33
<
_sk_load_f16_sse2
+
0x3b
>
HIDDEN
_sk_load_f16_dst_sse2
.
globl
_sk_load_f16_dst_sse2
FUNCTION
(
_sk_load_f16_dst_sse2
)
_sk_load_f16_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
124
/
/
sub
0x7c
%
esp
.
byte
15
41
157
120
255
255
255
/
/
movaps
%
xmm3
-
0x88
(
%
ebp
)
.
byte
15
41
85
136
/
/
movaps
%
xmm2
-
0x78
(
%
ebp
)
.
byte
102
15
127
77
152
/
/
movdqa
%
xmm1
-
0x68
(
%
ebp
)
.
byte
15
41
69
168
/
/
movaps
%
xmm0
-
0x58
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
5d23
<
_sk_load_f16_dst_sse2
+
0x22
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
3
/
/
shl
0x3
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
133
168
1
0
0
/
/
jne
5eed
<
_sk_load_f16_dst_sse2
+
0x1ec
>
.
byte
102
15
16
4
254
/
/
movupd
(
%
esi
%
edi
8
)
%
xmm0
.
byte
243
15
111
76
254
16
/
/
movdqu
0x10
(
%
esi
%
edi
8
)
%
xmm1
.
byte
102
15
40
208
/
/
movapd
%
xmm0
%
xmm2
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
97
216
/
/
punpcklwd
%
xmm0
%
xmm3
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
15
126
93
208
/
/
movd
%
xmm3
-
0x30
(
%
ebp
)
.
byte
102
15
112
195
229
/
/
pshufd
0xe5
%
xmm3
%
xmm0
.
byte
102
15
126
69
212
/
/
movd
%
xmm0
-
0x2c
(
%
ebp
)
.
byte
243
15
126
77
208
/
/
movq
-
0x30
(
%
ebp
)
%
xmm1
.
byte
102
15
112
195
231
/
/
pshufd
0xe7
%
xmm3
%
xmm0
.
byte
102
15
126
69
220
/
/
movd
%
xmm0
-
0x24
(
%
ebp
)
.
byte
102
15
112
195
78
/
/
pshufd
0x4e
%
xmm3
%
xmm0
.
byte
102
15
126
69
216
/
/
movd
%
xmm0
-
0x28
(
%
ebp
)
.
byte
243
15
126
69
216
/
/
movq
-
0x28
(
%
ebp
)
%
xmm0
.
byte
102
15
126
85
224
/
/
movd
%
xmm2
-
0x20
(
%
ebp
)
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
15
126
93
228
/
/
movd
%
xmm3
-
0x1c
(
%
ebp
)
.
byte
243
15
126
117
224
/
/
movq
-
0x20
(
%
ebp
)
%
xmm6
.
byte
102
15
112
218
231
/
/
pshufd
0xe7
%
xmm2
%
xmm3
.
byte
102
15
126
93
236
/
/
movd
%
xmm3
-
0x14
(
%
ebp
)
.
byte
102
15
112
210
78
/
/
pshufd
0x4e
%
xmm2
%
xmm2
.
byte
102
15
126
85
232
/
/
movd
%
xmm2
-
0x18
(
%
ebp
)
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
15
97
202
/
/
punpcklwd
%
xmm2
%
xmm1
.
byte
102
15
111
186
45
175
0
0
/
/
movdqa
0xaf2d
(
%
edx
)
%
xmm7
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
219
215
/
/
pand
%
xmm7
%
xmm2
.
byte
102
15
111
154
61
175
0
0
/
/
movdqa
0xaf3d
(
%
edx
)
%
xmm3
.
byte
102
15
219
203
/
/
pand
%
xmm3
%
xmm1
.
byte
102
15
111
162
77
175
0
0
/
/
movdqa
0xaf4d
(
%
edx
)
%
xmm4
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
102
233
/
/
pcmpgtd
%
xmm1
%
xmm5
.
byte
102
15
114
241
13
/
/
pslld
0xd
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
111
146
93
175
0
0
/
/
movdqa
0xaf5d
(
%
edx
)
%
xmm2
.
byte
102
15
127
85
184
/
/
movdqa
%
xmm2
-
0x48
(
%
ebp
)
.
byte
102
15
254
202
/
/
paddd
%
xmm2
%
xmm1
.
byte
102
15
223
233
/
/
pandn
%
xmm1
%
xmm5
.
byte
243
15
126
77
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm1
.
byte
102
15
127
105
16
/
/
movdqa
%
xmm5
0x10
(
%
ecx
)
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
15
97
194
/
/
punpcklwd
%
xmm2
%
xmm0
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
219
239
/
/
pand
%
xmm7
%
xmm5
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
102
15
102
216
/
/
pcmpgtd
%
xmm0
%
xmm3
.
byte
102
15
114
240
13
/
/
pslld
0xd
%
xmm0
.
byte
102
15
235
197
/
/
por
%
xmm5
%
xmm0
.
byte
102
15
111
109
184
/
/
movdqa
-
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
254
197
/
/
paddd
%
xmm5
%
xmm0
.
byte
102
15
223
216
/
/
pandn
%
xmm0
%
xmm3
.
byte
102
15
127
89
32
/
/
movdqa
%
xmm3
0x20
(
%
ecx
)
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
97
240
/
/
punpcklwd
%
xmm0
%
xmm6
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
219
199
/
/
pand
%
xmm7
%
xmm0
.
byte
102
15
219
242
/
/
pand
%
xmm2
%
xmm6
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
102
15
102
222
/
/
pcmpgtd
%
xmm6
%
xmm3
.
byte
102
15
114
246
13
/
/
pslld
0xd
%
xmm6
.
byte
102
15
235
240
/
/
por
%
xmm0
%
xmm6
.
byte
102
15
254
245
/
/
paddd
%
xmm5
%
xmm6
.
byte
102
15
223
222
/
/
pandn
%
xmm6
%
xmm3
.
byte
102
15
127
89
48
/
/
movdqa
%
xmm3
0x30
(
%
ecx
)
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
97
203
/
/
punpcklwd
%
xmm3
%
xmm1
.
byte
102
15
219
249
/
/
pand
%
xmm1
%
xmm7
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
102
225
/
/
pcmpgtd
%
xmm1
%
xmm4
.
byte
102
15
114
241
13
/
/
pslld
0xd
%
xmm1
.
byte
102
15
235
207
/
/
por
%
xmm7
%
xmm1
.
byte
102
15
254
205
/
/
paddd
%
xmm5
%
xmm1
.
byte
102
15
223
225
/
/
pandn
%
xmm1
%
xmm4
.
byte
102
15
127
97
64
/
/
movdqa
%
xmm4
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
15
40
77
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm1
.
byte
15
40
85
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm2
.
byte
15
40
157
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
129
196
140
0
0
0
/
/
add
0x8c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
242
15
16
4
254
/
/
movsd
(
%
esi
%
edi
8
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
131
251
1
/
/
cmp
0x1
%
ebx
.
byte
15
132
81
254
255
255
/
/
je
5d50
<
_sk_load_f16_dst_sse2
+
0x4f
>
.
byte
102
15
22
68
254
8
/
/
movhpd
0x8
(
%
esi
%
edi
8
)
%
xmm0
.
byte
131
251
3
/
/
cmp
0x3
%
ebx
.
byte
15
130
66
254
255
255
/
/
jb
5d50
<
_sk_load_f16_dst_sse2
+
0x4f
>
.
byte
243
15
126
76
254
16
/
/
movq
0x10
(
%
esi
%
edi
8
)
%
xmm1
.
byte
233
55
254
255
255
/
/
jmp
5d50
<
_sk_load_f16_dst_sse2
+
0x4f
>
HIDDEN
_sk_gather_f16_sse2
.
globl
_sk_gather_f16_sse2
FUNCTION
(
_sk_gather_f16_sse2
)
_sk_gather_f16_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
88
/
/
sub
0x58
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
254
211
/
/
paddd
%
xmm3
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
226
/
/
minps
%
xmm2
%
xmm4
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
95
209
/
/
maxps
%
xmm1
%
xmm2
.
byte
15
93
208
/
/
minps
%
xmm0
%
xmm2
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
110
73
4
/
/
movd
0x4
(
%
ecx
)
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
112
202
232
/
/
pshufd
0xe8
%
xmm2
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
243
15
91
204
/
/
cvttps2dq
%
xmm4
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
112
209
231
/
/
pshufd
0xe7
%
xmm1
%
xmm2
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
243
15
126
20
202
/
/
movq
(
%
edx
%
ecx
8
)
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
243
15
126
4
202
/
/
movq
(
%
edx
%
ecx
8
)
%
xmm0
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
243
15
126
28
202
/
/
movq
(
%
edx
%
ecx
8
)
%
xmm3
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
243
15
126
12
202
/
/
movq
(
%
edx
%
ecx
8
)
%
xmm1
.
byte
232
0
0
0
0
/
/
call
5fbf
<
_sk_gather_f16_sse2
+
0xa6
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
102
15
108
194
/
/
punpcklqdq
%
xmm2
%
xmm0
.
byte
102
15
108
217
/
/
punpcklqdq
%
xmm1
%
xmm3
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
102
15
97
224
/
/
punpcklwd
%
xmm0
%
xmm4
.
byte
102
15
105
216
/
/
punpckhwd
%
xmm0
%
xmm3
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
97
195
/
/
punpcklwd
%
xmm3
%
xmm0
.
byte
102
15
105
227
/
/
punpckhwd
%
xmm3
%
xmm4
.
byte
102
15
126
69
224
/
/
movd
%
xmm0
-
0x20
(
%
ebp
)
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
77
228
/
/
movd
%
xmm1
-
0x1c
(
%
ebp
)
.
byte
243
15
126
77
224
/
/
movq
-
0x20
(
%
ebp
)
%
xmm1
.
byte
102
15
112
208
231
/
/
pshufd
0xe7
%
xmm0
%
xmm2
.
byte
102
15
126
85
236
/
/
movd
%
xmm2
-
0x14
(
%
ebp
)
.
byte
102
15
112
192
78
/
/
pshufd
0x4e
%
xmm0
%
xmm0
.
byte
102
15
126
69
232
/
/
movd
%
xmm0
-
0x18
(
%
ebp
)
.
byte
243
15
126
109
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
126
101
240
/
/
movd
%
xmm4
-
0x10
(
%
ebp
)
.
byte
102
15
112
196
229
/
/
pshufd
0xe5
%
xmm4
%
xmm0
.
byte
102
15
126
69
244
/
/
movd
%
xmm0
-
0xc
(
%
ebp
)
.
byte
242
15
16
69
240
/
/
movsd
-
0x10
(
%
ebp
)
%
xmm0
.
byte
15
41
69
184
/
/
movaps
%
xmm0
-
0x48
(
%
ebp
)
.
byte
102
15
112
196
231
/
/
pshufd
0xe7
%
xmm4
%
xmm0
.
byte
102
15
126
69
252
/
/
movd
%
xmm0
-
0x4
(
%
ebp
)
.
byte
102
15
112
196
78
/
/
pshufd
0x4e
%
xmm4
%
xmm0
.
byte
102
15
126
69
248
/
/
movd
%
xmm0
-
0x8
(
%
ebp
)
.
byte
102
15
97
206
/
/
punpcklwd
%
xmm6
%
xmm1
.
byte
102
15
239
210
/
/
pxor
%
xmm2
%
xmm2
.
byte
102
15
111
177
145
172
0
0
/
/
movdqa
0xac91
(
%
ecx
)
%
xmm6
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
102
15
219
137
161
172
0
0
/
/
pand
0xaca1
(
%
ecx
)
%
xmm1
.
byte
102
15
111
129
177
172
0
0
/
/
movdqa
0xacb1
(
%
ecx
)
%
xmm0
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
102
217
/
/
pcmpgtd
%
xmm1
%
xmm3
.
byte
102
15
127
93
168
/
/
movdqa
%
xmm3
-
0x58
(
%
ebp
)
.
byte
102
15
114
241
13
/
/
pslld
0xd
%
xmm1
.
byte
102
15
235
204
/
/
por
%
xmm4
%
xmm1
.
byte
102
15
97
234
/
/
punpcklwd
%
xmm2
%
xmm5
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
111
213
/
/
movdqa
%
xmm5
%
xmm2
.
byte
102
15
219
214
/
/
pand
%
xmm6
%
xmm2
.
byte
102
15
111
153
161
172
0
0
/
/
movdqa
0xaca1
(
%
ecx
)
%
xmm3
.
byte
102
15
219
235
/
/
pand
%
xmm3
%
xmm5
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
102
229
/
/
pcmpgtd
%
xmm5
%
xmm4
.
byte
102
15
127
101
200
/
/
movdqa
%
xmm4
-
0x38
(
%
ebp
)
.
byte
102
15
114
245
13
/
/
pslld
0xd
%
xmm5
.
byte
102
15
235
234
/
/
por
%
xmm2
%
xmm5
.
byte
102
15
111
69
184
/
/
movdqa
-
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
97
199
/
/
punpcklwd
%
xmm7
%
xmm0
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
219
254
/
/
pand
%
xmm6
%
xmm7
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
111
145
177
172
0
0
/
/
movdqa
0xacb1
(
%
ecx
)
%
xmm2
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
102
216
/
/
pcmpgtd
%
xmm0
%
xmm3
.
byte
102
15
114
240
13
/
/
pslld
0xd
%
xmm0
.
byte
102
15
235
199
/
/
por
%
xmm7
%
xmm0
.
byte
243
15
126
125
248
/
/
movq
-
0x8
(
%
ebp
)
%
xmm7
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
97
252
/
/
punpcklwd
%
xmm4
%
xmm7
.
byte
102
15
219
247
/
/
pand
%
xmm7
%
xmm6
.
byte
102
15
219
185
161
172
0
0
/
/
pand
0xaca1
(
%
ecx
)
%
xmm7
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
102
215
/
/
pcmpgtd
%
xmm7
%
xmm2
.
byte
102
15
114
247
13
/
/
pslld
0xd
%
xmm7
.
byte
102
15
235
254
/
/
por
%
xmm6
%
xmm7
.
byte
102
15
111
161
193
172
0
0
/
/
movdqa
0xacc1
(
%
ecx
)
%
xmm4
.
byte
102
15
254
204
/
/
paddd
%
xmm4
%
xmm1
.
byte
102
15
254
236
/
/
paddd
%
xmm4
%
xmm5
.
byte
102
15
254
196
/
/
paddd
%
xmm4
%
xmm0
.
byte
102
15
254
252
/
/
paddd
%
xmm4
%
xmm7
.
byte
102
15
111
117
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm6
.
byte
102
15
223
241
/
/
pandn
%
xmm1
%
xmm6
.
byte
102
15
111
77
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm1
.
byte
102
15
223
205
/
/
pandn
%
xmm5
%
xmm1
.
byte
102
15
223
216
/
/
pandn
%
xmm0
%
xmm3
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
223
239
/
/
pandn
%
xmm7
%
xmm5
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
104
/
/
add
0x68
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_store_f16_sse2
.
globl
_sk_store_f16_sse2
FUNCTION
(
_sk_store_f16_sse2
)
_sk_store_f16_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
232
0
0
0
0
/
/
call
6183
<
_sk_store_f16_sse2
+
0x19
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
152
13
171
0
0
/
/
movdqa
0xab0d
(
%
eax
)
%
xmm3
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
219
227
/
/
pand
%
xmm3
%
xmm4
.
byte
102
15
111
136
61
171
0
0
/
/
movdqa
0xab3d
(
%
eax
)
%
xmm1
.
byte
102
15
127
77
232
/
/
movdqa
%
xmm1
-
0x18
(
%
ebp
)
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
114
246
3
/
/
pslld
0x3
%
xmm6
.
byte
102
15
219
241
/
/
pand
%
xmm1
%
xmm6
.
byte
102
15
254
244
/
/
paddd
%
xmm4
%
xmm6
.
byte
102
15
111
184
29
171
0
0
/
/
movdqa
0xab1d
(
%
eax
)
%
xmm7
.
byte
102
15
127
69
168
/
/
movdqa
%
xmm0
-
0x58
(
%
ebp
)
.
byte
102
15
219
199
/
/
pand
%
xmm7
%
xmm0
.
byte
102
15
111
168
45
171
0
0
/
/
movdqa
0xab2d
(
%
eax
)
%
xmm5
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
102
200
/
/
pcmpgtd
%
xmm0
%
xmm1
.
byte
102
15
111
128
125
167
0
0
/
/
movdqa
0xa77d
(
%
eax
)
%
xmm0
.
byte
102
15
127
69
216
/
/
movdqa
%
xmm0
-
0x28
(
%
ebp
)
.
byte
102
15
254
240
/
/
paddd
%
xmm0
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
15
223
206
/
/
pandn
%
xmm6
%
xmm1
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
102
15
111
242
/
/
movdqa
%
xmm2
%
xmm6
.
byte
102
15
114
246
3
/
/
pslld
0x3
%
xmm6
.
byte
102
15
219
117
232
/
/
pand
-
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
254
240
/
/
paddd
%
xmm0
%
xmm6
.
byte
102
15
127
85
152
/
/
movdqa
%
xmm2
-
0x68
(
%
ebp
)
.
byte
102
15
219
215
/
/
pand
%
xmm7
%
xmm2
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
102
226
/
/
pcmpgtd
%
xmm2
%
xmm4
.
byte
102
15
254
117
216
/
/
paddd
-
0x28
(
%
ebp
)
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
15
223
230
/
/
pandn
%
xmm6
%
xmm4
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
15
107
224
/
/
packssdw
%
xmm0
%
xmm4
.
byte
102
15
97
204
/
/
punpcklwd
%
xmm4
%
xmm1
.
byte
102
15
111
85
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
102
15
111
226
/
/
movdqa
%
xmm2
%
xmm4
.
byte
102
15
114
244
3
/
/
pslld
0x3
%
xmm4
.
byte
102
15
219
101
232
/
/
pand
-
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
254
224
/
/
paddd
%
xmm0
%
xmm4
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
219
199
/
/
pand
%
xmm7
%
xmm0
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
15
102
240
/
/
pcmpgtd
%
xmm0
%
xmm6
.
byte
102
15
111
85
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm2
.
byte
102
15
254
226
/
/
paddd
%
xmm2
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
223
244
/
/
pandn
%
xmm4
%
xmm6
.
byte
102
15
111
101
184
/
/
movdqa
-
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
114
240
3
/
/
pslld
0x3
%
xmm0
.
byte
102
15
219
69
232
/
/
pand
-
0x18
(
%
ebp
)
%
xmm0
.
byte
102
15
219
220
/
/
pand
%
xmm4
%
xmm3
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
102
15
102
239
/
/
pcmpgtd
%
xmm7
%
xmm5
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
223
232
/
/
pandn
%
xmm0
%
xmm5
.
byte
102
15
107
240
/
/
packssdw
%
xmm0
%
xmm6
.
byte
102
15
107
232
/
/
packssdw
%
xmm0
%
xmm5
.
byte
102
15
97
245
/
/
punpcklwd
%
xmm5
%
xmm6
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
193
230
3
/
/
shl
0x3
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
121
8
/
/
mov
0x8
(
%
ecx
)
%
edi
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
98
198
/
/
punpckldq
%
xmm6
%
xmm0
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
117
48
/
/
jne
6304
<
_sk_store_f16_sse2
+
0x19a
>
.
byte
15
17
4
214
/
/
movups
%
xmm0
(
%
esi
%
edx
8
)
.
byte
102
15
106
206
/
/
punpckhdq
%
xmm6
%
xmm1
.
byte
243
15
127
76
214
16
/
/
movdqu
%
xmm1
0x10
(
%
esi
%
edx
8
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
15
40
77
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm1
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
112
/
/
add
0x70
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
15
214
4
214
/
/
movq
%
xmm0
(
%
esi
%
edx
8
)
.
byte
131
255
1
/
/
cmp
0x1
%
edi
.
byte
116
212
/
/
je
62e2
<
_sk_store_f16_sse2
+
0x178
>
.
byte
102
15
23
68
214
8
/
/
movhpd
%
xmm0
0x8
(
%
esi
%
edx
8
)
.
byte
131
255
3
/
/
cmp
0x3
%
edi
.
byte
114
201
/
/
jb
62e2
<
_sk_store_f16_sse2
+
0x178
>
.
byte
102
15
106
206
/
/
punpckhdq
%
xmm6
%
xmm1
.
byte
102
15
214
76
214
16
/
/
movq
%
xmm1
0x10
(
%
esi
%
edx
8
)
.
byte
235
189
/
/
jmp
62e2
<
_sk_store_f16_sse2
+
0x178
>
HIDDEN
_sk_load_u16_be_sse2
.
globl
_sk_load_u16_be_sse2
FUNCTION
(
_sk_load_u16_be_sse2
)
_sk_load_u16_be_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
44
/
/
sub
0x2c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
139
49
/
/
mov
(
%
ecx
)
%
esi
.
byte
139
122
4
/
/
mov
0x4
(
%
edx
)
%
edi
.
byte
15
175
121
4
/
/
imul
0x4
(
%
ecx
)
%
edi
.
byte
1
255
/
/
add
%
edi
%
edi
.
byte
3
58
/
/
add
(
%
edx
)
%
edi
.
byte
139
89
8
/
/
mov
0x8
(
%
ecx
)
%
ebx
.
byte
193
230
2
/
/
shl
0x2
%
esi
.
byte
133
219
/
/
test
%
ebx
%
ebx
.
byte
232
0
0
0
0
/
/
call
6350
<
_sk_load_u16_be_sse2
+
0x2b
>
.
byte
90
/
/
pop
%
edx
.
byte
15
133
11
1
0
0
/
/
jne
6462
<
_sk_load_u16_be_sse2
+
0x13d
>
.
byte
102
15
16
4
119
/
/
movupd
(
%
edi
%
esi
2
)
%
xmm0
.
byte
243
15
111
76
119
16
/
/
movdqu
0x10
(
%
edi
%
esi
2
)
%
xmm1
.
byte
102
15
40
208
/
/
movapd
%
xmm0
%
xmm2
.
byte
102
15
97
209
/
/
punpcklwd
%
xmm1
%
xmm2
.
byte
102
15
105
193
/
/
punpckhwd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
15
112
233
78
/
/
pshufd
0x4e
%
xmm1
%
xmm5
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
15
126
77
216
/
/
movd
%
xmm1
-
0x28
(
%
ebp
)
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
126
69
220
/
/
movd
%
xmm0
-
0x24
(
%
ebp
)
.
byte
243
15
126
69
216
/
/
movq
-
0x28
(
%
ebp
)
%
xmm0
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
97
195
/
/
punpcklwd
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
162
144
166
0
0
/
/
movaps
0xa690
(
%
edx
)
%
xmm4
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
126
109
208
/
/
movd
%
xmm5
-
0x30
(
%
ebp
)
.
byte
102
15
112
205
229
/
/
pshufd
0xe5
%
xmm5
%
xmm1
.
byte
102
15
126
77
212
/
/
movd
%
xmm1
-
0x2c
(
%
ebp
)
.
byte
243
15
126
77
208
/
/
movq
-
0x30
(
%
ebp
)
%
xmm1
.
byte
102
15
97
203
/
/
punpcklwd
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
113
245
8
/
/
psllw
0x8
%
xmm5
.
byte
102
15
112
242
78
/
/
pshufd
0x4e
%
xmm2
%
xmm6
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
235
213
/
/
por
%
xmm5
%
xmm2
.
byte
102
15
126
85
232
/
/
movd
%
xmm2
-
0x18
(
%
ebp
)
.
byte
102
15
112
210
229
/
/
pshufd
0xe5
%
xmm2
%
xmm2
.
byte
102
15
126
85
236
/
/
movd
%
xmm2
-
0x14
(
%
ebp
)
.
byte
243
15
126
85
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm2
.
byte
102
15
97
211
/
/
punpcklwd
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
113
245
8
/
/
psllw
0x8
%
xmm5
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
102
15
235
245
/
/
por
%
xmm5
%
xmm6
.
byte
102
15
126
117
224
/
/
movd
%
xmm6
-
0x20
(
%
ebp
)
.
byte
102
15
112
238
229
/
/
pshufd
0xe5
%
xmm6
%
xmm5
.
byte
102
15
126
109
228
/
/
movd
%
xmm5
-
0x1c
(
%
ebp
)
.
byte
243
15
126
109
224
/
/
movq
-
0x20
(
%
ebp
)
%
xmm5
.
byte
102
15
97
235
/
/
punpcklwd
%
xmm3
%
xmm5
.
byte
15
91
221
/
/
cvtdq2ps
%
xmm5
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
60
/
/
add
0x3c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
242
15
16
4
119
/
/
movsd
(
%
edi
%
esi
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
131
251
1
/
/
cmp
0x1
%
ebx
.
byte
15
132
238
254
255
255
/
/
je
6362
<
_sk_load_u16_be_sse2
+
0x3d
>
.
byte
102
15
22
68
119
8
/
/
movhpd
0x8
(
%
edi
%
esi
2
)
%
xmm0
.
byte
131
251
3
/
/
cmp
0x3
%
ebx
.
byte
15
130
223
254
255
255
/
/
jb
6362
<
_sk_load_u16_be_sse2
+
0x3d
>
.
byte
243
15
126
76
119
16
/
/
movq
0x10
(
%
edi
%
esi
2
)
%
xmm1
.
byte
233
212
254
255
255
/
/
jmp
6362
<
_sk_load_u16_be_sse2
+
0x3d
>
HIDDEN
_sk_load_rgb_u16_be_sse2
.
globl
_sk_load_rgb_u16_be_sse2
FUNCTION
(
_sk_load_rgb_u16_be_sse2
)
_sk_load_rgb_u16_be_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
28
/
/
sub
0x1c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
139
121
8
/
/
mov
0x8
(
%
ecx
)
%
edi
.
byte
139
114
4
/
/
mov
0x4
(
%
edx
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
50
/
/
add
(
%
edx
)
%
esi
.
byte
107
25
3
/
/
imul
0x3
(
%
ecx
)
%
ebx
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
232
0
0
0
0
/
/
call
64b7
<
_sk_load_rgb_u16_be_sse2
+
0x29
>
.
byte
90
/
/
pop
%
edx
.
byte
15
133
240
0
0
0
/
/
jne
65ae
<
_sk_load_rgb_u16_be_sse2
+
0x120
>
.
byte
243
15
111
20
94
/
/
movdqu
(
%
esi
%
ebx
2
)
%
xmm2
.
byte
243
15
111
92
94
8
/
/
movdqu
0x8
(
%
esi
%
ebx
2
)
%
xmm3
.
byte
102
15
115
219
4
/
/
psrldq
0x4
%
xmm3
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
115
216
6
/
/
psrldq
0x6
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
115
217
6
/
/
psrldq
0x6
%
xmm1
.
byte
102
15
97
193
/
/
punpcklwd
%
xmm1
%
xmm0
.
byte
102
15
97
211
/
/
punpcklwd
%
xmm3
%
xmm2
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
97
200
/
/
punpcklwd
%
xmm0
%
xmm1
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
15
105
208
/
/
punpckhwd
%
xmm0
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
8
/
/
psllw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
15
126
77
224
/
/
movd
%
xmm1
-
0x20
(
%
ebp
)
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
126
69
228
/
/
movd
%
xmm0
-
0x1c
(
%
ebp
)
.
byte
243
15
126
69
224
/
/
movq
-
0x20
(
%
ebp
)
%
xmm0
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
97
195
/
/
punpcklwd
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
170
41
165
0
0
/
/
movaps
0xa529
(
%
edx
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
113
241
8
/
/
psllw
0x8
%
xmm1
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
235
225
/
/
por
%
xmm1
%
xmm4
.
byte
102
15
126
101
216
/
/
movd
%
xmm4
-
0x28
(
%
ebp
)
.
byte
102
15
112
204
229
/
/
pshufd
0xe5
%
xmm4
%
xmm1
.
byte
102
15
126
77
220
/
/
movd
%
xmm1
-
0x24
(
%
ebp
)
.
byte
243
15
126
77
216
/
/
movq
-
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
97
203
/
/
punpcklwd
%
xmm3
%
xmm1
.
byte
15
91
201
/
/
cvtdq2ps
%
xmm1
%
xmm1
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
102
15
111
226
/
/
movdqa
%
xmm2
%
xmm4
.
byte
102
15
113
244
8
/
/
psllw
0x8
%
xmm4
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
235
212
/
/
por
%
xmm4
%
xmm2
.
byte
102
15
126
85
232
/
/
movd
%
xmm2
-
0x18
(
%
ebp
)
.
byte
102
15
112
210
229
/
/
pshufd
0xe5
%
xmm2
%
xmm2
.
byte
102
15
126
85
236
/
/
movd
%
xmm2
-
0x14
(
%
ebp
)
.
byte
243
15
126
85
232
/
/
movq
-
0x18
(
%
ebp
)
%
xmm2
.
byte
102
15
97
211
/
/
punpcklwd
%
xmm3
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
141
112
8
/
/
lea
0x8
(
%
eax
)
%
esi
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
154
169
162
0
0
/
/
movaps
0xa2a9
(
%
edx
)
%
xmm3
.
byte
86
/
/
push
%
esi
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
44
/
/
add
0x2c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
15
110
20
94
/
/
movd
(
%
esi
%
ebx
2
)
%
xmm2
.
byte
102
15
196
84
94
4
2
/
/
pinsrw
0x2
0x4
(
%
esi
%
ebx
2
)
%
xmm2
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
131
255
1
/
/
cmp
0x1
%
edi
.
byte
117
13
/
/
jne
65d0
<
_sk_load_rgb_u16_be_sse2
+
0x142
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
233
16
255
255
255
/
/
jmp
64e0
<
_sk_load_rgb_u16_be_sse2
+
0x52
>
.
byte
102
15
110
68
94
6
/
/
movd
0x6
(
%
esi
%
ebx
2
)
%
xmm0
.
byte
102
15
196
68
94
10
2
/
/
pinsrw
0x2
0xa
(
%
esi
%
ebx
2
)
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
131
255
3
/
/
cmp
0x3
%
edi
.
byte
114
18
/
/
jb
65f8
<
_sk_load_rgb_u16_be_sse2
+
0x16a
>
.
byte
102
15
110
92
94
12
/
/
movd
0xc
(
%
esi
%
ebx
2
)
%
xmm3
.
byte
102
15
196
92
94
16
2
/
/
pinsrw
0x2
0x10
(
%
esi
%
ebx
2
)
%
xmm3
.
byte
233
232
254
255
255
/
/
jmp
64e0
<
_sk_load_rgb_u16_be_sse2
+
0x52
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
233
223
254
255
255
/
/
jmp
64e0
<
_sk_load_rgb_u16_be_sse2
+
0x52
>
HIDDEN
_sk_store_u16_be_sse2
.
globl
_sk_store_u16_be_sse2
FUNCTION
(
_sk_store_u16_be_sse2
)
_sk_store_u16_be_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
32
/
/
sub
0x20
%
esp
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
6612
<
_sk_store_u16_be_sse2
+
0x11
>
.
byte
88
/
/
pop
%
eax
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
40
168
78
161
0
0
/
/
movaps
0xa14e
(
%
eax
)
%
xmm5
.
byte
15
93
229
/
/
minps
%
xmm5
%
xmm4
.
byte
15
40
176
190
166
0
0
/
/
movaps
0xa6be
(
%
eax
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
102
15
91
228
/
/
cvtps2dq
%
xmm4
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
224
/
/
packssdw
%
xmm0
%
xmm4
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
113
247
8
/
/
psllw
0x8
%
xmm7
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
15
95
249
/
/
maxps
%
xmm1
%
xmm7
.
byte
15
93
253
/
/
minps
%
xmm5
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
102
15
91
255
/
/
cvtps2dq
%
xmm7
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
15
107
248
/
/
packssdw
%
xmm0
%
xmm7
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
235
251
/
/
por
%
xmm3
%
xmm7
.
byte
102
15
97
231
/
/
punpcklwd
%
xmm7
%
xmm4
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
95
218
/
/
maxps
%
xmm2
%
xmm3
.
byte
15
93
221
/
/
minps
%
xmm5
%
xmm3
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
102
15
91
251
/
/
cvtps2dq
%
xmm3
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
15
107
248
/
/
packssdw
%
xmm0
%
xmm7
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
235
251
/
/
por
%
xmm3
%
xmm7
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
95
93
232
/
/
maxps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
15
93
221
/
/
minps
%
xmm5
%
xmm3
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
102
15
91
219
/
/
cvtps2dq
%
xmm3
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
107
216
/
/
packssdw
%
xmm0
%
xmm3
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
113
245
8
/
/
psllw
0x8
%
xmm5
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
235
221
/
/
por
%
xmm5
%
xmm3
.
byte
102
15
97
251
/
/
punpcklwd
%
xmm3
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
139
119
4
/
/
mov
0x4
(
%
edi
)
%
esi
.
byte
15
175
113
4
/
/
imul
0x4
(
%
ecx
)
%
esi
.
byte
1
246
/
/
add
%
esi
%
esi
.
byte
3
55
/
/
add
(
%
edi
)
%
esi
.
byte
139
121
8
/
/
mov
0x8
(
%
ecx
)
%
edi
.
byte
193
226
2
/
/
shl
0x2
%
edx
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
117
49
/
/
jne
6741
<
_sk_store_u16_be_sse2
+
0x140
>
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
102
15
98
223
/
/
punpckldq
%
xmm7
%
xmm3
.
byte
243
15
127
28
86
/
/
movdqu
%
xmm3
(
%
esi
%
edx
2
)
.
byte
102
15
106
231
/
/
punpckhdq
%
xmm7
%
xmm4
.
byte
243
15
127
100
86
16
/
/
movdqu
%
xmm4
0x10
(
%
esi
%
edx
2
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
85
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm2
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
48
/
/
add
0x30
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
98
239
/
/
punpckldq
%
xmm7
%
xmm5
.
byte
102
15
214
44
86
/
/
movq
%
xmm5
(
%
esi
%
edx
2
)
.
byte
131
255
1
/
/
cmp
0x1
%
edi
.
byte
116
212
/
/
je
6727
<
_sk_store_u16_be_sse2
+
0x126
>
.
byte
102
15
23
108
86
8
/
/
movhpd
%
xmm5
0x8
(
%
esi
%
edx
2
)
.
byte
131
255
3
/
/
cmp
0x3
%
edi
.
byte
114
201
/
/
jb
6727
<
_sk_store_u16_be_sse2
+
0x126
>
.
byte
102
15
106
231
/
/
punpckhdq
%
xmm7
%
xmm4
.
byte
102
15
214
100
86
16
/
/
movq
%
xmm4
0x10
(
%
esi
%
edx
2
)
.
byte
235
189
/
/
jmp
6727
<
_sk_store_u16_be_sse2
+
0x126
>
HIDDEN
_sk_load_f32_sse2
.
globl
_sk_load_f32_sse2
FUNCTION
(
_sk_load_f32_sse2
)
_sk_load_f32_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
48
/
/
mov
(
%
eax
)
%
esi
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
139
86
4
/
/
mov
0x4
(
%
esi
)
%
edx
.
byte
15
175
81
4
/
/
imul
0x4
(
%
ecx
)
%
edx
.
byte
193
226
2
/
/
shl
0x2
%
edx
.
byte
3
22
/
/
add
(
%
esi
)
%
edx
.
byte
141
52
189
0
0
0
0
/
/
lea
0x0
(
%
edi
4
)
%
esi
.
byte
193
231
4
/
/
shl
0x4
%
edi
.
byte
15
16
36
23
/
/
movups
(
%
edi
%
edx
1
)
%
xmm4
.
byte
139
121
8
/
/
mov
0x8
(
%
ecx
)
%
edi
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
117
71
/
/
jne
67e1
<
_sk_load_f32_sse2
+
0x77
>
.
byte
15
16
84
178
16
/
/
movups
0x10
(
%
edx
%
esi
4
)
%
xmm2
.
byte
15
16
92
178
32
/
/
movups
0x20
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
16
68
178
48
/
/
movups
0x30
(
%
edx
%
esi
4
)
%
xmm0
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
20
234
/
/
unpcklps
%
xmm2
%
xmm5
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
15
21
226
/
/
unpckhps
%
xmm2
%
xmm4
.
byte
15
21
216
/
/
unpckhps
%
xmm0
%
xmm3
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
15
18
205
/
/
movhlps
%
xmm5
%
xmm1
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
15
18
220
/
/
movhlps
%
xmm4
%
xmm3
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
16
/
/
add
0x10
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
131
255
1
/
/
cmp
0x1
%
edi
.
byte
117
8
/
/
jne
67f1
<
_sk_load_f32_sse2
+
0x87
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
235
184
/
/
jmp
67a9
<
_sk_load_f32_sse2
+
0x3f
>
.
byte
15
16
84
178
16
/
/
movups
0x10
(
%
edx
%
esi
4
)
%
xmm2
.
byte
131
255
3
/
/
cmp
0x3
%
edi
.
byte
114
7
/
/
jb
6802
<
_sk_load_f32_sse2
+
0x98
>
.
byte
15
16
92
178
32
/
/
movups
0x20
(
%
edx
%
esi
4
)
%
xmm3
.
byte
235
167
/
/
jmp
67a9
<
_sk_load_f32_sse2
+
0x3f
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
235
162
/
/
jmp
67a9
<
_sk_load_f32_sse2
+
0x3f
>
HIDDEN
_sk_load_f32_dst_sse2
.
globl
_sk_load_f32_dst_sse2
FUNCTION
(
_sk_load_f32_dst_sse2
)
_sk_load_f32_dst_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
32
/
/
sub
0x20
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
48
/
/
mov
(
%
eax
)
%
esi
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
139
86
4
/
/
mov
0x4
(
%
esi
)
%
edx
.
byte
15
175
81
4
/
/
imul
0x4
(
%
ecx
)
%
edx
.
byte
193
226
2
/
/
shl
0x2
%
edx
.
byte
3
22
/
/
add
(
%
esi
)
%
edx
.
byte
141
52
189
0
0
0
0
/
/
lea
0x0
(
%
edi
4
)
%
esi
.
byte
193
231
4
/
/
shl
0x4
%
edi
.
byte
15
16
52
23
/
/
movups
(
%
edi
%
edx
1
)
%
xmm6
.
byte
139
121
8
/
/
mov
0x8
(
%
ecx
)
%
edi
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
117
97
/
/
jne
68a3
<
_sk_load_f32_dst_sse2
+
0x9c
>
.
byte
15
16
124
178
16
/
/
movups
0x10
(
%
edx
%
esi
4
)
%
xmm7
.
byte
15
16
92
178
32
/
/
movups
0x20
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
16
84
178
48
/
/
movups
0x30
(
%
edx
%
esi
4
)
%
xmm2
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
20
231
/
/
unpcklps
%
xmm7
%
xmm4
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
20
234
/
/
unpcklps
%
xmm2
%
xmm5
.
byte
15
21
247
/
/
unpckhps
%
xmm7
%
xmm6
.
byte
15
21
218
/
/
unpckhps
%
xmm2
%
xmm3
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
102
15
20
213
/
/
unpcklpd
%
xmm5
%
xmm2
.
byte
15
18
236
/
/
movhlps
%
xmm4
%
xmm5
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
102
15
20
227
/
/
unpcklpd
%
xmm3
%
xmm4
.
byte
15
18
222
/
/
movhlps
%
xmm6
%
xmm3
.
byte
102
15
41
81
16
/
/
movapd
%
xmm2
0x10
(
%
ecx
)
.
byte
15
41
105
32
/
/
movaps
%
xmm5
0x20
(
%
ecx
)
.
byte
102
15
41
97
48
/
/
movapd
%
xmm4
0x30
(
%
ecx
)
.
byte
15
41
89
64
/
/
movaps
%
xmm3
0x40
(
%
ecx
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
85
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
48
/
/
add
0x30
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
131
255
1
/
/
cmp
0x1
%
edi
.
byte
117
8
/
/
jne
68b3
<
_sk_load_f32_dst_sse2
+
0xac
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
235
158
/
/
jmp
6851
<
_sk_load_f32_dst_sse2
+
0x4a
>
.
byte
15
16
124
178
16
/
/
movups
0x10
(
%
edx
%
esi
4
)
%
xmm7
.
byte
131
255
3
/
/
cmp
0x3
%
edi
.
byte
114
7
/
/
jb
68c4
<
_sk_load_f32_dst_sse2
+
0xbd
>
.
byte
15
16
92
178
32
/
/
movups
0x20
(
%
edx
%
esi
4
)
%
xmm3
.
byte
235
141
/
/
jmp
6851
<
_sk_load_f32_dst_sse2
+
0x4a
>
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
235
136
/
/
jmp
6851
<
_sk_load_f32_dst_sse2
+
0x4a
>
HIDDEN
_sk_store_f32_sse2
.
globl
_sk_store_f32_sse2
FUNCTION
(
_sk_store_f32_sse2
)
_sk_store_f32_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
48
/
/
mov
(
%
eax
)
%
esi
.
byte
139
25
/
/
mov
(
%
ecx
)
%
ebx
.
byte
139
86
4
/
/
mov
0x4
(
%
esi
)
%
edx
.
byte
15
175
81
4
/
/
imul
0x4
(
%
ecx
)
%
edx
.
byte
193
226
2
/
/
shl
0x2
%
edx
.
byte
3
22
/
/
add
(
%
esi
)
%
edx
.
byte
141
52
157
0
0
0
0
/
/
lea
0x0
(
%
ebx
4
)
%
esi
.
byte
193
227
4
/
/
shl
0x4
%
ebx
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
20
233
/
/
unpcklps
%
xmm1
%
xmm5
.
byte
15
40
226
/
/
movaps
%
xmm2
%
xmm4
.
byte
15
20
227
/
/
unpcklps
%
xmm3
%
xmm4
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
102
15
20
244
/
/
unpcklpd
%
xmm4
%
xmm6
.
byte
139
121
8
/
/
mov
0x8
(
%
ecx
)
%
edi
.
byte
102
15
17
52
19
/
/
movupd
%
xmm6
(
%
ebx
%
edx
1
)
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
21
241
/
/
unpckhps
%
xmm1
%
xmm6
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
21
251
/
/
unpckhps
%
xmm3
%
xmm7
.
byte
15
18
229
/
/
movhlps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
102
15
20
239
/
/
unpcklpd
%
xmm7
%
xmm5
.
byte
133
255
/
/
test
%
edi
%
edi
.
byte
117
40
/
/
jne
694f
<
_sk_store_f32_sse2
+
0x86
>
.
byte
102
15
21
247
/
/
unpckhpd
%
xmm7
%
xmm6
.
byte
15
17
100
178
16
/
/
movups
%
xmm4
0x10
(
%
edx
%
esi
4
)
.
byte
102
15
17
108
178
32
/
/
movupd
%
xmm5
0x20
(
%
edx
%
esi
4
)
.
byte
102
15
17
116
178
48
/
/
movupd
%
xmm6
0x30
(
%
edx
%
esi
4
)
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
131
255
1
/
/
cmp
0x1
%
edi
.
byte
116
232
/
/
je
693c
<
_sk_store_f32_sse2
+
0x73
>
.
byte
15
17
100
178
16
/
/
movups
%
xmm4
0x10
(
%
edx
%
esi
4
)
.
byte
131
255
3
/
/
cmp
0x3
%
edi
.
byte
114
222
/
/
jb
693c
<
_sk_store_f32_sse2
+
0x73
>
.
byte
102
15
17
108
178
32
/
/
movupd
%
xmm5
0x20
(
%
edx
%
esi
4
)
.
byte
235
214
/
/
jmp
693c
<
_sk_store_f32_sse2
+
0x73
>
HIDDEN
_sk_repeat_x_sse2
.
globl
_sk_repeat_x_sse2
FUNCTION
(
_sk_repeat_x_sse2
)
_sk_repeat_x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
6971
<
_sk_repeat_x_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
16
34
/
/
movss
(
%
edx
)
%
xmm4
.
byte
243
15
16
106
4
/
/
movss
0x4
(
%
edx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
243
15
91
245
/
/
cvttps2dq
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
194
238
1
/
/
cmpltps
%
xmm6
%
xmm5
.
byte
15
84
168
239
157
0
0
/
/
andps
0x9def
(
%
eax
)
%
xmm5
.
byte
15
92
245
/
/
subps
%
xmm5
%
xmm6
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_repeat_y_sse2
.
globl
_sk_repeat_y_sse2
FUNCTION
(
_sk_repeat_y_sse2
)
_sk_repeat_y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
69c3
<
_sk_repeat_y_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
16
34
/
/
movss
(
%
edx
)
%
xmm4
.
byte
243
15
16
106
4
/
/
movss
0x4
(
%
edx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
243
15
91
245
/
/
cvttps2dq
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
194
238
1
/
/
cmpltps
%
xmm6
%
xmm5
.
byte
15
84
168
157
157
0
0
/
/
andps
0x9d9d
(
%
eax
)
%
xmm5
.
byte
15
92
245
/
/
subps
%
xmm5
%
xmm6
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_mirror_x_sse2
.
globl
_sk_mirror_x_sse2
FUNCTION
(
_sk_mirror_x_sse2
)
_sk_mirror_x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
6a15
<
_sk_mirror_x_sse2
+
0xb
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
243
15
16
34
/
/
movss
(
%
edx
)
%
xmm4
.
byte
243
15
16
106
4
/
/
movss
0x4
(
%
edx
)
%
xmm5
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
243
15
89
169
75
165
0
0
/
/
mulss
0xa54b
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
243
15
91
253
/
/
cvttps2dq
%
xmm5
%
xmm7
.
byte
15
91
255
/
/
cvtdq2ps
%
xmm7
%
xmm7
.
byte
15
194
239
1
/
/
cmpltps
%
xmm7
%
xmm5
.
byte
15
84
169
75
157
0
0
/
/
andps
0x9d4b
(
%
ecx
)
%
xmm5
.
byte
15
92
253
/
/
subps
%
xmm5
%
xmm7
.
byte
243
15
88
228
/
/
addss
%
xmm4
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
92
199
/
/
subps
%
xmm7
%
xmm0
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_mirror_y_sse2
.
globl
_sk_mirror_y_sse2
FUNCTION
(
_sk_mirror_y_sse2
)
_sk_mirror_y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
6a89
<
_sk_mirror_y_sse2
+
0xb
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
243
15
16
34
/
/
movss
(
%
edx
)
%
xmm4
.
byte
243
15
16
106
4
/
/
movss
0x4
(
%
edx
)
%
xmm5
.
byte
15
40
244
/
/
movaps
%
xmm4
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
243
15
89
169
215
164
0
0
/
/
mulss
0xa4d7
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
243
15
91
253
/
/
cvttps2dq
%
xmm5
%
xmm7
.
byte
15
91
255
/
/
cvtdq2ps
%
xmm7
%
xmm7
.
byte
15
194
239
1
/
/
cmpltps
%
xmm7
%
xmm5
.
byte
15
84
169
215
156
0
0
/
/
andps
0x9cd7
(
%
ecx
)
%
xmm5
.
byte
15
92
253
/
/
subps
%
xmm5
%
xmm7
.
byte
243
15
88
228
/
/
addss
%
xmm4
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
92
207
/
/
subps
%
xmm7
%
xmm1
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
84
204
/
/
andps
%
xmm4
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clamp_x_1_sse2
.
globl
_sk_clamp_x_1_sse2
FUNCTION
(
_sk_clamp_x_1_sse2
)
_sk_clamp_x_1_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
6afd
<
_sk_clamp_x_1_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
160
99
156
0
0
/
/
minps
0x9c63
(
%
eax
)
%
xmm4
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_repeat_x_1_sse2
.
globl
_sk_repeat_x_1_sse2
FUNCTION
(
_sk_repeat_x_1_sse2
)
_sk_repeat_x_1_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
6b2d
<
_sk_repeat_x_1_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
243
15
91
224
/
/
cvttps2dq
%
xmm0
%
xmm4
.
byte
15
91
236
/
/
cvtdq2ps
%
xmm4
%
xmm5
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
40
184
51
156
0
0
/
/
movaps
0x9c33
(
%
eax
)
%
xmm7
.
byte
15
84
247
/
/
andps
%
xmm7
%
xmm6
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
92
238
/
/
subps
%
xmm6
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
95
224
/
/
maxps
%
xmm0
%
xmm4
.
byte
15
93
231
/
/
minps
%
xmm7
%
xmm4
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_mirror_x_1_sse2
.
globl
_sk_mirror_x_1_sse2
FUNCTION
(
_sk_mirror_x_1_sse2
)
_sk_mirror_x_1_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
6b77
<
_sk_mirror_x_1_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
15
40
160
89
156
0
0
/
/
movaps
0x9c59
(
%
eax
)
%
xmm4
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
40
168
217
155
0
0
/
/
movaps
0x9bd9
(
%
eax
)
%
xmm5
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
243
15
91
245
/
/
cvttps2dq
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
194
238
1
/
/
cmpltps
%
xmm6
%
xmm5
.
byte
15
40
184
233
155
0
0
/
/
movaps
0x9be9
(
%
eax
)
%
xmm7
.
byte
15
84
239
/
/
andps
%
xmm7
%
xmm5
.
byte
15
92
245
/
/
subps
%
xmm5
%
xmm6
.
byte
15
88
246
/
/
addps
%
xmm6
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
95
196
/
/
maxps
%
xmm4
%
xmm0
.
byte
15
93
199
/
/
minps
%
xmm7
%
xmm0
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_decal_x_sse2
.
globl
_sk_decal_x_sse2
FUNCTION
(
_sk_decal_x_sse2
)
_sk_decal_x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
224
2
/
/
cmpleps
%
xmm0
%
xmm4
.
byte
243
15
16
106
64
/
/
movss
0x40
(
%
edx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
84
244
/
/
andps
%
xmm4
%
xmm6
.
byte
15
17
50
/
/
movups
%
xmm6
(
%
edx
)
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_decal_y_sse2
.
globl
_sk_decal_y_sse2
FUNCTION
(
_sk_decal_y_sse2
)
_sk_decal_y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
225
2
/
/
cmpleps
%
xmm1
%
xmm4
.
byte
243
15
16
106
68
/
/
movss
0x44
(
%
edx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
84
244
/
/
andps
%
xmm4
%
xmm6
.
byte
15
17
50
/
/
movups
%
xmm6
(
%
edx
)
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_decal_x_and_y_sse2
.
globl
_sk_decal_x_and_y_sse2
FUNCTION
(
_sk_decal_x_and_y_sse2
)
_sk_decal_x_and_y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
243
15
16
105
64
/
/
movss
0x40
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
194
232
2
/
/
cmpleps
%
xmm0
%
xmm5
.
byte
15
194
225
2
/
/
cmpleps
%
xmm1
%
xmm4
.
byte
15
84
229
/
/
andps
%
xmm5
%
xmm4
.
byte
243
15
16
105
68
/
/
movss
0x44
(
%
ecx
)
%
xmm5
.
byte
15
84
230
/
/
andps
%
xmm6
%
xmm4
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
84
244
/
/
andps
%
xmm4
%
xmm6
.
byte
15
17
49
/
/
movups
%
xmm6
(
%
ecx
)
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_check_decal_mask_sse2
.
globl
_sk_check_decal_mask_sse2
FUNCTION
(
_sk_check_decal_mask_sse2
)
_sk_check_decal_mask_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
15
16
33
/
/
movups
(
%
ecx
)
%
xmm4
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
15
84
204
/
/
andps
%
xmm4
%
xmm1
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
84
220
/
/
andps
%
xmm4
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_luminance_to_alpha_sse2
.
globl
_sk_luminance_to_alpha_sse2
FUNCTION
(
_sk_luminance_to_alpha_sse2
)
_sk_luminance_to_alpha_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
232
0
0
0
0
/
/
call
6cdb
<
_sk_luminance_to_alpha_sse2
+
0xe
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
89
128
5
160
0
0
/
/
mulps
0xa005
(
%
eax
)
%
xmm0
.
byte
15
89
136
21
160
0
0
/
/
mulps
0xa015
(
%
eax
)
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
89
152
37
160
0
0
/
/
mulps
0xa025
(
%
eax
)
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_translate_sse2
.
globl
_sk_matrix_translate_sse2
FUNCTION
(
_sk_matrix_translate_sse2
)
_sk_matrix_translate_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_scale_translate_sse2
.
globl
_sk_matrix_scale_translate_sse2
FUNCTION
(
_sk_matrix_scale_translate_sse2
)
_sk_matrix_scale_translate_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
243
15
16
113
8
/
/
movss
0x8
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
243
15
16
97
12
/
/
movss
0xc
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_2x3_sse2
.
globl
_sk_matrix_2x3_sse2
FUNCTION
(
_sk_matrix_2x3_sse2
)
_sk_matrix_2x3_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
243
15
16
73
4
/
/
movss
0x4
(
%
ecx
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
113
8
/
/
movss
0x8
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
121
16
/
/
movss
0x10
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
113
12
/
/
movss
0xc
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
121
20
/
/
movss
0x14
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_3x4_sse2
.
globl
_sk_matrix_3x4_sse2
FUNCTION
(
_sk_matrix_3x4_sse2
)
_sk_matrix_3x4_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
105
24
/
/
movss
0x18
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
243
15
16
113
36
/
/
movss
0x24
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
243
15
16
113
12
/
/
movss
0xc
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
88
245
/
/
addps
%
xmm5
%
xmm6
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
243
15
16
105
28
/
/
movss
0x1c
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
243
15
16
113
40
/
/
movss
0x28
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
234
/
/
mulps
%
xmm2
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
88
245
/
/
addps
%
xmm5
%
xmm6
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
243
15
16
113
32
/
/
movss
0x20
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
243
15
16
81
44
/
/
movss
0x2c
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
243
15
16
121
20
/
/
movss
0x14
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
249
/
/
mulps
%
xmm1
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_4x5_sse2
.
globl
_sk_matrix_4x5_sse2
FUNCTION
(
_sk_matrix_4x5_sse2
)
_sk_matrix_4x5_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
243
15
16
73
4
/
/
movss
0x4
(
%
ecx
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
113
48
/
/
movss
0x30
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
121
64
/
/
movss
0x40
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
243
15
16
121
32
/
/
movss
0x20
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
113
52
/
/
movss
0x34
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
121
68
/
/
movss
0x44
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
243
15
16
121
36
/
/
movss
0x24
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
243
15
16
113
20
/
/
movss
0x14
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
243
15
16
113
56
/
/
movss
0x38
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
121
72
/
/
movss
0x48
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
243
15
16
121
40
/
/
movss
0x28
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
243
15
16
113
24
/
/
movss
0x18
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
243
15
16
121
8
/
/
movss
0x8
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
243
15
16
113
60
/
/
movss
0x3c
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
243
15
16
89
76
/
/
movss
0x4c
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
88
243
/
/
addps
%
xmm3
%
xmm6
.
byte
243
15
16
89
44
/
/
movss
0x2c
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
243
15
16
81
28
/
/
movss
0x1c
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
243
15
16
89
12
/
/
movss
0xc
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_4x3_sse2
.
globl
_sk_matrix_4x3_sse2
FUNCTION
(
_sk_matrix_4x3_sse2
)
_sk_matrix_4x3_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
243
15
16
73
4
/
/
movss
0x4
(
%
ecx
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
81
16
/
/
movss
0x10
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
89
32
/
/
movss
0x20
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
81
20
/
/
movss
0x14
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
89
36
/
/
movss
0x24
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
89
24
/
/
movss
0x18
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
15
16
113
40
/
/
movss
0x28
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
243
15
16
89
12
/
/
movss
0xc
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
15
16
113
28
/
/
movss
0x1c
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
121
44
/
/
movss
0x2c
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_perspective_sse2
.
globl
_sk_matrix_perspective_sse2
FUNCTION
(
_sk_matrix_perspective_sse2
)
_sk_matrix_perspective_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
243
15
16
105
4
/
/
movss
0x4
(
%
ecx
)
%
xmm5
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
243
15
16
113
8
/
/
movss
0x8
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
105
20
/
/
movss
0x14
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
88
245
/
/
addps
%
xmm5
%
xmm6
.
byte
243
15
16
105
12
/
/
movss
0xc
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
243
15
16
113
28
/
/
movss
0x1c
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
243
15
16
73
32
/
/
movss
0x20
(
%
ecx
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
243
15
16
73
24
/
/
movss
0x18
(
%
ecx
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
83
201
/
/
rcpps
%
xmm1
%
xmm1
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_evenly_spaced_gradient_sse2
.
globl
_sk_evenly_spaced_gradient_sse2
FUNCTION
(
_sk_evenly_spaced_gradient_sse2
)
_sk_evenly_spaced_gradient_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
60
/
/
sub
0x3c
%
esp
.
byte
15
41
69
184
/
/
movaps
%
xmm0
-
0x48
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
7171
<
_sk_evenly_spaced_gradient_sse2
+
0x12
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
49
/
/
mov
(
%
ecx
)
%
esi
.
byte
139
65
4
/
/
mov
0x4
(
%
ecx
)
%
eax
.
byte
78
/
/
dec
%
esi
.
byte
102
15
110
206
/
/
movd
%
esi
%
xmm1
.
byte
102
15
112
201
0
/
/
pshufd
0x0
%
xmm1
%
xmm1
.
byte
102
15
111
146
159
155
0
0
/
/
movdqa
0x9b9f
(
%
edx
)
%
xmm2
.
byte
102
15
219
209
/
/
pand
%
xmm1
%
xmm2
.
byte
102
15
235
146
31
153
0
0
/
/
por
0x991f
(
%
edx
)
%
xmm2
.
byte
102
15
114
209
16
/
/
psrld
0x10
%
xmm1
.
byte
102
15
235
138
175
155
0
0
/
/
por
0x9baf
(
%
edx
)
%
xmm1
.
byte
15
88
138
191
155
0
0
/
/
addps
0x9bbf
(
%
edx
)
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
218
/
/
movd
%
xmm3
%
edx
.
byte
102
15
112
217
231
/
/
pshufd
0xe7
%
xmm1
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
243
15
16
52
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm6
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
243
15
16
36
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm4
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
12
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm1
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
139
65
20
/
/
mov
0x14
(
%
ecx
)
%
eax
.
byte
243
15
16
20
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm2
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
243
15
16
4
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm0
.
byte
243
15
16
60
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm7
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
102
15
20
230
/
/
unpcklpd
%
xmm6
%
xmm4
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
15
20
199
/
/
unpcklps
%
xmm7
%
xmm0
.
byte
139
65
8
/
/
mov
0x8
(
%
ecx
)
%
eax
.
byte
243
15
16
20
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm2
.
byte
243
15
16
52
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm6
.
byte
243
15
16
12
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm1
.
byte
243
15
16
60
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm7
.
byte
102
15
20
195
/
/
unpcklpd
%
xmm3
%
xmm0
.
byte
102
15
41
69
200
/
/
movapd
%
xmm0
-
0x38
(
%
ebp
)
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
139
65
24
/
/
mov
0x18
(
%
ecx
)
%
eax
.
byte
243
15
16
20
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm2
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
243
15
16
4
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm0
.
byte
243
15
16
60
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm7
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
15
20
199
/
/
unpcklps
%
xmm7
%
xmm0
.
byte
102
15
20
195
/
/
unpcklpd
%
xmm3
%
xmm0
.
byte
102
15
41
69
216
/
/
movapd
%
xmm0
-
0x28
(
%
ebp
)
.
byte
139
65
12
/
/
mov
0xc
(
%
ecx
)
%
eax
.
byte
243
15
16
20
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm2
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
243
15
16
60
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm7
.
byte
15
20
215
/
/
unpcklps
%
xmm7
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
139
65
28
/
/
mov
0x1c
(
%
ecx
)
%
eax
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
15
20
223
/
/
unpcklps
%
xmm7
%
xmm3
.
byte
243
15
16
60
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm7
.
byte
243
15
16
44
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm5
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
102
15
20
251
/
/
unpcklpd
%
xmm3
%
xmm7
.
byte
139
65
16
/
/
mov
0x10
(
%
ecx
)
%
eax
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
243
15
16
44
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
243
15
16
4
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm0
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
102
15
20
221
/
/
unpcklpd
%
xmm5
%
xmm3
.
byte
139
65
32
/
/
mov
0x20
(
%
ecx
)
%
eax
.
byte
243
15
16
4
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm0
.
byte
243
15
16
44
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
243
15
16
4
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm0
.
byte
243
15
16
52
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm6
.
byte
15
20
198
/
/
unpcklps
%
xmm6
%
xmm0
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
15
40
109
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm5
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
101
200
/
/
addps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
89
205
/
/
mulps
%
xmm5
%
xmm1
.
byte
15
88
77
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
89
221
/
/
mulps
%
xmm5
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
76
/
/
add
0x4c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_gradient_sse2
.
globl
_sk_gradient_sse2
FUNCTION
(
_sk_gradient_sse2
)
_sk_gradient_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
60
/
/
sub
0x3c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
139
10
/
/
mov
(
%
edx
)
%
ecx
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
131
249
2
/
/
cmp
0x2
%
ecx
.
byte
114
33
/
/
jb
7359
<
_sk_gradient_sse2
+
0x3a
>
.
byte
139
114
36
/
/
mov
0x24
(
%
edx
)
%
esi
.
byte
73
/
/
dec
%
ecx
.
byte
131
198
4
/
/
add
0x4
%
esi
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
243
15
16
22
/
/
movss
(
%
esi
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
194
208
2
/
/
cmpleps
%
xmm0
%
xmm2
.
byte
102
15
250
202
/
/
psubd
%
xmm2
%
xmm1
.
byte
131
198
4
/
/
add
0x4
%
esi
.
byte
73
/
/
dec
%
ecx
.
byte
117
234
/
/
jne
7343
<
_sk_gradient_sse2
+
0x24
>
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
139
66
4
/
/
mov
0x4
(
%
edx
)
%
eax
.
byte
243
15
16
12
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm1
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
243
15
16
36
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm4
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
139
66
20
/
/
mov
0x14
(
%
edx
)
%
eax
.
byte
243
15
16
12
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm1
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
243
15
16
44
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm5
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
15
20
226
/
/
unpcklps
%
xmm2
%
xmm4
.
byte
102
15
20
227
/
/
unpcklpd
%
xmm3
%
xmm4
.
byte
102
15
41
101
184
/
/
movapd
%
xmm4
-
0x48
(
%
ebp
)
.
byte
15
20
241
/
/
unpcklps
%
xmm1
%
xmm6
.
byte
15
20
239
/
/
unpcklps
%
xmm7
%
xmm5
.
byte
139
66
8
/
/
mov
0x8
(
%
edx
)
%
eax
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
243
15
16
12
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm1
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
102
15
41
109
200
/
/
movapd
%
xmm5
-
0x38
(
%
ebp
)
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
15
20
207
/
/
unpcklps
%
xmm7
%
xmm1
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
139
66
24
/
/
mov
0x18
(
%
edx
)
%
eax
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
243
15
16
36
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm4
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
15
20
231
/
/
unpcklps
%
xmm7
%
xmm4
.
byte
102
15
20
227
/
/
unpcklpd
%
xmm3
%
xmm4
.
byte
102
15
41
101
216
/
/
movapd
%
xmm4
-
0x28
(
%
ebp
)
.
byte
139
66
12
/
/
mov
0xc
(
%
edx
)
%
eax
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
15
20
215
/
/
unpcklps
%
xmm7
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
139
66
28
/
/
mov
0x1c
(
%
edx
)
%
eax
.
byte
243
15
16
60
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm7
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
223
/
/
unpcklps
%
xmm7
%
xmm3
.
byte
243
15
16
60
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm7
.
byte
243
15
16
44
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm5
.
byte
15
20
253
/
/
unpcklps
%
xmm5
%
xmm7
.
byte
102
15
20
251
/
/
unpcklpd
%
xmm3
%
xmm7
.
byte
139
66
16
/
/
mov
0x10
(
%
edx
)
%
eax
.
byte
243
15
16
28
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm3
.
byte
243
15
16
44
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
243
15
16
52
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm6
.
byte
15
20
222
/
/
unpcklps
%
xmm6
%
xmm3
.
byte
102
15
20
221
/
/
unpcklpd
%
xmm5
%
xmm3
.
byte
139
66
32
/
/
mov
0x20
(
%
edx
)
%
eax
.
byte
243
15
16
44
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm5
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
243
15
16
44
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm5
.
byte
243
15
16
36
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm4
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
15
40
101
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
88
101
200
/
/
addps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
77
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
137
193
/
/
mov
%
eax
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
76
/
/
add
0x4c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_evenly_spaced_2_stop_gradient_sse2
.
globl
_sk_evenly_spaced_2_stop_gradient_sse2
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_sse2
)
_sk_evenly_spaced_2_stop_gradient_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
1
/
/
movss
(
%
ecx
)
%
xmm0
.
byte
243
15
16
73
4
/
/
movss
0x4
(
%
ecx
)
%
xmm1
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
81
16
/
/
movss
0x10
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
243
15
16
81
20
/
/
movss
0x14
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
89
24
/
/
movss
0x18
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
243
15
16
89
12
/
/
movss
0xc
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
243
15
16
105
28
/
/
movss
0x1c
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_unit_angle_sse2
.
globl
_sk_xy_to_unit_angle_sse2
FUNCTION
(
_sk_xy_to_unit_angle_sse2
)
_sk_xy_to_unit_angle_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
24
/
/
sub
0x18
%
esp
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
7561
<
_sk_xy_to_unit_angle_sse2
+
0x18
>
.
byte
88
/
/
pop
%
eax
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
84
233
/
/
andps
%
xmm1
%
xmm5
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
92
226
/
/
subps
%
xmm2
%
xmm4
.
byte
15
84
226
/
/
andps
%
xmm2
%
xmm4
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
93
244
/
/
minps
%
xmm4
%
xmm6
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
95
252
/
/
maxps
%
xmm4
%
xmm7
.
byte
15
94
247
/
/
divps
%
xmm7
%
xmm6
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
89
255
/
/
mulps
%
xmm7
%
xmm7
.
byte
15
40
128
223
151
0
0
/
/
movaps
0x97df
(
%
eax
)
%
xmm0
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
88
128
239
151
0
0
/
/
addps
0x97ef
(
%
eax
)
%
xmm0
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
88
128
255
151
0
0
/
/
addps
0x97ff
(
%
eax
)
%
xmm0
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
88
128
15
152
0
0
/
/
addps
0x980f
(
%
eax
)
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
194
236
1
/
/
cmpltps
%
xmm4
%
xmm5
.
byte
15
40
176
31
152
0
0
/
/
movaps
0x981f
(
%
eax
)
%
xmm6
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
15
84
245
/
/
andps
%
xmm5
%
xmm6
.
byte
15
85
232
/
/
andnps
%
xmm0
%
xmm5
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
86
238
/
/
orps
%
xmm6
%
xmm5
.
byte
15
194
204
1
/
/
cmpltps
%
xmm4
%
xmm1
.
byte
15
40
128
239
145
0
0
/
/
movaps
0x91ef
(
%
eax
)
%
xmm0
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
84
193
/
/
andps
%
xmm1
%
xmm0
.
byte
15
85
205
/
/
andnps
%
xmm5
%
xmm1
.
byte
15
86
200
/
/
orps
%
xmm0
%
xmm1
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
194
196
1
/
/
cmpltps
%
xmm4
%
xmm0
.
byte
15
40
168
255
145
0
0
/
/
movaps
0x91ff
(
%
eax
)
%
xmm5
.
byte
15
92
233
/
/
subps
%
xmm1
%
xmm5
.
byte
15
84
232
/
/
andps
%
xmm0
%
xmm5
.
byte
15
85
193
/
/
andnps
%
xmm1
%
xmm0
.
byte
15
86
197
/
/
orps
%
xmm5
%
xmm0
.
byte
15
194
224
7
/
/
cmpordps
%
xmm0
%
xmm4
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
40
/
/
add
0x28
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_radius_sse2
.
globl
_sk_xy_to_radius_sse2
FUNCTION
(
_sk_xy_to_radius_sse2
)
_sk_xy_to_radius_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
89
228
/
/
mulps
%
xmm4
%
xmm4
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
15
81
196
/
/
sqrtps
%
xmm4
%
xmm0
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_negate_x_sse2
.
globl
_sk_negate_x_sse2
FUNCTION
(
_sk_negate_x_sse2
)
_sk_negate_x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
7658
<
_sk_negate_x_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
87
128
56
150
0
0
/
/
xorps
0x9638
(
%
eax
)
%
xmm0
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_2pt_conical_strip_sse2
.
globl
_sk_xy_to_2pt_conical_strip_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_strip_sse2
)
_sk_xy_to_2pt_conical_strip_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
97
64
/
/
movss
0x40
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
237
/
/
mulps
%
xmm5
%
xmm5
.
byte
15
92
229
/
/
subps
%
xmm5
%
xmm4
.
byte
15
81
228
/
/
sqrtps
%
xmm4
%
xmm4
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_2pt_conical_focal_on_circle_sse2
.
globl
_sk_xy_to_2pt_conical_focal_on_circle_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_focal_on_circle_sse2
)
_sk_xy_to_2pt_conical_focal_on_circle_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
89
228
/
/
mulps
%
xmm4
%
xmm4
.
byte
15
94
224
/
/
divps
%
xmm0
%
xmm4
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
141
72
4
/
/
lea
0x4
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_2pt_conical_well_behaved_sse2
.
globl
_sk_xy_to_2pt_conical_well_behaved_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_well_behaved_sse2
)
_sk_xy_to_2pt_conical_well_behaved_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
97
64
/
/
movss
0x40
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
237
/
/
mulps
%
xmm5
%
xmm5
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
15
81
197
/
/
sqrtps
%
xmm5
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_2pt_conical_greater_sse2
.
globl
_sk_xy_to_2pt_conical_greater_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_greater_sse2
)
_sk_xy_to_2pt_conical_greater_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
97
64
/
/
movss
0x40
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
237
/
/
mulps
%
xmm5
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
81
192
/
/
sqrtps
%
xmm0
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_2pt_conical_smaller_sse2
.
globl
_sk_xy_to_2pt_conical_smaller_sse2
FUNCTION
(
_sk_xy_to_2pt_conical_smaller_sse2
)
_sk_xy_to_2pt_conical_smaller_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
7750
<
_sk_xy_to_2pt_conical_smaller_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
243
15
16
98
64
/
/
movss
0x40
(
%
edx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
237
/
/
mulps
%
xmm5
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
81
192
/
/
sqrtps
%
xmm0
%
xmm0
.
byte
15
87
128
64
149
0
0
/
/
xorps
0x9540
(
%
eax
)
%
xmm0
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_alter_2pt_conical_compensate_focal_sse2
.
globl
_sk_alter_2pt_conical_compensate_focal_sse2
FUNCTION
(
_sk_alter_2pt_conical_compensate_focal_sse2
)
_sk_alter_2pt_conical_compensate_focal_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
97
68
/
/
movss
0x44
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_alter_2pt_conical_unswap_sse2
.
globl
_sk_alter_2pt_conical_unswap_sse2
FUNCTION
(
_sk_alter_2pt_conical_unswap_sse2
)
_sk_alter_2pt_conical_unswap_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
77c1
<
_sk_alter_2pt_conical_unswap_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
40
160
159
143
0
0
/
/
movaps
0x8f9f
(
%
eax
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_mask_2pt_conical_nan_sse2
.
globl
_sk_mask_2pt_conical_nan_sse2
FUNCTION
(
_sk_mask_2pt_conical_nan_sse2
)
_sk_mask_2pt_conical_nan_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
224
7
/
/
cmpordps
%
xmm0
%
xmm4
.
byte
15
17
34
/
/
movups
%
xmm4
(
%
edx
)
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_mask_2pt_conical_degenerates_sse2
.
globl
_sk_mask_2pt_conical_degenerates_sse2
FUNCTION
(
_sk_mask_2pt_conical_degenerates_sse2
)
_sk_mask_2pt_conical_degenerates_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
194
232
1
/
/
cmpltps
%
xmm0
%
xmm5
.
byte
15
17
42
/
/
movups
%
xmm5
(
%
edx
)
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
15
95
196
/
/
maxps
%
xmm4
%
xmm0
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_apply_vector_mask_sse2
.
globl
_sk_apply_vector_mask_sse2
FUNCTION
(
_sk_apply_vector_mask_sse2
)
_sk_apply_vector_mask_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
15
16
33
/
/
movups
(
%
ecx
)
%
xmm4
.
byte
15
84
196
/
/
andps
%
xmm4
%
xmm0
.
byte
15
84
204
/
/
andps
%
xmm4
%
xmm1
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
84
220
/
/
andps
%
xmm4
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_save_xy_sse2
.
globl
_sk_save_xy_sse2
FUNCTION
(
_sk_save_xy_sse2
)
_sk_save_xy_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
24
/
/
sub
0x18
%
esp
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
232
0
0
0
0
/
/
call
787a
<
_sk_save_xy_sse2
+
0x12
>
.
byte
88
/
/
pop
%
eax
.
byte
15
40
160
214
142
0
0
/
/
movaps
0x8ed6
(
%
eax
)
%
xmm4
.
byte
15
40
232
/
/
movaps
%
xmm0
%
xmm5
.
byte
15
88
236
/
/
addps
%
xmm4
%
xmm5
.
byte
243
15
91
245
/
/
cvttps2dq
%
xmm5
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
194
254
1
/
/
cmpltps
%
xmm6
%
xmm7
.
byte
15
40
144
230
142
0
0
/
/
movaps
0x8ee6
(
%
eax
)
%
xmm2
.
byte
15
84
250
/
/
andps
%
xmm2
%
xmm7
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
92
238
/
/
subps
%
xmm6
%
xmm5
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
243
15
91
244
/
/
cvttps2dq
%
xmm4
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
40
252
/
/
movaps
%
xmm4
%
xmm7
.
byte
15
194
254
1
/
/
cmpltps
%
xmm6
%
xmm7
.
byte
15
84
250
/
/
andps
%
xmm2
%
xmm7
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
92
230
/
/
subps
%
xmm6
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
15
17
1
/
/
movups
%
xmm0
(
%
ecx
)
.
byte
15
17
73
64
/
/
movups
%
xmm1
0x40
(
%
ecx
)
.
byte
15
17
169
128
0
0
0
/
/
movups
%
xmm5
0x80
(
%
ecx
)
.
byte
15
17
161
192
0
0
0
/
/
movups
%
xmm4
0xc0
(
%
ecx
)
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
141
80
8
/
/
lea
0x8
(
%
eax
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
82
/
/
push
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
40
/
/
add
0x28
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_accumulate_sse2
.
globl
_sk_accumulate_sse2
FUNCTION
(
_sk_accumulate_sse2
)
_sk_accumulate_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
17
/
/
mov
(
%
ecx
)
%
edx
.
byte
15
16
162
0
1
0
0
/
/
movups
0x100
(
%
edx
)
%
xmm4
.
byte
15
16
170
64
1
0
0
/
/
movups
0x140
(
%
edx
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
88
96
16
/
/
addps
0x10
(
%
eax
)
%
xmm4
.
byte
15
41
96
16
/
/
movaps
%
xmm4
0x10
(
%
eax
)
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
88
96
32
/
/
addps
0x20
(
%
eax
)
%
xmm4
.
byte
15
41
96
32
/
/
movaps
%
xmm4
0x20
(
%
eax
)
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
89
226
/
/
mulps
%
xmm2
%
xmm4
.
byte
15
88
96
48
/
/
addps
0x30
(
%
eax
)
%
xmm4
.
byte
15
41
96
48
/
/
movaps
%
xmm4
0x30
(
%
eax
)
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
88
104
64
/
/
addps
0x40
(
%
eax
)
%
xmm5
.
byte
15
41
104
64
/
/
movaps
%
xmm5
0x40
(
%
eax
)
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
82
/
/
push
%
edx
.
byte
80
/
/
push
%
eax
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bilinear_nx_sse2
.
globl
_sk_bilinear_nx_sse2
FUNCTION
(
_sk_bilinear_nx_sse2
)
_sk_bilinear_nx_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7962
<
_sk_bilinear_nx_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
6
/
/
movups
(
%
esi
)
%
xmm0
.
byte
15
16
166
128
0
0
0
/
/
movups
0x80
(
%
esi
)
%
xmm4
.
byte
15
88
128
46
148
0
0
/
/
addps
0x942e
(
%
eax
)
%
xmm0
.
byte
15
40
168
254
141
0
0
/
/
movaps
0x8dfe
(
%
eax
)
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
17
174
0
1
0
0
/
/
movups
%
xmm5
0x100
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bilinear_px_sse2
.
globl
_sk_bilinear_px_sse2
FUNCTION
(
_sk_bilinear_px_sse2
)
_sk_bilinear_px_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
79a8
<
_sk_bilinear_px_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
6
/
/
movups
(
%
esi
)
%
xmm0
.
byte
15
16
166
128
0
0
0
/
/
movups
0x80
(
%
esi
)
%
xmm4
.
byte
15
88
128
168
141
0
0
/
/
addps
0x8da8
(
%
eax
)
%
xmm0
.
byte
15
17
166
0
1
0
0
/
/
movups
%
xmm4
0x100
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bilinear_ny_sse2
.
globl
_sk_bilinear_ny_sse2
FUNCTION
(
_sk_bilinear_ny_sse2
)
_sk_bilinear_ny_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
79e4
<
_sk_bilinear_ny_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
78
64
/
/
movups
0x40
(
%
esi
)
%
xmm1
.
byte
15
16
166
192
0
0
0
/
/
movups
0xc0
(
%
esi
)
%
xmm4
.
byte
15
88
136
172
147
0
0
/
/
addps
0x93ac
(
%
eax
)
%
xmm1
.
byte
15
40
168
124
141
0
0
/
/
movaps
0x8d7c
(
%
eax
)
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
17
174
64
1
0
0
/
/
movups
%
xmm5
0x140
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bilinear_py_sse2
.
globl
_sk_bilinear_py_sse2
FUNCTION
(
_sk_bilinear_py_sse2
)
_sk_bilinear_py_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7a2b
<
_sk_bilinear_py_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
78
64
/
/
movups
0x40
(
%
esi
)
%
xmm1
.
byte
15
16
166
192
0
0
0
/
/
movups
0xc0
(
%
esi
)
%
xmm4
.
byte
15
88
136
37
141
0
0
/
/
addps
0x8d25
(
%
eax
)
%
xmm1
.
byte
15
17
166
64
1
0
0
/
/
movups
%
xmm4
0x140
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_n3x_sse2
.
globl
_sk_bicubic_n3x_sse2
FUNCTION
(
_sk_bicubic_n3x_sse2
)
_sk_bicubic_n3x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7a68
<
_sk_bicubic_n3x_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
6
/
/
movups
(
%
esi
)
%
xmm0
.
byte
15
16
166
128
0
0
0
/
/
movups
0x80
(
%
esi
)
%
xmm4
.
byte
15
88
128
56
147
0
0
/
/
addps
0x9338
(
%
eax
)
%
xmm0
.
byte
15
40
168
248
140
0
0
/
/
movaps
0x8cf8
(
%
eax
)
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
89
228
/
/
mulps
%
xmm4
%
xmm4
.
byte
15
89
168
72
147
0
0
/
/
mulps
0x9348
(
%
eax
)
%
xmm5
.
byte
15
88
168
232
142
0
0
/
/
addps
0x8ee8
(
%
eax
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
17
174
0
1
0
0
/
/
movups
%
xmm5
0x100
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_n1x_sse2
.
globl
_sk_bicubic_n1x_sse2
FUNCTION
(
_sk_bicubic_n1x_sse2
)
_sk_bicubic_n1x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7ac5
<
_sk_bicubic_n1x_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
6
/
/
movups
(
%
esi
)
%
xmm0
.
byte
15
16
166
128
0
0
0
/
/
movups
0x80
(
%
esi
)
%
xmm4
.
byte
15
88
128
203
146
0
0
/
/
addps
0x92cb
(
%
eax
)
%
xmm0
.
byte
15
40
168
155
140
0
0
/
/
movaps
0x8c9b
(
%
eax
)
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
40
160
251
146
0
0
/
/
movaps
0x92fb
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
160
11
147
0
0
/
/
addps
0x930b
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
160
139
140
0
0
/
/
addps
0x8c8b
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
160
27
147
0
0
/
/
addps
0x931b
(
%
eax
)
%
xmm4
.
byte
15
17
166
0
1
0
0
/
/
movups
%
xmm4
0x100
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_p1x_sse2
.
globl
_sk_bicubic_p1x_sse2
FUNCTION
(
_sk_bicubic_p1x_sse2
)
_sk_bicubic_p1x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7b30
<
_sk_bicubic_p1x_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
40
160
32
140
0
0
/
/
movaps
0x8c20
(
%
eax
)
%
xmm4
.
byte
15
16
6
/
/
movups
(
%
esi
)
%
xmm0
.
byte
15
16
174
128
0
0
0
/
/
movups
0x80
(
%
esi
)
%
xmm5
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
40
176
144
146
0
0
/
/
movaps
0x9290
(
%
eax
)
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
176
160
146
0
0
/
/
addps
0x92a0
(
%
eax
)
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
244
/
/
addps
%
xmm4
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
176
176
146
0
0
/
/
addps
0x92b0
(
%
eax
)
%
xmm6
.
byte
15
17
182
0
1
0
0
/
/
movups
%
xmm6
0x100
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_p3x_sse2
.
globl
_sk_bicubic_p3x_sse2
FUNCTION
(
_sk_bicubic_p3x_sse2
)
_sk_bicubic_p3x_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7b90
<
_sk_bicubic_p3x_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
6
/
/
movups
(
%
esi
)
%
xmm0
.
byte
15
16
166
128
0
0
0
/
/
movups
0x80
(
%
esi
)
%
xmm4
.
byte
15
88
128
64
146
0
0
/
/
addps
0x9240
(
%
eax
)
%
xmm0
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
89
237
/
/
mulps
%
xmm5
%
xmm5
.
byte
15
89
160
32
146
0
0
/
/
mulps
0x9220
(
%
eax
)
%
xmm4
.
byte
15
88
160
192
141
0
0
/
/
addps
0x8dc0
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
17
166
0
1
0
0
/
/
movups
%
xmm4
0x100
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_n3y_sse2
.
globl
_sk_bicubic_n3y_sse2
FUNCTION
(
_sk_bicubic_n3y_sse2
)
_sk_bicubic_n3y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7be3
<
_sk_bicubic_n3y_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
78
64
/
/
movups
0x40
(
%
esi
)
%
xmm1
.
byte
15
16
166
192
0
0
0
/
/
movups
0xc0
(
%
esi
)
%
xmm4
.
byte
15
88
136
189
145
0
0
/
/
addps
0x91bd
(
%
eax
)
%
xmm1
.
byte
15
40
168
125
139
0
0
/
/
movaps
0x8b7d
(
%
eax
)
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
89
228
/
/
mulps
%
xmm4
%
xmm4
.
byte
15
89
168
205
145
0
0
/
/
mulps
0x91cd
(
%
eax
)
%
xmm5
.
byte
15
88
168
109
141
0
0
/
/
addps
0x8d6d
(
%
eax
)
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
17
174
64
1
0
0
/
/
movups
%
xmm5
0x140
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_n1y_sse2
.
globl
_sk_bicubic_n1y_sse2
FUNCTION
(
_sk_bicubic_n1y_sse2
)
_sk_bicubic_n1y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7c41
<
_sk_bicubic_n1y_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
78
64
/
/
movups
0x40
(
%
esi
)
%
xmm1
.
byte
15
16
166
192
0
0
0
/
/
movups
0xc0
(
%
esi
)
%
xmm4
.
byte
15
88
136
79
145
0
0
/
/
addps
0x914f
(
%
eax
)
%
xmm1
.
byte
15
40
168
31
139
0
0
/
/
movaps
0x8b1f
(
%
eax
)
%
xmm5
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
40
160
127
145
0
0
/
/
movaps
0x917f
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
160
143
145
0
0
/
/
addps
0x918f
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
160
15
139
0
0
/
/
addps
0x8b0f
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
88
160
159
145
0
0
/
/
addps
0x919f
(
%
eax
)
%
xmm4
.
byte
15
17
166
64
1
0
0
/
/
movups
%
xmm4
0x140
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_p1y_sse2
.
globl
_sk_bicubic_p1y_sse2
FUNCTION
(
_sk_bicubic_p1y_sse2
)
_sk_bicubic_p1y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7cad
<
_sk_bicubic_p1y_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
40
160
163
138
0
0
/
/
movaps
0x8aa3
(
%
eax
)
%
xmm4
.
byte
15
16
78
64
/
/
movups
0x40
(
%
esi
)
%
xmm1
.
byte
15
16
174
192
0
0
0
/
/
movups
0xc0
(
%
esi
)
%
xmm5
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
40
176
19
145
0
0
/
/
movaps
0x9113
(
%
eax
)
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
176
35
145
0
0
/
/
addps
0x9123
(
%
eax
)
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
244
/
/
addps
%
xmm4
%
xmm6
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
88
176
51
145
0
0
/
/
addps
0x9133
(
%
eax
)
%
xmm6
.
byte
15
17
182
64
1
0
0
/
/
movups
%
xmm6
0x140
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bicubic_p3y_sse2
.
globl
_sk_bicubic_p3y_sse2
FUNCTION
(
_sk_bicubic_p3y_sse2
)
_sk_bicubic_p3y_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
86
/
/
push
%
esi
.
byte
80
/
/
push
%
eax
.
byte
232
0
0
0
0
/
/
call
7d0e
<
_sk_bicubic_p3y_sse2
+
0xa
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
16
78
64
/
/
movups
0x40
(
%
esi
)
%
xmm1
.
byte
15
16
166
192
0
0
0
/
/
movups
0xc0
(
%
esi
)
%
xmm4
.
byte
15
88
136
194
144
0
0
/
/
addps
0x90c2
(
%
eax
)
%
xmm1
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
89
237
/
/
mulps
%
xmm5
%
xmm5
.
byte
15
89
160
162
144
0
0
/
/
mulps
0x90a2
(
%
eax
)
%
xmm4
.
byte
15
88
160
66
140
0
0
/
/
addps
0x8c42
(
%
eax
)
%
xmm4
.
byte
15
89
229
/
/
mulps
%
xmm5
%
xmm4
.
byte
15
17
166
64
1
0
0
/
/
movups
%
xmm4
0x140
(
%
esi
)
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
81
/
/
push
%
ecx
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
20
/
/
add
0x14
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_callback_sse2
.
globl
_sk_callback_sse2
FUNCTION
(
_sk_callback_sse2
)
_sk_callback_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
12
/
/
sub
0xc
%
esp
.
byte
139
117
8
/
/
mov
0x8
(
%
ebp
)
%
esi
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
139
31
/
/
mov
(
%
edi
)
%
ebx
.
byte
139
70
8
/
/
mov
0x8
(
%
esi
)
%
eax
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
15
40
234
/
/
movaps
%
xmm2
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
15
21
193
/
/
unpckhps
%
xmm1
%
xmm0
.
byte
15
21
211
/
/
unpckhps
%
xmm3
%
xmm2
.
byte
15
40
204
/
/
movaps
%
xmm4
%
xmm1
.
byte
102
15
20
205
/
/
unpcklpd
%
xmm5
%
xmm1
.
byte
15
18
236
/
/
movhlps
%
xmm4
%
xmm5
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
102
15
20
218
/
/
unpcklpd
%
xmm2
%
xmm3
.
byte
102
15
17
75
4
/
/
movupd
%
xmm1
0x4
(
%
ebx
)
.
byte
15
18
208
/
/
movhlps
%
xmm0
%
xmm2
.
byte
15
17
107
20
/
/
movups
%
xmm5
0x14
(
%
ebx
)
.
byte
102
15
17
91
36
/
/
movupd
%
xmm3
0x24
(
%
ebx
)
.
byte
15
17
83
52
/
/
movups
%
xmm2
0x34
(
%
ebx
)
.
byte
133
192
/
/
test
%
eax
%
eax
.
byte
185
4
0
0
0
/
/
mov
0x4
%
ecx
.
byte
15
69
200
/
/
cmovne
%
eax
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
81
/
/
push
%
ecx
.
byte
83
/
/
push
%
ebx
.
byte
255
19
/
/
call
*
(
%
ebx
)
.
byte
131
196
16
/
/
add
0x10
%
esp
.
byte
139
131
4
1
0
0
/
/
mov
0x104
(
%
ebx
)
%
eax
.
byte
15
16
32
/
/
movups
(
%
eax
)
%
xmm4
.
byte
15
16
64
16
/
/
movups
0x10
(
%
eax
)
%
xmm0
.
byte
15
16
88
32
/
/
movups
0x20
(
%
eax
)
%
xmm3
.
byte
15
16
80
48
/
/
movups
0x30
(
%
eax
)
%
xmm2
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
15
21
224
/
/
unpckhps
%
xmm0
%
xmm4
.
byte
15
21
218
/
/
unpckhps
%
xmm2
%
xmm3
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
15
18
205
/
/
movhlps
%
xmm5
%
xmm1
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
15
18
220
/
/
movhlps
%
xmm4
%
xmm3
.
byte
141
71
8
/
/
lea
0x8
(
%
edi
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
80
/
/
push
%
eax
.
byte
86
/
/
push
%
esi
.
byte
255
87
4
/
/
call
*
0x4
(
%
edi
)
.
byte
131
196
28
/
/
add
0x1c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clut_3D_sse2
.
globl
_sk_clut_3D_sse2
FUNCTION
(
_sk_clut_3D_sse2
)
_sk_clut_3D_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
28
1
0
0
/
/
sub
0x11c
%
esp
.
byte
15
41
157
216
254
255
255
/
/
movaps
%
xmm3
-
0x128
(
%
ebp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
81
12
/
/
mov
0xc
(
%
ecx
)
%
edx
.
byte
141
114
255
/
/
lea
-
0x1
(
%
edx
)
%
esi
.
byte
102
15
110
198
/
/
movd
%
esi
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
41
165
248
254
255
255
/
/
movaps
%
xmm4
-
0x108
(
%
ebp
)
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
112
240
0
/
/
pshufd
0x0
%
xmm0
%
xmm6
.
byte
102
15
127
117
152
/
/
movdqa
%
xmm6
-
0x68
(
%
ebp
)
.
byte
139
81
8
/
/
mov
0x8
(
%
ecx
)
%
edx
.
byte
141
114
255
/
/
lea
-
0x1
(
%
edx
)
%
esi
.
byte
102
15
110
214
/
/
movd
%
esi
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
41
149
88
255
255
255
/
/
movaps
%
xmm2
-
0xa8
(
%
ebp
)
.
byte
243
15
91
234
/
/
cvttps2dq
%
xmm2
%
xmm5
.
byte
102
15
127
173
40
255
255
255
/
/
movdqa
%
xmm5
-
0xd8
(
%
ebp
)
.
byte
102
15
112
205
245
/
/
pshufd
0xf5
%
xmm5
%
xmm1
.
byte
102
15
244
206
/
/
pmuludq
%
xmm6
%
xmm1
.
byte
102
15
111
214
/
/
movdqa
%
xmm6
%
xmm2
.
byte
102
15
244
213
/
/
pmuludq
%
xmm5
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
98
209
/
/
punpckldq
%
xmm1
%
xmm2
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
127
109
168
/
/
movdqa
%
xmm5
-
0x58
(
%
ebp
)
.
byte
102
15
110
202
/
/
movd
%
edx
%
xmm1
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
139
81
4
/
/
mov
0x4
(
%
ecx
)
%
edx
.
byte
74
/
/
dec
%
edx
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
102
15
112
209
0
/
/
pshufd
0x0
%
xmm1
%
xmm2
.
byte
243
15
91
216
/
/
cvttps2dq
%
xmm0
%
xmm3
.
byte
102
15
127
93
184
/
/
movdqa
%
xmm3
-
0x48
(
%
ebp
)
.
byte
102
15
112
195
245
/
/
pshufd
0xf5
%
xmm3
%
xmm0
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
244
203
/
/
pmuludq
%
xmm3
%
xmm1
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
98
200
/
/
punpckldq
%
xmm0
%
xmm1
.
byte
102
15
127
77
136
/
/
movdqa
%
xmm1
-
0x78
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
7ef4
<
_sk_clut_3D_sse2
+
0xee
>
.
byte
90
/
/
pop
%
edx
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
102
15
127
133
8
255
255
255
/
/
movdqa
%
xmm0
-
0xf8
(
%
ebp
)
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
102
15
254
216
/
/
paddd
%
xmm0
%
xmm3
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
254
195
/
/
paddd
%
xmm3
%
xmm0
.
byte
102
15
111
162
12
143
0
0
/
/
movdqa
0x8f0c
(
%
edx
)
%
xmm4
.
byte
102
15
127
165
120
255
255
255
/
/
movdqa
%
xmm4
-
0x88
(
%
ebp
)
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
244
196
/
/
pmuludq
%
xmm4
%
xmm0
.
byte
102
15
244
204
/
/
pmuludq
%
xmm4
%
xmm1
.
byte
102
15
112
225
232
/
/
pshufd
0xe8
%
xmm1
%
xmm4
.
byte
102
15
112
200
232
/
/
pshufd
0xe8
%
xmm0
%
xmm1
.
byte
102
15
98
204
/
/
punpckldq
%
xmm4
%
xmm1
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
102
15
112
193
231
/
/
pshufd
0xe7
%
xmm1
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
139
9
/
/
mov
(
%
ecx
)
%
ecx
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
243
15
16
36
153
/
/
movss
(
%
ecx
%
ebx
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
44
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
102
15
20
236
/
/
unpcklpd
%
xmm4
%
xmm5
.
byte
102
15
118
228
/
/
pcmpeqd
%
xmm4
%
xmm4
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
250
196
/
/
psubd
%
xmm4
%
xmm0
.
byte
102
15
112
224
229
/
/
pshufd
0xe5
%
xmm0
%
xmm4
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
153
/
/
movss
(
%
ecx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
36
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
243
15
16
60
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm7
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
248
/
/
unpcklps
%
xmm0
%
xmm7
.
byte
102
15
20
252
/
/
unpcklpd
%
xmm4
%
xmm7
.
byte
102
15
111
130
172
136
0
0
/
/
movdqa
0x88ac
(
%
edx
)
%
xmm0
.
byte
102
15
127
133
104
255
255
255
/
/
movdqa
%
xmm0
-
0x98
(
%
ebp
)
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
193
231
/
/
pshufd
0xe7
%
xmm1
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
153
/
/
movss
(
%
ecx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
52
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm6
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
102
15
20
241
/
/
unpcklpd
%
xmm1
%
xmm6
.
byte
15
40
130
252
142
0
0
/
/
movaps
0x8efc
(
%
edx
)
%
xmm0
.
byte
15
40
141
248
254
255
255
/
/
movaps
-
0x108
(
%
ebp
)
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
41
77
200
/
/
movaps
%
xmm1
-
0x38
(
%
ebp
)
.
byte
15
40
141
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
41
141
72
255
255
255
/
/
movaps
%
xmm1
-
0xb8
(
%
ebp
)
.
byte
15
88
69
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
98
208
/
/
punpckldq
%
xmm0
%
xmm2
.
byte
102
15
127
149
24
255
255
255
/
/
movdqa
%
xmm2
-
0xe8
(
%
ebp
)
.
byte
102
15
254
218
/
/
paddd
%
xmm2
%
xmm3
.
byte
102
15
112
195
245
/
/
pshufd
0xf5
%
xmm3
%
xmm0
.
byte
102
15
111
141
120
255
255
255
/
/
movdqa
-
0x88
(
%
ebp
)
%
xmm1
.
byte
102
15
244
217
/
/
pmuludq
%
xmm1
%
xmm3
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
112
200
232
/
/
pshufd
0xe8
%
xmm0
%
xmm1
.
byte
102
15
112
195
232
/
/
pshufd
0xe8
%
xmm3
%
xmm0
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
243
15
16
12
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm1
.
byte
243
15
16
20
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
12
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm1
.
byte
243
15
16
28
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
102
15
20
218
/
/
unpcklpd
%
xmm2
%
xmm3
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
118
210
/
/
pcmpeqd
%
xmm2
%
xmm2
.
byte
102
15
250
202
/
/
psubd
%
xmm2
%
xmm1
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
15
126
210
/
/
movd
%
xmm2
%
edx
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
243
15
16
20
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm2
.
byte
243
15
16
12
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
243
15
16
20
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm2
.
byte
243
15
16
36
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm4
.
byte
15
20
212
/
/
unpcklps
%
xmm4
%
xmm2
.
byte
102
15
20
209
/
/
unpcklpd
%
xmm1
%
xmm2
.
byte
102
15
254
133
104
255
255
255
/
/
paddd
-
0x98
(
%
ebp
)
%
xmm0
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
243
15
16
12
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm1
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
102
15
20
204
/
/
unpcklpd
%
xmm4
%
xmm1
.
byte
15
91
69
184
/
/
cvtdq2ps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
40
101
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
41
101
216
/
/
movaps
%
xmm4
-
0x28
(
%
ebp
)
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
15
92
215
/
/
subps
%
xmm7
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
41
149
56
255
255
255
/
/
movaps
%
xmm2
-
0xc8
(
%
ebp
)
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
41
141
232
254
255
255
/
/
movaps
%
xmm1
-
0x118
(
%
ebp
)
.
byte
243
15
91
133
72
255
255
255
/
/
cvttps2dq
-
0xb8
(
%
ebp
)
%
xmm0
.
byte
102
15
112
224
245
/
/
pshufd
0xf5
%
xmm0
%
xmm4
.
byte
102
15
111
77
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm1
.
byte
102
15
244
225
/
/
pmuludq
%
xmm1
%
xmm4
.
byte
102
15
111
233
/
/
movdqa
%
xmm1
%
xmm5
.
byte
102
15
244
232
/
/
pmuludq
%
xmm0
%
xmm5
.
byte
102
15
112
253
232
/
/
pshufd
0xe8
%
xmm5
%
xmm7
.
byte
102
15
112
196
232
/
/
pshufd
0xe8
%
xmm4
%
xmm0
.
byte
102
15
98
248
/
/
punpckldq
%
xmm0
%
xmm7
.
byte
102
15
127
125
152
/
/
movdqa
%
xmm7
-
0x68
(
%
ebp
)
.
byte
102
15
254
189
8
255
255
255
/
/
paddd
-
0xf8
(
%
ebp
)
%
xmm7
.
byte
102
15
111
199
/
/
movdqa
%
xmm7
%
xmm0
.
byte
102
15
254
69
136
/
/
paddd
-
0x78
(
%
ebp
)
%
xmm0
.
byte
102
15
112
224
245
/
/
pshufd
0xf5
%
xmm0
%
xmm4
.
byte
102
15
111
141
120
255
255
255
/
/
movdqa
-
0x88
(
%
ebp
)
%
xmm1
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
244
225
/
/
pmuludq
%
xmm1
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
232
232
/
/
pshufd
0xe8
%
xmm0
%
xmm5
.
byte
102
15
98
236
/
/
punpckldq
%
xmm4
%
xmm5
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
197
229
/
/
pshufd
0xe5
%
xmm5
%
xmm0
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
197
78
/
/
pshufd
0x4e
%
xmm5
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
197
231
/
/
pshufd
0xe7
%
xmm5
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
36
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
243
15
16
4
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm0
.
byte
243
15
16
20
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm2
.
byte
15
20
208
/
/
unpcklps
%
xmm0
%
xmm2
.
byte
102
15
20
212
/
/
unpcklpd
%
xmm4
%
xmm2
.
byte
102
15
41
149
72
255
255
255
/
/
movapd
%
xmm2
-
0xb8
(
%
ebp
)
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
250
195
/
/
psubd
%
xmm3
%
xmm0
.
byte
102
15
118
210
/
/
pcmpeqd
%
xmm2
%
xmm2
.
byte
102
15
112
224
229
/
/
pshufd
0xe5
%
xmm0
%
xmm4
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
36
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm4
.
byte
243
15
16
4
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm0
.
byte
15
20
196
/
/
unpcklps
%
xmm4
%
xmm0
.
byte
243
15
16
36
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm4
.
byte
243
15
16
52
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm6
.
byte
15
20
230
/
/
unpcklps
%
xmm6
%
xmm4
.
byte
102
15
20
224
/
/
unpcklpd
%
xmm0
%
xmm4
.
byte
102
15
111
157
104
255
255
255
/
/
movdqa
-
0x98
(
%
ebp
)
%
xmm3
.
byte
102
15
254
235
/
/
paddd
%
xmm3
%
xmm5
.
byte
102
15
112
197
229
/
/
pshufd
0xe5
%
xmm5
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
197
78
/
/
pshufd
0x4e
%
xmm5
%
xmm0
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
102
15
112
197
231
/
/
pshufd
0xe7
%
xmm5
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
52
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm6
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
243
15
16
44
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm5
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
102
15
254
189
24
255
255
255
/
/
paddd
-
0xe8
(
%
ebp
)
%
xmm7
.
byte
102
15
112
199
245
/
/
pshufd
0xf5
%
xmm7
%
xmm0
.
byte
102
15
244
249
/
/
pmuludq
%
xmm1
%
xmm7
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
112
240
232
/
/
pshufd
0xe8
%
xmm0
%
xmm6
.
byte
102
15
112
199
232
/
/
pshufd
0xe8
%
xmm7
%
xmm0
.
byte
102
15
98
198
/
/
punpckldq
%
xmm6
%
xmm0
.
byte
102
15
126
248
/
/
movd
%
xmm7
%
eax
.
byte
102
15
112
240
229
/
/
pshufd
0xe5
%
xmm0
%
xmm6
.
byte
102
15
126
242
/
/
movd
%
xmm6
%
edx
.
byte
102
15
112
240
78
/
/
pshufd
0x4e
%
xmm0
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
112
240
231
/
/
pshufd
0xe7
%
xmm0
%
xmm6
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
243
15
16
52
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm6
.
byte
243
15
16
60
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
243
15
16
52
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm6
.
byte
243
15
16
12
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm1
.
byte
15
20
206
/
/
unpcklps
%
xmm6
%
xmm1
.
byte
102
15
20
207
/
/
unpcklpd
%
xmm7
%
xmm1
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
250
242
/
/
psubd
%
xmm2
%
xmm6
.
byte
102
15
112
254
229
/
/
pshufd
0xe5
%
xmm6
%
xmm7
.
byte
102
15
126
248
/
/
movd
%
xmm7
%
eax
.
byte
102
15
112
254
78
/
/
pshufd
0x4e
%
xmm6
%
xmm7
.
byte
102
15
126
250
/
/
movd
%
xmm7
%
edx
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
112
246
231
/
/
pshufd
0xe7
%
xmm6
%
xmm6
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
243
15
16
52
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm6
.
byte
243
15
16
60
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
243
15
16
20
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm2
.
byte
243
15
16
52
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm6
.
byte
15
20
214
/
/
unpcklps
%
xmm6
%
xmm2
.
byte
102
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm2
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
102
15
254
199
/
/
paddd
%
xmm7
%
xmm0
.
byte
102
15
112
240
229
/
/
pshufd
0xe5
%
xmm0
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
240
78
/
/
pshufd
0x4e
%
xmm0
%
xmm6
.
byte
102
15
126
242
/
/
movd
%
xmm6
%
edx
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
52
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm6
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
243
15
16
28
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm3
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
15
40
181
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm6
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
91
133
40
255
255
255
/
/
cvtdq2ps
-
0xd8
(
%
ebp
)
%
xmm0
.
byte
15
40
165
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm4
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
41
165
88
255
255
255
/
/
movaps
%
xmm4
-
0xa8
(
%
ebp
)
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
41
77
184
/
/
movaps
%
xmm1
-
0x48
(
%
ebp
)
.
byte
15
40
133
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm0
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
41
149
56
255
255
255
/
/
movaps
%
xmm2
-
0xc8
(
%
ebp
)
.
byte
15
40
133
232
254
255
255
/
/
movaps
-
0x118
(
%
ebp
)
%
xmm0
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
89
220
/
/
mulps
%
xmm4
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
41
157
40
255
255
255
/
/
movaps
%
xmm3
-
0xd8
(
%
ebp
)
.
byte
243
15
91
77
200
/
/
cvttps2dq
-
0x38
(
%
ebp
)
%
xmm1
.
byte
102
15
127
77
200
/
/
movdqa
%
xmm1
-
0x38
(
%
ebp
)
.
byte
102
15
111
69
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
15
111
77
136
/
/
movdqa
-
0x78
(
%
ebp
)
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
112
209
245
/
/
pshufd
0xf5
%
xmm1
%
xmm2
.
byte
102
15
111
133
120
255
255
255
/
/
movdqa
-
0x88
(
%
ebp
)
%
xmm0
.
byte
102
15
244
200
/
/
pmuludq
%
xmm0
%
xmm1
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
203
229
/
/
pshufd
0xe5
%
xmm3
%
xmm1
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
112
203
78
/
/
pshufd
0x4e
%
xmm3
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
203
231
/
/
pshufd
0xe7
%
xmm3
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
243
15
16
12
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm1
.
byte
243
15
16
20
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
196
/
/
unpcklps
%
xmm4
%
xmm0
.
byte
102
15
20
194
/
/
unpcklpd
%
xmm2
%
xmm0
.
byte
102
15
41
69
168
/
/
movapd
%
xmm0
-
0x58
(
%
ebp
)
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
118
192
/
/
pcmpeqd
%
xmm0
%
xmm0
.
byte
102
15
250
208
/
/
psubd
%
xmm0
%
xmm2
.
byte
102
15
112
226
229
/
/
pshufd
0xe5
%
xmm2
%
xmm4
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
102
15
112
226
78
/
/
pshufd
0x4e
%
xmm2
%
xmm4
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
243
15
16
20
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm2
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
15
20
226
/
/
unpcklps
%
xmm2
%
xmm4
.
byte
243
15
16
20
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm2
.
byte
243
15
16
44
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm5
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
102
15
20
212
/
/
unpcklpd
%
xmm4
%
xmm2
.
byte
102
15
254
223
/
/
paddd
%
xmm7
%
xmm3
.
byte
102
15
112
227
229
/
/
pshufd
0xe5
%
xmm3
%
xmm4
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
112
219
231
/
/
pshufd
0xe7
%
xmm3
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
243
15
16
28
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm3
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
15
20
227
/
/
unpcklps
%
xmm3
%
xmm4
.
byte
243
15
16
44
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm5
.
byte
243
15
16
28
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm3
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
102
15
20
236
/
/
unpcklpd
%
xmm4
%
xmm5
.
byte
102
15
111
230
/
/
movdqa
%
xmm6
%
xmm4
.
byte
102
15
254
165
24
255
255
255
/
/
paddd
-
0xe8
(
%
ebp
)
%
xmm4
.
byte
102
15
112
220
245
/
/
pshufd
0xf5
%
xmm4
%
xmm3
.
byte
102
15
111
141
120
255
255
255
/
/
movdqa
-
0x88
(
%
ebp
)
%
xmm1
.
byte
102
15
244
225
/
/
pmuludq
%
xmm1
%
xmm4
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
244
217
/
/
pmuludq
%
xmm1
%
xmm3
.
byte
102
15
112
227
232
/
/
pshufd
0xe8
%
xmm3
%
xmm4
.
byte
102
15
112
222
232
/
/
pshufd
0xe8
%
xmm6
%
xmm3
.
byte
102
15
98
220
/
/
punpckldq
%
xmm4
%
xmm3
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
227
229
/
/
pshufd
0xe5
%
xmm3
%
xmm4
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
112
227
231
/
/
pshufd
0xe7
%
xmm3
%
xmm4
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
243
15
16
36
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm4
.
byte
243
15
16
52
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
243
15
16
36
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm4
.
byte
243
15
16
12
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm1
.
byte
15
20
204
/
/
unpcklps
%
xmm4
%
xmm1
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
102
15
250
224
/
/
psubd
%
xmm0
%
xmm4
.
byte
102
15
112
244
229
/
/
pshufd
0xe5
%
xmm4
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
244
78
/
/
pshufd
0x4e
%
xmm4
%
xmm6
.
byte
102
15
126
242
/
/
movd
%
xmm6
%
edx
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
112
228
231
/
/
pshufd
0xe7
%
xmm4
%
xmm4
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
243
15
16
36
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm4
.
byte
243
15
16
4
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm0
.
byte
15
20
196
/
/
unpcklps
%
xmm4
%
xmm0
.
byte
243
15
16
60
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm7
.
byte
243
15
16
36
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm4
.
byte
15
20
252
/
/
unpcklps
%
xmm4
%
xmm7
.
byte
102
15
20
248
/
/
unpcklpd
%
xmm0
%
xmm7
.
byte
102
15
111
165
104
255
255
255
/
/
movdqa
-
0x98
(
%
ebp
)
%
xmm4
.
byte
102
15
254
220
/
/
paddd
%
xmm4
%
xmm3
.
byte
102
15
112
195
229
/
/
pshufd
0xe5
%
xmm3
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
195
78
/
/
pshufd
0x4e
%
xmm3
%
xmm0
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
112
195
231
/
/
pshufd
0xe7
%
xmm3
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
28
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm3
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
243
15
16
52
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm6
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
102
15
20
243
/
/
unpcklpd
%
xmm3
%
xmm6
.
byte
15
40
93
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm3
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
41
77
168
/
/
movaps
%
xmm1
-
0x58
(
%
ebp
)
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
88
250
/
/
addps
%
xmm2
%
xmm7
.
byte
15
92
245
/
/
subps
%
xmm5
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
88
245
/
/
addps
%
xmm5
%
xmm6
.
byte
102
15
111
69
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm0
.
byte
102
15
254
69
200
/
/
paddd
-
0x38
(
%
ebp
)
%
xmm0
.
byte
102
15
127
69
152
/
/
movdqa
%
xmm0
-
0x68
(
%
ebp
)
.
byte
102
15
111
85
136
/
/
movdqa
-
0x78
(
%
ebp
)
%
xmm2
.
byte
102
15
254
208
/
/
paddd
%
xmm0
%
xmm2
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
157
120
255
255
255
/
/
movdqa
-
0x88
(
%
ebp
)
%
xmm3
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
244
195
/
/
pmuludq
%
xmm3
%
xmm0
.
byte
102
15
112
200
232
/
/
pshufd
0xe8
%
xmm0
%
xmm1
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
243
15
16
12
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm1
.
byte
243
15
16
20
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
12
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm1
.
byte
243
15
16
44
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm5
.
byte
15
20
233
/
/
unpcklps
%
xmm1
%
xmm5
.
byte
102
15
20
234
/
/
unpcklpd
%
xmm2
%
xmm5
.
byte
102
15
41
109
136
/
/
movapd
%
xmm5
-
0x78
(
%
ebp
)
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
118
210
/
/
pcmpeqd
%
xmm2
%
xmm2
.
byte
102
15
250
202
/
/
psubd
%
xmm2
%
xmm1
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
15
126
210
/
/
movd
%
xmm2
%
edx
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
243
15
16
12
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm1
.
byte
243
15
16
20
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
44
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm5
.
byte
243
15
16
12
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm1
.
byte
15
20
233
/
/
unpcklps
%
xmm1
%
xmm5
.
byte
102
15
20
234
/
/
unpcklpd
%
xmm2
%
xmm5
.
byte
102
15
41
109
200
/
/
movapd
%
xmm5
-
0x38
(
%
ebp
)
.
byte
102
15
254
196
/
/
paddd
%
xmm4
%
xmm0
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
12
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
36
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm4
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
102
15
20
225
/
/
unpcklpd
%
xmm1
%
xmm4
.
byte
102
15
111
77
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm1
.
byte
102
15
254
141
24
255
255
255
/
/
paddd
-
0xe8
(
%
ebp
)
%
xmm1
.
byte
102
15
112
193
245
/
/
pshufd
0xf5
%
xmm1
%
xmm0
.
byte
102
15
244
203
/
/
pmuludq
%
xmm3
%
xmm1
.
byte
102
15
244
195
/
/
pmuludq
%
xmm3
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
209
232
/
/
pshufd
0xe8
%
xmm1
%
xmm2
.
byte
102
15
98
208
/
/
punpckldq
%
xmm0
%
xmm2
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
243
15
16
4
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm0
.
byte
243
15
16
12
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
28
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm3
.
byte
243
15
16
4
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm0
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
250
203
/
/
psubd
%
xmm3
%
xmm1
.
byte
102
15
112
217
229
/
/
pshufd
0xe5
%
xmm1
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
218
/
/
movd
%
xmm3
%
edx
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
243
15
16
12
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm1
.
byte
243
15
16
28
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
243
15
16
12
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm1
.
byte
243
15
16
44
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm5
.
byte
15
20
205
/
/
unpcklps
%
xmm5
%
xmm1
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
102
15
254
149
104
255
255
255
/
/
paddd
-
0x98
(
%
ebp
)
%
xmm2
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
126
218
/
/
movd
%
xmm3
%
edx
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
243
15
16
20
185
/
/
movss
(
%
ecx
%
edi
4
)
%
xmm2
.
byte
243
15
16
28
145
/
/
movss
(
%
ecx
%
edx
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
20
177
/
/
movss
(
%
ecx
%
esi
4
)
%
xmm2
.
byte
243
15
16
44
129
/
/
movss
(
%
ecx
%
eax
4
)
%
xmm5
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
15
40
109
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm5
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
40
109
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm5
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
40
101
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm4
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
40
157
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
92
207
/
/
subps
%
xmm7
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
214
/
/
addps
%
xmm6
%
xmm2
.
byte
15
91
157
8
255
255
255
/
/
cvtdq2ps
-
0xf8
(
%
ebp
)
%
xmm3
.
byte
15
40
165
248
254
255
255
/
/
movaps
-
0x108
(
%
ebp
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
40
157
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm3
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
40
157
40
255
255
255
/
/
movaps
-
0xd8
(
%
ebp
)
%
xmm3
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
157
216
254
255
255
/
/
movaps
-
0x128
(
%
ebp
)
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
129
196
44
1
0
0
/
/
add
0x12c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clut_4D_sse2
.
globl
_sk_clut_4D_sse2
FUNCTION
(
_sk_clut_4D_sse2
)
_sk_clut_4D_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
76
1
0
0
/
/
sub
0x14c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
139
74
16
/
/
mov
0x10
(
%
edx
)
%
ecx
.
byte
141
113
255
/
/
lea
-
0x1
(
%
ecx
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
102
15
110
217
/
/
movd
%
ecx
%
xmm3
.
byte
102
15
112
251
0
/
/
pshufd
0x0
%
xmm3
%
xmm7
.
byte
102
15
127
189
24
255
255
255
/
/
movdqa
%
xmm7
-
0xe8
(
%
ebp
)
.
byte
139
74
12
/
/
mov
0xc
(
%
edx
)
%
ecx
.
byte
141
113
255
/
/
lea
-
0x1
(
%
ecx
)
%
esi
.
byte
102
15
110
230
/
/
movd
%
esi
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
89
226
/
/
mulps
%
xmm2
%
xmm4
.
byte
15
41
165
248
254
255
255
/
/
movaps
%
xmm4
-
0x108
(
%
ebp
)
.
byte
243
15
91
244
/
/
cvttps2dq
%
xmm4
%
xmm6
.
byte
102
15
127
181
216
254
255
255
/
/
movdqa
%
xmm6
-
0x128
(
%
ebp
)
.
byte
102
15
112
214
245
/
/
pshufd
0xf5
%
xmm6
%
xmm2
.
byte
102
15
244
215
/
/
pmuludq
%
xmm7
%
xmm2
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
244
230
/
/
pmuludq
%
xmm6
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
98
226
/
/
punpckldq
%
xmm2
%
xmm4
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
127
181
8
255
255
255
/
/
movdqa
%
xmm6
-
0xf8
(
%
ebp
)
.
byte
102
15
110
209
/
/
movd
%
ecx
%
xmm2
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
244
216
/
/
pmuludq
%
xmm0
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
98
211
/
/
punpckldq
%
xmm3
%
xmm2
.
byte
139
74
8
/
/
mov
0x8
(
%
edx
)
%
ecx
.
byte
141
113
255
/
/
lea
-
0x1
(
%
ecx
)
%
esi
.
byte
102
15
110
222
/
/
movd
%
esi
%
xmm3
.
byte
102
15
112
219
0
/
/
pshufd
0x0
%
xmm3
%
xmm3
.
byte
15
91
219
/
/
cvtdq2ps
%
xmm3
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
41
157
56
255
255
255
/
/
movaps
%
xmm3
-
0xc8
(
%
ebp
)
.
byte
102
15
112
250
0
/
/
pshufd
0x0
%
xmm2
%
xmm7
.
byte
102
15
127
189
88
255
255
255
/
/
movdqa
%
xmm7
-
0xa8
(
%
ebp
)
.
byte
243
15
91
227
/
/
cvttps2dq
%
xmm3
%
xmm4
.
byte
102
15
127
165
40
255
255
255
/
/
movdqa
%
xmm4
-
0xd8
(
%
ebp
)
.
byte
102
15
112
204
245
/
/
pshufd
0xf5
%
xmm4
%
xmm1
.
byte
102
15
244
207
/
/
pmuludq
%
xmm7
%
xmm1
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
102
15
244
220
/
/
pmuludq
%
xmm4
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
98
217
/
/
punpckldq
%
xmm1
%
xmm3
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
102
15
127
165
72
255
255
255
/
/
movdqa
%
xmm4
-
0xb8
(
%
ebp
)
.
byte
102
15
110
201
/
/
movd
%
ecx
%
xmm1
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
139
74
4
/
/
mov
0x4
(
%
edx
)
%
ecx
.
byte
73
/
/
dec
%
ecx
.
byte
102
15
110
209
/
/
movd
%
ecx
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
102
15
112
217
0
/
/
pshufd
0x0
%
xmm1
%
xmm3
.
byte
102
15
127
93
168
/
/
movdqa
%
xmm3
-
0x58
(
%
ebp
)
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
102
15
127
133
104
255
255
255
/
/
movdqa
%
xmm0
-
0x98
(
%
ebp
)
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
244
203
/
/
pmuludq
%
xmm3
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
15
112
218
232
/
/
pshufd
0xe8
%
xmm2
%
xmm3
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
98
217
/
/
punpckldq
%
xmm1
%
xmm3
.
byte
102
15
127
93
152
/
/
movdqa
%
xmm3
-
0x68
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
8a80
<
_sk_clut_4D_sse2
+
0x165
>
.
byte
89
/
/
pop
%
ecx
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
41
133
168
254
255
255
/
/
movaps
%
xmm0
-
0x158
(
%
ebp
)
.
byte
243
15
91
200
/
/
cvttps2dq
%
xmm0
%
xmm1
.
byte
102
15
127
141
200
254
255
255
/
/
movdqa
%
xmm1
-
0x138
(
%
ebp
)
.
byte
102
15
254
241
/
/
paddd
%
xmm1
%
xmm6
.
byte
102
15
127
117
200
/
/
movdqa
%
xmm6
-
0x38
(
%
ebp
)
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
254
214
/
/
paddd
%
xmm6
%
xmm2
.
byte
102
15
254
218
/
/
paddd
%
xmm2
%
xmm3
.
byte
102
15
111
161
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm4
.
byte
102
15
112
203
245
/
/
pshufd
0xf5
%
xmm3
%
xmm1
.
byte
102
15
244
220
/
/
pmuludq
%
xmm4
%
xmm3
.
byte
102
15
244
204
/
/
pmuludq
%
xmm4
%
xmm1
.
byte
102
15
112
233
232
/
/
pshufd
0xe8
%
xmm1
%
xmm5
.
byte
102
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm1
.
byte
102
15
98
205
/
/
punpckldq
%
xmm5
%
xmm1
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
112
217
229
/
/
pshufd
0xe5
%
xmm1
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
219
/
/
movd
%
xmm3
%
ebx
.
byte
102
15
112
217
231
/
/
pshufd
0xe7
%
xmm1
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
139
18
/
/
mov
(
%
edx
)
%
edx
.
byte
243
15
16
28
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm3
.
byte
243
15
16
52
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
36
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm4
.
byte
15
20
227
/
/
unpcklps
%
xmm3
%
xmm4
.
byte
102
15
20
230
/
/
unpcklpd
%
xmm6
%
xmm4
.
byte
102
15
41
101
184
/
/
movapd
%
xmm4
-
0x48
(
%
ebp
)
.
byte
102
15
118
228
/
/
pcmpeqd
%
xmm4
%
xmm4
.
byte
102
15
111
217
/
/
movdqa
%
xmm1
%
xmm3
.
byte
102
15
250
220
/
/
psubd
%
xmm4
%
xmm3
.
byte
102
15
118
237
/
/
pcmpeqd
%
xmm5
%
xmm5
.
byte
102
15
112
243
229
/
/
pshufd
0xe5
%
xmm3
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
243
78
/
/
pshufd
0x4e
%
xmm3
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
219
231
/
/
pshufd
0xe7
%
xmm3
%
xmm3
.
byte
102
15
126
219
/
/
movd
%
xmm3
%
ebx
.
byte
243
15
16
28
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm3
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
60
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm7
.
byte
15
20
223
/
/
unpcklps
%
xmm7
%
xmm3
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
102
15
41
157
232
254
255
255
/
/
movapd
%
xmm3
-
0x118
(
%
ebp
)
.
byte
102
15
254
137
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm1
.
byte
102
15
112
241
229
/
/
pshufd
0xe5
%
xmm1
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
241
78
/
/
pshufd
0x4e
%
xmm1
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
241
/
/
unpcklps
%
xmm1
%
xmm6
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
60
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm7
.
byte
15
20
223
/
/
unpcklps
%
xmm7
%
xmm3
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
15
40
185
112
131
0
0
/
/
movaps
0x8370
(
%
ecx
)
%
xmm7
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
41
133
120
255
255
255
/
/
movaps
%
xmm0
-
0x88
(
%
ebp
)
.
byte
15
40
133
248
254
255
255
/
/
movaps
-
0x108
(
%
ebp
)
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
41
69
136
/
/
movaps
%
xmm0
-
0x78
(
%
ebp
)
.
byte
15
40
133
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm0
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
41
133
184
254
255
255
/
/
movaps
%
xmm0
-
0x148
(
%
ebp
)
.
byte
15
88
125
216
/
/
addps
-
0x28
(
%
ebp
)
%
xmm7
.
byte
243
15
91
255
/
/
cvttps2dq
%
xmm7
%
xmm7
.
byte
102
15
112
199
245
/
/
pshufd
0xf5
%
xmm7
%
xmm0
.
byte
102
15
111
117
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm6
.
byte
102
15
244
198
/
/
pmuludq
%
xmm6
%
xmm0
.
byte
102
15
244
247
/
/
pmuludq
%
xmm7
%
xmm6
.
byte
102
15
112
230
232
/
/
pshufd
0xe8
%
xmm6
%
xmm4
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
98
224
/
/
punpckldq
%
xmm0
%
xmm4
.
byte
102
15
127
101
168
/
/
movdqa
%
xmm4
-
0x58
(
%
ebp
)
.
byte
102
15
254
212
/
/
paddd
%
xmm4
%
xmm2
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
161
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm4
.
byte
102
15
244
212
/
/
pmuludq
%
xmm4
%
xmm2
.
byte
102
15
244
196
/
/
pmuludq
%
xmm4
%
xmm0
.
byte
102
15
112
240
232
/
/
pshufd
0xe8
%
xmm0
%
xmm6
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
198
/
/
punpckldq
%
xmm6
%
xmm0
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
208
229
/
/
pshufd
0xe5
%
xmm0
%
xmm2
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
208
231
/
/
pshufd
0xe7
%
xmm0
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
52
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
243
15
16
20
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm2
.
byte
243
15
16
12
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm1
.
byte
15
20
202
/
/
unpcklps
%
xmm2
%
xmm1
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
250
213
/
/
psubd
%
xmm5
%
xmm2
.
byte
102
15
112
242
229
/
/
pshufd
0xe5
%
xmm2
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
242
78
/
/
pshufd
0x4e
%
xmm2
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
243
15
16
44
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm5
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
102
15
20
214
/
/
unpcklpd
%
xmm6
%
xmm2
.
byte
102
15
254
129
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm0
.
byte
102
15
112
232
229
/
/
pshufd
0xe5
%
xmm0
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
44
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
243
15
16
60
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm7
.
byte
243
15
16
52
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm6
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
102
15
20
253
/
/
unpcklpd
%
xmm5
%
xmm7
.
byte
15
91
173
104
255
255
255
/
/
cvtdq2ps
-
0x98
(
%
ebp
)
%
xmm5
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
41
69
216
/
/
movaps
%
xmm0
-
0x28
(
%
ebp
)
.
byte
15
40
109
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm5
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
15
41
141
104
255
255
255
/
/
movaps
%
xmm1
-
0x98
(
%
ebp
)
.
byte
15
40
141
232
254
255
255
/
/
movaps
-
0x118
(
%
ebp
)
%
xmm1
.
byte
15
92
209
/
/
subps
%
xmm1
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
41
85
184
/
/
movaps
%
xmm2
-
0x48
(
%
ebp
)
.
byte
15
92
251
/
/
subps
%
xmm3
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
88
251
/
/
addps
%
xmm3
%
xmm7
.
byte
243
15
91
141
184
254
255
255
/
/
cvttps2dq
-
0x148
(
%
ebp
)
%
xmm1
.
byte
102
15
112
217
245
/
/
pshufd
0xf5
%
xmm1
%
xmm3
.
byte
102
15
111
133
88
255
255
255
/
/
movdqa
-
0xa8
(
%
ebp
)
%
xmm0
.
byte
102
15
244
216
/
/
pmuludq
%
xmm0
%
xmm3
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
244
233
/
/
pmuludq
%
xmm1
%
xmm5
.
byte
102
15
112
197
232
/
/
pshufd
0xe8
%
xmm5
%
xmm0
.
byte
102
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm1
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
15
127
133
88
255
255
255
/
/
movdqa
%
xmm0
-
0xa8
(
%
ebp
)
.
byte
102
15
111
85
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
254
208
/
/
paddd
%
xmm0
%
xmm2
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
254
93
152
/
/
paddd
-
0x68
(
%
ebp
)
%
xmm3
.
byte
102
15
112
203
245
/
/
pshufd
0xf5
%
xmm3
%
xmm1
.
byte
102
15
244
220
/
/
pmuludq
%
xmm4
%
xmm3
.
byte
102
15
244
204
/
/
pmuludq
%
xmm4
%
xmm1
.
byte
102
15
112
233
232
/
/
pshufd
0xe8
%
xmm1
%
xmm5
.
byte
102
15
112
203
232
/
/
pshufd
0xe8
%
xmm3
%
xmm1
.
byte
102
15
98
205
/
/
punpckldq
%
xmm5
%
xmm1
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
217
229
/
/
pshufd
0xe5
%
xmm1
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
217
231
/
/
pshufd
0xe7
%
xmm1
%
xmm3
.
byte
102
15
126
219
/
/
movd
%
xmm3
%
ebx
.
byte
243
15
16
28
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm3
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
198
/
/
unpcklps
%
xmm6
%
xmm0
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
102
15
41
69
200
/
/
movapd
%
xmm0
-
0x38
(
%
ebp
)
.
byte
102
15
111
233
/
/
movdqa
%
xmm1
%
xmm5
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
250
235
/
/
psubd
%
xmm3
%
xmm5
.
byte
102
15
112
245
229
/
/
pshufd
0xe5
%
xmm5
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
245
78
/
/
pshufd
0x4e
%
xmm5
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
126
239
/
/
movd
%
xmm5
%
edi
.
byte
102
15
112
237
231
/
/
pshufd
0xe7
%
xmm5
%
xmm5
.
byte
102
15
126
235
/
/
movd
%
xmm5
%
ebx
.
byte
243
15
16
44
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm5
.
byte
243
15
16
4
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm0
.
byte
15
20
197
/
/
unpcklps
%
xmm5
%
xmm0
.
byte
243
15
16
52
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm6
.
byte
243
15
16
44
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm5
.
byte
15
20
245
/
/
unpcklps
%
xmm5
%
xmm6
.
byte
102
15
20
240
/
/
unpcklpd
%
xmm0
%
xmm6
.
byte
102
15
254
137
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm1
.
byte
102
15
112
193
229
/
/
pshufd
0xe5
%
xmm1
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
193
231
/
/
pshufd
0xe7
%
xmm1
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
44
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
243
15
16
36
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm4
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
102
15
20
229
/
/
unpcklpd
%
xmm5
%
xmm4
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
254
77
168
/
/
paddd
-
0x58
(
%
ebp
)
%
xmm1
.
byte
102
15
112
193
245
/
/
pshufd
0xf5
%
xmm1
%
xmm0
.
byte
102
15
111
145
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm2
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
233
232
/
/
pshufd
0xe8
%
xmm1
%
xmm5
.
byte
102
15
98
232
/
/
punpckldq
%
xmm0
%
xmm5
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
197
229
/
/
pshufd
0xe5
%
xmm5
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
197
78
/
/
pshufd
0x4e
%
xmm5
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
197
231
/
/
pshufd
0xe7
%
xmm5
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
4
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm0
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
243
15
16
20
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
15
20
208
/
/
unpcklpd
%
xmm0
%
xmm2
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
250
195
/
/
psubd
%
xmm3
%
xmm0
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
102
15
20
217
/
/
unpcklpd
%
xmm1
%
xmm3
.
byte
102
15
254
169
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm5
.
byte
102
15
112
197
229
/
/
pshufd
0xe5
%
xmm5
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
197
78
/
/
pshufd
0x4e
%
xmm5
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
126
239
/
/
movd
%
xmm5
%
edi
.
byte
102
15
112
197
231
/
/
pshufd
0xe7
%
xmm5
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
102
15
20
233
/
/
unpcklpd
%
xmm1
%
xmm5
.
byte
15
40
77
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm1
.
byte
15
92
209
/
/
subps
%
xmm1
%
xmm2
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
92
222
/
/
subps
%
xmm6
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
88
236
/
/
addps
%
xmm4
%
xmm5
.
byte
15
91
133
40
255
255
255
/
/
cvtdq2ps
-
0xd8
(
%
ebp
)
%
xmm0
.
byte
15
40
141
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
41
141
56
255
255
255
/
/
movaps
%
xmm1
-
0xc8
(
%
ebp
)
.
byte
15
40
133
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm0
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
41
149
40
255
255
255
/
/
movaps
%
xmm2
-
0xd8
(
%
ebp
)
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
41
157
104
255
255
255
/
/
movaps
%
xmm3
-
0x98
(
%
ebp
)
.
byte
15
92
239
/
/
subps
%
xmm7
%
xmm5
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
88
239
/
/
addps
%
xmm7
%
xmm5
.
byte
15
41
109
184
/
/
movaps
%
xmm5
-
0x48
(
%
ebp
)
.
byte
243
15
91
69
136
/
/
cvttps2dq
-
0x78
(
%
ebp
)
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
111
149
24
255
255
255
/
/
movdqa
-
0xe8
(
%
ebp
)
%
xmm2
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
15
112
226
232
/
/
pshufd
0xe8
%
xmm2
%
xmm4
.
byte
102
15
112
193
232
/
/
pshufd
0xe8
%
xmm1
%
xmm0
.
byte
102
15
98
224
/
/
punpckldq
%
xmm0
%
xmm4
.
byte
102
15
127
165
24
255
255
255
/
/
movdqa
%
xmm4
-
0xe8
(
%
ebp
)
.
byte
102
15
254
165
200
254
255
255
/
/
paddd
-
0x138
(
%
ebp
)
%
xmm4
.
byte
102
15
127
101
200
/
/
movdqa
%
xmm4
-
0x38
(
%
ebp
)
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
254
141
72
255
255
255
/
/
paddd
-
0xb8
(
%
ebp
)
%
xmm1
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
254
85
152
/
/
paddd
-
0x68
(
%
ebp
)
%
xmm2
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
153
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm3
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
244
195
/
/
pmuludq
%
xmm3
%
xmm0
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
102
15
112
216
232
/
/
pshufd
0xe8
%
xmm0
%
xmm3
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
195
/
/
punpckldq
%
xmm3
%
xmm0
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
208
229
/
/
pshufd
0xe5
%
xmm0
%
xmm2
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
208
231
/
/
pshufd
0xe7
%
xmm0
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
20
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm2
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
226
/
/
unpcklps
%
xmm2
%
xmm4
.
byte
102
15
20
227
/
/
unpcklpd
%
xmm3
%
xmm4
.
byte
102
15
41
101
136
/
/
movapd
%
xmm4
-
0x78
(
%
ebp
)
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
118
246
/
/
pcmpeqd
%
xmm6
%
xmm6
.
byte
102
15
250
214
/
/
psubd
%
xmm6
%
xmm2
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
36
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm4
.
byte
243
15
16
44
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm5
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
102
15
20
227
/
/
unpcklpd
%
xmm3
%
xmm4
.
byte
102
15
254
129
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm0
.
byte
102
15
112
216
229
/
/
pshufd
0xe5
%
xmm0
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
216
78
/
/
pshufd
0x4e
%
xmm0
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
243
15
16
44
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm5
.
byte
15
20
213
/
/
unpcklps
%
xmm5
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
102
15
254
77
168
/
/
paddd
-
0x58
(
%
ebp
)
%
xmm1
.
byte
102
15
112
217
245
/
/
pshufd
0xf5
%
xmm1
%
xmm3
.
byte
102
15
244
207
/
/
pmuludq
%
xmm7
%
xmm1
.
byte
102
15
244
223
/
/
pmuludq
%
xmm7
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
249
232
/
/
pshufd
0xe8
%
xmm1
%
xmm7
.
byte
102
15
98
251
/
/
punpckldq
%
xmm3
%
xmm7
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
207
229
/
/
pshufd
0xe5
%
xmm7
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
207
78
/
/
pshufd
0x4e
%
xmm7
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
207
231
/
/
pshufd
0xe7
%
xmm7
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
102
15
20
195
/
/
unpcklpd
%
xmm3
%
xmm0
.
byte
102
15
111
207
/
/
movdqa
%
xmm7
%
xmm1
.
byte
102
15
250
206
/
/
psubd
%
xmm6
%
xmm1
.
byte
102
15
112
217
229
/
/
pshufd
0xe5
%
xmm1
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
243
15
16
52
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm6
.
byte
15
20
238
/
/
unpcklps
%
xmm6
%
xmm5
.
byte
102
15
20
235
/
/
unpcklpd
%
xmm3
%
xmm5
.
byte
102
15
111
137
32
125
0
0
/
/
movdqa
0x7d20
(
%
ecx
)
%
xmm1
.
byte
102
15
254
249
/
/
paddd
%
xmm1
%
xmm7
.
byte
102
15
112
223
229
/
/
pshufd
0xe5
%
xmm7
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
223
78
/
/
pshufd
0x4e
%
xmm7
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
255
/
/
movd
%
xmm7
%
edi
.
byte
102
15
112
223
231
/
/
pshufd
0xe7
%
xmm7
%
xmm3
.
byte
102
15
126
219
/
/
movd
%
xmm3
%
ebx
.
byte
243
15
16
28
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm3
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
243
15
16
60
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm7
.
byte
243
15
16
28
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm3
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
102
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm7
.
byte
15
40
117
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
41
69
136
/
/
movaps
%
xmm0
-
0x78
(
%
ebp
)
.
byte
15
92
236
/
/
subps
%
xmm4
%
xmm5
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
88
236
/
/
addps
%
xmm4
%
xmm5
.
byte
15
92
250
/
/
subps
%
xmm2
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
88
250
/
/
addps
%
xmm2
%
xmm7
.
byte
102
15
111
85
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
254
149
88
255
255
255
/
/
paddd
-
0xa8
(
%
ebp
)
%
xmm2
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
111
226
/
/
movdqa
%
xmm2
%
xmm4
.
byte
102
15
254
69
152
/
/
paddd
-
0x68
(
%
ebp
)
%
xmm0
.
byte
102
15
112
208
245
/
/
pshufd
0xf5
%
xmm0
%
xmm2
.
byte
102
15
111
153
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm3
.
byte
102
15
244
195
/
/
pmuludq
%
xmm3
%
xmm0
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
112
218
232
/
/
pshufd
0xe8
%
xmm2
%
xmm3
.
byte
102
15
112
208
232
/
/
pshufd
0xe8
%
xmm0
%
xmm2
.
byte
102
15
98
211
/
/
punpckldq
%
xmm3
%
xmm2
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
243
15
16
4
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm0
.
byte
243
15
16
52
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm6
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
102
15
20
243
/
/
unpcklpd
%
xmm3
%
xmm6
.
byte
102
15
41
117
200
/
/
movapd
%
xmm6
-
0x38
(
%
ebp
)
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
250
195
/
/
psubd
%
xmm3
%
xmm0
.
byte
102
15
112
216
229
/
/
pshufd
0xe5
%
xmm0
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
216
78
/
/
pshufd
0x4e
%
xmm0
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
28
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm3
.
byte
243
15
16
4
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm0
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
52
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm6
.
byte
15
20
222
/
/
unpcklps
%
xmm6
%
xmm3
.
byte
102
15
20
216
/
/
unpcklpd
%
xmm0
%
xmm3
.
byte
102
15
41
157
232
254
255
255
/
/
movapd
%
xmm3
-
0x118
(
%
ebp
)
.
byte
102
15
254
209
/
/
paddd
%
xmm1
%
xmm2
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
254
85
168
/
/
paddd
-
0x58
(
%
ebp
)
%
xmm2
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
137
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm1
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
112
240
232
/
/
pshufd
0xe8
%
xmm0
%
xmm6
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
198
/
/
punpckldq
%
xmm6
%
xmm0
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
224
229
/
/
pshufd
0xe5
%
xmm0
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
102
15
112
224
231
/
/
pshufd
0xe7
%
xmm0
%
xmm4
.
byte
102
15
126
227
/
/
movd
%
xmm4
%
ebx
.
byte
243
15
16
36
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm4
.
byte
243
15
16
52
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
243
15
16
36
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm4
.
byte
243
15
16
12
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm1
.
byte
15
20
204
/
/
unpcklps
%
xmm4
%
xmm1
.
byte
102
15
20
206
/
/
unpcklpd
%
xmm6
%
xmm1
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
118
210
/
/
pcmpeqd
%
xmm2
%
xmm2
.
byte
102
15
250
226
/
/
psubd
%
xmm2
%
xmm4
.
byte
102
15
112
244
229
/
/
pshufd
0xe5
%
xmm4
%
xmm6
.
byte
102
15
126
240
/
/
movd
%
xmm6
%
eax
.
byte
102
15
112
244
78
/
/
pshufd
0x4e
%
xmm4
%
xmm6
.
byte
102
15
126
246
/
/
movd
%
xmm6
%
esi
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
102
15
112
228
231
/
/
pshufd
0xe7
%
xmm4
%
xmm4
.
byte
102
15
126
227
/
/
movd
%
xmm4
%
ebx
.
byte
243
15
16
36
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm4
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
212
/
/
unpcklps
%
xmm4
%
xmm2
.
byte
102
15
20
214
/
/
unpcklpd
%
xmm6
%
xmm2
.
byte
102
15
254
129
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm0
.
byte
102
15
112
224
229
/
/
pshufd
0xe5
%
xmm0
%
xmm4
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
102
15
112
224
78
/
/
pshufd
0x4e
%
xmm0
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
36
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
243
15
16
52
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm6
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
102
15
20
244
/
/
unpcklpd
%
xmm4
%
xmm6
.
byte
15
40
101
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
40
165
232
254
255
255
/
/
movaps
-
0x118
(
%
ebp
)
%
xmm4
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
92
243
/
/
subps
%
xmm3
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
88
243
/
/
addps
%
xmm3
%
xmm6
.
byte
15
40
93
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm3
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
40
133
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm0
.
byte
15
89
200
/
/
mulps
%
xmm0
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
15
89
240
/
/
mulps
%
xmm0
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
91
133
216
254
255
255
/
/
cvtdq2ps
-
0x128
(
%
ebp
)
%
xmm0
.
byte
15
40
141
248
254
255
255
/
/
movaps
-
0x108
(
%
ebp
)
%
xmm1
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
41
141
248
254
255
255
/
/
movaps
%
xmm1
-
0x108
(
%
ebp
)
.
byte
15
40
133
40
255
255
255
/
/
movaps
-
0xd8
(
%
ebp
)
%
xmm0
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
89
217
/
/
mulps
%
xmm1
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
15
41
157
40
255
255
255
/
/
movaps
%
xmm3
-
0xd8
(
%
ebp
)
.
byte
15
40
133
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm0
.
byte
15
92
208
/
/
subps
%
xmm0
%
xmm2
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
41
149
104
255
255
255
/
/
movaps
%
xmm2
-
0x98
(
%
ebp
)
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
92
240
/
/
subps
%
xmm0
%
xmm6
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
88
240
/
/
addps
%
xmm0
%
xmm6
.
byte
15
41
181
216
254
255
255
/
/
movaps
%
xmm6
-
0x128
(
%
ebp
)
.
byte
243
15
91
141
120
255
255
255
/
/
cvttps2dq
-
0x88
(
%
ebp
)
%
xmm1
.
byte
102
15
127
77
136
/
/
movdqa
%
xmm1
-
0x78
(
%
ebp
)
.
byte
102
15
111
133
8
255
255
255
/
/
movdqa
-
0xf8
(
%
ebp
)
%
xmm0
.
byte
102
15
254
193
/
/
paddd
%
xmm1
%
xmm0
.
byte
102
15
127
133
8
255
255
255
/
/
movdqa
%
xmm0
-
0xf8
(
%
ebp
)
.
byte
102
15
111
141
72
255
255
255
/
/
movdqa
-
0xb8
(
%
ebp
)
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
111
93
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm3
.
byte
102
15
254
217
/
/
paddd
%
xmm1
%
xmm3
.
byte
102
15
112
211
245
/
/
pshufd
0xf5
%
xmm3
%
xmm2
.
byte
102
15
111
129
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm0
.
byte
102
15
244
216
/
/
pmuludq
%
xmm0
%
xmm3
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
112
226
232
/
/
pshufd
0xe8
%
xmm2
%
xmm4
.
byte
102
15
112
211
232
/
/
pshufd
0xe8
%
xmm3
%
xmm2
.
byte
102
15
98
212
/
/
punpckldq
%
xmm4
%
xmm2
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
218
231
/
/
pshufd
0xe7
%
xmm2
%
xmm3
.
byte
102
15
126
219
/
/
movd
%
xmm3
%
ebx
.
byte
243
15
16
28
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm3
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
102
15
41
133
120
255
255
255
/
/
movapd
%
xmm0
-
0x88
(
%
ebp
)
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
118
192
/
/
pcmpeqd
%
xmm0
%
xmm0
.
byte
102
15
250
216
/
/
psubd
%
xmm0
%
xmm3
.
byte
102
15
112
235
229
/
/
pshufd
0xe5
%
xmm3
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
102
15
112
235
78
/
/
pshufd
0x4e
%
xmm3
%
xmm5
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
219
231
/
/
pshufd
0xe7
%
xmm3
%
xmm3
.
byte
102
15
126
219
/
/
movd
%
xmm3
%
ebx
.
byte
243
15
16
28
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm3
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
243
15
16
28
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm3
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
102
15
254
145
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm2
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
52
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm6
.
byte
243
15
16
20
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm2
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
102
15
20
243
/
/
unpcklpd
%
xmm3
%
xmm6
.
byte
102
15
254
77
168
/
/
paddd
-
0x58
(
%
ebp
)
%
xmm1
.
byte
102
15
112
209
245
/
/
pshufd
0xf5
%
xmm1
%
xmm2
.
byte
102
15
244
207
/
/
pmuludq
%
xmm7
%
xmm1
.
byte
102
15
244
215
/
/
pmuludq
%
xmm7
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
203
229
/
/
pshufd
0xe5
%
xmm3
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
203
78
/
/
pshufd
0x4e
%
xmm3
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
203
231
/
/
pshufd
0xe7
%
xmm3
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
60
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm7
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
231
/
/
unpcklps
%
xmm7
%
xmm4
.
byte
102
15
20
226
/
/
unpcklpd
%
xmm2
%
xmm4
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
250
208
/
/
psubd
%
xmm0
%
xmm2
.
byte
102
15
112
250
229
/
/
pshufd
0xe5
%
xmm2
%
xmm7
.
byte
102
15
126
248
/
/
movd
%
xmm7
%
eax
.
byte
102
15
112
250
78
/
/
pshufd
0x4e
%
xmm2
%
xmm7
.
byte
102
15
126
254
/
/
movd
%
xmm7
%
esi
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
60
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm7
.
byte
15
20
250
/
/
unpcklps
%
xmm2
%
xmm7
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
208
/
/
unpcklps
%
xmm0
%
xmm2
.
byte
102
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm2
.
byte
102
15
111
137
32
125
0
0
/
/
movdqa
0x7d20
(
%
ecx
)
%
xmm1
.
byte
102
15
254
217
/
/
paddd
%
xmm1
%
xmm3
.
byte
102
15
112
195
229
/
/
pshufd
0xe5
%
xmm3
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
195
78
/
/
pshufd
0x4e
%
xmm3
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
195
231
/
/
pshufd
0xe7
%
xmm3
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
60
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm7
.
byte
15
20
248
/
/
unpcklps
%
xmm0
%
xmm7
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
102
15
20
223
/
/
unpcklpd
%
xmm7
%
xmm3
.
byte
15
40
189
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm7
.
byte
15
92
231
/
/
subps
%
xmm7
%
xmm4
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
88
231
/
/
addps
%
xmm7
%
xmm4
.
byte
15
41
165
120
255
255
255
/
/
movaps
%
xmm4
-
0x88
(
%
ebp
)
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
92
222
/
/
subps
%
xmm6
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
222
/
/
addps
%
xmm6
%
xmm3
.
byte
15
41
93
184
/
/
movaps
%
xmm3
-
0x48
(
%
ebp
)
.
byte
102
15
111
149
8
255
255
255
/
/
movdqa
-
0xf8
(
%
ebp
)
%
xmm2
.
byte
102
15
254
149
88
255
255
255
/
/
paddd
-
0xa8
(
%
ebp
)
%
xmm2
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
254
69
152
/
/
paddd
-
0x68
(
%
ebp
)
%
xmm0
.
byte
102
15
112
224
245
/
/
pshufd
0xf5
%
xmm0
%
xmm4
.
byte
102
15
111
169
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm5
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
102
15
244
195
/
/
pmuludq
%
xmm3
%
xmm0
.
byte
102
15
244
227
/
/
pmuludq
%
xmm3
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
240
232
/
/
pshufd
0xe8
%
xmm0
%
xmm6
.
byte
102
15
98
244
/
/
punpckldq
%
xmm4
%
xmm6
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
198
229
/
/
pshufd
0xe5
%
xmm6
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
198
78
/
/
pshufd
0x4e
%
xmm6
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
198
231
/
/
pshufd
0xe7
%
xmm6
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
243
15
16
4
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm0
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
102
15
20
229
/
/
unpcklpd
%
xmm5
%
xmm4
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
250
195
/
/
psubd
%
xmm3
%
xmm0
.
byte
102
15
112
232
229
/
/
pshufd
0xe5
%
xmm0
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
60
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm7
.
byte
15
20
248
/
/
unpcklps
%
xmm0
%
xmm7
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
232
/
/
unpcklps
%
xmm0
%
xmm5
.
byte
102
15
20
239
/
/
unpcklpd
%
xmm7
%
xmm5
.
byte
102
15
254
241
/
/
paddd
%
xmm1
%
xmm6
.
byte
102
15
112
198
229
/
/
pshufd
0xe5
%
xmm6
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
198
78
/
/
pshufd
0x4e
%
xmm6
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
126
247
/
/
movd
%
xmm6
%
edi
.
byte
102
15
112
198
231
/
/
pshufd
0xe7
%
xmm6
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
60
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm7
.
byte
15
20
248
/
/
unpcklps
%
xmm0
%
xmm7
.
byte
243
15
16
52
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm6
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
240
/
/
unpcklps
%
xmm0
%
xmm6
.
byte
102
15
20
247
/
/
unpcklpd
%
xmm7
%
xmm6
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
254
77
168
/
/
paddd
-
0x58
(
%
ebp
)
%
xmm1
.
byte
102
15
112
193
245
/
/
pshufd
0xf5
%
xmm1
%
xmm0
.
byte
102
15
111
145
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm2
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
249
232
/
/
pshufd
0xe8
%
xmm1
%
xmm7
.
byte
102
15
98
248
/
/
punpckldq
%
xmm0
%
xmm7
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
199
229
/
/
pshufd
0xe5
%
xmm7
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
199
78
/
/
pshufd
0x4e
%
xmm7
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
199
231
/
/
pshufd
0xe7
%
xmm7
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
4
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm0
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
243
15
16
20
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
102
15
20
208
/
/
unpcklpd
%
xmm0
%
xmm2
.
byte
102
15
111
199
/
/
movdqa
%
xmm7
%
xmm0
.
byte
102
15
250
195
/
/
psubd
%
xmm3
%
xmm0
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
28
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm3
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
216
/
/
unpcklps
%
xmm0
%
xmm3
.
byte
102
15
20
217
/
/
unpcklpd
%
xmm1
%
xmm3
.
byte
102
15
254
185
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm7
.
byte
102
15
112
199
229
/
/
pshufd
0xe5
%
xmm7
%
xmm0
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
199
78
/
/
pshufd
0x4e
%
xmm7
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
126
255
/
/
movd
%
xmm7
%
edi
.
byte
102
15
112
199
231
/
/
pshufd
0xe7
%
xmm7
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
60
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm7
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
248
/
/
unpcklps
%
xmm0
%
xmm7
.
byte
102
15
20
249
/
/
unpcklpd
%
xmm1
%
xmm7
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
92
221
/
/
subps
%
xmm5
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
15
92
254
/
/
subps
%
xmm6
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
88
254
/
/
addps
%
xmm6
%
xmm7
.
byte
15
40
141
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm1
.
byte
15
92
209
/
/
subps
%
xmm1
%
xmm2
.
byte
15
40
133
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm0
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
15
41
149
8
255
255
255
/
/
movaps
%
xmm2
-
0xf8
(
%
ebp
)
.
byte
15
40
77
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm1
.
byte
15
92
217
/
/
subps
%
xmm1
%
xmm3
.
byte
15
89
216
/
/
mulps
%
xmm0
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
41
157
120
255
255
255
/
/
movaps
%
xmm3
-
0x88
(
%
ebp
)
.
byte
15
40
77
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm1
.
byte
15
92
249
/
/
subps
%
xmm1
%
xmm7
.
byte
15
89
248
/
/
mulps
%
xmm0
%
xmm7
.
byte
15
88
249
/
/
addps
%
xmm1
%
xmm7
.
byte
15
41
125
184
/
/
movaps
%
xmm7
-
0x48
(
%
ebp
)
.
byte
102
15
111
133
24
255
255
255
/
/
movdqa
-
0xe8
(
%
ebp
)
%
xmm0
.
byte
102
15
254
69
136
/
/
paddd
-
0x78
(
%
ebp
)
%
xmm0
.
byte
102
15
127
133
24
255
255
255
/
/
movdqa
%
xmm0
-
0xe8
(
%
ebp
)
.
byte
102
15
111
141
72
255
255
255
/
/
movdqa
-
0xb8
(
%
ebp
)
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
233
/
/
movdqa
%
xmm1
%
xmm5
.
byte
102
15
254
69
152
/
/
paddd
-
0x68
(
%
ebp
)
%
xmm0
.
byte
102
15
112
200
245
/
/
pshufd
0xf5
%
xmm0
%
xmm1
.
byte
102
15
111
145
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm2
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
102
15
244
202
/
/
pmuludq
%
xmm2
%
xmm1
.
byte
102
15
111
250
/
/
movdqa
%
xmm2
%
xmm7
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
208
232
/
/
pshufd
0xe8
%
xmm0
%
xmm2
.
byte
102
15
98
209
/
/
punpckldq
%
xmm1
%
xmm2
.
byte
102
15
126
192
/
/
movd
%
xmm0
%
eax
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
102
15
41
133
72
255
255
255
/
/
movapd
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
118
246
/
/
pcmpeqd
%
xmm6
%
xmm6
.
byte
102
15
250
206
/
/
psubd
%
xmm6
%
xmm1
.
byte
102
15
112
217
229
/
/
pshufd
0xe5
%
xmm1
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
243
15
16
12
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm1
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
204
/
/
unpcklps
%
xmm4
%
xmm1
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
102
15
254
145
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm2
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
212
/
/
unpcklps
%
xmm4
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
254
101
168
/
/
paddd
-
0x58
(
%
ebp
)
%
xmm4
.
byte
102
15
112
220
245
/
/
pshufd
0xf5
%
xmm4
%
xmm3
.
byte
102
15
244
231
/
/
pmuludq
%
xmm7
%
xmm4
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
244
223
/
/
pmuludq
%
xmm7
%
xmm3
.
byte
102
15
112
227
232
/
/
pshufd
0xe8
%
xmm3
%
xmm4
.
byte
102
15
112
221
232
/
/
pshufd
0xe8
%
xmm5
%
xmm3
.
byte
102
15
98
220
/
/
punpckldq
%
xmm4
%
xmm3
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
102
15
112
227
229
/
/
pshufd
0xe5
%
xmm3
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
102
15
112
227
231
/
/
pshufd
0xe7
%
xmm3
%
xmm4
.
byte
102
15
126
227
/
/
movd
%
xmm4
%
ebx
.
byte
243
15
16
36
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm4
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
15
20
236
/
/
unpcklps
%
xmm4
%
xmm5
.
byte
243
15
16
36
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm4
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
196
/
/
unpcklps
%
xmm4
%
xmm0
.
byte
102
15
20
197
/
/
unpcklpd
%
xmm5
%
xmm0
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
102
15
250
230
/
/
psubd
%
xmm6
%
xmm4
.
byte
102
15
112
236
229
/
/
pshufd
0xe5
%
xmm4
%
xmm5
.
byte
102
15
126
232
/
/
movd
%
xmm5
%
eax
.
byte
102
15
112
236
78
/
/
pshufd
0x4e
%
xmm4
%
xmm5
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
102
15
112
228
231
/
/
pshufd
0xe7
%
xmm4
%
xmm4
.
byte
102
15
126
227
/
/
movd
%
xmm4
%
ebx
.
byte
243
15
16
36
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm4
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
243
15
16
60
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm7
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
252
/
/
unpcklps
%
xmm4
%
xmm7
.
byte
102
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm7
.
byte
102
15
254
153
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm3
.
byte
102
15
112
227
229
/
/
pshufd
0xe5
%
xmm3
%
xmm4
.
byte
102
15
126
224
/
/
movd
%
xmm4
%
eax
.
byte
102
15
112
227
78
/
/
pshufd
0x4e
%
xmm3
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
112
219
231
/
/
pshufd
0xe7
%
xmm3
%
xmm3
.
byte
102
15
126
219
/
/
movd
%
xmm3
%
ebx
.
byte
243
15
16
28
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm3
.
byte
243
15
16
52
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
243
15
16
44
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm5
.
byte
243
15
16
28
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm3
.
byte
15
20
235
/
/
unpcklps
%
xmm3
%
xmm5
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
15
40
165
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm4
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
41
133
72
255
255
255
/
/
movaps
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
15
92
249
/
/
subps
%
xmm1
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
88
249
/
/
addps
%
xmm1
%
xmm7
.
byte
15
92
234
/
/
subps
%
xmm2
%
xmm5
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
102
15
111
133
24
255
255
255
/
/
movdqa
-
0xe8
(
%
ebp
)
%
xmm0
.
byte
102
15
254
133
88
255
255
255
/
/
paddd
-
0xa8
(
%
ebp
)
%
xmm0
.
byte
102
15
111
85
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm2
.
byte
102
15
254
208
/
/
paddd
%
xmm0
%
xmm2
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
112
194
245
/
/
pshufd
0xf5
%
xmm2
%
xmm0
.
byte
102
15
111
137
128
131
0
0
/
/
movdqa
0x8380
(
%
ecx
)
%
xmm1
.
byte
102
15
244
209
/
/
pmuludq
%
xmm1
%
xmm2
.
byte
102
15
244
193
/
/
pmuludq
%
xmm1
%
xmm0
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
112
200
232
/
/
pshufd
0xe8
%
xmm0
%
xmm1
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
243
15
16
36
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm4
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
102
15
20
226
/
/
unpcklpd
%
xmm2
%
xmm4
.
byte
102
15
41
101
152
/
/
movapd
%
xmm4
-
0x68
(
%
ebp
)
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
118
210
/
/
pcmpeqd
%
xmm2
%
xmm2
.
byte
102
15
250
202
/
/
psubd
%
xmm2
%
xmm1
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
15
126
208
/
/
movd
%
xmm2
%
eax
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
20
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
36
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm4
.
byte
243
15
16
12
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm1
.
byte
15
20
225
/
/
unpcklps
%
xmm1
%
xmm4
.
byte
102
15
20
226
/
/
unpcklpd
%
xmm2
%
xmm4
.
byte
102
15
41
165
88
255
255
255
/
/
movapd
%
xmm4
-
0xa8
(
%
ebp
)
.
byte
102
15
254
129
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm0
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
36
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm4
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
224
/
/
unpcklps
%
xmm0
%
xmm4
.
byte
102
15
20
225
/
/
unpcklpd
%
xmm1
%
xmm4
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
254
77
168
/
/
paddd
-
0x58
(
%
ebp
)
%
xmm1
.
byte
102
15
112
193
245
/
/
pshufd
0xf5
%
xmm1
%
xmm0
.
byte
102
15
244
206
/
/
pmuludq
%
xmm6
%
xmm1
.
byte
102
15
244
198
/
/
pmuludq
%
xmm6
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
209
232
/
/
pshufd
0xe8
%
xmm1
%
xmm2
.
byte
102
15
98
208
/
/
punpckldq
%
xmm0
%
xmm2
.
byte
102
15
126
200
/
/
movd
%
xmm1
%
eax
.
byte
102
15
112
194
229
/
/
pshufd
0xe5
%
xmm2
%
xmm0
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
102
15
112
194
78
/
/
pshufd
0x4e
%
xmm2
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
194
231
/
/
pshufd
0xe7
%
xmm2
%
xmm0
.
byte
102
15
126
195
/
/
movd
%
xmm0
%
ebx
.
byte
243
15
16
4
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm0
.
byte
243
15
16
12
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm1
.
byte
15
20
200
/
/
unpcklps
%
xmm0
%
xmm1
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
243
15
16
4
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm0
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
193
/
/
unpcklpd
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
118
219
/
/
pcmpeqd
%
xmm3
%
xmm3
.
byte
102
15
250
203
/
/
psubd
%
xmm3
%
xmm1
.
byte
102
15
112
217
229
/
/
pshufd
0xe5
%
xmm1
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
203
/
/
movd
%
xmm1
%
ebx
.
byte
243
15
16
12
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm1
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
243
15
16
12
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm1
.
byte
243
15
16
52
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm6
.
byte
15
20
206
/
/
unpcklps
%
xmm6
%
xmm1
.
byte
102
15
20
203
/
/
unpcklpd
%
xmm3
%
xmm1
.
byte
102
15
254
145
32
125
0
0
/
/
paddd
0x7d20
(
%
ecx
)
%
xmm2
.
byte
102
15
112
218
229
/
/
pshufd
0xe5
%
xmm2
%
xmm3
.
byte
102
15
126
216
/
/
movd
%
xmm3
%
eax
.
byte
102
15
112
218
78
/
/
pshufd
0x4e
%
xmm2
%
xmm3
.
byte
102
15
126
222
/
/
movd
%
xmm3
%
esi
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
112
210
231
/
/
pshufd
0xe7
%
xmm2
%
xmm2
.
byte
102
15
126
211
/
/
movd
%
xmm2
%
ebx
.
byte
243
15
16
20
154
/
/
movss
(
%
edx
%
ebx
4
)
%
xmm2
.
byte
243
15
16
28
178
/
/
movss
(
%
edx
%
esi
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
243
15
16
20
186
/
/
movss
(
%
edx
%
edi
4
)
%
xmm2
.
byte
243
15
16
52
130
/
/
movss
(
%
edx
%
eax
4
)
%
xmm6
.
byte
15
20
214
/
/
unpcklps
%
xmm6
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
15
40
117
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
198
/
/
addps
%
xmm6
%
xmm0
.
byte
15
40
181
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm6
.
byte
15
92
206
/
/
subps
%
xmm6
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
40
165
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm4
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
40
157
56
255
255
255
/
/
movaps
-
0xc8
(
%
ebp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
92
207
/
/
subps
%
xmm7
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
92
213
/
/
subps
%
xmm5
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
165
8
255
255
255
/
/
movaps
-
0xf8
(
%
ebp
)
%
xmm4
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
40
157
248
254
255
255
/
/
movaps
-
0x108
(
%
ebp
)
%
xmm3
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
196
/
/
addps
%
xmm4
%
xmm0
.
byte
15
40
165
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm4
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
40
101
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm4
.
byte
15
92
212
/
/
subps
%
xmm4
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
91
157
200
254
255
255
/
/
cvtdq2ps
-
0x138
(
%
ebp
)
%
xmm3
.
byte
15
40
165
168
254
255
255
/
/
movaps
-
0x158
(
%
ebp
)
%
xmm4
.
byte
15
92
227
/
/
subps
%
xmm3
%
xmm4
.
byte
15
40
157
40
255
255
255
/
/
movaps
-
0xd8
(
%
ebp
)
%
xmm3
.
byte
15
92
195
/
/
subps
%
xmm3
%
xmm0
.
byte
15
89
196
/
/
mulps
%
xmm4
%
xmm0
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
40
157
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm3
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
40
157
216
254
255
255
/
/
movaps
-
0x128
(
%
ebp
)
%
xmm3
.
byte
15
92
211
/
/
subps
%
xmm3
%
xmm2
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
141
66
8
/
/
lea
0x8
(
%
edx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
153
224
124
0
0
/
/
movaps
0x7ce0
(
%
ecx
)
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
129
196
92
1
0
0
/
/
add
0x15c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_gauss_a_to_rgba_sse2
.
globl
_sk_gauss_a_to_rgba_sse2
FUNCTION
(
_sk_gauss_a_to_rgba_sse2
)
_sk_gauss_a_to_rgba_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
232
0
0
0
0
/
/
call
9ed5
<
_sk_gauss_a_to_rgba_sse2
+
0xb
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
15
40
128
59
111
0
0
/
/
movaps
0x6f3b
(
%
eax
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
128
75
111
0
0
/
/
addps
0x6f4b
(
%
eax
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
128
91
111
0
0
/
/
addps
0x6f5b
(
%
eax
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
128
107
111
0
0
/
/
addps
0x6f6b
(
%
eax
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
128
123
111
0
0
/
/
addps
0x6f7b
(
%
eax
)
%
xmm0
.
byte
141
65
4
/
/
lea
0x4
(
%
ecx
)
%
eax
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
80
/
/
push
%
eax
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
17
/
/
call
*
(
%
ecx
)
.
byte
131
196
24
/
/
add
0x18
%
esp
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_bilerp_clamp_8888_sse2
.
globl
_sk_bilerp_clamp_8888_sse2
FUNCTION
(
_sk_bilerp_clamp_8888_sse2
)
_sk_bilerp_clamp_8888_sse2
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
44
1
0
0
/
/
sub
0x12c
%
esp
.
byte
232
0
0
0
0
/
/
call
9f33
<
_sk_bilerp_clamp_8888_sse2
+
0x11
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
56
/
/
mov
(
%
eax
)
%
edi
.
byte
15
40
158
29
104
0
0
/
/
movaps
0x681d
(
%
esi
)
%
xmm3
.
byte
15
41
133
8
255
255
255
/
/
movaps
%
xmm0
-
0xf8
(
%
ebp
)
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
88
227
/
/
addps
%
xmm3
%
xmm4
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
15
194
208
1
/
/
cmpltps
%
xmm0
%
xmm2
.
byte
15
40
174
45
104
0
0
/
/
movaps
0x682d
(
%
esi
)
%
xmm5
.
byte
15
84
213
/
/
andps
%
xmm5
%
xmm2
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
92
224
/
/
subps
%
xmm0
%
xmm4
.
byte
15
41
141
120
255
255
255
/
/
movaps
%
xmm1
-
0x88
(
%
ebp
)
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
243
15
91
195
/
/
cvttps2dq
%
xmm3
%
xmm0
.
byte
15
91
192
/
/
cvtdq2ps
%
xmm0
%
xmm0
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
194
200
1
/
/
cmpltps
%
xmm0
%
xmm1
.
byte
15
84
205
/
/
andps
%
xmm5
%
xmm1
.
byte
15
92
193
/
/
subps
%
xmm1
%
xmm0
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
243
15
16
79
8
/
/
movss
0x8
(
%
edi
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
102
15
118
192
/
/
pcmpeqd
%
xmm0
%
xmm0
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
127
141
40
255
255
255
/
/
movdqa
%
xmm1
-
0xd8
(
%
ebp
)
.
byte
243
15
16
79
12
/
/
movss
0xc
(
%
edi
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
102
15
254
200
/
/
paddd
%
xmm0
%
xmm1
.
byte
102
15
127
77
152
/
/
movdqa
%
xmm1
-
0x68
(
%
ebp
)
.
byte
15
40
197
/
/
movaps
%
xmm5
%
xmm0
.
byte
15
41
165
24
255
255
255
/
/
movaps
%
xmm4
-
0xe8
(
%
ebp
)
.
byte
15
92
196
/
/
subps
%
xmm4
%
xmm0
.
byte
15
41
133
88
255
255
255
/
/
movaps
%
xmm0
-
0xa8
(
%
ebp
)
.
byte
15
41
93
136
/
/
movaps
%
xmm3
-
0x78
(
%
ebp
)
.
byte
15
92
235
/
/
subps
%
xmm3
%
xmm5
.
byte
15
41
109
184
/
/
movaps
%
xmm5
-
0x48
(
%
ebp
)
.
byte
139
23
/
/
mov
(
%
edi
)
%
edx
.
byte
102
15
110
71
4
/
/
movd
0x4
(
%
edi
)
%
xmm0
.
byte
102
15
112
192
0
/
/
pshufd
0x0
%
xmm0
%
xmm0
.
byte
102
15
127
69
168
/
/
movdqa
%
xmm0
-
0x58
(
%
ebp
)
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
243
15
16
134
49
112
0
0
/
/
movss
0x7031
(
%
esi
)
%
xmm0
.
byte
15
40
150
237
104
0
0
/
/
movaps
0x68ed
(
%
esi
)
%
xmm2
.
byte
15
41
149
56
255
255
255
/
/
movaps
%
xmm2
-
0xc8
(
%
ebp
)
.
byte
15
40
150
45
106
0
0
/
/
movaps
0x6a2d
(
%
esi
)
%
xmm2
.
byte
15
41
149
72
255
255
255
/
/
movaps
%
xmm2
-
0xb8
(
%
ebp
)
.
byte
243
15
16
150
53
112
0
0
/
/
movss
0x7035
(
%
esi
)
%
xmm2
.
byte
243
15
17
85
236
/
/
movss
%
xmm2
-
0x14
(
%
ebp
)
.
byte
243
15
16
150
45
112
0
0
/
/
movss
0x702d
(
%
esi
)
%
xmm2
.
byte
243
15
17
85
240
/
/
movss
%
xmm2
-
0x10
(
%
ebp
)
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
243
15
17
69
232
/
/
movss
%
xmm0
-
0x18
(
%
ebp
)
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
88
149
120
255
255
255
/
/
addps
-
0x88
(
%
ebp
)
%
xmm2
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
95
194
/
/
maxps
%
xmm2
%
xmm0
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
41
165
104
255
255
255
/
/
movaps
%
xmm4
-
0x98
(
%
ebp
)
.
byte
15
46
226
/
/
ucomiss
%
xmm2
%
xmm4
.
byte
15
40
85
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm2
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
119
8
/
/
ja
a071
<
_sk_bilerp_clamp_8888_sse2
+
0x14f
>
.
byte
15
40
85
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm2
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
93
69
152
/
/
minps
-
0x68
(
%
ebp
)
%
xmm0
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
111
101
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm4
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
15
112
234
232
/
/
pshufd
0xe8
%
xmm2
%
xmm5
.
byte
102
15
112
192
245
/
/
pshufd
0xf5
%
xmm0
%
xmm0
.
byte
102
15
112
212
245
/
/
pshufd
0xf5
%
xmm4
%
xmm2
.
byte
102
15
244
208
/
/
pmuludq
%
xmm0
%
xmm2
.
byte
102
15
112
194
232
/
/
pshufd
0xe8
%
xmm2
%
xmm0
.
byte
102
15
98
232
/
/
punpckldq
%
xmm0
%
xmm5
.
byte
102
15
127
173
248
254
255
255
/
/
movdqa
%
xmm5
-
0x108
(
%
ebp
)
.
byte
243
15
16
69
232
/
/
movss
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
157
232
254
255
255
/
/
movaps
%
xmm3
-
0x118
(
%
ebp
)
.
byte
15
41
181
216
254
255
255
/
/
movaps
%
xmm6
-
0x128
(
%
ebp
)
.
byte
102
15
127
141
200
254
255
255
/
/
movdqa
%
xmm1
-
0x138
(
%
ebp
)
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
88
141
8
255
255
255
/
/
addps
-
0xf8
(
%
ebp
)
%
xmm1
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
95
225
/
/
maxps
%
xmm1
%
xmm4
.
byte
15
93
165
40
255
255
255
/
/
minps
-
0xd8
(
%
ebp
)
%
xmm4
.
byte
243
15
91
204
/
/
cvttps2dq
%
xmm4
%
xmm1
.
byte
102
15
254
141
248
254
255
255
/
/
paddd
-
0x108
(
%
ebp
)
%
xmm1
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
225
229
/
/
pshufd
0xe5
%
xmm1
%
xmm4
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
15
126
227
/
/
movd
%
xmm4
%
ebx
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
102
15
110
12
138
/
/
movd
(
%
edx
%
ecx
4
)
%
xmm1
.
byte
102
15
110
36
154
/
/
movd
(
%
edx
%
ebx
4
)
%
xmm4
.
byte
102
15
98
225
/
/
punpckldq
%
xmm1
%
xmm4
.
byte
102
15
110
44
178
/
/
movd
(
%
edx
%
esi
4
)
%
xmm5
.
byte
102
15
110
12
186
/
/
movd
(
%
edx
%
edi
4
)
%
xmm1
.
byte
102
15
98
233
/
/
punpckldq
%
xmm1
%
xmm5
.
byte
102
15
108
236
/
/
punpcklqdq
%
xmm4
%
xmm5
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
111
189
56
255
255
255
/
/
movdqa
-
0xc8
(
%
ebp
)
%
xmm7
.
byte
102
15
219
207
/
/
pand
%
xmm7
%
xmm1
.
byte
15
91
241
/
/
cvtdq2ps
%
xmm1
%
xmm6
.
byte
15
40
157
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm3
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
114
209
8
/
/
psrld
0x8
%
xmm1
.
byte
102
15
219
207
/
/
pand
%
xmm7
%
xmm1
.
byte
15
91
209
/
/
cvtdq2ps
%
xmm1
%
xmm2
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
219
231
/
/
pand
%
xmm7
%
xmm4
.
byte
15
91
252
/
/
cvtdq2ps
%
xmm4
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
102
15
114
213
24
/
/
psrld
0x18
%
xmm5
.
byte
15
91
229
/
/
cvtdq2ps
%
xmm5
%
xmm4
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
46
193
/
/
ucomiss
%
xmm1
%
xmm0
.
byte
15
40
173
24
255
255
255
/
/
movaps
-
0xe8
(
%
ebp
)
%
xmm5
.
byte
119
7
/
/
ja
a190
<
_sk_bilerp_clamp_8888_sse2
+
0x26e
>
.
byte
15
40
173
88
255
255
255
/
/
movaps
-
0xa8
(
%
ebp
)
%
xmm5
.
byte
15
89
109
200
/
/
mulps
-
0x38
(
%
ebp
)
%
xmm5
.
byte
15
89
245
/
/
mulps
%
xmm5
%
xmm6
.
byte
15
40
77
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
88
206
/
/
addps
%
xmm6
%
xmm1
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
15
89
213
/
/
mulps
%
xmm5
%
xmm2
.
byte
15
40
141
200
254
255
255
/
/
movaps
-
0x138
(
%
ebp
)
%
xmm1
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
15
89
253
/
/
mulps
%
xmm5
%
xmm7
.
byte
15
40
181
216
254
255
255
/
/
movaps
-
0x128
(
%
ebp
)
%
xmm6
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
40
157
232
254
255
255
/
/
movaps
-
0x118
(
%
ebp
)
%
xmm3
.
byte
15
88
221
/
/
addps
%
xmm5
%
xmm3
.
byte
243
15
88
69
236
/
/
addss
-
0x14
(
%
ebp
)
%
xmm0
.
byte
243
15
16
85
240
/
/
movss
-
0x10
(
%
ebp
)
%
xmm2
.
byte
15
46
208
/
/
ucomiss
%
xmm0
%
xmm2
.
byte
15
131
211
254
255
255
/
/
jae
a0af
<
_sk_bilerp_clamp_8888_sse2
+
0x18d
>
.
byte
15
40
165
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm4
.
byte
243
15
88
101
236
/
/
addss
-
0x14
(
%
ebp
)
%
xmm4
.
byte
243
15
16
69
240
/
/
movss
-
0x10
(
%
ebp
)
%
xmm0
.
byte
15
46
196
/
/
ucomiss
%
xmm4
%
xmm0
.
byte
15
131
72
254
255
255
/
/
jae
a03e
<
_sk_bilerp_clamp_8888_sse2
+
0x11c
>
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
131
236
8
/
/
sub
0x8
%
esp
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
81
/
/
push
%
ecx
.
byte
255
117
8
/
/
pushl
0x8
(
%
ebp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
129
196
60
1
0
0
/
/
add
0x13c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
144
/
/
nop
.
byte
144
/
/
nop
HIDDEN
_sk_start_pipeline_sse2_lowp
.
globl
_sk_start_pipeline_sse2_lowp
FUNCTION
(
_sk_start_pipeline_sse2_lowp
)
_sk_start_pipeline_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
59
125
20
/
/
cmp
0x14
(
%
ebp
)
%
edi
.
byte
15
131
193
0
0
0
/
/
jae
a2ee
<
_sk_start_pipeline_sse2_lowp
+
0xd6
>
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
117
24
/
/
mov
0x18
(
%
ebp
)
%
esi
.
byte
139
30
/
/
mov
(
%
esi
)
%
ebx
.
byte
131
198
4
/
/
add
0x4
%
esi
.
byte
141
64
8
/
/
lea
0x8
(
%
eax
)
%
eax
.
byte
137
69
236
/
/
mov
%
eax
-
0x14
(
%
ebp
)
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
137
125
240
/
/
mov
%
edi
-
0x10
(
%
ebp
)
.
byte
57
85
236
/
/
cmp
%
edx
-
0x14
(
%
ebp
)
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
137
223
/
/
mov
%
ebx
%
edi
.
byte
137
243
/
/
mov
%
esi
%
ebx
.
byte
119
77
/
/
ja
a2a0
<
_sk_start_pipeline_sse2_lowp
+
0x88
>
.
byte
139
117
8
/
/
mov
0x8
(
%
ebp
)
%
esi
.
byte
15
41
68
36
64
/
/
movaps
%
xmm0
0x40
(
%
esp
)
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
139
69
240
/
/
mov
-
0x10
(
%
ebp
)
%
eax
.
byte
137
68
36
12
/
/
mov
%
eax
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
92
36
4
/
/
mov
%
ebx
0x4
(
%
esp
)
.
byte
199
4
36
0
0
0
0
/
/
movl
0x0
(
%
esp
)
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
255
215
/
/
call
*
%
edi
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
141
70
8
/
/
lea
0x8
(
%
esi
)
%
eax
.
byte
131
198
16
/
/
add
0x10
%
esi
.
byte
57
214
/
/
cmp
%
edx
%
esi
.
byte
137
198
/
/
mov
%
eax
%
esi
.
byte
118
182
/
/
jbe
a256
<
_sk_start_pipeline_sse2_lowp
+
0x3e
>
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
41
193
/
/
sub
%
eax
%
ecx
.
byte
137
222
/
/
mov
%
ebx
%
esi
.
byte
137
251
/
/
mov
%
edi
%
ebx
.
byte
139
125
240
/
/
mov
-
0x10
(
%
ebp
)
%
edi
.
byte
116
55
/
/
je
a2e4
<
_sk_start_pipeline_sse2_lowp
+
0xcc
>
.
byte
15
41
68
36
64
/
/
movaps
%
xmm0
0x40
(
%
esp
)
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
68
36
8
/
/
mov
%
eax
0x8
(
%
esp
)
.
byte
137
116
36
4
/
/
mov
%
esi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
255
211
/
/
call
*
%
ebx
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
71
/
/
inc
%
edi
.
byte
59
125
20
/
/
cmp
0x14
(
%
ebp
)
%
edi
.
byte
15
133
86
255
255
255
/
/
jne
a244
<
_sk_start_pipeline_sse2_lowp
+
0x2c
>
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_just_return_sse2_lowp
.
globl
_sk_just_return_sse2_lowp
FUNCTION
(
_sk_just_return_sse2_lowp
)
_sk_just_return_sse2_lowp
:
.
byte
195
/
/
ret
HIDDEN
_sk_seed_shader_sse2_lowp
.
globl
_sk_seed_shader_sse2_lowp
FUNCTION
(
_sk_seed_shader_sse2_lowp
)
_sk_seed_shader_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
a304
<
_sk_seed_shader_sse2_lowp
+
0xd
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
40
109
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm5
.
byte
15
40
117
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm6
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
139
62
/
/
mov
(
%
esi
)
%
edi
.
byte
15
16
7
/
/
movups
(
%
edi
)
%
xmm0
.
byte
15
16
79
16
/
/
movups
0x10
(
%
edi
)
%
xmm1
.
byte
139
125
16
/
/
mov
0x10
(
%
ebp
)
%
edi
.
byte
102
15
110
215
/
/
movd
%
edi
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
88
202
/
/
addps
%
xmm2
%
xmm1
.
byte
102
15
110
210
/
/
movd
%
edx
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
15
91
210
/
/
cvtdq2ps
%
xmm2
%
xmm2
.
byte
15
88
145
76
100
0
0
/
/
addps
0x644c
(
%
ecx
)
%
xmm2
.
byte
141
78
8
/
/
lea
0x8
(
%
esi
)
%
ecx
.
byte
15
41
116
36
64
/
/
movaps
%
xmm6
0x40
(
%
esp
)
.
byte
15
41
108
36
48
/
/
movaps
%
xmm5
0x30
(
%
esp
)
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
124
36
8
/
/
mov
%
edi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
255
86
4
/
/
call
*
0x4
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_translate_sse2_lowp
.
globl
_sk_matrix_translate_sse2_lowp
FUNCTION
(
_sk_matrix_translate_sse2_lowp
)
_sk_matrix_translate_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
139
62
/
/
mov
(
%
esi
)
%
edi
.
byte
243
15
16
63
/
/
movss
(
%
edi
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
243
15
16
127
4
/
/
movss
0x4
(
%
edi
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
141
126
8
/
/
lea
0x8
(
%
esi
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
86
4
/
/
call
*
0x4
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_scale_translate_sse2_lowp
.
globl
_sk_matrix_scale_translate_sse2_lowp
FUNCTION
(
_sk_matrix_scale_translate_sse2_lowp
)
_sk_matrix_scale_translate_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
16
63
/
/
movss
(
%
edi
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
207
/
/
mulps
%
xmm7
%
xmm1
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
243
15
16
127
8
/
/
movss
0x8
(
%
edi
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
88
199
/
/
addps
%
xmm7
%
xmm0
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
243
15
16
127
4
/
/
movss
0x4
(
%
edi
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
89
223
/
/
mulps
%
xmm7
%
xmm3
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
243
15
16
127
12
/
/
movss
0xc
(
%
edi
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
88
223
/
/
addps
%
xmm7
%
xmm3
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
141
121
8
/
/
lea
0x8
(
%
ecx
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_2x3_sse2_lowp
.
globl
_sk_matrix_2x3_sse2_lowp
FUNCTION
(
_sk_matrix_2x3_sse2_lowp
)
_sk_matrix_2x3_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
16
7
/
/
movss
(
%
edi
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
243
15
16
119
8
/
/
movss
0x8
(
%
edi
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
111
16
/
/
movss
0x10
(
%
edi
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
89
214
/
/
mulps
%
xmm6
%
xmm2
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
15
88
245
/
/
addps
%
xmm5
%
xmm6
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
233
/
/
movaps
%
xmm1
%
xmm5
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
88
238
/
/
addps
%
xmm6
%
xmm5
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
243
15
16
119
12
/
/
movss
0xc
(
%
edi
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
89
222
/
/
mulps
%
xmm6
%
xmm3
.
byte
15
89
244
/
/
mulps
%
xmm4
%
xmm6
.
byte
243
15
16
87
20
/
/
movss
0x14
(
%
edi
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
243
15
16
87
4
/
/
movss
0x4
(
%
edi
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
88
218
/
/
addps
%
xmm2
%
xmm3
.
byte
15
40
85
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm2
.
byte
141
121
8
/
/
lea
0x8
(
%
ecx
)
%
edi
.
byte
15
41
84
36
64
/
/
movaps
%
xmm2
0x40
(
%
esp
)
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
40
77
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm1
.
byte
15
41
76
36
16
/
/
movaps
%
xmm1
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
222
/
/
movaps
%
xmm6
%
xmm3
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_matrix_perspective_sse2_lowp
.
globl
_sk_matrix_perspective_sse2_lowp
FUNCTION
(
_sk_matrix_perspective_sse2_lowp
)
_sk_matrix_perspective_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
112
/
/
sub
0x70
%
esp
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
9
/
/
movss
(
%
ecx
)
%
xmm1
.
byte
243
15
16
113
4
/
/
movss
0x4
(
%
ecx
)
%
xmm6
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
97
8
/
/
movss
0x8
(
%
ecx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
88
244
/
/
addps
%
xmm4
%
xmm6
.
byte
15
88
252
/
/
addps
%
xmm4
%
xmm7
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
88
230
/
/
addps
%
xmm6
%
xmm4
.
byte
15
89
77
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
243
15
16
113
16
/
/
movss
0x10
(
%
ecx
)
%
xmm6
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
243
15
16
65
20
/
/
movss
0x14
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
40
253
/
/
movaps
%
xmm5
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
89
242
/
/
mulps
%
xmm2
%
xmm6
.
byte
15
88
240
/
/
addps
%
xmm0
%
xmm6
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
243
15
16
65
12
/
/
movss
0xc
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
89
208
/
/
mulps
%
xmm0
%
xmm2
.
byte
15
88
242
/
/
addps
%
xmm2
%
xmm6
.
byte
15
89
69
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
243
15
16
65
28
/
/
movss
0x1c
(
%
ecx
)
%
xmm0
.
byte
15
198
192
0
/
/
shufps
0x0
%
xmm0
%
xmm0
.
byte
15
89
232
/
/
mulps
%
xmm0
%
xmm5
.
byte
15
89
69
216
/
/
mulps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
243
15
16
81
32
/
/
movss
0x20
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
243
15
16
81
24
/
/
movss
0x18
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
88
216
/
/
addps
%
xmm0
%
xmm3
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
89
85
232
/
/
mulps
-
0x18
(
%
ebp
)
%
xmm2
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
83
219
/
/
rcpps
%
xmm3
%
xmm3
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
40
93
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm3
.
byte
15
83
210
/
/
rcpps
%
xmm2
%
xmm2
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
15
40
85
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm2
.
byte
141
120
8
/
/
lea
0x8
(
%
eax
)
%
edi
.
byte
15
41
84
36
64
/
/
movaps
%
xmm2
0x40
(
%
esp
)
.
byte
15
41
92
36
48
/
/
movaps
%
xmm3
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
214
/
/
movaps
%
xmm6
%
xmm2
.
byte
15
40
223
/
/
movaps
%
xmm7
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
112
/
/
add
0x70
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_uniform_color_sse2_lowp
.
globl
_sk_uniform_color_sse2_lowp
FUNCTION
(
_sk_uniform_color_sse2_lowp
)
_sk_uniform_color_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
15
183
126
18
/
/
movzwl
0x12
(
%
esi
)
%
edi
.
byte
102
15
110
207
/
/
movd
%
edi
%
xmm1
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
110
70
16
/
/
movd
0x10
(
%
esi
)
%
xmm0
.
byte
242
15
112
192
0
/
/
pshuflw
0x0
%
xmm0
%
xmm0
.
byte
102
15
112
192
80
/
/
pshufd
0x50
%
xmm0
%
xmm0
.
byte
242
15
112
201
0
/
/
pshuflw
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
201
80
/
/
pshufd
0x50
%
xmm1
%
xmm1
.
byte
102
15
110
86
20
/
/
movd
0x14
(
%
esi
)
%
xmm2
.
byte
242
15
112
210
0
/
/
pshuflw
0x0
%
xmm2
%
xmm2
.
byte
102
15
112
210
80
/
/
pshufd
0x50
%
xmm2
%
xmm2
.
byte
15
183
118
22
/
/
movzwl
0x16
(
%
esi
)
%
esi
.
byte
102
15
110
222
/
/
movd
%
esi
%
xmm3
.
byte
242
15
112
219
0
/
/
pshuflw
0x0
%
xmm3
%
xmm3
.
byte
102
15
112
219
80
/
/
pshufd
0x50
%
xmm3
%
xmm3
.
byte
141
114
8
/
/
lea
0x8
(
%
edx
)
%
esi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
116
36
4
/
/
mov
%
esi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
82
4
/
/
call
*
0x4
(
%
edx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_black_color_sse2_lowp
.
globl
_sk_black_color_sse2_lowp
FUNCTION
(
_sk_black_color_sse2_lowp
)
_sk_black_color_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
232
0
0
0
0
/
/
call
a723
<
_sk_black_color_sse2_lowp
+
0xe
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
141
95
4
/
/
lea
0x4
(
%
edi
)
%
ebx
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
92
36
4
/
/
mov
%
ebx
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
152
61
103
0
0
/
/
movaps
0x673d
(
%
eax
)
%
xmm3
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
23
/
/
call
*
(
%
edi
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_white_color_sse2_lowp
.
globl
_sk_white_color_sse2_lowp
FUNCTION
(
_sk_white_color_sse2_lowp
)
_sk_white_color_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
232
0
0
0
0
/
/
call
a78e
<
_sk_white_color_sse2_lowp
+
0xe
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
141
95
4
/
/
lea
0x4
(
%
edi
)
%
ebx
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
92
36
4
/
/
mov
%
ebx
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
128
210
102
0
0
/
/
movaps
0x66d2
(
%
eax
)
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
255
23
/
/
call
*
(
%
edi
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_set_rgb_sse2_lowp
.
globl
_sk_set_rgb_sse2_lowp
FUNCTION
(
_sk_set_rgb_sse2_lowp
)
_sk_set_rgb_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
a7f8
<
_sk_set_rgb_sse2_lowp
+
0xd
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
16
150
116
103
0
0
/
/
movss
0x6774
(
%
esi
)
%
xmm2
.
byte
243
15
16
7
/
/
movss
(
%
edi
)
%
xmm0
.
byte
243
15
89
194
/
/
mulss
%
xmm2
%
xmm0
.
byte
243
15
16
142
104
103
0
0
/
/
movss
0x6768
(
%
esi
)
%
xmm1
.
byte
243
15
88
193
/
/
addss
%
xmm1
%
xmm0
.
byte
243
15
44
240
/
/
cvttss2si
%
xmm0
%
esi
.
byte
102
15
110
198
/
/
movd
%
esi
%
xmm0
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
243
15
16
127
4
/
/
movss
0x4
(
%
edi
)
%
xmm7
.
byte
243
15
89
250
/
/
mulss
%
xmm2
%
xmm7
.
byte
243
15
88
249
/
/
addss
%
xmm1
%
xmm7
.
byte
243
15
89
87
8
/
/
mulss
0x8
(
%
edi
)
%
xmm2
.
byte
243
15
44
255
/
/
cvttss2si
%
xmm7
%
edi
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
242
15
112
192
0
/
/
pshuflw
0x0
%
xmm0
%
xmm0
.
byte
102
15
112
192
80
/
/
pshufd
0x50
%
xmm0
%
xmm0
.
byte
243
15
88
209
/
/
addss
%
xmm1
%
xmm2
.
byte
102
15
110
207
/
/
movd
%
edi
%
xmm1
.
byte
242
15
112
201
0
/
/
pshuflw
0x0
%
xmm1
%
xmm1
.
byte
102
15
112
201
80
/
/
pshufd
0x50
%
xmm1
%
xmm1
.
byte
243
15
44
250
/
/
cvttss2si
%
xmm2
%
edi
.
byte
102
15
110
215
/
/
movd
%
edi
%
xmm2
.
byte
242
15
112
210
0
/
/
pshuflw
0x0
%
xmm2
%
xmm2
.
byte
102
15
112
210
80
/
/
pshufd
0x50
%
xmm2
%
xmm2
.
byte
141
121
8
/
/
lea
0x8
(
%
ecx
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clamp_a_sse2_lowp
.
globl
_sk_clamp_a_sse2_lowp
FUNCTION
(
_sk_clamp_a_sse2_lowp
)
_sk_clamp_a_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
a8bc
<
_sk_clamp_a_sse2_lowp
+
0xd
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
102
15
111
166
180
101
0
0
/
/
movdqa
0x65b4
(
%
esi
)
%
xmm4
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
239
236
/
/
pxor
%
xmm4
%
xmm5
.
byte
102
15
111
243
/
/
movdqa
%
xmm3
%
xmm6
.
byte
102
15
239
244
/
/
pxor
%
xmm4
%
xmm6
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
101
253
/
/
pcmpgtw
%
xmm5
%
xmm7
.
byte
102
15
219
199
/
/
pand
%
xmm7
%
xmm0
.
byte
102
15
223
251
/
/
pandn
%
xmm3
%
xmm7
.
byte
102
15
235
199
/
/
por
%
xmm7
%
xmm0
.
byte
102
15
111
233
/
/
movdqa
%
xmm1
%
xmm5
.
byte
102
15
239
236
/
/
pxor
%
xmm4
%
xmm5
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
101
253
/
/
pcmpgtw
%
xmm5
%
xmm7
.
byte
15
40
109
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
219
207
/
/
pand
%
xmm7
%
xmm1
.
byte
102
15
223
251
/
/
pandn
%
xmm3
%
xmm7
.
byte
102
15
235
207
/
/
por
%
xmm7
%
xmm1
.
byte
15
40
125
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm7
.
byte
102
15
239
226
/
/
pxor
%
xmm2
%
xmm4
.
byte
102
15
101
244
/
/
pcmpgtw
%
xmm4
%
xmm6
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
102
15
219
214
/
/
pand
%
xmm6
%
xmm2
.
byte
102
15
223
243
/
/
pandn
%
xmm3
%
xmm6
.
byte
102
15
235
214
/
/
por
%
xmm6
%
xmm2
.
byte
15
40
117
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm6
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
41
116
36
64
/
/
movaps
%
xmm6
0x40
(
%
esp
)
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
124
36
32
/
/
movaps
%
xmm7
0x20
(
%
esp
)
.
byte
15
41
108
36
16
/
/
movaps
%
xmm5
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clamp_a_dst_sse2_lowp
.
globl
_sk_clamp_a_dst_sse2_lowp
FUNCTION
(
_sk_clamp_a_dst_sse2_lowp
)
_sk_clamp_a_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
128
0
0
0
/
/
sub
0x80
%
esp
.
byte
15
41
93
200
/
/
movaps
%
xmm3
-
0x38
(
%
ebp
)
.
byte
15
41
85
216
/
/
movaps
%
xmm2
-
0x28
(
%
ebp
)
.
byte
15
41
77
232
/
/
movaps
%
xmm1
-
0x18
(
%
ebp
)
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
232
0
0
0
0
/
/
call
a983
<
_sk_clamp_a_dst_sse2_lowp
+
0x1f
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
101
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
111
77
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm1
.
byte
102
15
111
144
237
100
0
0
/
/
movdqa
0x64ed
(
%
eax
)
%
xmm2
.
byte
102
15
111
249
/
/
movdqa
%
xmm1
%
xmm7
.
byte
102
15
239
250
/
/
pxor
%
xmm2
%
xmm7
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
239
242
/
/
pxor
%
xmm2
%
xmm6
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
101
239
/
/
pcmpgtw
%
xmm7
%
xmm5
.
byte
102
15
219
205
/
/
pand
%
xmm5
%
xmm1
.
byte
102
15
223
236
/
/
pandn
%
xmm4
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
111
77
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
111
249
/
/
movdqa
%
xmm1
%
xmm7
.
byte
102
15
239
250
/
/
pxor
%
xmm2
%
xmm7
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
101
199
/
/
pcmpgtw
%
xmm7
%
xmm0
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
102
15
223
196
/
/
pandn
%
xmm4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
111
77
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm1
.
byte
102
15
239
209
/
/
pxor
%
xmm1
%
xmm2
.
byte
102
15
101
242
/
/
pcmpgtw
%
xmm2
%
xmm6
.
byte
102
15
219
206
/
/
pand
%
xmm6
%
xmm1
.
byte
102
15
223
244
/
/
pandn
%
xmm4
%
xmm6
.
byte
102
15
235
241
/
/
por
%
xmm1
%
xmm6
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
127
116
36
48
/
/
movdqa
%
xmm6
0x30
(
%
esp
)
.
byte
102
15
127
68
36
32
/
/
movdqa
%
xmm0
0x20
(
%
esp
)
.
byte
102
15
127
108
36
16
/
/
movdqa
%
xmm5
0x10
(
%
esp
)
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
15
40
77
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm1
.
byte
15
40
85
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm2
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
129
196
128
0
0
0
/
/
add
0x80
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_premul_sse2_lowp
.
globl
_sk_premul_sse2_lowp
FUNCTION
(
_sk_premul_sse2_lowp
)
_sk_premul_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
aa52
<
_sk_premul_sse2_lowp
+
0xd
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
15
111
186
14
100
0
0
/
/
movdqa
0x640e
(
%
edx
)
%
xmm7
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
139
85
12
/
/
mov
0xc
(
%
ebp
)
%
edx
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
141
122
4
/
/
lea
0x4
(
%
edx
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
18
/
/
call
*
(
%
edx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_premul_dst_sse2_lowp
.
globl
_sk_premul_dst_sse2_lowp
FUNCTION
(
_sk_premul_dst_sse2_lowp
)
_sk_premul_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
aae7
<
_sk_premul_dst_sse2_lowp
+
0x1a
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
101
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
213
236
/
/
pmullw
%
xmm4
%
xmm5
.
byte
102
15
111
176
121
99
0
0
/
/
movdqa
0x6379
(
%
eax
)
%
xmm6
.
byte
102
15
253
238
/
/
paddw
%
xmm6
%
xmm5
.
byte
102
15
111
125
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm7
.
byte
102
15
213
252
/
/
pmullw
%
xmm4
%
xmm7
.
byte
102
15
253
254
/
/
paddw
%
xmm6
%
xmm7
.
byte
102
15
111
69
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm0
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
15
253
198
/
/
paddw
%
xmm6
%
xmm0
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
127
68
36
48
/
/
movdqa
%
xmm0
0x30
(
%
esp
)
.
byte
102
15
127
124
36
32
/
/
movdqa
%
xmm7
0x20
(
%
esp
)
.
byte
102
15
127
108
36
16
/
/
movdqa
%
xmm5
0x10
(
%
esp
)
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
96
/
/
add
0x60
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_force_opaque_sse2_lowp
.
globl
_sk_force_opaque_sse2_lowp
FUNCTION
(
_sk_force_opaque_sse2_lowp
)
_sk_force_opaque_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
232
0
0
0
0
/
/
call
ab85
<
_sk_force_opaque_sse2_lowp
+
0xe
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
40
109
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm5
.
byte
15
40
117
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm6
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
141
95
4
/
/
lea
0x4
(
%
edi
)
%
ebx
.
byte
15
41
116
36
64
/
/
movaps
%
xmm6
0x40
(
%
esp
)
.
byte
15
41
108
36
48
/
/
movaps
%
xmm5
0x30
(
%
esp
)
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
92
36
4
/
/
mov
%
ebx
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
152
219
98
0
0
/
/
movaps
0x62db
(
%
eax
)
%
xmm3
.
byte
255
23
/
/
call
*
(
%
edi
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_force_opaque_dst_sse2_lowp
.
globl
_sk_force_opaque_dst_sse2_lowp
FUNCTION
(
_sk_force_opaque_dst_sse2_lowp
)
_sk_force_opaque_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
232
0
0
0
0
/
/
call
abe7
<
_sk_force_opaque_dst_sse2_lowp
+
0xe
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
141
95
4
/
/
lea
0x4
(
%
edi
)
%
ebx
.
byte
15
40
184
121
98
0
0
/
/
movaps
0x6279
(
%
eax
)
%
xmm7
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
92
36
4
/
/
mov
%
ebx
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
255
23
/
/
call
*
(
%
edi
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_swap_rb_sse2_lowp
.
globl
_sk_swap_rb_sse2_lowp
FUNCTION
(
_sk_swap_rb_sse2_lowp
)
_sk_swap_rb_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
194
/
/
movaps
%
xmm2
%
xmm0
.
byte
15
40
212
/
/
movaps
%
xmm4
%
xmm2
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_move_src_dst_sse2_lowp
.
globl
_sk_move_src_dst_sse2_lowp
FUNCTION
(
_sk_move_src_dst_sse2_lowp
)
_sk_move_src_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_move_dst_src_sse2_lowp
.
globl
_sk_move_dst_src_sse2_lowp
FUNCTION
(
_sk_move_dst_src_sse2_lowp
)
_sk_move_dst_src_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_invert_sse2_lowp
.
globl
_sk_invert_sse2_lowp
FUNCTION
(
_sk_invert_sse2_lowp
)
_sk_invert_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
ad36
<
_sk_invert_sse2_lowp
+
0xd
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
102
15
111
160
42
97
0
0
/
/
movdqa
0x612a
(
%
eax
)
%
xmm4
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
249
232
/
/
psubw
%
xmm0
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
249
241
/
/
psubw
%
xmm1
%
xmm6
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
249
250
/
/
psubw
%
xmm2
%
xmm7
.
byte
15
40
69
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm0
.
byte
102
15
249
227
/
/
psubw
%
xmm3
%
xmm4
.
byte
15
40
77
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm1
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
120
4
/
/
lea
0x4
(
%
eax
)
%
edi
.
byte
15
41
76
36
64
/
/
movaps
%
xmm1
0x40
(
%
esp
)
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
111
215
/
/
movdqa
%
xmm7
%
xmm2
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_clear_sse2_lowp
.
globl
_sk_clear_sse2_lowp
FUNCTION
(
_sk_clear_sse2_lowp
)
_sk_clear_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcatop_sse2_lowp
.
globl
_sk_srcatop_sse2_lowp
FUNCTION
(
_sk_srcatop_sse2_lowp
)
_sk_srcatop_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
ae2e
<
_sk_srcatop_sse2_lowp
+
0x19
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
117
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
15
111
128
50
96
0
0
/
/
movdqa
0x6032
(
%
eax
)
%
xmm0
.
byte
102
15
253
200
/
/
paddw
%
xmm0
%
xmm1
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
15
253
208
/
/
paddw
%
xmm0
%
xmm2
.
byte
102
15
213
238
/
/
pmullw
%
xmm6
%
xmm5
.
byte
102
15
253
232
/
/
paddw
%
xmm0
%
xmm5
.
byte
102
15
111
230
/
/
movdqa
%
xmm6
%
xmm4
.
byte
102
15
213
224
/
/
pmullw
%
xmm0
%
xmm4
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
249
195
/
/
psubw
%
xmm3
%
xmm0
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
213
93
24
/
/
pmullw
0x18
(
%
ebp
)
%
xmm3
.
byte
102
15
253
203
/
/
paddw
%
xmm3
%
xmm1
.
byte
102
15
111
93
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm3
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
213
251
/
/
pmullw
%
xmm3
%
xmm7
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
111
125
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
253
232
/
/
paddw
%
xmm0
%
xmm5
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
116
36
64
/
/
movdqa
%
xmm6
0x40
(
%
esp
)
.
byte
102
15
127
124
36
48
/
/
movdqa
%
xmm7
0x30
(
%
esp
)
.
byte
102
15
127
92
36
32
/
/
movdqa
%
xmm3
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
111
213
/
/
movdqa
%
xmm5
%
xmm2
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstatop_sse2_lowp
.
globl
_sk_dstatop_sse2_lowp
FUNCTION
(
_sk_dstatop_sse2_lowp
)
_sk_dstatop_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
af02
<
_sk_dstatop_sse2_lowp
+
0xd
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
101
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm4
.
byte
102
15
111
117
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
213
243
/
/
pmullw
%
xmm3
%
xmm6
.
byte
102
15
111
184
94
95
0
0
/
/
movdqa
0x5f5e
(
%
eax
)
%
xmm7
.
byte
102
15
253
247
/
/
paddw
%
xmm7
%
xmm6
.
byte
102
15
213
227
/
/
pmullw
%
xmm3
%
xmm4
.
byte
102
15
253
231
/
/
paddw
%
xmm7
%
xmm4
.
byte
102
15
111
109
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
213
235
/
/
pmullw
%
xmm3
%
xmm5
.
byte
102
15
253
239
/
/
paddw
%
xmm7
%
xmm5
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
102
15
249
125
72
/
/
psubw
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
253
198
/
/
paddw
%
xmm6
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
15
253
204
/
/
paddw
%
xmm4
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
15
253
213
/
/
paddw
%
xmm5
%
xmm2
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcin_sse2_lowp
.
globl
_sk_srcin_sse2_lowp
FUNCTION
(
_sk_srcin_sse2_lowp
)
_sk_srcin_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
afc3
<
_sk_srcin_sse2_lowp
+
0xd
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
111
117
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
15
111
190
157
94
0
0
/
/
movdqa
0x5e9d
(
%
esi
)
%
xmm7
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
213
222
/
/
pmullw
%
xmm6
%
xmm3
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
15
40
125
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm7
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
116
36
64
/
/
movdqa
%
xmm6
0x40
(
%
esp
)
.
byte
15
41
124
36
48
/
/
movaps
%
xmm7
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstin_sse2_lowp
.
globl
_sk_dstin_sse2_lowp
FUNCTION
(
_sk_dstin_sse2_lowp
)
_sk_dstin_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
b05a
<
_sk_dstin_sse2_lowp
+
0xd
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
85
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
111
109
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
111
117
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
15
111
184
6
94
0
0
/
/
movdqa
0x5e06
(
%
eax
)
%
xmm7
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
111
101
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
213
220
/
/
pmullw
%
xmm4
%
xmm3
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
102
15
127
108
36
32
/
/
movdqa
%
xmm5
0x20
(
%
esp
)
.
byte
102
15
127
116
36
16
/
/
movdqa
%
xmm6
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcout_sse2_lowp
.
globl
_sk_srcout_sse2_lowp
FUNCTION
(
_sk_srcout_sse2_lowp
)
_sk_srcout_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
b102
<
_sk_srcout_sse2_lowp
+
0xd
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
111
109
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
111
182
94
93
0
0
/
/
movdqa
0x5d5e
(
%
esi
)
%
xmm6
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
249
253
/
/
psubw
%
xmm5
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
15
40
125
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm7
.
byte
102
15
253
198
/
/
paddw
%
xmm6
%
xmm0
.
byte
102
15
253
206
/
/
paddw
%
xmm6
%
xmm1
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
15
253
222
/
/
paddw
%
xmm6
%
xmm3
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
108
36
64
/
/
movdqa
%
xmm5
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
124
36
32
/
/
movaps
%
xmm7
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstout_sse2_lowp
.
globl
_sk_dstout_sse2_lowp
FUNCTION
(
_sk_dstout_sse2_lowp
)
_sk_dstout_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
232
0
0
0
0
/
/
call
b1a5
<
_sk_dstout_sse2_lowp
+
0x11
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
109
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
111
117
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
111
184
187
92
0
0
/
/
movdqa
0x5cbb
(
%
eax
)
%
xmm7
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
102
15
249
216
/
/
psubw
%
xmm0
%
xmm3
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
213
77
40
/
/
pmullw
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
213
213
/
/
pmullw
%
xmm5
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
111
101
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
213
220
/
/
pmullw
%
xmm4
%
xmm3
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
102
15
127
108
36
48
/
/
movdqa
%
xmm5
0x30
(
%
esp
)
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
102
15
127
116
36
16
/
/
movdqa
%
xmm6
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcover_sse2_lowp
.
globl
_sk_srcover_sse2_lowp
FUNCTION
(
_sk_srcover_sse2_lowp
)
_sk_srcover_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
232
0
0
0
0
/
/
call
b261
<
_sk_srcover_sse2_lowp
+
0x19
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
136
255
91
0
0
/
/
movdqa
0x5bff
(
%
eax
)
%
xmm1
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
249
195
/
/
psubw
%
xmm3
%
xmm0
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
213
125
24
/
/
pmullw
0x18
(
%
ebp
)
%
xmm7
.
byte
102
15
253
249
/
/
paddw
%
xmm1
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
253
231
/
/
paddw
%
xmm7
%
xmm4
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
213
109
40
/
/
pmullw
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
253
233
/
/
paddw
%
xmm1
%
xmm5
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
253
213
/
/
paddw
%
xmm5
%
xmm2
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
111
125
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
213
247
/
/
pmullw
%
xmm7
%
xmm6
.
byte
102
15
253
241
/
/
paddw
%
xmm1
%
xmm6
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
102
15
111
85
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm2
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
15
111
117
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
116
36
64
/
/
movdqa
%
xmm6
0x40
(
%
esp
)
.
byte
102
15
127
124
36
48
/
/
movdqa
%
xmm7
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
96
/
/
add
0x60
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_dstover_sse2_lowp
.
globl
_sk_dstover_sse2_lowp
FUNCTION
(
_sk_dstover_sse2_lowp
)
_sk_dstover_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
b331
<
_sk_dstover_sse2_lowp
+
0xd
>
.
byte
95
/
/
pop
%
edi
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
102
15
111
101
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm4
.
byte
102
15
111
109
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
111
183
47
91
0
0
/
/
movdqa
0x5b2f
(
%
edi
)
%
xmm6
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
249
253
/
/
psubw
%
xmm5
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
15
111
125
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm7
.
byte
102
15
253
198
/
/
paddw
%
xmm6
%
xmm0
.
byte
102
15
253
206
/
/
paddw
%
xmm6
%
xmm1
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
15
253
222
/
/
paddw
%
xmm6
%
xmm3
.
byte
102
15
111
117
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
198
/
/
paddw
%
xmm6
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
212
/
/
paddw
%
xmm4
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
253
221
/
/
paddw
%
xmm5
%
xmm3
.
byte
141
120
4
/
/
lea
0x4
(
%
eax
)
%
edi
.
byte
102
15
127
108
36
64
/
/
movdqa
%
xmm5
0x40
(
%
esp
)
.
byte
102
15
127
100
36
48
/
/
movdqa
%
xmm4
0x30
(
%
esp
)
.
byte
102
15
127
124
36
32
/
/
movdqa
%
xmm7
0x20
(
%
esp
)
.
byte
102
15
127
116
36
16
/
/
movdqa
%
xmm6
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_modulate_sse2_lowp
.
globl
_sk_modulate_sse2_lowp
FUNCTION
(
_sk_modulate_sse2_lowp
)
_sk_modulate_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
b3e6
<
_sk_modulate_sse2_lowp
+
0xd
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
109
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
111
117
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
15
111
184
122
90
0
0
/
/
movdqa
0x5a7a
(
%
eax
)
%
xmm7
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
213
85
56
/
/
pmullw
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
111
101
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
213
220
/
/
pmullw
%
xmm4
%
xmm3
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
102
15
127
108
36
32
/
/
movdqa
%
xmm5
0x20
(
%
esp
)
.
byte
102
15
127
116
36
16
/
/
movdqa
%
xmm6
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_multiply_sse2_lowp
.
globl
_sk_multiply_sse2_lowp
FUNCTION
(
_sk_multiply_sse2_lowp
)
_sk_multiply_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
b482
<
_sk_multiply_sse2_lowp
+
0xd
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
111
160
222
89
0
0
/
/
movdqa
0x59de
(
%
eax
)
%
xmm4
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
15
253
244
/
/
paddw
%
xmm4
%
xmm6
.
byte
102
15
249
117
72
/
/
psubw
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
249
243
/
/
psubw
%
xmm3
%
xmm6
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
213
253
/
/
pmullw
%
xmm5
%
xmm7
.
byte
102
15
253
252
/
/
paddw
%
xmm4
%
xmm7
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
111
125
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm7
.
byte
102
15
253
252
/
/
paddw
%
xmm4
%
xmm7
.
byte
102
15
111
109
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
249
253
/
/
psubw
%
xmm5
%
xmm7
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
213
125
40
/
/
pmullw
0x28
(
%
ebp
)
%
xmm7
.
byte
102
15
253
252
/
/
paddw
%
xmm4
%
xmm7
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
111
125
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
253
252
/
/
paddw
%
xmm4
%
xmm7
.
byte
102
15
249
253
/
/
psubw
%
xmm5
%
xmm7
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
213
125
56
/
/
pmullw
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
253
252
/
/
paddw
%
xmm4
%
xmm7
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
213
220
/
/
pmullw
%
xmm4
%
xmm3
.
byte
102
15
253
220
/
/
paddw
%
xmm4
%
xmm3
.
byte
102
15
213
245
/
/
pmullw
%
xmm5
%
xmm6
.
byte
102
15
253
222
/
/
paddw
%
xmm6
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
108
36
64
/
/
movdqa
%
xmm5
0x40
(
%
esp
)
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_plus__sse2_lowp
.
globl
_sk_plus__sse2_lowp
FUNCTION
(
_sk_plus__sse2_lowp
)
_sk_plus__sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
b583
<
_sk_plus__sse2_lowp
+
0x15
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
253
77
24
/
/
paddw
0x18
(
%
ebp
)
%
xmm1
.
byte
102
15
111
168
253
88
0
0
/
/
movdqa
0x58fd
(
%
eax
)
%
xmm5
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
217
197
/
/
psubusw
%
xmm5
%
xmm0
.
byte
102
15
127
109
232
/
/
movdqa
%
xmm5
-
0x18
(
%
ebp
)
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
117
199
/
/
pcmpeqw
%
xmm7
%
xmm0
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
102
15
111
176
221
88
0
0
/
/
movdqa
0x58dd
(
%
eax
)
%
xmm6
.
byte
102
15
223
198
/
/
pandn
%
xmm6
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
253
101
40
/
/
paddw
0x28
(
%
ebp
)
%
xmm4
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
217
205
/
/
psubusw
%
xmm5
%
xmm1
.
byte
102
15
117
207
/
/
pcmpeqw
%
xmm7
%
xmm1
.
byte
102
15
219
225
/
/
pand
%
xmm1
%
xmm4
.
byte
102
15
223
206
/
/
pandn
%
xmm6
%
xmm1
.
byte
102
15
235
204
/
/
por
%
xmm4
%
xmm1
.
byte
102
15
253
85
56
/
/
paddw
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
111
226
/
/
movdqa
%
xmm2
%
xmm4
.
byte
102
15
217
229
/
/
psubusw
%
xmm5
%
xmm4
.
byte
102
15
117
231
/
/
pcmpeqw
%
xmm7
%
xmm4
.
byte
102
15
219
212
/
/
pand
%
xmm4
%
xmm2
.
byte
102
15
223
230
/
/
pandn
%
xmm6
%
xmm4
.
byte
102
15
235
226
/
/
por
%
xmm2
%
xmm4
.
byte
102
15
111
85
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
217
109
232
/
/
psubusw
-
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
117
239
/
/
pcmpeqw
%
xmm7
%
xmm5
.
byte
102
15
219
221
/
/
pand
%
xmm5
%
xmm3
.
byte
102
15
223
238
/
/
pandn
%
xmm6
%
xmm5
.
byte
102
15
235
235
/
/
por
%
xmm3
%
xmm5
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
84
36
64
/
/
movdqa
%
xmm2
0x40
(
%
esp
)
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
40
85
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
32
/
/
movaps
%
xmm2
0x20
(
%
esp
)
.
byte
15
40
85
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
16
/
/
movaps
%
xmm2
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
96
/
/
add
0x60
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_screen_sse2_lowp
.
globl
_sk_screen_sse2_lowp
FUNCTION
(
_sk_screen_sse2_lowp
)
_sk_screen_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
b67b
<
_sk_screen_sse2_lowp
+
0x15
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
109
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
111
117
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
15
111
176
229
87
0
0
/
/
movdqa
0x57e5
(
%
eax
)
%
xmm6
.
byte
102
15
253
206
/
/
paddw
%
xmm6
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
253
204
/
/
paddw
%
xmm4
%
xmm1
.
byte
102
15
213
229
/
/
pmullw
%
xmm5
%
xmm4
.
byte
102
15
253
230
/
/
paddw
%
xmm6
%
xmm4
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
249
204
/
/
psubw
%
xmm4
%
xmm1
.
byte
102
15
111
125
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
253
226
/
/
paddw
%
xmm2
%
xmm4
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
249
226
/
/
psubw
%
xmm2
%
xmm4
.
byte
102
15
111
85
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm2
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
253
235
/
/
paddw
%
xmm3
%
xmm5
.
byte
102
15
213
218
/
/
pmullw
%
xmm2
%
xmm3
.
byte
102
15
253
222
/
/
paddw
%
xmm6
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
249
235
/
/
psubw
%
xmm3
%
xmm5
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
84
36
64
/
/
movdqa
%
xmm2
0x40
(
%
esp
)
.
byte
102
15
127
124
36
48
/
/
movdqa
%
xmm7
0x30
(
%
esp
)
.
byte
15
40
85
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
32
/
/
movaps
%
xmm2
0x20
(
%
esp
)
.
byte
15
40
85
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
16
/
/
movaps
%
xmm2
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xor__sse2_lowp
.
globl
_sk_xor__sse2_lowp
FUNCTION
(
_sk_xor__sse2_lowp
)
_sk_xor__sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
102
15
127
93
232
/
/
movdqa
%
xmm3
-
0x18
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
b75b
<
_sk_xor__sse2_lowp
+
0x12
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
176
5
87
0
0
/
/
movdqa
0x5705
(
%
eax
)
%
xmm6
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
249
125
72
/
/
psubw
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
249
235
/
/
psubw
%
xmm3
%
xmm5
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
102
15
213
93
24
/
/
pmullw
0x18
(
%
ebp
)
%
xmm3
.
byte
102
15
253
195
/
/
paddw
%
xmm3
%
xmm0
.
byte
102
15
213
207
/
/
pmullw
%
xmm7
%
xmm1
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
102
15
213
93
40
/
/
pmullw
0x28
(
%
ebp
)
%
xmm3
.
byte
102
15
253
203
/
/
paddw
%
xmm3
%
xmm1
.
byte
102
15
213
215
/
/
pmullw
%
xmm7
%
xmm2
.
byte
102
15
111
93
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm3
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
213
227
/
/
pmullw
%
xmm3
%
xmm4
.
byte
102
15
253
212
/
/
paddw
%
xmm4
%
xmm2
.
byte
102
15
213
125
232
/
/
pmullw
-
0x18
(
%
ebp
)
%
xmm7
.
byte
102
15
111
101
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
213
236
/
/
pmullw
%
xmm4
%
xmm5
.
byte
102
15
253
239
/
/
paddw
%
xmm7
%
xmm5
.
byte
102
15
253
198
/
/
paddw
%
xmm6
%
xmm0
.
byte
102
15
253
206
/
/
paddw
%
xmm6
%
xmm1
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
15
253
238
/
/
paddw
%
xmm6
%
xmm5
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
102
15
127
92
36
48
/
/
movdqa
%
xmm3
0x30
(
%
esp
)
.
byte
15
40
93
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
32
/
/
movaps
%
xmm3
0x20
(
%
esp
)
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
96
/
/
add
0x60
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_darken_sse2_lowp
.
globl
_sk_darken_sse2_lowp
FUNCTION
(
_sk_darken_sse2_lowp
)
_sk_darken_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
144
0
0
0
/
/
sub
0x90
%
esp
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
102
15
127
125
216
/
/
movdqa
%
xmm7
-
0x28
(
%
ebp
)
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
b850
<
_sk_darken_sse2_lowp
+
0x25
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
117
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
15
111
222
/
/
movdqa
%
xmm6
%
xmm3
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
111
184
32
86
0
0
/
/
movdqa
0x5620
(
%
eax
)
%
xmm7
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
239
247
/
/
pxor
%
xmm7
%
xmm6
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
239
239
/
/
pxor
%
xmm7
%
xmm5
.
byte
102
15
101
238
/
/
pcmpgtw
%
xmm6
%
xmm5
.
byte
102
15
219
197
/
/
pand
%
xmm5
%
xmm0
.
byte
102
15
223
233
/
/
pandn
%
xmm1
%
xmm5
.
byte
102
15
235
232
/
/
por
%
xmm0
%
xmm5
.
byte
102
15
111
128
16
86
0
0
/
/
movdqa
0x5610
(
%
eax
)
%
xmm0
.
byte
102
15
127
69
232
/
/
movdqa
%
xmm0
-
0x18
(
%
ebp
)
.
byte
102
15
253
232
/
/
paddw
%
xmm0
%
xmm5
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
249
213
/
/
psubw
%
xmm5
%
xmm2
.
byte
102
15
127
85
184
/
/
movdqa
%
xmm2
-
0x48
(
%
ebp
)
.
byte
102
15
111
77
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
253
244
/
/
paddw
%
xmm4
%
xmm6
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
213
224
/
/
pmullw
%
xmm0
%
xmm4
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
111
93
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm3
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
239
239
/
/
pxor
%
xmm7
%
xmm5
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
239
199
/
/
pxor
%
xmm7
%
xmm0
.
byte
102
15
101
197
/
/
pcmpgtw
%
xmm5
%
xmm0
.
byte
102
15
219
200
/
/
pand
%
xmm0
%
xmm1
.
byte
102
15
223
196
/
/
pandn
%
xmm4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
253
69
232
/
/
paddw
-
0x18
(
%
ebp
)
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
249
240
/
/
psubw
%
xmm0
%
xmm6
.
byte
102
15
111
109
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
111
69
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm0
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
213
194
/
/
pmullw
%
xmm2
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
239
207
/
/
pxor
%
xmm7
%
xmm1
.
byte
102
15
239
248
/
/
pxor
%
xmm0
%
xmm7
.
byte
102
15
101
249
/
/
pcmpgtw
%
xmm1
%
xmm7
.
byte
102
15
219
199
/
/
pand
%
xmm7
%
xmm0
.
byte
102
15
223
250
/
/
pandn
%
xmm2
%
xmm7
.
byte
102
15
235
248
/
/
por
%
xmm0
%
xmm7
.
byte
102
15
111
77
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm1
.
byte
102
15
253
249
/
/
paddw
%
xmm1
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
249
231
/
/
psubw
%
xmm7
%
xmm4
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
249
195
/
/
psubw
%
xmm3
%
xmm0
.
byte
102
15
111
125
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
253
194
/
/
paddw
%
xmm2
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
124
36
64
/
/
movdqa
%
xmm7
0x40
(
%
esp
)
.
byte
102
15
127
108
36
48
/
/
movdqa
%
xmm5
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
129
196
144
0
0
0
/
/
add
0x90
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_lighten_sse2_lowp
.
globl
_sk_lighten_sse2_lowp
FUNCTION
(
_sk_lighten_sse2_lowp
)
_sk_lighten_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
144
0
0
0
/
/
sub
0x90
%
esp
.
byte
102
15
111
243
/
/
movdqa
%
xmm3
%
xmm6
.
byte
102
15
127
117
216
/
/
movdqa
%
xmm6
-
0x28
(
%
ebp
)
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
b9f9
<
_sk_lighten_sse2_lowp
+
0x25
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
213
77
72
/
/
pmullw
0x48
(
%
ebp
)
%
xmm1
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
15
111
184
119
84
0
0
/
/
movdqa
0x5477
(
%
eax
)
%
xmm7
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
239
247
/
/
pxor
%
xmm7
%
xmm6
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
239
239
/
/
pxor
%
xmm7
%
xmm5
.
byte
102
15
101
238
/
/
pcmpgtw
%
xmm6
%
xmm5
.
byte
102
15
219
205
/
/
pand
%
xmm5
%
xmm1
.
byte
102
15
223
232
/
/
pandn
%
xmm0
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
111
128
103
84
0
0
/
/
movdqa
0x5467
(
%
eax
)
%
xmm0
.
byte
102
15
253
232
/
/
paddw
%
xmm0
%
xmm5
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
127
85
232
/
/
movdqa
%
xmm2
-
0x18
(
%
ebp
)
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
249
221
/
/
psubw
%
xmm5
%
xmm3
.
byte
102
15
127
93
184
/
/
movdqa
%
xmm3
-
0x48
(
%
ebp
)
.
byte
102
15
111
77
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
253
244
/
/
paddw
%
xmm4
%
xmm6
.
byte
102
15
111
69
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
213
224
/
/
pmullw
%
xmm0
%
xmm4
.
byte
102
15
111
93
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm3
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
239
239
/
/
pxor
%
xmm7
%
xmm5
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
239
199
/
/
pxor
%
xmm7
%
xmm0
.
byte
102
15
101
197
/
/
pcmpgtw
%
xmm5
%
xmm0
.
byte
102
15
219
224
/
/
pand
%
xmm0
%
xmm4
.
byte
102
15
223
193
/
/
pandn
%
xmm1
%
xmm0
.
byte
102
15
235
196
/
/
por
%
xmm4
%
xmm0
.
byte
102
15
253
194
/
/
paddw
%
xmm2
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
249
240
/
/
psubw
%
xmm0
%
xmm6
.
byte
102
15
111
109
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
111
85
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
253
226
/
/
paddw
%
xmm2
%
xmm4
.
byte
102
15
213
85
72
/
/
pmullw
0x48
(
%
ebp
)
%
xmm2
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
239
207
/
/
pxor
%
xmm7
%
xmm1
.
byte
102
15
239
248
/
/
pxor
%
xmm0
%
xmm7
.
byte
102
15
101
249
/
/
pcmpgtw
%
xmm1
%
xmm7
.
byte
102
15
219
215
/
/
pand
%
xmm7
%
xmm2
.
byte
102
15
223
248
/
/
pandn
%
xmm0
%
xmm7
.
byte
102
15
235
250
/
/
por
%
xmm2
%
xmm7
.
byte
102
15
111
77
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm1
.
byte
102
15
253
249
/
/
paddw
%
xmm1
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
249
231
/
/
psubw
%
xmm7
%
xmm4
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
249
/
/
movdqa
%
xmm1
%
xmm7
.
byte
102
15
249
195
/
/
psubw
%
xmm3
%
xmm0
.
byte
102
15
111
85
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm2
.
byte
102
15
213
194
/
/
pmullw
%
xmm2
%
xmm0
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
84
36
64
/
/
movdqa
%
xmm2
0x40
(
%
esp
)
.
byte
102
15
127
108
36
48
/
/
movdqa
%
xmm5
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
129
196
144
0
0
0
/
/
add
0x90
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_difference_sse2_lowp
.
globl
_sk_difference_sse2_lowp
FUNCTION
(
_sk_difference_sse2_lowp
)
_sk_difference_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
160
0
0
0
/
/
sub
0xa0
%
esp
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
102
15
127
125
200
/
/
movdqa
%
xmm7
-
0x38
(
%
ebp
)
.
byte
15
41
85
184
/
/
movaps
%
xmm2
-
0x48
(
%
ebp
)
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
bb97
<
_sk_difference_sse2_lowp
+
0x25
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
117
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
15
111
222
/
/
movdqa
%
xmm6
%
xmm3
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
111
184
217
82
0
0
/
/
movdqa
0x52d9
(
%
eax
)
%
xmm7
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
239
247
/
/
pxor
%
xmm7
%
xmm6
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
239
239
/
/
pxor
%
xmm7
%
xmm5
.
byte
102
15
101
238
/
/
pcmpgtw
%
xmm6
%
xmm5
.
byte
102
15
219
205
/
/
pand
%
xmm5
%
xmm1
.
byte
102
15
223
232
/
/
pandn
%
xmm0
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
111
128
201
82
0
0
/
/
movdqa
0x52c9
(
%
eax
)
%
xmm0
.
byte
102
15
127
69
216
/
/
movdqa
%
xmm0
-
0x28
(
%
ebp
)
.
byte
102
15
253
232
/
/
paddw
%
xmm0
%
xmm5
.
byte
102
15
113
213
7
/
/
psrlw
0x7
%
xmm5
.
byte
102
15
111
128
249
82
0
0
/
/
movdqa
0x52f9
(
%
eax
)
%
xmm0
.
byte
102
15
127
69
232
/
/
movdqa
%
xmm0
-
0x18
(
%
ebp
)
.
byte
102
15
219
232
/
/
pand
%
xmm0
%
xmm5
.
byte
102
15
249
213
/
/
psubw
%
xmm5
%
xmm2
.
byte
102
15
127
85
168
/
/
movdqa
%
xmm2
-
0x58
(
%
ebp
)
.
byte
102
15
111
77
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
253
244
/
/
paddw
%
xmm4
%
xmm6
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
213
224
/
/
pmullw
%
xmm0
%
xmm4
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
111
93
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm3
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
239
239
/
/
pxor
%
xmm7
%
xmm5
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
239
199
/
/
pxor
%
xmm7
%
xmm0
.
byte
102
15
101
197
/
/
pcmpgtw
%
xmm5
%
xmm0
.
byte
102
15
219
224
/
/
pand
%
xmm0
%
xmm4
.
byte
102
15
223
193
/
/
pandn
%
xmm1
%
xmm0
.
byte
102
15
235
196
/
/
por
%
xmm4
%
xmm0
.
byte
102
15
253
69
216
/
/
paddw
-
0x28
(
%
ebp
)
%
xmm0
.
byte
102
15
113
208
7
/
/
psrlw
0x7
%
xmm0
.
byte
102
15
219
69
232
/
/
pand
-
0x18
(
%
ebp
)
%
xmm0
.
byte
102
15
249
240
/
/
psubw
%
xmm0
%
xmm6
.
byte
102
15
111
109
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
111
69
184
/
/
movdqa
-
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
213
194
/
/
pmullw
%
xmm2
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
239
207
/
/
pxor
%
xmm7
%
xmm1
.
byte
102
15
239
248
/
/
pxor
%
xmm0
%
xmm7
.
byte
102
15
101
249
/
/
pcmpgtw
%
xmm1
%
xmm7
.
byte
102
15
219
215
/
/
pand
%
xmm7
%
xmm2
.
byte
102
15
223
248
/
/
pandn
%
xmm0
%
xmm7
.
byte
102
15
235
250
/
/
por
%
xmm2
%
xmm7
.
byte
102
15
111
77
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
253
249
/
/
paddw
%
xmm1
%
xmm7
.
byte
102
15
113
215
7
/
/
psrlw
0x7
%
xmm7
.
byte
102
15
219
125
232
/
/
pand
-
0x18
(
%
ebp
)
%
xmm7
.
byte
102
15
249
231
/
/
psubw
%
xmm7
%
xmm4
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
249
/
/
movdqa
%
xmm1
%
xmm7
.
byte
102
15
249
195
/
/
psubw
%
xmm3
%
xmm0
.
byte
102
15
111
85
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm2
.
byte
102
15
213
194
/
/
pmullw
%
xmm2
%
xmm0
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
84
36
64
/
/
movdqa
%
xmm2
0x40
(
%
esp
)
.
byte
102
15
127
108
36
48
/
/
movdqa
%
xmm5
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
129
196
160
0
0
0
/
/
add
0xa0
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_exclusion_sse2_lowp
.
globl
_sk_exclusion_sse2_lowp
FUNCTION
(
_sk_exclusion_sse2_lowp
)
_sk_exclusion_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
bd4b
<
_sk_exclusion_sse2_lowp
+
0x15
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
253
193
/
/
paddw
%
xmm1
%
xmm0
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
15
111
184
21
81
0
0
/
/
movdqa
0x5115
(
%
eax
)
%
xmm7
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
113
209
7
/
/
psrlw
0x7
%
xmm1
.
byte
102
15
111
176
69
81
0
0
/
/
movdqa
0x5145
(
%
eax
)
%
xmm6
.
byte
102
15
219
206
/
/
pand
%
xmm6
%
xmm1
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
15
111
109
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
253
204
/
/
paddw
%
xmm4
%
xmm1
.
byte
102
15
213
229
/
/
pmullw
%
xmm5
%
xmm4
.
byte
102
15
253
231
/
/
paddw
%
xmm7
%
xmm4
.
byte
102
15
113
212
7
/
/
psrlw
0x7
%
xmm4
.
byte
102
15
219
230
/
/
pand
%
xmm6
%
xmm4
.
byte
102
15
249
204
/
/
psubw
%
xmm4
%
xmm1
.
byte
102
15
111
109
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
253
226
/
/
paddw
%
xmm2
%
xmm4
.
byte
102
15
213
213
/
/
pmullw
%
xmm5
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
113
210
7
/
/
psrlw
0x7
%
xmm2
.
byte
102
15
219
214
/
/
pand
%
xmm6
%
xmm2
.
byte
102
15
249
226
/
/
psubw
%
xmm2
%
xmm4
.
byte
102
15
111
215
/
/
movdqa
%
xmm7
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
111
117
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
116
36
64
/
/
movdqa
%
xmm6
0x40
(
%
esp
)
.
byte
102
15
127
108
36
48
/
/
movdqa
%
xmm5
0x30
(
%
esp
)
.
byte
15
40
85
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
32
/
/
movaps
%
xmm2
0x20
(
%
esp
)
.
byte
15
40
85
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
16
/
/
movaps
%
xmm2
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_hardlight_sse2_lowp
.
globl
_sk_hardlight_sse2_lowp
FUNCTION
(
_sk_hardlight_sse2_lowp
)
_sk_hardlight_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
176
0
0
0
/
/
sub
0xb0
%
esp
.
byte
15
41
85
152
/
/
movaps
%
xmm2
-
0x68
(
%
ebp
)
.
byte
102
15
111
233
/
/
movdqa
%
xmm1
%
xmm5
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
232
0
0
0
0
/
/
call
be45
<
_sk_hardlight_sse2_lowp
+
0x1c
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
184
27
80
0
0
/
/
movdqa
0x501b
(
%
eax
)
%
xmm7
.
byte
102
15
127
125
200
/
/
movdqa
%
xmm7
-
0x38
(
%
ebp
)
.
byte
102
15
111
215
/
/
movdqa
%
xmm7
%
xmm2
.
byte
102
15
249
85
72
/
/
psubw
0x48
(
%
ebp
)
%
xmm2
.
byte
102
15
127
85
168
/
/
movdqa
%
xmm2
-
0x58
(
%
ebp
)
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
249
250
/
/
psubw
%
xmm2
%
xmm7
.
byte
102
15
127
125
232
/
/
movdqa
%
xmm7
-
0x18
(
%
ebp
)
.
byte
102
15
111
117
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm6
.
byte
102
15
213
254
/
/
pmullw
%
xmm6
%
xmm7
.
byte
102
15
253
248
/
/
paddw
%
xmm0
%
xmm7
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
249
196
/
/
psubw
%
xmm4
%
xmm0
.
byte
102
15
111
93
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm3
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
249
206
/
/
psubw
%
xmm6
%
xmm1
.
byte
102
15
213
200
/
/
pmullw
%
xmm0
%
xmm1
.
byte
102
15
111
243
/
/
movdqa
%
xmm3
%
xmm6
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
127
93
216
/
/
movdqa
%
xmm3
-
0x28
(
%
ebp
)
.
byte
102
15
213
243
/
/
pmullw
%
xmm3
%
xmm6
.
byte
102
15
253
201
/
/
paddw
%
xmm1
%
xmm1
.
byte
102
15
111
214
/
/
movdqa
%
xmm6
%
xmm2
.
byte
102
15
249
209
/
/
psubw
%
xmm1
%
xmm2
.
byte
102
15
253
228
/
/
paddw
%
xmm4
%
xmm4
.
byte
102
15
111
128
43
80
0
0
/
/
movdqa
0x502b
(
%
eax
)
%
xmm0
.
byte
102
15
239
216
/
/
pxor
%
xmm0
%
xmm3
.
byte
102
15
127
93
184
/
/
movdqa
%
xmm3
-
0x48
(
%
ebp
)
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
239
200
/
/
pxor
%
xmm0
%
xmm1
.
byte
102
15
101
203
/
/
pcmpgtw
%
xmm3
%
xmm1
.
byte
102
15
213
101
24
/
/
pmullw
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
219
209
/
/
pand
%
xmm1
%
xmm2
.
byte
102
15
223
204
/
/
pandn
%
xmm4
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
253
125
200
/
/
paddw
-
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
111
85
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm2
.
byte
102
15
213
213
/
/
pmullw
%
xmm5
%
xmm2
.
byte
102
15
111
93
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm3
.
byte
102
15
111
125
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm7
.
byte
102
15
213
251
/
/
pmullw
%
xmm3
%
xmm7
.
byte
102
15
253
250
/
/
paddw
%
xmm2
%
xmm7
.
byte
102
15
111
85
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm2
.
byte
102
15
249
213
/
/
psubw
%
xmm5
%
xmm2
.
byte
102
15
111
101
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
249
227
/
/
psubw
%
xmm3
%
xmm4
.
byte
102
15
213
226
/
/
pmullw
%
xmm2
%
xmm4
.
byte
102
15
253
228
/
/
paddw
%
xmm4
%
xmm4
.
byte
102
15
111
214
/
/
movdqa
%
xmm6
%
xmm2
.
byte
102
15
249
212
/
/
psubw
%
xmm4
%
xmm2
.
byte
102
15
253
237
/
/
paddw
%
xmm5
%
xmm5
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
239
224
/
/
pxor
%
xmm0
%
xmm4
.
byte
102
15
101
101
184
/
/
pcmpgtw
-
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
213
235
/
/
pmullw
%
xmm3
%
xmm5
.
byte
102
15
219
212
/
/
pand
%
xmm4
%
xmm2
.
byte
102
15
223
229
/
/
pandn
%
xmm5
%
xmm4
.
byte
102
15
235
226
/
/
por
%
xmm2
%
xmm4
.
byte
102
15
253
125
200
/
/
paddw
-
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
253
231
/
/
paddw
%
xmm7
%
xmm4
.
byte
102
15
111
93
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm3
.
byte
102
15
111
85
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm2
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
15
111
109
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
213
109
56
/
/
pmullw
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
253
234
/
/
paddw
%
xmm2
%
xmm5
.
byte
102
15
111
85
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm2
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
253
219
/
/
paddw
%
xmm3
%
xmm3
.
byte
102
15
239
195
/
/
pxor
%
xmm3
%
xmm0
.
byte
102
15
101
69
184
/
/
pcmpgtw
-
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
111
125
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
249
125
56
/
/
psubw
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
213
250
/
/
pmullw
%
xmm2
%
xmm7
.
byte
102
15
253
255
/
/
paddw
%
xmm7
%
xmm7
.
byte
102
15
249
247
/
/
psubw
%
xmm7
%
xmm6
.
byte
102
15
111
125
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
213
223
/
/
pmullw
%
xmm7
%
xmm3
.
byte
102
15
219
240
/
/
pand
%
xmm0
%
xmm6
.
byte
102
15
223
195
/
/
pandn
%
xmm3
%
xmm0
.
byte
102
15
235
198
/
/
por
%
xmm6
%
xmm0
.
byte
102
15
111
93
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm3
.
byte
102
15
253
235
/
/
paddw
%
xmm3
%
xmm5
.
byte
102
15
253
232
/
/
paddw
%
xmm0
%
xmm5
.
byte
102
15
111
69
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
111
85
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm2
.
byte
102
15
213
208
/
/
pmullw
%
xmm0
%
xmm2
.
byte
102
15
253
211
/
/
paddw
%
xmm3
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
111
93
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm3
.
byte
102
15
253
218
/
/
paddw
%
xmm2
%
xmm3
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
68
36
64
/
/
movdqa
%
xmm0
0x40
(
%
esp
)
.
byte
102
15
127
124
36
48
/
/
movdqa
%
xmm7
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
111
213
/
/
movdqa
%
xmm5
%
xmm2
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
129
196
176
0
0
0
/
/
add
0xb0
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_overlay_sse2_lowp
.
globl
_sk_overlay_sse2_lowp
FUNCTION
(
_sk_overlay_sse2_lowp
)
_sk_overlay_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
208
0
0
0
/
/
sub
0xd0
%
esp
.
byte
15
41
85
152
/
/
movaps
%
xmm2
-
0x68
(
%
ebp
)
.
byte
15
41
141
120
255
255
255
/
/
movaps
%
xmm1
-
0x88
(
%
ebp
)
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
c067
<
_sk_overlay_sse2_lowp
+
0x1f
>
.
byte
88
/
/
pop
%
eax
.
byte
102
15
111
69
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
111
184
249
77
0
0
/
/
movdqa
0x4df9
(
%
eax
)
%
xmm7
.
byte
102
15
127
125
232
/
/
movdqa
%
xmm7
-
0x18
(
%
ebp
)
.
byte
102
15
111
215
/
/
movdqa
%
xmm7
%
xmm2
.
byte
102
15
249
208
/
/
psubw
%
xmm0
%
xmm2
.
byte
102
15
127
85
168
/
/
movdqa
%
xmm2
-
0x58
(
%
ebp
)
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
213
209
/
/
pmullw
%
xmm1
%
xmm2
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
249
248
/
/
psubw
%
xmm0
%
xmm7
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
213
101
24
/
/
pmullw
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
253
226
/
/
paddw
%
xmm2
%
xmm4
.
byte
102
15
127
101
216
/
/
movdqa
%
xmm4
-
0x28
(
%
ebp
)
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
127
69
200
/
/
movdqa
%
xmm0
-
0x38
(
%
ebp
)
.
byte
102
15
249
233
/
/
psubw
%
xmm1
%
xmm5
.
byte
102
15
111
230
/
/
movdqa
%
xmm6
%
xmm4
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
111
93
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm3
.
byte
102
15
249
211
/
/
psubw
%
xmm3
%
xmm2
.
byte
102
15
213
213
/
/
pmullw
%
xmm5
%
xmm2
.
byte
102
15
213
240
/
/
pmullw
%
xmm0
%
xmm6
.
byte
102
15
253
210
/
/
paddw
%
xmm2
%
xmm2
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
249
234
/
/
psubw
%
xmm2
%
xmm5
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
15
111
144
9
78
0
0
/
/
movdqa
0x4e09
(
%
eax
)
%
xmm2
.
byte
102
15
127
85
184
/
/
movdqa
%
xmm2
-
0x48
(
%
ebp
)
.
byte
102
15
239
226
/
/
pxor
%
xmm2
%
xmm4
.
byte
102
15
239
194
/
/
pxor
%
xmm2
%
xmm0
.
byte
102
15
101
196
/
/
pcmpgtw
%
xmm4
%
xmm0
.
byte
102
15
127
101
136
/
/
movdqa
%
xmm4
-
0x78
(
%
ebp
)
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
15
253
201
/
/
paddw
%
xmm1
%
xmm1
.
byte
102
15
219
232
/
/
pand
%
xmm0
%
xmm5
.
byte
102
15
223
193
/
/
pandn
%
xmm1
%
xmm0
.
byte
102
15
235
197
/
/
por
%
xmm5
%
xmm0
.
byte
102
15
111
85
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm2
.
byte
102
15
111
77
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
253
202
/
/
paddw
%
xmm2
%
xmm1
.
byte
102
15
253
200
/
/
paddw
%
xmm0
%
xmm1
.
byte
102
15
127
77
216
/
/
movdqa
%
xmm1
-
0x28
(
%
ebp
)
.
byte
102
15
111
69
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm0
.
byte
102
15
111
141
120
255
255
255
/
/
movdqa
-
0x88
(
%
ebp
)
%
xmm1
.
byte
102
15
213
193
/
/
pmullw
%
xmm1
%
xmm0
.
byte
102
15
111
85
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm2
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
102
15
213
218
/
/
pmullw
%
xmm2
%
xmm3
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
102
15
111
69
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm0
.
byte
102
15
249
193
/
/
psubw
%
xmm1
%
xmm0
.
byte
102
15
111
109
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
249
234
/
/
psubw
%
xmm2
%
xmm5
.
byte
102
15
213
232
/
/
pmullw
%
xmm0
%
xmm5
.
byte
102
15
253
237
/
/
paddw
%
xmm5
%
xmm5
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
249
197
/
/
psubw
%
xmm5
%
xmm0
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
253
237
/
/
paddw
%
xmm5
%
xmm5
.
byte
102
15
239
109
184
/
/
pxor
-
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
101
236
/
/
pcmpgtw
%
xmm4
%
xmm5
.
byte
102
15
213
202
/
/
pmullw
%
xmm2
%
xmm1
.
byte
102
15
253
201
/
/
paddw
%
xmm1
%
xmm1
.
byte
102
15
219
197
/
/
pand
%
xmm5
%
xmm0
.
byte
102
15
223
233
/
/
pandn
%
xmm1
%
xmm5
.
byte
102
15
235
232
/
/
por
%
xmm0
%
xmm5
.
byte
102
15
253
93
232
/
/
paddw
-
0x18
(
%
ebp
)
%
xmm3
.
byte
102
15
253
221
/
/
paddw
%
xmm5
%
xmm3
.
byte
102
15
111
109
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm5
.
byte
102
15
111
69
168
/
/
movdqa
-
0x58
(
%
ebp
)
%
xmm0
.
byte
102
15
213
197
/
/
pmullw
%
xmm5
%
xmm0
.
byte
102
15
111
77
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm1
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
213
225
/
/
pmullw
%
xmm1
%
xmm4
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
253
192
/
/
paddw
%
xmm0
%
xmm0
.
byte
102
15
239
69
184
/
/
pxor
-
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
101
69
136
/
/
pcmpgtw
-
0x78
(
%
ebp
)
%
xmm0
.
byte
102
15
111
85
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
249
213
/
/
psubw
%
xmm5
%
xmm2
.
byte
102
15
111
77
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm1
.
byte
102
15
111
233
/
/
movdqa
%
xmm1
%
xmm5
.
byte
102
15
111
77
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm1
.
byte
102
15
249
233
/
/
psubw
%
xmm1
%
xmm5
.
byte
102
15
213
234
/
/
pmullw
%
xmm2
%
xmm5
.
byte
102
15
253
237
/
/
paddw
%
xmm5
%
xmm5
.
byte
102
15
249
245
/
/
psubw
%
xmm5
%
xmm6
.
byte
102
15
111
85
152
/
/
movdqa
-
0x68
(
%
ebp
)
%
xmm2
.
byte
102
15
213
209
/
/
pmullw
%
xmm1
%
xmm2
.
byte
102
15
253
210
/
/
paddw
%
xmm2
%
xmm2
.
byte
102
15
219
240
/
/
pand
%
xmm0
%
xmm6
.
byte
102
15
223
194
/
/
pandn
%
xmm2
%
xmm0
.
byte
102
15
235
198
/
/
por
%
xmm6
%
xmm0
.
byte
102
15
111
85
232
/
/
movdqa
-
0x18
(
%
ebp
)
%
xmm2
.
byte
102
15
253
226
/
/
paddw
%
xmm2
%
xmm4
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
111
69
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
213
248
/
/
pmullw
%
xmm0
%
xmm7
.
byte
102
15
253
250
/
/
paddw
%
xmm2
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
111
109
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
253
239
/
/
paddw
%
xmm7
%
xmm5
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
102
15
111
77
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
102
15
127
68
36
64
/
/
movdqa
%
xmm0
0x40
(
%
esp
)
.
byte
15
40
69
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
129
196
208
0
0
0
/
/
add
0xd0
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_8888_sse2_lowp
.
globl
_sk_load_8888_sse2_lowp
FUNCTION
(
_sk_load_8888_sse2_lowp
)
_sk_load_8888_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
193
227
2
/
/
shl
0x2
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
c2d6
<
_sk_load_8888_sse2_lowp
+
0x30
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
32
/
/
ja
c2fc
<
_sk_load_8888_sse2_lowp
+
0x56
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
50
1
0
0
/
/
mov
0x132
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
243
15
16
28
179
/
/
movss
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
235
73
/
/
jmp
c345
<
_sk_load_8888_sse2_lowp
+
0x9f
>
.
byte
102
15
16
28
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
15
16
116
179
16
/
/
movups
0x10
(
%
ebx
%
esi
4
)
%
xmm6
.
byte
235
61
/
/
jmp
c345
<
_sk_load_8888_sse2_lowp
+
0x9f
>
.
byte
102
15
110
68
179
8
/
/
movd
0x8
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
15
18
28
179
/
/
movlpd
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
235
43
/
/
jmp
c345
<
_sk_load_8888_sse2_lowp
+
0x9f
>
.
byte
102
15
110
68
179
24
/
/
movd
0x18
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
102
15
112
240
69
/
/
pshufd
0x45
%
xmm0
%
xmm6
.
byte
243
15
16
68
179
20
/
/
movss
0x14
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
15
198
198
0
/
/
shufps
0x0
%
xmm6
%
xmm0
.
byte
15
198
198
226
/
/
shufps
0xe2
%
xmm6
%
xmm0
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
243
15
16
68
179
16
/
/
movss
0x10
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
243
15
16
240
/
/
movss
%
xmm0
%
xmm6
.
byte
102
15
16
28
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
102
15
40
203
/
/
movapd
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
15
111
175
138
75
0
0
/
/
movdqa
0x4b8a
(
%
edi
)
%
xmm5
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
219
197
/
/
pand
%
xmm5
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
40
211
/
/
movapd
%
xmm3
%
xmm2
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
114
214
24
/
/
psrld
0x18
%
xmm6
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
15
107
222
/
/
packssdw
%
xmm6
%
xmm3
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
212
/
/
packssdw
%
xmm4
%
xmm2
.
byte
102
15
219
213
/
/
pand
%
xmm5
%
xmm2
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
28
0
/
/
sbb
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
61
0
0
0
50
/
/
cmp
0x32000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
106
0
/
/
add
%
ch
0x0
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
96
/
/
pusha
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
79
0
/
/
add
%
cl
0x0
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_load_8888_dst_sse2_lowp
.
globl
_sk_load_8888_dst_sse2_lowp
FUNCTION
(
_sk_load_8888_dst_sse2_lowp
)
_sk_load_8888_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
140
0
0
0
/
/
sub
0x8c
%
esp
.
byte
102
15
127
93
184
/
/
movdqa
%
xmm3
-
0x48
(
%
ebp
)
.
byte
102
15
127
85
200
/
/
movdqa
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
193
227
2
/
/
shl
0x2
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
c461
<
_sk_load_8888_dst_sse2_lowp
+
0x3d
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
32
/
/
ja
c487
<
_sk_load_8888_dst_sse2_lowp
+
0x63
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
59
1
0
0
/
/
mov
0x13b
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
243
15
16
60
179
/
/
movss
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
235
73
/
/
jmp
c4d0
<
_sk_load_8888_dst_sse2_lowp
+
0xac
>
.
byte
102
15
16
60
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
15
16
92
179
16
/
/
movups
0x10
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
235
61
/
/
jmp
c4d0
<
_sk_load_8888_dst_sse2_lowp
+
0xac
>
.
byte
102
15
110
84
179
8
/
/
movd
0x8
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
102
15
112
250
69
/
/
pshufd
0x45
%
xmm2
%
xmm7
.
byte
102
15
18
60
179
/
/
movlpd
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
235
43
/
/
jmp
c4d0
<
_sk_load_8888_dst_sse2_lowp
+
0xac
>
.
byte
102
15
110
84
179
24
/
/
movd
0x18
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
102
15
112
218
69
/
/
pshufd
0x45
%
xmm2
%
xmm3
.
byte
243
15
16
84
179
20
/
/
movss
0x14
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
15
198
211
0
/
/
shufps
0x0
%
xmm3
%
xmm2
.
byte
15
198
211
226
/
/
shufps
0xe2
%
xmm3
%
xmm2
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
243
15
16
84
179
16
/
/
movss
0x10
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
243
15
16
218
/
/
movss
%
xmm2
%
xmm3
.
byte
102
15
16
60
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
102
15
40
247
/
/
movapd
%
xmm7
%
xmm6
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
107
242
/
/
packssdw
%
xmm2
%
xmm6
.
byte
102
15
111
151
255
73
0
0
/
/
movdqa
0x49ff
(
%
edi
)
%
xmm2
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
40
239
/
/
movapd
%
xmm7
%
xmm5
.
byte
102
15
114
213
16
/
/
psrld
0x10
%
xmm5
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
107
251
/
/
packssdw
%
xmm3
%
xmm7
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
114
229
16
/
/
psrad
0x10
%
xmm5
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
236
/
/
packssdw
%
xmm4
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
127
124
36
64
/
/
movdqa
%
xmm7
0x40
(
%
esp
)
.
byte
102
15
127
116
36
32
/
/
movdqa
%
xmm6
0x20
(
%
esp
)
.
byte
102
15
127
76
36
16
/
/
movdqa
%
xmm1
0x10
(
%
esp
)
.
byte
102
15
127
108
36
48
/
/
movdqa
%
xmm5
0x30
(
%
esp
)
.
byte
15
40
77
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
129
196
140
0
0
0
/
/
add
0x8c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
28
0
/
/
sbb
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
61
0
0
0
50
/
/
cmp
0x32000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
106
0
/
/
add
%
ch
0x0
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
96
/
/
pusha
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
79
0
/
/
add
%
cl
0x0
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_store_8888_sse2_lowp
.
globl
_sk_store_8888_sse2_lowp
FUNCTION
(
_sk_store_8888_sse2_lowp
)
_sk_store_8888_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
108
/
/
sub
0x6c
%
esp
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
113
244
8
/
/
psllw
0x8
%
xmm4
.
byte
102
15
235
224
/
/
por
%
xmm0
%
xmm4
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
97
238
/
/
punpcklwd
%
xmm6
%
xmm5
.
byte
102
15
127
93
216
/
/
movdqa
%
xmm3
-
0x28
(
%
ebp
)
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
15
235
218
/
/
por
%
xmm2
%
xmm3
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
97
251
/
/
punpcklwd
%
xmm3
%
xmm7
.
byte
102
15
235
253
/
/
por
%
xmm5
%
xmm7
.
byte
102
15
105
230
/
/
punpckhwd
%
xmm6
%
xmm4
.
byte
102
15
105
243
/
/
punpckhwd
%
xmm3
%
xmm6
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
102
15
235
244
/
/
por
%
xmm4
%
xmm6
.
byte
102
15
111
101
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm4
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
93
8
/
/
mov
0x8
(
%
ebp
)
%
ebx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
121
4
/
/
mov
0x4
(
%
ecx
)
%
edi
.
byte
15
175
250
/
/
imul
%
edx
%
edi
.
byte
193
231
2
/
/
shl
0x2
%
edi
.
byte
3
57
/
/
add
(
%
ecx
)
%
edi
.
byte
128
227
7
/
/
and
0x7
%
bl
.
byte
254
203
/
/
dec
%
bl
.
byte
128
251
6
/
/
cmp
0x6
%
bl
.
byte
232
0
0
0
0
/
/
call
c62c
<
_sk_store_8888_sse2_lowp
+
0x74
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
119
19
/
/
ja
c649
<
_sk_store_8888_sse2_lowp
+
0x91
>
.
byte
15
182
219
/
/
movzbl
%
bl
%
ebx
.
byte
3
140
153
184
0
0
0
/
/
add
0xb8
(
%
ecx
%
ebx
4
)
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
102
15
126
60
183
/
/
movd
%
xmm7
(
%
edi
%
esi
4
)
.
byte
235
93
/
/
jmp
c6a6
<
_sk_store_8888_sse2_lowp
+
0xee
>
.
byte
243
15
127
60
183
/
/
movdqu
%
xmm7
(
%
edi
%
esi
4
)
.
byte
243
15
127
116
183
16
/
/
movdqu
%
xmm6
0x10
(
%
edi
%
esi
4
)
.
byte
235
80
/
/
jmp
c6a6
<
_sk_store_8888_sse2_lowp
+
0xee
>
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
102
15
112
223
78
/
/
pshufd
0x4e
%
xmm7
%
xmm3
.
byte
102
15
126
92
183
8
/
/
movd
%
xmm3
0x8
(
%
edi
%
esi
4
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
102
15
111
230
/
/
movdqa
%
xmm6
%
xmm4
.
byte
102
15
214
60
183
/
/
movq
%
xmm7
(
%
edi
%
esi
4
)
.
byte
235
48
/
/
jmp
c6a6
<
_sk_store_8888_sse2_lowp
+
0xee
>
.
byte
102
15
112
222
78
/
/
pshufd
0x4e
%
xmm6
%
xmm3
.
byte
102
15
126
92
183
24
/
/
movd
%
xmm3
0x18
(
%
edi
%
esi
4
)
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
102
15
112
222
229
/
/
pshufd
0xe5
%
xmm6
%
xmm3
.
byte
102
15
126
92
183
20
/
/
movd
%
xmm3
0x14
(
%
edi
%
esi
4
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
102
15
111
101
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm4
.
byte
102
15
126
116
183
16
/
/
movd
%
xmm6
0x10
(
%
edi
%
esi
4
)
.
byte
243
15
127
60
183
/
/
movdqu
%
xmm7
(
%
edi
%
esi
4
)
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
102
15
127
100
36
48
/
/
movdqa
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
108
/
/
add
0x6c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
22
/
/
push
%
ss
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
67
0
/
/
add
%
al
0x0
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
42
0
/
/
sub
(
%
eax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
117
0
/
/
jne
c6f2
<
_sk_store_8888_sse2_lowp
+
0x13a
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
111
/
/
outsl
%
ds
:
(
%
esi
)
(
%
dx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
89
0
/
/
add
%
bl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_load_bgra_sse2_lowp
.
globl
_sk_load_bgra_sse2_lowp
FUNCTION
(
_sk_load_bgra_sse2_lowp
)
_sk_load_bgra_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
193
227
2
/
/
shl
0x2
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
c730
<
_sk_load_bgra_sse2_lowp
+
0x30
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
32
/
/
ja
c756
<
_sk_load_bgra_sse2_lowp
+
0x56
>
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
52
1
0
0
/
/
mov
0x134
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
243
15
16
28
179
/
/
movss
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
235
73
/
/
jmp
c79f
<
_sk_load_bgra_sse2_lowp
+
0x9f
>
.
byte
102
15
16
28
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
15
16
116
179
16
/
/
movups
0x10
(
%
ebx
%
esi
4
)
%
xmm6
.
byte
235
61
/
/
jmp
c79f
<
_sk_load_bgra_sse2_lowp
+
0x9f
>
.
byte
102
15
110
68
179
8
/
/
movd
0x8
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
102
15
18
28
179
/
/
movlpd
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
235
43
/
/
jmp
c79f
<
_sk_load_bgra_sse2_lowp
+
0x9f
>
.
byte
102
15
110
68
179
24
/
/
movd
0x18
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
102
15
112
240
69
/
/
pshufd
0x45
%
xmm0
%
xmm6
.
byte
243
15
16
68
179
20
/
/
movss
0x14
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
15
198
198
0
/
/
shufps
0x0
%
xmm6
%
xmm0
.
byte
15
198
198
226
/
/
shufps
0xe2
%
xmm6
%
xmm0
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
243
15
16
68
179
16
/
/
movss
0x10
(
%
ebx
%
esi
4
)
%
xmm0
.
byte
243
15
16
240
/
/
movss
%
xmm0
%
xmm6
.
byte
102
15
16
28
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
102
15
40
203
/
/
movapd
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
15
111
175
48
71
0
0
/
/
movdqa
0x4730
(
%
edi
)
%
xmm5
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
219
213
/
/
pand
%
xmm5
%
xmm2
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
40
195
/
/
movapd
%
xmm3
%
xmm0
.
byte
102
15
114
208
16
/
/
psrld
0x10
%
xmm0
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
114
214
24
/
/
psrld
0x18
%
xmm6
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
15
107
222
/
/
packssdw
%
xmm6
%
xmm3
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
196
/
/
packssdw
%
xmm4
%
xmm0
.
byte
102
15
219
197
/
/
pand
%
xmm5
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
28
0
/
/
sbb
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
61
0
0
0
50
/
/
cmp
0x32000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
106
0
/
/
add
%
ch
0x0
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
96
/
/
pusha
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
79
0
/
/
add
%
cl
0x0
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_load_bgra_dst_sse2_lowp
.
globl
_sk_load_bgra_dst_sse2_lowp
FUNCTION
(
_sk_load_bgra_dst_sse2_lowp
)
_sk_load_bgra_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
140
0
0
0
/
/
sub
0x8c
%
esp
.
byte
102
15
127
93
184
/
/
movdqa
%
xmm3
-
0x48
(
%
ebp
)
.
byte
102
15
127
85
200
/
/
movdqa
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
193
227
2
/
/
shl
0x2
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
c8bd
<
_sk_load_bgra_dst_sse2_lowp
+
0x3d
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
32
/
/
ja
c8e3
<
_sk_load_bgra_dst_sse2_lowp
+
0x63
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
59
1
0
0
/
/
mov
0x13b
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
243
15
16
60
179
/
/
movss
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
235
73
/
/
jmp
c92c
<
_sk_load_bgra_dst_sse2_lowp
+
0xac
>
.
byte
102
15
16
60
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
15
16
92
179
16
/
/
movups
0x10
(
%
ebx
%
esi
4
)
%
xmm3
.
byte
235
61
/
/
jmp
c92c
<
_sk_load_bgra_dst_sse2_lowp
+
0xac
>
.
byte
102
15
110
84
179
8
/
/
movd
0x8
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
102
15
112
250
69
/
/
pshufd
0x45
%
xmm2
%
xmm7
.
byte
102
15
18
60
179
/
/
movlpd
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
235
43
/
/
jmp
c92c
<
_sk_load_bgra_dst_sse2_lowp
+
0xac
>
.
byte
102
15
110
84
179
24
/
/
movd
0x18
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
102
15
112
218
69
/
/
pshufd
0x45
%
xmm2
%
xmm3
.
byte
243
15
16
84
179
20
/
/
movss
0x14
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
15
198
211
0
/
/
shufps
0x0
%
xmm3
%
xmm2
.
byte
15
198
211
226
/
/
shufps
0xe2
%
xmm3
%
xmm2
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
243
15
16
84
179
16
/
/
movss
0x10
(
%
ebx
%
esi
4
)
%
xmm2
.
byte
243
15
16
218
/
/
movss
%
xmm2
%
xmm3
.
byte
102
15
16
60
179
/
/
movupd
(
%
ebx
%
esi
4
)
%
xmm7
.
byte
102
15
40
247
/
/
movapd
%
xmm7
%
xmm6
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
107
242
/
/
packssdw
%
xmm2
%
xmm6
.
byte
102
15
111
151
163
69
0
0
/
/
movdqa
0x45a3
(
%
edi
)
%
xmm2
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
219
202
/
/
pand
%
xmm2
%
xmm1
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
40
239
/
/
movapd
%
xmm7
%
xmm5
.
byte
102
15
114
213
16
/
/
psrld
0x10
%
xmm5
.
byte
102
15
114
215
24
/
/
psrld
0x18
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
107
251
/
/
packssdw
%
xmm3
%
xmm7
.
byte
102
15
114
245
16
/
/
pslld
0x10
%
xmm5
.
byte
102
15
114
229
16
/
/
psrad
0x10
%
xmm5
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
236
/
/
packssdw
%
xmm4
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
127
124
36
64
/
/
movdqa
%
xmm7
0x40
(
%
esp
)
.
byte
102
15
127
76
36
48
/
/
movdqa
%
xmm1
0x30
(
%
esp
)
.
byte
102
15
127
116
36
32
/
/
movdqa
%
xmm6
0x20
(
%
esp
)
.
byte
102
15
127
108
36
16
/
/
movdqa
%
xmm5
0x10
(
%
esp
)
.
byte
15
40
77
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
93
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
129
196
140
0
0
0
/
/
add
0x8c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
28
0
/
/
sbb
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
61
0
0
0
50
/
/
cmp
0x32000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
106
0
/
/
add
%
ch
0x0
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
96
/
/
pusha
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
79
0
/
/
add
%
cl
0x0
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_store_bgra_sse2_lowp
.
globl
_sk_store_bgra_sse2_lowp
FUNCTION
(
_sk_store_bgra_sse2_lowp
)
_sk_store_bgra_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
108
/
/
sub
0x6c
%
esp
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
113
244
8
/
/
psllw
0x8
%
xmm4
.
byte
102
15
235
226
/
/
por
%
xmm2
%
xmm4
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
97
238
/
/
punpcklwd
%
xmm6
%
xmm5
.
byte
102
15
127
93
216
/
/
movdqa
%
xmm3
-
0x28
(
%
ebp
)
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
15
235
216
/
/
por
%
xmm0
%
xmm3
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
97
251
/
/
punpcklwd
%
xmm3
%
xmm7
.
byte
102
15
235
253
/
/
por
%
xmm5
%
xmm7
.
byte
102
15
105
230
/
/
punpckhwd
%
xmm6
%
xmm4
.
byte
102
15
105
243
/
/
punpckhwd
%
xmm3
%
xmm6
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
102
15
235
244
/
/
por
%
xmm4
%
xmm6
.
byte
102
15
111
101
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm4
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
93
8
/
/
mov
0x8
(
%
ebp
)
%
ebx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
121
4
/
/
mov
0x4
(
%
ecx
)
%
edi
.
byte
15
175
250
/
/
imul
%
edx
%
edi
.
byte
193
231
2
/
/
shl
0x2
%
edi
.
byte
3
57
/
/
add
(
%
ecx
)
%
edi
.
byte
128
227
7
/
/
and
0x7
%
bl
.
byte
254
203
/
/
dec
%
bl
.
byte
128
251
6
/
/
cmp
0x6
%
bl
.
byte
232
0
0
0
0
/
/
call
ca88
<
_sk_store_bgra_sse2_lowp
+
0x74
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
119
19
/
/
ja
caa5
<
_sk_store_bgra_sse2_lowp
+
0x91
>
.
byte
15
182
219
/
/
movzbl
%
bl
%
ebx
.
byte
3
140
153
184
0
0
0
/
/
add
0xb8
(
%
ecx
%
ebx
4
)
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
102
15
126
60
183
/
/
movd
%
xmm7
(
%
edi
%
esi
4
)
.
byte
235
93
/
/
jmp
cb02
<
_sk_store_bgra_sse2_lowp
+
0xee
>
.
byte
243
15
127
60
183
/
/
movdqu
%
xmm7
(
%
edi
%
esi
4
)
.
byte
243
15
127
116
183
16
/
/
movdqu
%
xmm6
0x10
(
%
edi
%
esi
4
)
.
byte
235
80
/
/
jmp
cb02
<
_sk_store_bgra_sse2_lowp
+
0xee
>
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
102
15
112
223
78
/
/
pshufd
0x4e
%
xmm7
%
xmm3
.
byte
102
15
126
92
183
8
/
/
movd
%
xmm3
0x8
(
%
edi
%
esi
4
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
102
15
111
230
/
/
movdqa
%
xmm6
%
xmm4
.
byte
102
15
214
60
183
/
/
movq
%
xmm7
(
%
edi
%
esi
4
)
.
byte
235
48
/
/
jmp
cb02
<
_sk_store_bgra_sse2_lowp
+
0xee
>
.
byte
102
15
112
222
78
/
/
pshufd
0x4e
%
xmm6
%
xmm3
.
byte
102
15
126
92
183
24
/
/
movd
%
xmm3
0x18
(
%
edi
%
esi
4
)
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
102
15
112
222
229
/
/
pshufd
0xe5
%
xmm6
%
xmm3
.
byte
102
15
126
92
183
20
/
/
movd
%
xmm3
0x14
(
%
edi
%
esi
4
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
102
15
111
101
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm4
.
byte
102
15
126
116
183
16
/
/
movd
%
xmm6
0x10
(
%
edi
%
esi
4
)
.
byte
243
15
127
60
183
/
/
movdqu
%
xmm7
(
%
edi
%
esi
4
)
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
102
15
127
100
36
48
/
/
movdqa
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
108
/
/
add
0x6c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
22
/
/
push
%
ss
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
67
0
/
/
add
%
al
0x0
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
42
0
/
/
sub
(
%
eax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
117
0
/
/
jne
cb4e
<
_sk_store_bgra_sse2_lowp
+
0x13a
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
111
/
/
outsl
%
ds
:
(
%
esi
)
(
%
dx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
89
0
/
/
add
%
bl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_gather_8888_sse2_lowp
.
globl
_sk_gather_8888_sse2_lowp
FUNCTION
(
_sk_gather_8888_sse2_lowp
)
_sk_gather_8888_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
232
0
0
0
0
/
/
call
cb69
<
_sk_gather_8888_sse2_lowp
+
0xd
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
16
103
8
/
/
movss
0x8
(
%
edi
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
118
237
/
/
pcmpeqd
%
xmm5
%
xmm5
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
95
198
/
/
maxps
%
xmm6
%
xmm0
.
byte
15
95
206
/
/
maxps
%
xmm6
%
xmm1
.
byte
15
93
204
/
/
minps
%
xmm4
%
xmm1
.
byte
15
93
196
/
/
minps
%
xmm4
%
xmm0
.
byte
243
15
16
103
12
/
/
movss
0xc
(
%
edi
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
95
222
/
/
maxps
%
xmm6
%
xmm3
.
byte
15
95
214
/
/
maxps
%
xmm6
%
xmm2
.
byte
15
93
212
/
/
minps
%
xmm4
%
xmm2
.
byte
15
93
220
/
/
minps
%
xmm4
%
xmm3
.
byte
139
55
/
/
mov
(
%
edi
)
%
esi
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
103
4
/
/
movd
0x4
(
%
edi
)
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
102
15
112
234
245
/
/
pshufd
0xf5
%
xmm2
%
xmm5
.
byte
102
15
244
236
/
/
pmuludq
%
xmm4
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
112
251
245
/
/
pshufd
0xf5
%
xmm3
%
xmm7
.
byte
102
15
244
252
/
/
pmuludq
%
xmm4
%
xmm7
.
byte
102
15
244
226
/
/
pmuludq
%
xmm2
%
xmm4
.
byte
102
15
112
212
232
/
/
pshufd
0xe8
%
xmm4
%
xmm2
.
byte
102
15
112
229
232
/
/
pshufd
0xe8
%
xmm5
%
xmm4
.
byte
102
15
98
212
/
/
punpckldq
%
xmm4
%
xmm2
.
byte
102
15
244
243
/
/
pmuludq
%
xmm3
%
xmm6
.
byte
102
15
112
222
232
/
/
pshufd
0xe8
%
xmm6
%
xmm3
.
byte
102
15
112
231
232
/
/
pshufd
0xe8
%
xmm7
%
xmm4
.
byte
102
15
98
220
/
/
punpckldq
%
xmm4
%
xmm3
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
254
203
/
/
paddd
%
xmm3
%
xmm1
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
15
112
216
231
/
/
pshufd
0xe7
%
xmm0
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
243
15
16
28
190
/
/
movss
(
%
esi
%
edi
4
)
%
xmm3
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
110
36
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
102
15
110
28
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
112
233
231
/
/
pshufd
0xe7
%
xmm1
%
xmm5
.
byte
102
15
110
52
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm6
.
byte
102
15
126
239
/
/
movd
%
xmm5
%
edi
.
byte
102
15
110
60
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm7
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
110
20
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm2
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
102
15
110
4
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm0
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
110
12
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm1
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
102
15
98
101
232
/
/
punpckldq
-
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
98
222
/
/
punpckldq
%
xmm6
%
xmm3
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
108
220
/
/
punpcklqdq
%
xmm4
%
xmm3
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
102
15
98
215
/
/
punpckldq
%
xmm7
%
xmm2
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
15
108
194
/
/
punpcklqdq
%
xmm2
%
xmm0
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
102
15
107
202
/
/
packssdw
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
111
224
/
/
movdqa
%
xmm0
%
xmm4
.
byte
102
15
114
208
24
/
/
psrld
0x18
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
107
216
/
/
packssdw
%
xmm0
%
xmm3
.
byte
102
15
111
186
247
66
0
0
/
/
movdqa
0x42f7
(
%
edx
)
%
xmm7
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
15
114
212
16
/
/
psrld
0x10
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
107
212
/
/
packssdw
%
xmm4
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
219
199
/
/
pand
%
xmm7
%
xmm0
.
byte
102
15
219
215
/
/
pand
%
xmm7
%
xmm2
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
84
36
4
/
/
mov
%
edx
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
96
/
/
add
0x60
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_gather_bgra_sse2_lowp
.
globl
_sk_gather_bgra_sse2_lowp
FUNCTION
(
_sk_gather_bgra_sse2_lowp
)
_sk_gather_bgra_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
cd64
<
_sk_gather_bgra_sse2_lowp
+
0xd
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
16
103
8
/
/
movss
0x8
(
%
edi
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
118
237
/
/
pcmpeqd
%
xmm5
%
xmm5
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
95
198
/
/
maxps
%
xmm6
%
xmm0
.
byte
15
95
206
/
/
maxps
%
xmm6
%
xmm1
.
byte
15
93
204
/
/
minps
%
xmm4
%
xmm1
.
byte
15
93
196
/
/
minps
%
xmm4
%
xmm0
.
byte
243
15
16
103
12
/
/
movss
0xc
(
%
edi
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
95
222
/
/
maxps
%
xmm6
%
xmm3
.
byte
15
95
214
/
/
maxps
%
xmm6
%
xmm2
.
byte
15
93
212
/
/
minps
%
xmm4
%
xmm2
.
byte
15
93
220
/
/
minps
%
xmm4
%
xmm3
.
byte
139
55
/
/
mov
(
%
edi
)
%
esi
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
103
4
/
/
movd
0x4
(
%
edi
)
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
102
15
112
234
245
/
/
pshufd
0xf5
%
xmm2
%
xmm5
.
byte
102
15
244
236
/
/
pmuludq
%
xmm4
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
112
251
245
/
/
pshufd
0xf5
%
xmm3
%
xmm7
.
byte
102
15
244
252
/
/
pmuludq
%
xmm4
%
xmm7
.
byte
102
15
244
226
/
/
pmuludq
%
xmm2
%
xmm4
.
byte
102
15
112
212
232
/
/
pshufd
0xe8
%
xmm4
%
xmm2
.
byte
102
15
112
229
232
/
/
pshufd
0xe8
%
xmm5
%
xmm4
.
byte
102
15
98
212
/
/
punpckldq
%
xmm4
%
xmm2
.
byte
102
15
244
243
/
/
pmuludq
%
xmm3
%
xmm6
.
byte
102
15
112
222
232
/
/
pshufd
0xe8
%
xmm6
%
xmm3
.
byte
102
15
112
231
232
/
/
pshufd
0xe8
%
xmm7
%
xmm4
.
byte
102
15
98
220
/
/
punpckldq
%
xmm4
%
xmm3
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
254
203
/
/
paddd
%
xmm3
%
xmm1
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
15
112
216
231
/
/
pshufd
0xe7
%
xmm0
%
xmm3
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
102
15
110
36
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm4
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
110
52
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm6
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
102
15
110
28
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm3
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
193
78
/
/
pshufd
0x4e
%
xmm1
%
xmm0
.
byte
102
15
112
209
231
/
/
pshufd
0xe7
%
xmm1
%
xmm2
.
byte
102
15
110
44
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm5
.
byte
102
15
126
215
/
/
movd
%
xmm2
%
edi
.
byte
102
15
110
60
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm7
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
110
4
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm0
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
102
15
110
20
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm2
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
110
12
190
/
/
movd
(
%
esi
%
edi
4
)
%
xmm1
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
102
15
98
244
/
/
punpckldq
%
xmm4
%
xmm6
.
byte
102
15
98
221
/
/
punpckldq
%
xmm5
%
xmm3
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
108
222
/
/
punpcklqdq
%
xmm6
%
xmm3
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
102
15
98
199
/
/
punpckldq
%
xmm7
%
xmm0
.
byte
102
15
98
209
/
/
punpckldq
%
xmm1
%
xmm2
.
byte
102
15
108
208
/
/
punpcklqdq
%
xmm0
%
xmm2
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
111
250
/
/
movdqa
%
xmm2
%
xmm7
.
byte
102
15
114
210
24
/
/
psrld
0x18
%
xmm2
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
107
218
/
/
packssdw
%
xmm2
%
xmm3
.
byte
102
15
111
162
252
64
0
0
/
/
movdqa
0x40fc
(
%
edx
)
%
xmm4
.
byte
102
15
114
208
16
/
/
psrld
0x10
%
xmm0
.
byte
102
15
114
215
16
/
/
psrld
0x10
%
xmm7
.
byte
102
15
114
247
16
/
/
pslld
0x10
%
xmm7
.
byte
102
15
114
231
16
/
/
psrad
0x10
%
xmm7
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
199
/
/
packssdw
%
xmm7
%
xmm0
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
219
212
/
/
pand
%
xmm4
%
xmm2
.
byte
102
15
219
196
/
/
pand
%
xmm4
%
xmm0
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
84
36
4
/
/
mov
%
edx
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_565_sse2_lowp
.
globl
_sk_load_565_sse2_lowp
FUNCTION
(
_sk_load_565_sse2_lowp
)
_sk_load_565_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
117
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm6
.
byte
15
40
109
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm5
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
cf88
<
_sk_load_565_sse2_lowp
+
0x3b
>
.
byte
94
/
/
pop
%
esi
.
byte
139
125
16
/
/
mov
0x10
(
%
ebp
)
%
edi
.
byte
119
28
/
/
ja
cfaa
<
_sk_load_565_sse2_lowp
+
0x5d
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
142
244
0
0
0
/
/
mov
0xf4
(
%
esi
%
ecx
4
)
%
ecx
.
byte
1
241
/
/
add
%
esi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
183
12
123
/
/
movzwl
(
%
ebx
%
edi
2
)
%
ecx
.
byte
102
15
110
193
/
/
movd
%
ecx
%
xmm0
.
byte
235
59
/
/
jmp
cfe5
<
_sk_load_565_sse2_lowp
+
0x98
>
.
byte
243
15
111
4
123
/
/
movdqu
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
235
52
/
/
jmp
cfe5
<
_sk_load_565_sse2_lowp
+
0x98
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
68
123
4
2
/
/
pinsrw
0x2
0x4
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
243
15
16
12
123
/
/
movss
(
%
ebx
%
edi
2
)
%
xmm1
.
byte
243
15
16
193
/
/
movss
%
xmm1
%
xmm0
.
byte
235
30
/
/
jmp
cfe5
<
_sk_load_565_sse2_lowp
+
0x98
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
68
123
12
6
/
/
pinsrw
0x6
0xc
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
102
15
196
68
123
10
5
/
/
pinsrw
0x5
0xa
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
102
15
196
68
123
8
4
/
/
pinsrw
0x4
0x8
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
102
15
18
4
123
/
/
movlpd
(
%
ebx
%
edi
2
)
%
xmm0
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
219
190
24
63
0
0
/
/
pand
0x3f18
(
%
esi
)
%
xmm7
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
113
209
5
/
/
psrlw
0x5
%
xmm1
.
byte
102
15
219
142
40
63
0
0
/
/
pand
0x3f28
(
%
esi
)
%
xmm1
.
byte
102
15
111
150
56
63
0
0
/
/
movdqa
0x3f38
(
%
esi
)
%
xmm2
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
113
208
13
/
/
psrlw
0xd
%
xmm0
.
byte
102
15
235
199
/
/
por
%
xmm7
%
xmm0
.
byte
102
15
111
249
/
/
movdqa
%
xmm1
%
xmm7
.
byte
102
15
113
247
2
/
/
psllw
0x2
%
xmm7
.
byte
102
15
113
209
4
/
/
psrlw
0x4
%
xmm1
.
byte
102
15
235
207
/
/
por
%
xmm7
%
xmm1
.
byte
102
15
111
250
/
/
movdqa
%
xmm2
%
xmm7
.
byte
102
15
113
247
3
/
/
psllw
0x3
%
xmm7
.
byte
102
15
113
210
2
/
/
psrlw
0x2
%
xmm2
.
byte
102
15
235
215
/
/
por
%
xmm7
%
xmm2
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
116
36
64
/
/
movaps
%
xmm6
0x40
(
%
esp
)
.
byte
15
41
108
36
48
/
/
movaps
%
xmm5
0x30
(
%
esp
)
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
139
93
20
/
/
mov
0x14
(
%
ebp
)
%
ebx
.
byte
137
92
36
12
/
/
mov
%
ebx
0xc
(
%
esp
)
.
byte
137
124
36
8
/
/
mov
%
edi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
15
40
158
216
62
0
0
/
/
movaps
0x3ed8
(
%
esi
)
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
41
0
/
/
sub
%
eax
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
88
/
/
pop
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
81
0
/
/
add
%
dl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_load_565_dst_sse2_lowp
.
globl
_sk_load_565_dst_sse2_lowp
FUNCTION
(
_sk_load_565_dst_sse2_lowp
)
_sk_load_565_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
d0c3
<
_sk_load_565_dst_sse2_lowp
+
0x2b
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
d0e5
<
_sk_load_565_dst_sse2_lowp
+
0x4d
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
249
0
0
0
/
/
mov
0xf9
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
183
12
115
/
/
movzwl
(
%
ebx
%
esi
2
)
%
ecx
.
byte
102
15
110
225
/
/
movd
%
ecx
%
xmm4
.
byte
235
59
/
/
jmp
d120
<
_sk_load_565_dst_sse2_lowp
+
0x88
>
.
byte
243
15
111
36
115
/
/
movdqu
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
235
52
/
/
jmp
d120
<
_sk_load_565_dst_sse2_lowp
+
0x88
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
100
115
4
2
/
/
pinsrw
0x2
0x4
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
243
15
16
44
115
/
/
movss
(
%
ebx
%
esi
2
)
%
xmm5
.
byte
243
15
16
229
/
/
movss
%
xmm5
%
xmm4
.
byte
235
30
/
/
jmp
d120
<
_sk_load_565_dst_sse2_lowp
+
0x88
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
100
115
12
6
/
/
pinsrw
0x6
0xc
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
196
100
115
10
5
/
/
pinsrw
0x5
0xa
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
196
100
115
8
4
/
/
pinsrw
0x4
0x8
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
18
36
115
/
/
movlpd
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
102
15
219
183
221
61
0
0
/
/
pand
0x3ddd
(
%
edi
)
%
xmm6
.
byte
102
15
111
236
/
/
movdqa
%
xmm4
%
xmm5
.
byte
102
15
113
213
5
/
/
psrlw
0x5
%
xmm5
.
byte
102
15
219
175
237
61
0
0
/
/
pand
0x3ded
(
%
edi
)
%
xmm5
.
byte
102
15
111
191
253
61
0
0
/
/
movdqa
0x3dfd
(
%
edi
)
%
xmm7
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
102
15
113
212
13
/
/
psrlw
0xd
%
xmm4
.
byte
102
15
235
230
/
/
por
%
xmm6
%
xmm4
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
15
113
246
2
/
/
psllw
0x2
%
xmm6
.
byte
102
15
113
213
4
/
/
psrlw
0x4
%
xmm5
.
byte
102
15
235
238
/
/
por
%
xmm6
%
xmm5
.
byte
102
15
111
247
/
/
movdqa
%
xmm7
%
xmm6
.
byte
102
15
113
246
3
/
/
psllw
0x3
%
xmm6
.
byte
102
15
113
215
2
/
/
psrlw
0x2
%
xmm7
.
byte
102
15
235
254
/
/
por
%
xmm6
%
xmm7
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
40
183
157
61
0
0
/
/
movaps
0x3d9d
(
%
edi
)
%
xmm6
.
byte
15
41
116
36
64
/
/
movaps
%
xmm6
0x40
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
127
124
36
48
/
/
movdqa
%
xmm7
0x30
(
%
esp
)
.
byte
102
15
127
100
36
16
/
/
movdqa
%
xmm4
0x10
(
%
esp
)
.
byte
102
15
127
108
36
32
/
/
movdqa
%
xmm5
0x20
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
41
0
/
/
sub
%
eax
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
88
/
/
pop
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
81
0
/
/
add
%
dl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_store_565_sse2_lowp
.
globl
_sk_store_565_sse2_lowp
FUNCTION
(
_sk_store_565_sse2_lowp
)
_sk_store_565_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
102
15
111
226
/
/
movdqa
%
xmm2
%
xmm4
.
byte
232
0
0
0
0
/
/
call
d1ea
<
_sk_store_565_sse2_lowp
+
0x12
>
.
byte
95
/
/
pop
%
edi
.
byte
15
40
109
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
102
15
113
242
8
/
/
psllw
0x8
%
xmm2
.
byte
102
15
219
151
230
60
0
0
/
/
pand
0x3ce6
(
%
edi
)
%
xmm2
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
113
246
3
/
/
psllw
0x3
%
xmm6
.
byte
102
15
219
183
246
60
0
0
/
/
pand
0x3cf6
(
%
edi
)
%
xmm6
.
byte
102
15
235
242
/
/
por
%
xmm2
%
xmm6
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
113
210
3
/
/
psrlw
0x3
%
xmm2
.
byte
102
15
235
214
/
/
por
%
xmm6
%
xmm2
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
15
40
125
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm7
.
byte
119
22
/
/
ja
d262
<
_sk_store_565_sse2_lowp
+
0x8a
>
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
3
188
143
242
0
0
0
/
/
add
0xf2
(
%
edi
%
ecx
4
)
%
edi
.
byte
255
231
/
/
jmp
*
%
edi
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
102
137
12
115
/
/
mov
%
cx
(
%
ebx
%
esi
2
)
.
byte
235
59
/
/
jmp
d29d
<
_sk_store_565_sse2_lowp
+
0xc5
>
.
byte
243
15
127
20
115
/
/
movdqu
%
xmm2
(
%
ebx
%
esi
2
)
.
byte
235
52
/
/
jmp
d29d
<
_sk_store_565_sse2_lowp
+
0xc5
>
.
byte
102
15
197
202
2
/
/
pextrw
0x2
%
xmm2
%
ecx
.
byte
102
137
76
115
4
/
/
mov
%
cx
0x4
(
%
ebx
%
esi
2
)
.
byte
102
15
126
20
115
/
/
movd
%
xmm2
(
%
ebx
%
esi
2
)
.
byte
235
35
/
/
jmp
d29d
<
_sk_store_565_sse2_lowp
+
0xc5
>
.
byte
102
15
197
202
6
/
/
pextrw
0x6
%
xmm2
%
ecx
.
byte
102
137
76
115
12
/
/
mov
%
cx
0xc
(
%
ebx
%
esi
2
)
.
byte
102
15
197
202
5
/
/
pextrw
0x5
%
xmm2
%
ecx
.
byte
102
137
76
115
10
/
/
mov
%
cx
0xa
(
%
ebx
%
esi
2
)
.
byte
102
15
197
202
4
/
/
pextrw
0x4
%
xmm2
%
ecx
.
byte
102
137
76
115
8
/
/
mov
%
cx
0x8
(
%
ebx
%
esi
2
)
.
byte
102
15
214
20
115
/
/
movq
%
xmm2
(
%
ebx
%
esi
2
)
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
108
36
64
/
/
movaps
%
xmm5
0x40
(
%
esp
)
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
41
124
36
16
/
/
movaps
%
xmm7
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
110
/
/
outsb
%
ds
:
(
%
esi
)
(
%
dx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
137
0
0
0
127
/
/
add
%
cl
0x7f000000
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
174
0
0
0
164
/
/
add
%
ch
-
0x5c000000
(
%
esi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
154
0
0
0
144
/
/
add
%
bl
-
0x70000000
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_gather_565_sse2_lowp
.
globl
_sk_gather_565_sse2_lowp
FUNCTION
(
_sk_gather_565_sse2_lowp
)
_sk_gather_565_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
108
/
/
sub
0x6c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
243
15
16
98
8
/
/
movss
0x8
(
%
edx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
118
237
/
/
pcmpeqd
%
xmm5
%
xmm5
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
95
198
/
/
maxps
%
xmm6
%
xmm0
.
byte
15
95
206
/
/
maxps
%
xmm6
%
xmm1
.
byte
15
93
204
/
/
minps
%
xmm4
%
xmm1
.
byte
15
93
196
/
/
minps
%
xmm4
%
xmm0
.
byte
243
15
16
98
12
/
/
movss
0xc
(
%
edx
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
95
222
/
/
maxps
%
xmm6
%
xmm3
.
byte
15
95
214
/
/
maxps
%
xmm6
%
xmm2
.
byte
15
93
212
/
/
minps
%
xmm4
%
xmm2
.
byte
15
93
220
/
/
minps
%
xmm4
%
xmm3
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
98
4
/
/
movd
0x4
(
%
edx
)
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
102
15
112
234
245
/
/
pshufd
0xf5
%
xmm2
%
xmm5
.
byte
102
15
244
236
/
/
pmuludq
%
xmm4
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
244
242
/
/
pmuludq
%
xmm2
%
xmm6
.
byte
102
15
112
211
245
/
/
pshufd
0xf5
%
xmm3
%
xmm2
.
byte
102
15
244
212
/
/
pmuludq
%
xmm4
%
xmm2
.
byte
232
0
0
0
0
/
/
call
d370
<
_sk_gather_565_sse2_lowp
+
0x78
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
50
/
/
mov
(
%
edx
)
%
esi
.
byte
102
15
112
246
232
/
/
pshufd
0xe8
%
xmm6
%
xmm6
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
102
15
98
245
/
/
punpckldq
%
xmm5
%
xmm6
.
byte
102
15
244
227
/
/
pmuludq
%
xmm3
%
xmm4
.
byte
102
15
112
220
232
/
/
pshufd
0xe8
%
xmm4
%
xmm3
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
98
218
/
/
punpckldq
%
xmm2
%
xmm3
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
198
/
/
paddd
%
xmm6
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
254
203
/
/
paddd
%
xmm3
%
xmm1
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
208
229
/
/
pshufd
0xe5
%
xmm0
%
xmm2
.
byte
102
15
127
85
216
/
/
movdqa
%
xmm2
-
0x28
(
%
ebp
)
.
byte
102
15
112
216
78
/
/
pshufd
0x4e
%
xmm0
%
xmm3
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
15
112
233
231
/
/
pshufd
0xe7
%
xmm1
%
xmm5
.
byte
102
15
126
239
/
/
movd
%
xmm5
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
239
/
/
movd
%
edi
%
xmm5
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
231
/
/
movd
%
edi
%
xmm4
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
247
/
/
movd
%
edi
%
xmm6
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
215
/
/
movd
%
edi
%
xmm2
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
255
/
/
movd
%
edi
%
xmm7
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
207
/
/
movd
%
edi
%
xmm1
.
byte
102
15
111
69
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm0
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
15
183
20
86
/
/
movzwl
(
%
esi
%
edx
2
)
%
edx
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
102
15
97
229
/
/
punpcklwd
%
xmm5
%
xmm4
.
byte
102
15
110
239
/
/
movd
%
edi
%
xmm5
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
102
15
97
242
/
/
punpcklwd
%
xmm2
%
xmm6
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
102
15
98
244
/
/
punpckldq
%
xmm4
%
xmm6
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
102
15
97
207
/
/
punpcklwd
%
xmm7
%
xmm1
.
byte
15
40
125
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
97
197
/
/
punpcklwd
%
xmm5
%
xmm0
.
byte
102
15
98
193
/
/
punpckldq
%
xmm1
%
xmm0
.
byte
102
15
108
198
/
/
punpcklqdq
%
xmm6
%
xmm0
.
byte
102
15
111
232
/
/
movdqa
%
xmm0
%
xmm5
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
219
169
48
59
0
0
/
/
pand
0x3b30
(
%
ecx
)
%
xmm5
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
113
209
5
/
/
psrlw
0x5
%
xmm1
.
byte
102
15
219
137
64
59
0
0
/
/
pand
0x3b40
(
%
ecx
)
%
xmm1
.
byte
102
15
111
145
80
59
0
0
/
/
movdqa
0x3b50
(
%
ecx
)
%
xmm2
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
113
208
13
/
/
psrlw
0xd
%
xmm0
.
byte
102
15
235
197
/
/
por
%
xmm5
%
xmm0
.
byte
102
15
111
233
/
/
movdqa
%
xmm1
%
xmm5
.
byte
102
15
113
245
2
/
/
psllw
0x2
%
xmm5
.
byte
102
15
113
209
4
/
/
psrlw
0x4
%
xmm1
.
byte
102
15
235
205
/
/
por
%
xmm5
%
xmm1
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
113
245
3
/
/
psllw
0x3
%
xmm5
.
byte
102
15
113
210
2
/
/
psrlw
0x2
%
xmm2
.
byte
102
15
235
213
/
/
por
%
xmm5
%
xmm2
.
byte
15
40
109
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm5
.
byte
141
88
8
/
/
lea
0x8
(
%
eax
)
%
ebx
.
byte
15
41
108
36
64
/
/
movaps
%
xmm5
0x40
(
%
esp
)
.
byte
15
41
124
36
48
/
/
movaps
%
xmm7
0x30
(
%
esp
)
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
92
36
4
/
/
mov
%
ebx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
15
40
153
240
58
0
0
/
/
movaps
0x3af0
(
%
ecx
)
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
108
/
/
add
0x6c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_4444_sse2_lowp
.
globl
_sk_load_4444_sse2_lowp
FUNCTION
(
_sk_load_4444_sse2_lowp
)
_sk_load_4444_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
d52a
<
_sk_load_4444_sse2_lowp
+
0x37
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
d54c
<
_sk_load_4444_sse2_lowp
+
0x59
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
250
0
0
0
/
/
mov
0xfa
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
183
12
115
/
/
movzwl
(
%
ebx
%
esi
2
)
%
ecx
.
byte
102
15
110
225
/
/
movd
%
ecx
%
xmm4
.
byte
235
59
/
/
jmp
d587
<
_sk_load_4444_sse2_lowp
+
0x94
>
.
byte
243
15
111
36
115
/
/
movdqu
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
235
52
/
/
jmp
d587
<
_sk_load_4444_sse2_lowp
+
0x94
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
100
115
4
2
/
/
pinsrw
0x2
0x4
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
243
15
16
4
115
/
/
movss
(
%
ebx
%
esi
2
)
%
xmm0
.
byte
243
15
16
224
/
/
movss
%
xmm0
%
xmm4
.
byte
235
30
/
/
jmp
d587
<
_sk_load_4444_sse2_lowp
+
0x94
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
100
115
12
6
/
/
pinsrw
0x6
0xc
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
196
100
115
10
5
/
/
pinsrw
0x5
0xa
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
196
100
115
8
4
/
/
pinsrw
0x4
0x8
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
18
36
115
/
/
movlpd
(
%
ebx
%
esi
2
)
%
xmm4
.
byte
102
15
111
204
/
/
movdqa
%
xmm4
%
xmm1
.
byte
102
15
113
209
12
/
/
psrlw
0xc
%
xmm1
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
111
135
198
57
0
0
/
/
movdqa
0x39c6
(
%
edi
)
%
xmm0
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
102
15
113
211
4
/
/
psrlw
0x4
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
102
15
219
224
/
/
pand
%
xmm0
%
xmm4
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
4
/
/
psllw
0x4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
4
/
/
psllw
0x4
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
242
4
/
/
psllw
0x4
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
102
15
113
243
4
/
/
psllw
0x4
%
xmm3
.
byte
102
15
235
220
/
/
por
%
xmm4
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
41
0
/
/
sub
%
eax
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
88
/
/
pop
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
81
0
/
/
add
%
dl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_load_4444_dst_sse2_lowp
.
globl
_sk_load_4444_dst_sse2_lowp
FUNCTION
(
_sk_load_4444_dst_sse2_lowp
)
_sk_load_4444_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
108
/
/
sub
0x6c
%
esp
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
d66f
<
_sk_load_4444_dst_sse2_lowp
+
0x2f
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
d691
<
_sk_load_4444_dst_sse2_lowp
+
0x51
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
253
0
0
0
/
/
mov
0xfd
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
183
12
115
/
/
movzwl
(
%
ebx
%
esi
2
)
%
ecx
.
byte
102
15
110
249
/
/
movd
%
ecx
%
xmm7
.
byte
235
59
/
/
jmp
d6cc
<
_sk_load_4444_dst_sse2_lowp
+
0x8c
>
.
byte
243
15
111
60
115
/
/
movdqu
(
%
ebx
%
esi
2
)
%
xmm7
.
byte
235
52
/
/
jmp
d6cc
<
_sk_load_4444_dst_sse2_lowp
+
0x8c
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
196
124
115
4
2
/
/
pinsrw
0x2
0x4
(
%
ebx
%
esi
2
)
%
xmm7
.
byte
243
15
16
44
115
/
/
movss
(
%
ebx
%
esi
2
)
%
xmm5
.
byte
243
15
16
253
/
/
movss
%
xmm5
%
xmm7
.
byte
235
30
/
/
jmp
d6cc
<
_sk_load_4444_dst_sse2_lowp
+
0x8c
>
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
196
124
115
12
6
/
/
pinsrw
0x6
0xc
(
%
ebx
%
esi
2
)
%
xmm7
.
byte
102
15
196
124
115
10
5
/
/
pinsrw
0x5
0xa
(
%
ebx
%
esi
2
)
%
xmm7
.
byte
102
15
196
124
115
8
4
/
/
pinsrw
0x4
0x8
(
%
ebx
%
esi
2
)
%
xmm7
.
byte
102
15
18
60
115
/
/
movlpd
(
%
ebx
%
esi
2
)
%
xmm7
.
byte
102
15
111
239
/
/
movdqa
%
xmm7
%
xmm5
.
byte
102
15
113
213
12
/
/
psrlw
0xc
%
xmm5
.
byte
102
15
111
247
/
/
movdqa
%
xmm7
%
xmm6
.
byte
102
15
113
214
8
/
/
psrlw
0x8
%
xmm6
.
byte
102
15
111
159
129
56
0
0
/
/
movdqa
0x3881
(
%
edi
)
%
xmm3
.
byte
102
15
219
243
/
/
pand
%
xmm3
%
xmm6
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
113
212
4
/
/
psrlw
0x4
%
xmm4
.
byte
102
15
219
227
/
/
pand
%
xmm3
%
xmm4
.
byte
102
15
219
251
/
/
pand
%
xmm3
%
xmm7
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
102
15
113
243
4
/
/
psllw
0x4
%
xmm3
.
byte
102
15
235
221
/
/
por
%
xmm5
%
xmm3
.
byte
102
15
111
238
/
/
movdqa
%
xmm6
%
xmm5
.
byte
102
15
113
245
4
/
/
psllw
0x4
%
xmm5
.
byte
102
15
235
238
/
/
por
%
xmm6
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
113
246
4
/
/
psllw
0x4
%
xmm6
.
byte
102
15
235
244
/
/
por
%
xmm4
%
xmm6
.
byte
102
15
111
231
/
/
movdqa
%
xmm7
%
xmm4
.
byte
102
15
113
244
4
/
/
psllw
0x4
%
xmm4
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
102
15
127
92
36
16
/
/
movdqa
%
xmm3
0x10
(
%
esp
)
.
byte
102
15
127
116
36
48
/
/
movdqa
%
xmm6
0x30
(
%
esp
)
.
byte
102
15
127
108
36
32
/
/
movdqa
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
93
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
108
/
/
add
0x6c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
41
0
/
/
sub
%
eax
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
88
/
/
pop
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
81
0
/
/
add
%
dl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_store_4444_sse2_lowp
.
globl
_sk_store_4444_sse2_lowp
FUNCTION
(
_sk_store_4444_sse2_lowp
)
_sk_store_4444_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
232
0
0
0
0
/
/
call
d79a
<
_sk_store_4444_sse2_lowp
+
0x12
>
.
byte
94
/
/
pop
%
esi
.
byte
102
15
111
216
/
/
movdqa
%
xmm0
%
xmm3
.
byte
102
15
113
243
8
/
/
psllw
0x8
%
xmm3
.
byte
102
15
219
158
102
55
0
0
/
/
pand
0x3766
(
%
esi
)
%
xmm3
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
113
246
4
/
/
psllw
0x4
%
xmm6
.
byte
102
15
219
182
118
55
0
0
/
/
pand
0x3776
(
%
esi
)
%
xmm6
.
byte
102
15
235
243
/
/
por
%
xmm3
%
xmm6
.
byte
102
15
111
174
134
55
0
0
/
/
movdqa
0x3786
(
%
esi
)
%
xmm5
.
byte
102
15
219
234
/
/
pand
%
xmm2
%
xmm5
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
102
15
113
211
4
/
/
psrlw
0x4
%
xmm3
.
byte
102
15
235
221
/
/
por
%
xmm5
%
xmm3
.
byte
15
40
109
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
235
222
/
/
por
%
xmm6
%
xmm3
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
139
125
16
/
/
mov
0x10
(
%
ebp
)
%
edi
.
byte
15
40
125
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm7
.
byte
119
22
/
/
ja
d822
<
_sk_store_4444_sse2_lowp
+
0x9a
>
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
3
180
142
2
1
0
0
/
/
add
0x102
(
%
esi
%
ecx
4
)
%
esi
.
byte
255
230
/
/
jmp
*
%
esi
.
byte
102
15
126
217
/
/
movd
%
xmm3
%
ecx
.
byte
102
137
12
123
/
/
mov
%
cx
(
%
ebx
%
edi
2
)
.
byte
235
59
/
/
jmp
d85d
<
_sk_store_4444_sse2_lowp
+
0xd5
>
.
byte
243
15
127
28
123
/
/
movdqu
%
xmm3
(
%
ebx
%
edi
2
)
.
byte
235
52
/
/
jmp
d85d
<
_sk_store_4444_sse2_lowp
+
0xd5
>
.
byte
102
15
197
203
2
/
/
pextrw
0x2
%
xmm3
%
ecx
.
byte
102
137
76
123
4
/
/
mov
%
cx
0x4
(
%
ebx
%
edi
2
)
.
byte
102
15
126
28
123
/
/
movd
%
xmm3
(
%
ebx
%
edi
2
)
.
byte
235
35
/
/
jmp
d85d
<
_sk_store_4444_sse2_lowp
+
0xd5
>
.
byte
102
15
197
203
6
/
/
pextrw
0x6
%
xmm3
%
ecx
.
byte
102
137
76
123
12
/
/
mov
%
cx
0xc
(
%
ebx
%
edi
2
)
.
byte
102
15
197
203
5
/
/
pextrw
0x5
%
xmm3
%
ecx
.
byte
102
137
76
123
10
/
/
mov
%
cx
0xa
(
%
ebx
%
edi
2
)
.
byte
102
15
197
203
4
/
/
pextrw
0x4
%
xmm3
%
ecx
.
byte
102
137
76
123
8
/
/
mov
%
cx
0x8
(
%
ebx
%
edi
2
)
.
byte
102
15
214
28
123
/
/
movq
%
xmm3
(
%
ebx
%
edi
2
)
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
15
41
108
36
48
/
/
movaps
%
xmm5
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
41
124
36
16
/
/
movaps
%
xmm7
0x10
(
%
esp
)
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
124
36
8
/
/
mov
%
edi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
126
0
/
/
jle
d89e
<
_sk_store_4444_sse2_lowp
+
0x116
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
153
/
/
cltd
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
143
0
0
0
190
/
/
add
%
cl
-
0x42000000
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
180
0
0
0
170
0
/
/
add
%
dh
0xaa0000
(
%
eax
%
eax
1
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
160
/
/
.
byte
0xa0
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_gather_4444_sse2_lowp
.
globl
_sk_gather_4444_sse2_lowp
FUNCTION
(
_sk_gather_4444_sse2_lowp
)
_sk_gather_4444_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
d8c5
<
_sk_gather_4444_sse2_lowp
+
0xd
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
16
103
8
/
/
movss
0x8
(
%
edi
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
118
237
/
/
pcmpeqd
%
xmm5
%
xmm5
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
87
246
/
/
xorps
%
xmm6
%
xmm6
.
byte
15
95
198
/
/
maxps
%
xmm6
%
xmm0
.
byte
15
95
206
/
/
maxps
%
xmm6
%
xmm1
.
byte
15
93
204
/
/
minps
%
xmm4
%
xmm1
.
byte
15
93
196
/
/
minps
%
xmm4
%
xmm0
.
byte
243
15
16
103
12
/
/
movss
0xc
(
%
edi
)
%
xmm4
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
102
15
254
229
/
/
paddd
%
xmm5
%
xmm4
.
byte
15
95
222
/
/
maxps
%
xmm6
%
xmm3
.
byte
15
95
214
/
/
maxps
%
xmm6
%
xmm2
.
byte
15
93
212
/
/
minps
%
xmm4
%
xmm2
.
byte
15
93
220
/
/
minps
%
xmm4
%
xmm3
.
byte
139
55
/
/
mov
(
%
edi
)
%
esi
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
102
15
110
103
4
/
/
movd
0x4
(
%
edi
)
%
xmm4
.
byte
102
15
112
228
0
/
/
pshufd
0x0
%
xmm4
%
xmm4
.
byte
102
15
112
234
245
/
/
pshufd
0xf5
%
xmm2
%
xmm5
.
byte
102
15
244
236
/
/
pmuludq
%
xmm4
%
xmm5
.
byte
102
15
111
244
/
/
movdqa
%
xmm4
%
xmm6
.
byte
102
15
112
251
245
/
/
pshufd
0xf5
%
xmm3
%
xmm7
.
byte
102
15
244
252
/
/
pmuludq
%
xmm4
%
xmm7
.
byte
102
15
244
226
/
/
pmuludq
%
xmm2
%
xmm4
.
byte
102
15
112
212
232
/
/
pshufd
0xe8
%
xmm4
%
xmm2
.
byte
102
15
112
229
232
/
/
pshufd
0xe8
%
xmm5
%
xmm4
.
byte
102
15
98
212
/
/
punpckldq
%
xmm4
%
xmm2
.
byte
102
15
244
243
/
/
pmuludq
%
xmm3
%
xmm6
.
byte
102
15
112
222
232
/
/
pshufd
0xe8
%
xmm6
%
xmm3
.
byte
102
15
112
231
232
/
/
pshufd
0xe8
%
xmm7
%
xmm4
.
byte
102
15
98
220
/
/
punpckldq
%
xmm4
%
xmm3
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
254
203
/
/
paddd
%
xmm3
%
xmm1
.
byte
102
15
112
248
78
/
/
pshufd
0x4e
%
xmm0
%
xmm7
.
byte
102
15
112
216
231
/
/
pshufd
0xe7
%
xmm0
%
xmm3
.
byte
102
15
112
225
78
/
/
pshufd
0x4e
%
xmm1
%
xmm4
.
byte
102
15
112
233
231
/
/
pshufd
0xe7
%
xmm1
%
xmm5
.
byte
102
15
126
239
/
/
movd
%
xmm5
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
239
/
/
movd
%
edi
%
xmm5
.
byte
102
15
126
231
/
/
movd
%
xmm4
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
247
/
/
movd
%
edi
%
xmm6
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
102
15
112
201
229
/
/
pshufd
0xe5
%
xmm1
%
xmm1
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
215
/
/
movd
%
edi
%
xmm2
.
byte
102
15
126
207
/
/
movd
%
xmm1
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
231
/
/
movd
%
edi
%
xmm4
.
byte
102
15
126
223
/
/
movd
%
xmm3
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
223
/
/
movd
%
edi
%
xmm3
.
byte
102
15
126
255
/
/
movd
%
xmm7
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
207
/
/
movd
%
edi
%
xmm1
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
102
15
110
255
/
/
movd
%
edi
%
xmm7
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
15
183
60
126
/
/
movzwl
(
%
esi
%
edi
2
)
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
102
15
110
199
/
/
movd
%
edi
%
xmm0
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
102
15
97
245
/
/
punpcklwd
%
xmm5
%
xmm6
.
byte
102
15
97
212
/
/
punpcklwd
%
xmm4
%
xmm2
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
98
214
/
/
punpckldq
%
xmm6
%
xmm2
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
102
15
97
203
/
/
punpcklwd
%
xmm3
%
xmm1
.
byte
102
15
97
248
/
/
punpcklwd
%
xmm0
%
xmm7
.
byte
102
15
98
249
/
/
punpckldq
%
xmm1
%
xmm7
.
byte
102
15
108
250
/
/
punpcklqdq
%
xmm2
%
xmm7
.
byte
102
15
111
207
/
/
movdqa
%
xmm7
%
xmm1
.
byte
102
15
113
209
12
/
/
psrlw
0xc
%
xmm1
.
byte
102
15
111
215
/
/
movdqa
%
xmm7
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
111
130
43
54
0
0
/
/
movdqa
0x362b
(
%
edx
)
%
xmm0
.
byte
102
15
219
208
/
/
pand
%
xmm0
%
xmm2
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
102
15
113
211
4
/
/
psrlw
0x4
%
xmm3
.
byte
102
15
219
216
/
/
pand
%
xmm0
%
xmm3
.
byte
102
15
219
248
/
/
pand
%
xmm0
%
xmm7
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
113
240
4
/
/
psllw
0x4
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
4
/
/
psllw
0x4
%
xmm1
.
byte
102
15
235
202
/
/
por
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
242
4
/
/
psllw
0x4
%
xmm2
.
byte
102
15
235
211
/
/
por
%
xmm3
%
xmm2
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
102
15
113
243
4
/
/
psllw
0x4
%
xmm3
.
byte
102
15
235
223
/
/
por
%
xmm7
%
xmm3
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
141
81
8
/
/
lea
0x8
(
%
ecx
)
%
edx
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
84
36
4
/
/
mov
%
edx
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_a8_sse2_lowp
.
globl
_sk_load_a8_sse2_lowp
FUNCTION
(
_sk_load_a8_sse2_lowp
)
_sk_load_a8_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
111
69
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm0
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
dae2
<
_sk_load_a8_sse2_lowp
+
0x3a
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
db04
<
_sk_load_a8_sse2_lowp
+
0x5c
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
198
0
0
0
/
/
mov
0xc6
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
182
12
51
/
/
movzbl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
217
/
/
movd
%
ecx
%
xmm3
.
byte
235
90
/
/
jmp
db5e
<
_sk_load_a8_sse2_lowp
+
0xb6
>
.
byte
243
15
126
28
51
/
/
movq
(
%
ebx
%
esi
1
)
%
xmm3
.
byte
102
15
96
216
/
/
punpcklbw
%
xmm0
%
xmm3
.
byte
235
79
/
/
jmp
db5e
<
_sk_load_a8_sse2_lowp
+
0xb6
>
.
byte
15
182
76
51
2
/
/
movzbl
0x2
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
217
2
/
/
pinsrw
0x2
%
ecx
%
xmm3
.
byte
15
183
12
51
/
/
movzwl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
233
/
/
movd
%
ecx
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
243
15
16
221
/
/
movss
%
xmm5
%
xmm3
.
byte
235
47
/
/
jmp
db5e
<
_sk_load_a8_sse2_lowp
+
0xb6
>
.
byte
15
182
76
51
6
/
/
movzbl
0x6
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
217
6
/
/
pinsrw
0x6
%
ecx
%
xmm3
.
byte
15
182
76
51
5
/
/
movzbl
0x5
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
217
5
/
/
pinsrw
0x5
%
ecx
%
xmm3
.
byte
15
182
76
51
4
/
/
movzbl
0x4
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
217
4
/
/
pinsrw
0x4
%
ecx
%
xmm3
.
byte
102
15
110
44
51
/
/
movd
(
%
ebx
%
esi
1
)
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
242
15
16
221
/
/
movsd
%
xmm5
%
xmm3
.
byte
102
15
219
159
126
51
0
0
/
/
pand
0x337e
(
%
edi
)
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
102
15
127
68
36
16
/
/
movdqa
%
xmm0
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
59
0
/
/
cmp
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
45
0
0
0
111
/
/
sub
0x6f000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
101
0
/
/
add
%
ah
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
91
/
/
pop
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
77
0
/
/
add
%
cl
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
HIDDEN
_sk_load_a8_dst_sse2_lowp
.
globl
_sk_load_a8_dst_sse2_lowp
FUNCTION
(
_sk_load_a8_dst_sse2_lowp
)
_sk_load_a8_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
dbed
<
_sk_load_a8_dst_sse2_lowp
+
0x29
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
dc0f
<
_sk_load_a8_dst_sse2_lowp
+
0x4b
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
195
0
0
0
/
/
mov
0xc3
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
182
12
51
/
/
movzbl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
225
/
/
movd
%
ecx
%
xmm4
.
byte
235
90
/
/
jmp
dc69
<
_sk_load_a8_dst_sse2_lowp
+
0xa5
>
.
byte
243
15
126
36
51
/
/
movq
(
%
ebx
%
esi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
235
79
/
/
jmp
dc69
<
_sk_load_a8_dst_sse2_lowp
+
0xa5
>
.
byte
15
182
76
51
2
/
/
movzbl
0x2
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
225
2
/
/
pinsrw
0x2
%
ecx
%
xmm4
.
byte
15
183
12
51
/
/
movzwl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
233
/
/
movd
%
ecx
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
243
15
16
229
/
/
movss
%
xmm5
%
xmm4
.
byte
235
47
/
/
jmp
dc69
<
_sk_load_a8_dst_sse2_lowp
+
0xa5
>
.
byte
15
182
76
51
6
/
/
movzbl
0x6
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
225
6
/
/
pinsrw
0x6
%
ecx
%
xmm4
.
byte
15
182
76
51
5
/
/
movzbl
0x5
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
225
5
/
/
pinsrw
0x5
%
ecx
%
xmm4
.
byte
15
182
76
51
4
/
/
movzbl
0x4
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
225
4
/
/
pinsrw
0x4
%
ecx
%
xmm4
.
byte
102
15
110
44
51
/
/
movd
(
%
ebx
%
esi
1
)
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
102
15
219
167
115
50
0
0
/
/
pand
0x3273
(
%
edi
)
%
xmm4
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
102
15
127
100
36
64
/
/
movdqa
%
xmm4
0x40
(
%
esp
)
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
127
100
36
48
/
/
movdqa
%
xmm4
0x30
(
%
esp
)
.
byte
102
15
127
100
36
32
/
/
movdqa
%
xmm4
0x20
(
%
esp
)
.
byte
102
15
127
100
36
16
/
/
movdqa
%
xmm4
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
59
0
/
/
cmp
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
45
0
0
0
111
/
/
sub
0x6f000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
101
0
/
/
add
%
ah
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
91
/
/
pop
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
77
0
/
/
add
%
cl
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
HIDDEN
_sk_store_a8_sse2_lowp
.
globl
_sk_store_a8_sse2_lowp
FUNCTION
(
_sk_store_a8_sse2_lowp
)
_sk_store_a8_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
121
4
/
/
mov
0x4
(
%
ecx
)
%
edi
.
byte
15
175
125
20
/
/
imul
0x14
(
%
ebp
)
%
edi
.
byte
3
57
/
/
add
(
%
ecx
)
%
edi
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
dd05
<
_sk_store_a8_sse2_lowp
+
0x39
>
.
byte
91
/
/
pop
%
ebx
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
26
/
/
ja
dd25
<
_sk_store_a8_sse2_lowp
+
0x59
>
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
139
223
0
0
0
/
/
mov
0xdf
(
%
ebx
%
ecx
4
)
%
ecx
.
byte
1
217
/
/
add
%
ebx
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
102
15
126
217
/
/
movd
%
xmm3
%
ecx
.
byte
136
12
55
/
/
mov
%
cl
(
%
edi
%
esi
1
)
.
byte
233
136
0
0
0
/
/
jmp
ddad
<
_sk_store_a8_sse2_lowp
+
0xe1
>
.
byte
102
15
111
163
91
49
0
0
/
/
movdqa
0x315b
(
%
ebx
)
%
xmm4
.
byte
102
15
219
227
/
/
pand
%
xmm3
%
xmm4
.
byte
102
15
103
228
/
/
packuswb
%
xmm4
%
xmm4
.
byte
102
15
214
36
55
/
/
movq
%
xmm4
(
%
edi
%
esi
1
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
235
109
/
/
jmp
ddad
<
_sk_store_a8_sse2_lowp
+
0xe1
>
.
byte
102
15
197
203
2
/
/
pextrw
0x2
%
xmm3
%
ecx
.
byte
136
76
55
2
/
/
mov
%
cl
0x2
(
%
edi
%
esi
1
)
.
byte
102
15
111
163
91
49
0
0
/
/
movdqa
0x315b
(
%
ebx
)
%
xmm4
.
byte
102
15
219
227
/
/
pand
%
xmm3
%
xmm4
.
byte
102
15
103
228
/
/
packuswb
%
xmm4
%
xmm4
.
byte
102
15
126
225
/
/
movd
%
xmm4
%
ecx
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
102
137
12
55
/
/
mov
%
cx
(
%
edi
%
esi
1
)
.
byte
235
70
/
/
jmp
ddad
<
_sk_store_a8_sse2_lowp
+
0xe1
>
.
byte
102
15
197
203
6
/
/
pextrw
0x6
%
xmm3
%
ecx
.
byte
136
76
55
6
/
/
mov
%
cl
0x6
(
%
edi
%
esi
1
)
.
byte
102
15
197
203
5
/
/
pextrw
0x5
%
xmm3
%
ecx
.
byte
136
76
55
5
/
/
mov
%
cl
0x5
(
%
edi
%
esi
1
)
.
byte
102
15
197
203
4
/
/
pextrw
0x4
%
xmm3
%
ecx
.
byte
136
76
55
4
/
/
mov
%
cl
0x4
(
%
edi
%
esi
1
)
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
102
15
111
163
91
49
0
0
/
/
movdqa
0x315b
(
%
ebx
)
%
xmm4
.
byte
102
15
219
227
/
/
pand
%
xmm3
%
xmm4
.
byte
102
15
103
228
/
/
packuswb
%
xmm4
%
xmm4
.
byte
102
15
126
36
55
/
/
movd
%
xmm4
(
%
edi
%
esi
1
)
.
byte
15
40
229
/
/
movaps
%
xmm5
%
xmm4
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
20
0
/
/
adc
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
59
/
/
add
%
bh
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
125
0
/
/
add
%
bh
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
116
0
/
/
je
ddf6
<
_sk_store_a8_sse2_lowp
+
0x12a
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
107
0
0
/
/
imul
0x0
(
%
eax
)
%
eax
.
byte
0
98
0
/
/
add
%
ah
0x0
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
HIDDEN
_sk_gather_a8_sse2_lowp
.
globl
_sk_gather_a8_sse2_lowp
FUNCTION
(
_sk_gather_a8_sse2_lowp
)
_sk_gather_a8_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
105
8
/
/
movss
0x8
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
102
15
118
246
/
/
pcmpeqd
%
xmm6
%
xmm6
.
byte
102
15
254
238
/
/
paddd
%
xmm6
%
xmm5
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
196
/
/
maxps
%
xmm4
%
xmm0
.
byte
15
95
204
/
/
maxps
%
xmm4
%
xmm1
.
byte
15
93
205
/
/
minps
%
xmm5
%
xmm1
.
byte
15
93
197
/
/
minps
%
xmm5
%
xmm0
.
byte
243
15
16
105
12
/
/
movss
0xc
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
102
15
254
238
/
/
paddd
%
xmm6
%
xmm5
.
byte
15
95
220
/
/
maxps
%
xmm4
%
xmm3
.
byte
15
95
212
/
/
maxps
%
xmm4
%
xmm2
.
byte
15
93
213
/
/
minps
%
xmm5
%
xmm2
.
byte
15
93
221
/
/
minps
%
xmm5
%
xmm3
.
byte
243
15
91
234
/
/
cvttps2dq
%
xmm2
%
xmm5
.
byte
102
15
110
81
4
/
/
movd
0x4
(
%
ecx
)
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
102
15
111
242
/
/
movdqa
%
xmm2
%
xmm6
.
byte
102
15
244
245
/
/
pmuludq
%
xmm5
%
xmm6
.
byte
102
15
112
237
245
/
/
pshufd
0xf5
%
xmm5
%
xmm5
.
byte
102
15
244
234
/
/
pmuludq
%
xmm2
%
xmm5
.
byte
102
15
112
246
232
/
/
pshufd
0xe8
%
xmm6
%
xmm6
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
102
15
98
245
/
/
punpckldq
%
xmm5
%
xmm6
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
198
/
/
paddd
%
xmm6
%
xmm0
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
15
126
234
/
/
movd
%
xmm5
%
edx
.
byte
102
15
112
232
231
/
/
pshufd
0xe7
%
xmm0
%
xmm5
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
139
9
/
/
mov
(
%
ecx
)
%
ecx
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
102
15
126
199
/
/
movd
%
xmm0
%
edi
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
15
182
20
17
/
/
movzbl
(
%
ecx
%
edx
1
)
%
edx
.
byte
15
182
52
49
/
/
movzbl
(
%
ecx
%
esi
1
)
%
esi
.
byte
193
230
8
/
/
shl
0x8
%
esi
.
byte
9
214
/
/
or
%
edx
%
esi
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
195
245
/
/
pshufd
0xf5
%
xmm3
%
xmm0
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
15
182
60
57
/
/
movzbl
(
%
ecx
%
edi
1
)
%
edi
.
byte
15
182
20
17
/
/
movzbl
(
%
ecx
%
edx
1
)
%
edx
.
byte
193
226
8
/
/
shl
0x8
%
edx
.
byte
9
250
/
/
or
%
edi
%
edx
.
byte
139
125
8
/
/
mov
0x8
(
%
ebp
)
%
edi
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
98
208
/
/
punpckldq
%
xmm0
%
xmm2
.
byte
243
15
91
193
/
/
cvttps2dq
%
xmm1
%
xmm0
.
byte
102
15
254
194
/
/
paddd
%
xmm2
%
xmm0
.
byte
102
15
110
218
/
/
movd
%
edx
%
xmm3
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
200
229
/
/
pshufd
0xe5
%
xmm0
%
xmm1
.
byte
102
15
196
222
1
/
/
pinsrw
0x1
%
esi
%
xmm3
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
102
15
112
200
78
/
/
pshufd
0x4e
%
xmm0
%
xmm1
.
byte
15
182
20
17
/
/
movzbl
(
%
ecx
%
edx
1
)
%
edx
.
byte
15
182
52
49
/
/
movzbl
(
%
ecx
%
esi
1
)
%
esi
.
byte
193
230
8
/
/
shl
0x8
%
esi
.
byte
9
214
/
/
or
%
edx
%
esi
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
112
192
231
/
/
pshufd
0xe7
%
xmm0
%
xmm0
.
byte
102
15
196
222
2
/
/
pinsrw
0x2
%
esi
%
xmm3
.
byte
102
15
126
198
/
/
movd
%
xmm0
%
esi
.
byte
15
182
20
17
/
/
movzbl
(
%
ecx
%
edx
1
)
%
edx
.
byte
15
182
12
49
/
/
movzbl
(
%
ecx
%
esi
1
)
%
ecx
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
193
225
8
/
/
shl
0x8
%
ecx
.
byte
9
209
/
/
or
%
edx
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
15
40
85
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm2
.
byte
102
15
196
217
3
/
/
pinsrw
0x3
%
ecx
%
xmm3
.
byte
102
15
96
220
/
/
punpcklbw
%
xmm4
%
xmm3
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
41
84
36
48
/
/
movaps
%
xmm2
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
60
36
/
/
mov
%
edi
(
%
esp
)
.
byte
15
87
192
/
/
xorps
%
xmm0
%
xmm0
.
byte
15
87
201
/
/
xorps
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_load_g8_sse2_lowp
.
globl
_sk_load_g8_sse2_lowp
FUNCTION
(
_sk_load_g8_sse2_lowp
)
_sk_load_g8_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
15
40
93
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm3
.
byte
15
40
85
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm2
.
byte
15
40
77
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm1
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
dfb8
<
_sk_load_g8_sse2_lowp
+
0x39
>
.
byte
94
/
/
pop
%
esi
.
byte
139
125
16
/
/
mov
0x10
(
%
ebp
)
%
edi
.
byte
119
28
/
/
ja
dfda
<
_sk_load_g8_sse2_lowp
+
0x5b
>
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
142
200
0
0
0
/
/
mov
0xc8
(
%
esi
%
ecx
4
)
%
ecx
.
byte
1
241
/
/
add
%
esi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
182
12
59
/
/
movzbl
(
%
ebx
%
edi
1
)
%
ecx
.
byte
102
15
110
193
/
/
movd
%
ecx
%
xmm0
.
byte
235
90
/
/
jmp
e034
<
_sk_load_g8_sse2_lowp
+
0xb5
>
.
byte
243
15
126
4
59
/
/
movq
(
%
ebx
%
edi
1
)
%
xmm0
.
byte
102
15
96
192
/
/
punpcklbw
%
xmm0
%
xmm0
.
byte
235
79
/
/
jmp
e034
<
_sk_load_g8_sse2_lowp
+
0xb5
>
.
byte
15
182
76
59
2
/
/
movzbl
0x2
(
%
ebx
%
edi
1
)
%
ecx
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
193
2
/
/
pinsrw
0x2
%
ecx
%
xmm0
.
byte
15
183
12
59
/
/
movzwl
(
%
ebx
%
edi
1
)
%
ecx
.
byte
102
15
110
233
/
/
movd
%
ecx
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
243
15
16
197
/
/
movss
%
xmm5
%
xmm0
.
byte
235
47
/
/
jmp
e034
<
_sk_load_g8_sse2_lowp
+
0xb5
>
.
byte
15
182
76
59
6
/
/
movzbl
0x6
(
%
ebx
%
edi
1
)
%
ecx
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
196
193
6
/
/
pinsrw
0x6
%
ecx
%
xmm0
.
byte
15
182
76
59
5
/
/
movzbl
0x5
(
%
ebx
%
edi
1
)
%
ecx
.
byte
102
15
196
193
5
/
/
pinsrw
0x5
%
ecx
%
xmm0
.
byte
15
182
76
59
4
/
/
movzbl
0x4
(
%
ebx
%
edi
1
)
%
ecx
.
byte
102
15
196
193
4
/
/
pinsrw
0x4
%
ecx
%
xmm0
.
byte
102
15
110
44
59
/
/
movd
(
%
ebx
%
edi
1
)
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
242
15
16
197
/
/
movsd
%
xmm5
%
xmm0
.
byte
102
15
219
134
168
46
0
0
/
/
pand
0x2ea8
(
%
esi
)
%
xmm0
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
41
92
36
48
/
/
movaps
%
xmm3
0x30
(
%
esp
)
.
byte
15
41
84
36
32
/
/
movaps
%
xmm2
0x20
(
%
esp
)
.
byte
15
41
76
36
16
/
/
movaps
%
xmm1
0x10
(
%
esp
)
.
byte
139
93
20
/
/
mov
0x14
(
%
ebp
)
%
ebx
.
byte
137
92
36
12
/
/
mov
%
ebx
0xc
(
%
esp
)
.
byte
137
124
36
8
/
/
mov
%
edi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
15
40
158
168
46
0
0
/
/
movaps
0x2ea8
(
%
esi
)
%
xmm3
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
59
0
/
/
cmp
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
45
0
0
0
111
/
/
sub
0x6f000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
101
0
/
/
add
%
ah
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
91
/
/
pop
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
77
0
/
/
add
%
cl
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
HIDDEN
_sk_load_g8_dst_sse2_lowp
.
globl
_sk_load_g8_dst_sse2_lowp
FUNCTION
(
_sk_load_g8_dst_sse2_lowp
)
_sk_load_g8_dst_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
e0c5
<
_sk_load_g8_dst_sse2_lowp
+
0x29
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
e0e7
<
_sk_load_g8_dst_sse2_lowp
+
0x4b
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
195
0
0
0
/
/
mov
0xc3
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
182
12
51
/
/
movzbl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
225
/
/
movd
%
ecx
%
xmm4
.
byte
235
90
/
/
jmp
e141
<
_sk_load_g8_dst_sse2_lowp
+
0xa5
>
.
byte
243
15
126
36
51
/
/
movq
(
%
ebx
%
esi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
235
79
/
/
jmp
e141
<
_sk_load_g8_dst_sse2_lowp
+
0xa5
>
.
byte
15
182
76
51
2
/
/
movzbl
0x2
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
225
2
/
/
pinsrw
0x2
%
ecx
%
xmm4
.
byte
15
183
12
51
/
/
movzwl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
233
/
/
movd
%
ecx
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
243
15
16
229
/
/
movss
%
xmm5
%
xmm4
.
byte
235
47
/
/
jmp
e141
<
_sk_load_g8_dst_sse2_lowp
+
0xa5
>
.
byte
15
182
76
51
6
/
/
movzbl
0x6
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
225
6
/
/
pinsrw
0x6
%
ecx
%
xmm4
.
byte
15
182
76
51
5
/
/
movzbl
0x5
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
225
5
/
/
pinsrw
0x5
%
ecx
%
xmm4
.
byte
15
182
76
51
4
/
/
movzbl
0x4
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
225
4
/
/
pinsrw
0x4
%
ecx
%
xmm4
.
byte
102
15
110
44
51
/
/
movd
(
%
ebx
%
esi
1
)
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
102
15
219
167
155
45
0
0
/
/
pand
0x2d9b
(
%
edi
)
%
xmm4
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
40
175
155
45
0
0
/
/
movaps
0x2d9b
(
%
edi
)
%
xmm5
.
byte
15
41
108
36
64
/
/
movaps
%
xmm5
0x40
(
%
esp
)
.
byte
102
15
127
100
36
48
/
/
movdqa
%
xmm4
0x30
(
%
esp
)
.
byte
102
15
127
100
36
32
/
/
movdqa
%
xmm4
0x20
(
%
esp
)
.
byte
102
15
127
100
36
16
/
/
movdqa
%
xmm4
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
59
0
/
/
cmp
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
45
0
0
0
111
/
/
sub
0x6f000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
101
0
/
/
add
%
ah
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
91
/
/
pop
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
77
0
/
/
add
%
cl
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
HIDDEN
_sk_luminance_to_alpha_sse2_lowp
.
globl
_sk_luminance_to_alpha_sse2_lowp
FUNCTION
(
_sk_luminance_to_alpha_sse2_lowp
)
_sk_luminance_to_alpha_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
232
0
0
0
0
/
/
call
e1b5
<
_sk_luminance_to_alpha_sse2_lowp
+
0x11
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
85
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm2
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
40
109
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm5
.
byte
15
40
117
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm6
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
102
15
213
128
123
45
0
0
/
/
pmullw
0x2d7b
(
%
eax
)
%
xmm0
.
byte
102
15
213
136
139
45
0
0
/
/
pmullw
0x2d8b
(
%
eax
)
%
xmm1
.
byte
102
15
253
200
/
/
paddw
%
xmm0
%
xmm1
.
byte
102
15
213
152
155
45
0
0
/
/
pmullw
0x2d9b
(
%
eax
)
%
xmm3
.
byte
102
15
253
217
/
/
paddw
%
xmm1
%
xmm3
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
71
4
/
/
lea
0x4
(
%
edi
)
%
eax
.
byte
15
41
116
36
64
/
/
movaps
%
xmm6
0x40
(
%
esp
)
.
byte
15
41
108
36
48
/
/
movaps
%
xmm5
0x30
(
%
esp
)
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
41
84
36
16
/
/
movaps
%
xmm2
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
68
36
4
/
/
mov
%
eax
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
239
201
/
/
pxor
%
xmm1
%
xmm1
.
byte
15
87
210
/
/
xorps
%
xmm2
%
xmm2
.
byte
255
23
/
/
call
*
(
%
edi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_gather_g8_sse2_lowp
.
globl
_sk_gather_g8_sse2_lowp
FUNCTION
(
_sk_gather_g8_sse2_lowp
)
_sk_gather_g8_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
105
8
/
/
movss
0x8
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
102
15
118
246
/
/
pcmpeqd
%
xmm6
%
xmm6
.
byte
102
15
254
238
/
/
paddd
%
xmm6
%
xmm5
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
196
/
/
maxps
%
xmm4
%
xmm0
.
byte
15
95
204
/
/
maxps
%
xmm4
%
xmm1
.
byte
15
93
205
/
/
minps
%
xmm5
%
xmm1
.
byte
15
93
197
/
/
minps
%
xmm5
%
xmm0
.
byte
243
15
16
105
12
/
/
movss
0xc
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
102
15
254
238
/
/
paddd
%
xmm6
%
xmm5
.
byte
15
95
220
/
/
maxps
%
xmm4
%
xmm3
.
byte
15
95
212
/
/
maxps
%
xmm4
%
xmm2
.
byte
15
93
213
/
/
minps
%
xmm5
%
xmm2
.
byte
15
93
221
/
/
minps
%
xmm5
%
xmm3
.
byte
243
15
91
234
/
/
cvttps2dq
%
xmm2
%
xmm5
.
byte
102
15
110
81
4
/
/
movd
0x4
(
%
ecx
)
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
102
15
111
242
/
/
movdqa
%
xmm2
%
xmm6
.
byte
102
15
244
245
/
/
pmuludq
%
xmm5
%
xmm6
.
byte
102
15
112
237
245
/
/
pshufd
0xf5
%
xmm5
%
xmm5
.
byte
102
15
244
234
/
/
pmuludq
%
xmm2
%
xmm5
.
byte
102
15
112
246
232
/
/
pshufd
0xe8
%
xmm6
%
xmm6
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
102
15
98
245
/
/
punpckldq
%
xmm5
%
xmm6
.
byte
243
15
91
192
/
/
cvttps2dq
%
xmm0
%
xmm0
.
byte
102
15
254
198
/
/
paddd
%
xmm6
%
xmm0
.
byte
102
15
112
232
78
/
/
pshufd
0x4e
%
xmm0
%
xmm5
.
byte
102
15
126
234
/
/
movd
%
xmm5
%
edx
.
byte
102
15
112
232
231
/
/
pshufd
0xe7
%
xmm0
%
xmm5
.
byte
102
15
126
238
/
/
movd
%
xmm5
%
esi
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
91
219
/
/
cvttps2dq
%
xmm3
%
xmm3
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
102
15
112
192
229
/
/
pshufd
0xe5
%
xmm0
%
xmm0
.
byte
15
182
20
23
/
/
movzbl
(
%
edi
%
edx
1
)
%
edx
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
193
230
8
/
/
shl
0x8
%
esi
.
byte
9
214
/
/
or
%
edx
%
esi
.
byte
102
15
126
194
/
/
movd
%
xmm0
%
edx
.
byte
102
15
112
195
245
/
/
pshufd
0xf5
%
xmm3
%
xmm0
.
byte
102
15
244
194
/
/
pmuludq
%
xmm2
%
xmm0
.
byte
15
182
12
15
/
/
movzbl
(
%
edi
%
ecx
1
)
%
ecx
.
byte
15
182
20
23
/
/
movzbl
(
%
edi
%
edx
1
)
%
edx
.
byte
193
226
8
/
/
shl
0x8
%
edx
.
byte
9
202
/
/
or
%
ecx
%
edx
.
byte
232
0
0
0
0
/
/
call
e2fa
<
_sk_gather_g8_sse2_lowp
+
0xc9
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
244
211
/
/
pmuludq
%
xmm3
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
98
208
/
/
punpckldq
%
xmm0
%
xmm2
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
102
15
254
202
/
/
paddd
%
xmm2
%
xmm1
.
byte
102
15
110
194
/
/
movd
%
edx
%
xmm0
.
byte
102
15
126
202
/
/
movd
%
xmm1
%
edx
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
15
196
198
1
/
/
pinsrw
0x1
%
esi
%
xmm0
.
byte
102
15
126
214
/
/
movd
%
xmm2
%
esi
.
byte
102
15
112
209
78
/
/
pshufd
0x4e
%
xmm1
%
xmm2
.
byte
15
182
20
23
/
/
movzbl
(
%
edi
%
edx
1
)
%
edx
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
193
230
8
/
/
shl
0x8
%
esi
.
byte
9
214
/
/
or
%
edx
%
esi
.
byte
102
15
126
210
/
/
movd
%
xmm2
%
edx
.
byte
102
15
112
201
231
/
/
pshufd
0xe7
%
xmm1
%
xmm1
.
byte
102
15
196
198
2
/
/
pinsrw
0x2
%
esi
%
xmm0
.
byte
102
15
126
206
/
/
movd
%
xmm1
%
esi
.
byte
15
182
20
23
/
/
movzbl
(
%
edi
%
edx
1
)
%
edx
.
byte
15
182
52
55
/
/
movzbl
(
%
edi
%
esi
1
)
%
esi
.
byte
139
125
8
/
/
mov
0x8
(
%
ebp
)
%
edi
.
byte
193
230
8
/
/
shl
0x8
%
esi
.
byte
9
214
/
/
or
%
edx
%
esi
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
102
15
196
198
3
/
/
pinsrw
0x3
%
esi
%
xmm0
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
77
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm1
.
byte
15
40
85
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm2
.
byte
15
40
93
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm3
.
byte
102
15
96
196
/
/
punpcklbw
%
xmm4
%
xmm0
.
byte
15
40
101
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm4
.
byte
141
88
8
/
/
lea
0x8
(
%
eax
)
%
ebx
.
byte
15
41
100
36
64
/
/
movaps
%
xmm4
0x40
(
%
esp
)
.
byte
15
41
92
36
48
/
/
movaps
%
xmm3
0x30
(
%
esp
)
.
byte
15
41
84
36
32
/
/
movaps
%
xmm2
0x20
(
%
esp
)
.
byte
15
41
76
36
16
/
/
movaps
%
xmm1
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
92
36
4
/
/
mov
%
ebx
0x4
(
%
esp
)
.
byte
137
60
36
/
/
mov
%
edi
(
%
esp
)
.
byte
15
40
153
102
43
0
0
/
/
movaps
0x2b66
(
%
ecx
)
%
xmm3
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
111
208
/
/
movdqa
%
xmm0
%
xmm2
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_scale_1_float_sse2_lowp
.
globl
_sk_scale_1_float_sse2_lowp
FUNCTION
(
_sk_scale_1_float_sse2_lowp
)
_sk_scale_1_float_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
e3cb
<
_sk_scale_1_float_sse2_lowp
+
0xd
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
243
15
16
39
/
/
movss
(
%
edi
)
%
xmm4
.
byte
243
15
89
166
161
43
0
0
/
/
mulss
0x2ba1
(
%
esi
)
%
xmm4
.
byte
243
15
88
166
149
43
0
0
/
/
addss
0x2b95
(
%
esi
)
%
xmm4
.
byte
243
15
44
252
/
/
cvttss2si
%
xmm4
%
edi
.
byte
102
15
110
239
/
/
movd
%
edi
%
xmm5
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
15
40
125
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm7
.
byte
242
15
112
237
0
/
/
pshuflw
0x0
%
xmm5
%
xmm5
.
byte
102
15
112
237
80
/
/
pshufd
0x50
%
xmm5
%
xmm5
.
byte
102
15
213
197
/
/
pmullw
%
xmm5
%
xmm0
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
15
213
213
/
/
pmullw
%
xmm5
%
xmm2
.
byte
102
15
213
221
/
/
pmullw
%
xmm5
%
xmm3
.
byte
102
15
111
174
149
42
0
0
/
/
movdqa
0x2a95
(
%
esi
)
%
xmm5
.
byte
102
15
253
197
/
/
paddw
%
xmm5
%
xmm0
.
byte
102
15
253
205
/
/
paddw
%
xmm5
%
xmm1
.
byte
102
15
253
213
/
/
paddw
%
xmm5
%
xmm2
.
byte
102
15
253
221
/
/
paddw
%
xmm5
%
xmm3
.
byte
15
40
109
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm5
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
113
8
/
/
lea
0x8
(
%
ecx
)
%
esi
.
byte
15
41
108
36
64
/
/
movaps
%
xmm5
0x40
(
%
esp
)
.
byte
15
41
124
36
48
/
/
movaps
%
xmm7
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
116
36
4
/
/
mov
%
esi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_lerp_1_float_sse2_lowp
.
globl
_sk_lerp_1_float_sse2_lowp
FUNCTION
(
_sk_lerp_1_float_sse2_lowp
)
_sk_lerp_1_float_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
102
15
111
218
/
/
movdqa
%
xmm2
%
xmm3
.
byte
102
15
111
209
/
/
movdqa
%
xmm1
%
xmm2
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
e499
<
_sk_lerp_1_float_sse2_lowp
+
0x1d
>
.
byte
89
/
/
pop
%
ecx
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
16
/
/
mov
(
%
eax
)
%
edx
.
byte
243
15
16
50
/
/
movss
(
%
edx
)
%
xmm6
.
byte
243
15
89
177
211
42
0
0
/
/
mulss
0x2ad3
(
%
ecx
)
%
xmm6
.
byte
243
15
88
177
199
42
0
0
/
/
addss
0x2ac7
(
%
ecx
)
%
xmm6
.
byte
243
15
44
214
/
/
cvttss2si
%
xmm6
%
edx
.
byte
102
15
110
242
/
/
movd
%
edx
%
xmm6
.
byte
242
15
112
246
0
/
/
pshuflw
0x0
%
xmm6
%
xmm6
.
byte
102
15
112
246
80
/
/
pshufd
0x50
%
xmm6
%
xmm6
.
byte
102
15
111
129
199
41
0
0
/
/
movdqa
0x29c7
(
%
ecx
)
%
xmm0
.
byte
102
15
213
206
/
/
pmullw
%
xmm6
%
xmm1
.
byte
102
15
253
200
/
/
paddw
%
xmm0
%
xmm1
.
byte
102
15
213
214
/
/
pmullw
%
xmm6
%
xmm2
.
byte
102
15
253
208
/
/
paddw
%
xmm0
%
xmm2
.
byte
102
15
213
222
/
/
pmullw
%
xmm6
%
xmm3
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
102
15
213
230
/
/
pmullw
%
xmm6
%
xmm4
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
249
198
/
/
psubw
%
xmm6
%
xmm0
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
213
245
/
/
pmullw
%
xmm5
%
xmm6
.
byte
102
15
253
206
/
/
paddw
%
xmm6
%
xmm1
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
213
117
40
/
/
pmullw
0x28
(
%
ebp
)
%
xmm6
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
15
111
117
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm6
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
102
15
213
254
/
/
pmullw
%
xmm6
%
xmm7
.
byte
102
15
253
223
/
/
paddw
%
xmm7
%
xmm3
.
byte
102
15
111
125
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
213
199
/
/
pmullw
%
xmm7
%
xmm0
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
141
120
8
/
/
lea
0x8
(
%
eax
)
%
edi
.
byte
102
15
127
124
36
64
/
/
movdqa
%
xmm7
0x40
(
%
esp
)
.
byte
102
15
127
116
36
48
/
/
movdqa
%
xmm6
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
102
15
127
108
36
16
/
/
movdqa
%
xmm5
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_scale_u8_sse2_lowp
.
globl
_sk_scale_u8_sse2_lowp
FUNCTION
(
_sk_scale_u8_sse2_lowp
)
_sk_scale_u8_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
92
/
/
sub
0x5c
%
esp
.
byte
15
40
125
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm7
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
e5c2
<
_sk_scale_u8_sse2_lowp
+
0x31
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
e5e4
<
_sk_scale_u8_sse2_lowp
+
0x53
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
2
1
0
0
/
/
mov
0x102
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
182
12
51
/
/
movzbl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
225
/
/
movd
%
ecx
%
xmm4
.
byte
235
90
/
/
jmp
e63e
<
_sk_scale_u8_sse2_lowp
+
0xad
>
.
byte
243
15
126
36
51
/
/
movq
(
%
ebx
%
esi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
235
79
/
/
jmp
e63e
<
_sk_scale_u8_sse2_lowp
+
0xad
>
.
byte
15
182
76
51
2
/
/
movzbl
0x2
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
225
2
/
/
pinsrw
0x2
%
ecx
%
xmm4
.
byte
15
183
12
51
/
/
movzwl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
233
/
/
movd
%
ecx
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
243
15
16
229
/
/
movss
%
xmm5
%
xmm4
.
byte
235
47
/
/
jmp
e63e
<
_sk_scale_u8_sse2_lowp
+
0xad
>
.
byte
15
182
76
51
6
/
/
movzbl
0x6
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
102
15
196
225
6
/
/
pinsrw
0x6
%
ecx
%
xmm4
.
byte
15
182
76
51
5
/
/
movzbl
0x5
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
225
5
/
/
pinsrw
0x5
%
ecx
%
xmm4
.
byte
15
182
76
51
4
/
/
movzbl
0x4
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
225
4
/
/
pinsrw
0x4
%
ecx
%
xmm4
.
byte
102
15
110
44
51
/
/
movd
(
%
ebx
%
esi
1
)
%
xmm5
.
byte
102
15
96
232
/
/
punpcklbw
%
xmm0
%
xmm5
.
byte
242
15
16
229
/
/
movsd
%
xmm5
%
xmm4
.
byte
102
15
219
167
158
40
0
0
/
/
pand
0x289e
(
%
edi
)
%
xmm4
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
15
111
175
158
40
0
0
/
/
movdqa
0x289e
(
%
edi
)
%
xmm5
.
byte
102
15
253
197
/
/
paddw
%
xmm5
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
213
204
/
/
pmullw
%
xmm4
%
xmm1
.
byte
102
15
253
205
/
/
paddw
%
xmm5
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
213
212
/
/
pmullw
%
xmm4
%
xmm2
.
byte
102
15
253
213
/
/
paddw
%
xmm5
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
213
227
/
/
pmullw
%
xmm3
%
xmm4
.
byte
102
15
253
229
/
/
paddw
%
xmm5
%
xmm4
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
40
93
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
64
/
/
movaps
%
xmm3
0x40
(
%
esp
)
.
byte
15
41
124
36
48
/
/
movaps
%
xmm7
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
111
220
/
/
movdqa
%
xmm4
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
92
/
/
add
0x5c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
59
0
/
/
cmp
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
45
0
0
0
111
/
/
sub
0x6f000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
101
0
/
/
add
%
ah
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
91
/
/
pop
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
77
0
/
/
add
%
cl
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
HIDDEN
_sk_lerp_u8_sse2_lowp
.
globl
_sk_lerp_u8_sse2_lowp
FUNCTION
(
_sk_lerp_u8_sse2_lowp
)
_sk_lerp_u8_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
108
/
/
sub
0x6c
%
esp
.
byte
102
15
127
93
216
/
/
movdqa
%
xmm3
-
0x28
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
e70e
<
_sk_lerp_u8_sse2_lowp
+
0x2e
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
e730
<
_sk_lerp_u8_sse2_lowp
+
0x50
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
74
1
0
0
/
/
mov
0x14a
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
182
12
51
/
/
movzbl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
217
/
/
movd
%
ecx
%
xmm3
.
byte
235
90
/
/
jmp
e78a
<
_sk_lerp_u8_sse2_lowp
+
0xaa
>
.
byte
243
15
126
28
51
/
/
movq
(
%
ebx
%
esi
1
)
%
xmm3
.
byte
102
15
96
216
/
/
punpcklbw
%
xmm0
%
xmm3
.
byte
235
79
/
/
jmp
e78a
<
_sk_lerp_u8_sse2_lowp
+
0xaa
>
.
byte
15
182
76
51
2
/
/
movzbl
0x2
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
217
2
/
/
pinsrw
0x2
%
ecx
%
xmm3
.
byte
15
183
12
51
/
/
movzwl
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
110
225
/
/
movd
%
ecx
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
243
15
16
220
/
/
movss
%
xmm4
%
xmm3
.
byte
235
47
/
/
jmp
e78a
<
_sk_lerp_u8_sse2_lowp
+
0xaa
>
.
byte
15
182
76
51
6
/
/
movzbl
0x6
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
217
6
/
/
pinsrw
0x6
%
ecx
%
xmm3
.
byte
15
182
76
51
5
/
/
movzbl
0x5
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
217
5
/
/
pinsrw
0x5
%
ecx
%
xmm3
.
byte
15
182
76
51
4
/
/
movzbl
0x4
(
%
ebx
%
esi
1
)
%
ecx
.
byte
102
15
196
217
4
/
/
pinsrw
0x4
%
ecx
%
xmm3
.
byte
102
15
110
36
51
/
/
movd
(
%
ebx
%
esi
1
)
%
xmm4
.
byte
102
15
96
224
/
/
punpcklbw
%
xmm0
%
xmm4
.
byte
242
15
16
220
/
/
movsd
%
xmm4
%
xmm3
.
byte
102
15
219
159
82
39
0
0
/
/
pand
0x2752
(
%
edi
)
%
xmm3
.
byte
102
15
111
183
82
39
0
0
/
/
movdqa
0x2752
(
%
edi
)
%
xmm6
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
102
15
239
230
/
/
pxor
%
xmm6
%
xmm4
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
111
109
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm5
.
byte
102
15
213
253
/
/
pmullw
%
xmm5
%
xmm7
.
byte
102
15
213
195
/
/
pmullw
%
xmm3
%
xmm0
.
byte
102
15
253
198
/
/
paddw
%
xmm6
%
xmm0
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
111
109
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
213
253
/
/
pmullw
%
xmm5
%
xmm7
.
byte
102
15
213
203
/
/
pmullw
%
xmm3
%
xmm1
.
byte
102
15
253
206
/
/
paddw
%
xmm6
%
xmm1
.
byte
102
15
253
207
/
/
paddw
%
xmm7
%
xmm1
.
byte
102
15
111
252
/
/
movdqa
%
xmm4
%
xmm7
.
byte
102
15
111
109
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
213
253
/
/
pmullw
%
xmm5
%
xmm7
.
byte
102
15
213
211
/
/
pmullw
%
xmm3
%
xmm2
.
byte
102
15
253
214
/
/
paddw
%
xmm6
%
xmm2
.
byte
102
15
253
215
/
/
paddw
%
xmm7
%
xmm2
.
byte
102
15
213
93
216
/
/
pmullw
-
0x28
(
%
ebp
)
%
xmm3
.
byte
102
15
253
222
/
/
paddw
%
xmm6
%
xmm3
.
byte
102
15
111
117
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm6
.
byte
102
15
213
230
/
/
pmullw
%
xmm6
%
xmm4
.
byte
102
15
253
220
/
/
paddw
%
xmm4
%
xmm3
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
102
15
127
116
36
64
/
/
movdqa
%
xmm6
0x40
(
%
esp
)
.
byte
102
15
127
108
36
48
/
/
movdqa
%
xmm5
0x30
(
%
esp
)
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
108
/
/
add
0x6c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
59
0
/
/
cmp
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
45
0
0
0
111
/
/
sub
0x6f000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
101
0
/
/
add
%
ah
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
91
/
/
pop
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
77
0
/
/
add
%
cl
0x0
(
%
ebp
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
HIDDEN
_sk_scale_565_sse2_lowp
.
globl
_sk_scale_565_sse2_lowp
FUNCTION
(
_sk_scale_565_sse2_lowp
)
_sk_scale_565_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
188
0
0
0
/
/
sub
0xbc
%
esp
.
byte
102
15
111
243
/
/
movdqa
%
xmm3
%
xmm6
.
byte
15
41
85
136
/
/
movaps
%
xmm2
-
0x78
(
%
ebp
)
.
byte
15
41
77
152
/
/
movaps
%
xmm1
-
0x68
(
%
ebp
)
.
byte
15
41
69
168
/
/
movaps
%
xmm0
-
0x58
(
%
ebp
)
.
byte
102
15
111
125
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
e8b7
<
_sk_scale_565_sse2_lowp
+
0x43
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
e8d9
<
_sk_scale_565_sse2_lowp
+
0x65
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
249
1
0
0
/
/
mov
0x1f9
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
183
12
115
/
/
movzwl
(
%
ebx
%
esi
2
)
%
ecx
.
byte
102
15
110
217
/
/
movd
%
ecx
%
xmm3
.
byte
235
59
/
/
jmp
e914
<
_sk_scale_565_sse2_lowp
+
0xa0
>
.
byte
243
15
111
28
115
/
/
movdqu
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
235
52
/
/
jmp
e914
<
_sk_scale_565_sse2_lowp
+
0xa0
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
92
115
4
2
/
/
pinsrw
0x2
0x4
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
243
15
16
12
115
/
/
movss
(
%
ebx
%
esi
2
)
%
xmm1
.
byte
243
15
16
217
/
/
movss
%
xmm1
%
xmm3
.
byte
235
30
/
/
jmp
e914
<
_sk_scale_565_sse2_lowp
+
0xa0
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
92
115
12
6
/
/
pinsrw
0x6
0xc
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
196
92
115
10
5
/
/
pinsrw
0x5
0xa
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
196
92
115
8
4
/
/
pinsrw
0x4
0x8
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
18
28
115
/
/
movlpd
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
219
143
233
37
0
0
/
/
pand
0x25e9
(
%
edi
)
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
210
5
/
/
psrlw
0x5
%
xmm2
.
byte
102
15
219
151
249
37
0
0
/
/
pand
0x25f9
(
%
edi
)
%
xmm2
.
byte
102
15
111
135
9
38
0
0
/
/
movdqa
0x2609
(
%
edi
)
%
xmm0
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
102
15
113
211
13
/
/
psrlw
0xd
%
xmm3
.
byte
102
15
235
217
/
/
por
%
xmm1
%
xmm3
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
2
/
/
psllw
0x2
%
xmm1
.
byte
102
15
113
210
4
/
/
psrlw
0x4
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
113
241
3
/
/
psllw
0x3
%
xmm1
.
byte
102
15
113
208
2
/
/
psrlw
0x2
%
xmm0
.
byte
102
15
235
193
/
/
por
%
xmm1
%
xmm0
.
byte
102
15
127
69
200
/
/
movdqa
%
xmm0
-
0x38
(
%
ebp
)
.
byte
102
15
111
167
185
37
0
0
/
/
movdqa
0x25b9
(
%
edi
)
%
xmm4
.
byte
15
41
117
184
/
/
movaps
%
xmm6
-
0x48
(
%
ebp
)
.
byte
102
15
239
244
/
/
pxor
%
xmm4
%
xmm6
.
byte
102
15
239
252
/
/
pxor
%
xmm4
%
xmm7
.
byte
102
15
101
254
/
/
pcmpgtw
%
xmm6
%
xmm7
.
byte
102
15
111
240
/
/
movdqa
%
xmm0
%
xmm6
.
byte
102
15
239
244
/
/
pxor
%
xmm4
%
xmm6
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
239
204
/
/
pxor
%
xmm4
%
xmm1
.
byte
102
15
101
241
/
/
pcmpgtw
%
xmm1
%
xmm6
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
223
200
/
/
pandn
%
xmm0
%
xmm1
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
239
204
/
/
pxor
%
xmm4
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
235
196
/
/
por
%
xmm4
%
xmm0
.
byte
102
15
127
69
216
/
/
movdqa
%
xmm0
-
0x28
(
%
ebp
)
.
byte
102
15
101
200
/
/
pcmpgtw
%
xmm0
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
219
193
/
/
pand
%
xmm1
%
xmm0
.
byte
102
15
223
205
/
/
pandn
%
xmm5
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
15
111
109
200
/
/
movdqa
-
0x38
(
%
ebp
)
%
xmm5
.
byte
102
15
111
197
/
/
movdqa
%
xmm5
%
xmm0
.
byte
102
15
219
198
/
/
pand
%
xmm6
%
xmm0
.
byte
102
15
223
242
/
/
pandn
%
xmm2
%
xmm6
.
byte
102
15
235
240
/
/
por
%
xmm0
%
xmm6
.
byte
102
15
239
230
/
/
pxor
%
xmm6
%
xmm4
.
byte
102
15
101
101
216
/
/
pcmpgtw
-
0x28
(
%
ebp
)
%
xmm4
.
byte
102
15
219
244
/
/
pand
%
xmm4
%
xmm6
.
byte
102
15
223
227
/
/
pandn
%
xmm3
%
xmm4
.
byte
102
15
235
230
/
/
por
%
xmm6
%
xmm4
.
byte
102
15
219
207
/
/
pand
%
xmm7
%
xmm1
.
byte
102
15
223
252
/
/
pandn
%
xmm4
%
xmm7
.
byte
102
15
235
249
/
/
por
%
xmm1
%
xmm7
.
byte
102
15
213
93
168
/
/
pmullw
-
0x58
(
%
ebp
)
%
xmm3
.
byte
102
15
213
85
152
/
/
pmullw
-
0x68
(
%
ebp
)
%
xmm2
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
213
101
136
/
/
pmullw
-
0x78
(
%
ebp
)
%
xmm4
.
byte
102
15
213
125
184
/
/
pmullw
-
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
111
135
169
37
0
0
/
/
movdqa
0x25a9
(
%
edi
)
%
xmm0
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
102
15
253
208
/
/
paddw
%
xmm0
%
xmm2
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
253
248
/
/
paddw
%
xmm0
%
xmm7
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
40
69
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
64
/
/
movaps
%
xmm0
0x40
(
%
esp
)
.
byte
15
40
69
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
129
196
188
0
0
0
/
/
add
0xbc
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
102
144
/
/
xchg
%
ax
%
ax
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
41
0
/
/
sub
%
eax
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
88
/
/
pop
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
81
0
/
/
add
%
dl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_lerp_565_sse2_lowp
.
globl
_sk_lerp_565_sse2_lowp
FUNCTION
(
_sk_lerp_565_sse2_lowp
)
_sk_lerp_565_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
188
0
0
0
/
/
sub
0xbc
%
esp
.
byte
102
15
111
243
/
/
movdqa
%
xmm3
%
xmm6
.
byte
15
41
85
136
/
/
movaps
%
xmm2
-
0x78
(
%
ebp
)
.
byte
15
41
77
152
/
/
movaps
%
xmm1
-
0x68
(
%
ebp
)
.
byte
15
41
69
168
/
/
movaps
%
xmm0
-
0x58
(
%
ebp
)
.
byte
102
15
111
125
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
89
4
/
/
mov
0x4
(
%
ecx
)
%
ebx
.
byte
15
175
93
20
/
/
imul
0x14
(
%
ebp
)
%
ebx
.
byte
1
219
/
/
add
%
ebx
%
ebx
.
byte
3
25
/
/
add
(
%
ecx
)
%
ebx
.
byte
137
209
/
/
mov
%
edx
%
ecx
.
byte
128
225
7
/
/
and
0x7
%
cl
.
byte
254
201
/
/
dec
%
cl
.
byte
128
249
6
/
/
cmp
0x6
%
cl
.
byte
232
0
0
0
0
/
/
call
eb0f
<
_sk_lerp_565_sse2_lowp
+
0x43
>
.
byte
95
/
/
pop
%
edi
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
119
28
/
/
ja
eb31
<
_sk_lerp_565_sse2_lowp
+
0x65
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
15
182
201
/
/
movzbl
%
cl
%
ecx
.
byte
139
140
143
73
2
0
0
/
/
mov
0x249
(
%
edi
%
ecx
4
)
%
ecx
.
byte
1
249
/
/
add
%
edi
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
15
183
12
115
/
/
movzwl
(
%
ebx
%
esi
2
)
%
ecx
.
byte
102
15
110
217
/
/
movd
%
ecx
%
xmm3
.
byte
235
59
/
/
jmp
eb6c
<
_sk_lerp_565_sse2_lowp
+
0xa0
>
.
byte
243
15
111
28
115
/
/
movdqu
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
235
52
/
/
jmp
eb6c
<
_sk_lerp_565_sse2_lowp
+
0xa0
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
92
115
4
2
/
/
pinsrw
0x2
0x4
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
243
15
16
12
115
/
/
movss
(
%
ebx
%
esi
2
)
%
xmm1
.
byte
243
15
16
217
/
/
movss
%
xmm1
%
xmm3
.
byte
235
30
/
/
jmp
eb6c
<
_sk_lerp_565_sse2_lowp
+
0xa0
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
196
92
115
12
6
/
/
pinsrw
0x6
0xc
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
196
92
115
10
5
/
/
pinsrw
0x5
0xa
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
196
92
115
8
4
/
/
pinsrw
0x4
0x8
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
18
28
115
/
/
movlpd
(
%
ebx
%
esi
2
)
%
xmm3
.
byte
102
15
111
203
/
/
movdqa
%
xmm3
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
219
143
145
35
0
0
/
/
pand
0x2391
(
%
edi
)
%
xmm1
.
byte
102
15
111
211
/
/
movdqa
%
xmm3
%
xmm2
.
byte
102
15
113
210
5
/
/
psrlw
0x5
%
xmm2
.
byte
102
15
219
151
161
35
0
0
/
/
pand
0x23a1
(
%
edi
)
%
xmm2
.
byte
102
15
111
175
177
35
0
0
/
/
movdqa
0x23b1
(
%
edi
)
%
xmm5
.
byte
102
15
219
235
/
/
pand
%
xmm3
%
xmm5
.
byte
102
15
113
211
13
/
/
psrlw
0xd
%
xmm3
.
byte
102
15
235
217
/
/
por
%
xmm1
%
xmm3
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
113
241
2
/
/
psllw
0x2
%
xmm1
.
byte
102
15
113
210
4
/
/
psrlw
0x4
%
xmm2
.
byte
102
15
235
209
/
/
por
%
xmm1
%
xmm2
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
113
241
3
/
/
psllw
0x3
%
xmm1
.
byte
102
15
113
213
2
/
/
psrlw
0x2
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
127
109
216
/
/
movdqa
%
xmm5
-
0x28
(
%
ebp
)
.
byte
102
15
111
167
97
35
0
0
/
/
movdqa
0x2361
(
%
edi
)
%
xmm4
.
byte
102
15
111
198
/
/
movdqa
%
xmm6
%
xmm0
.
byte
102
15
111
200
/
/
movdqa
%
xmm0
%
xmm1
.
byte
102
15
239
204
/
/
pxor
%
xmm4
%
xmm1
.
byte
102
15
239
252
/
/
pxor
%
xmm4
%
xmm7
.
byte
102
15
101
249
/
/
pcmpgtw
%
xmm1
%
xmm7
.
byte
102
15
111
245
/
/
movdqa
%
xmm5
%
xmm6
.
byte
102
15
239
244
/
/
pxor
%
xmm4
%
xmm6
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
239
204
/
/
pxor
%
xmm4
%
xmm1
.
byte
102
15
101
241
/
/
pcmpgtw
%
xmm1
%
xmm6
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
102
15
223
205
/
/
pandn
%
xmm5
%
xmm1
.
byte
102
15
127
69
184
/
/
movdqa
%
xmm0
-
0x48
(
%
ebp
)
.
byte
102
15
111
234
/
/
movdqa
%
xmm2
%
xmm5
.
byte
102
15
219
238
/
/
pand
%
xmm6
%
xmm5
.
byte
102
15
235
233
/
/
por
%
xmm1
%
xmm5
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
102
15
239
204
/
/
pxor
%
xmm4
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
235
196
/
/
por
%
xmm4
%
xmm0
.
byte
102
15
127
69
200
/
/
movdqa
%
xmm0
-
0x38
(
%
ebp
)
.
byte
102
15
101
200
/
/
pcmpgtw
%
xmm0
%
xmm1
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
219
193
/
/
pand
%
xmm1
%
xmm0
.
byte
102
15
223
205
/
/
pandn
%
xmm5
%
xmm1
.
byte
102
15
235
200
/
/
por
%
xmm0
%
xmm1
.
byte
102
15
111
69
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm0
.
byte
102
15
219
198
/
/
pand
%
xmm6
%
xmm0
.
byte
102
15
223
242
/
/
pandn
%
xmm2
%
xmm6
.
byte
102
15
235
240
/
/
por
%
xmm0
%
xmm6
.
byte
102
15
239
230
/
/
pxor
%
xmm6
%
xmm4
.
byte
102
15
101
101
200
/
/
pcmpgtw
-
0x38
(
%
ebp
)
%
xmm4
.
byte
102
15
219
244
/
/
pand
%
xmm4
%
xmm6
.
byte
102
15
223
227
/
/
pandn
%
xmm3
%
xmm4
.
byte
102
15
235
230
/
/
por
%
xmm6
%
xmm4
.
byte
102
15
219
207
/
/
pand
%
xmm7
%
xmm1
.
byte
102
15
223
252
/
/
pandn
%
xmm4
%
xmm7
.
byte
102
15
235
249
/
/
por
%
xmm1
%
xmm7
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
213
93
168
/
/
pmullw
-
0x58
(
%
ebp
)
%
xmm3
.
byte
102
15
111
143
81
35
0
0
/
/
movdqa
0x2351
(
%
edi
)
%
xmm1
.
byte
102
15
239
193
/
/
pxor
%
xmm1
%
xmm0
.
byte
102
15
111
101
24
/
/
movdqa
0x18
(
%
ebp
)
%
xmm4
.
byte
102
15
213
196
/
/
pmullw
%
xmm4
%
xmm0
.
byte
102
15
253
217
/
/
paddw
%
xmm1
%
xmm3
.
byte
102
15
253
216
/
/
paddw
%
xmm0
%
xmm3
.
byte
102
15
111
194
/
/
movdqa
%
xmm2
%
xmm0
.
byte
102
15
213
85
152
/
/
pmullw
-
0x68
(
%
ebp
)
%
xmm2
.
byte
102
15
239
193
/
/
pxor
%
xmm1
%
xmm0
.
byte
102
15
111
109
40
/
/
movdqa
0x28
(
%
ebp
)
%
xmm5
.
byte
102
15
213
197
/
/
pmullw
%
xmm5
%
xmm0
.
byte
102
15
253
209
/
/
paddw
%
xmm1
%
xmm2
.
byte
102
15
253
208
/
/
paddw
%
xmm0
%
xmm2
.
byte
102
15
111
101
216
/
/
movdqa
-
0x28
(
%
ebp
)
%
xmm4
.
byte
102
15
111
196
/
/
movdqa
%
xmm4
%
xmm0
.
byte
102
15
213
101
136
/
/
pmullw
-
0x78
(
%
ebp
)
%
xmm4
.
byte
102
15
239
193
/
/
pxor
%
xmm1
%
xmm0
.
byte
102
15
111
117
56
/
/
movdqa
0x38
(
%
ebp
)
%
xmm6
.
byte
102
15
213
198
/
/
pmullw
%
xmm6
%
xmm0
.
byte
102
15
253
225
/
/
paddw
%
xmm1
%
xmm4
.
byte
102
15
253
224
/
/
paddw
%
xmm0
%
xmm4
.
byte
102
15
111
199
/
/
movdqa
%
xmm7
%
xmm0
.
byte
102
15
213
125
184
/
/
pmullw
-
0x48
(
%
ebp
)
%
xmm7
.
byte
102
15
239
193
/
/
pxor
%
xmm1
%
xmm0
.
byte
102
15
253
249
/
/
paddw
%
xmm1
%
xmm7
.
byte
102
15
111
77
72
/
/
movdqa
0x48
(
%
ebp
)
%
xmm1
.
byte
102
15
213
193
/
/
pmullw
%
xmm1
%
xmm0
.
byte
102
15
253
248
/
/
paddw
%
xmm0
%
xmm7
.
byte
102
15
113
211
8
/
/
psrlw
0x8
%
xmm3
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
102
15
127
76
36
64
/
/
movdqa
%
xmm1
0x40
(
%
esp
)
.
byte
102
15
127
116
36
48
/
/
movdqa
%
xmm6
0x30
(
%
esp
)
.
byte
102
15
127
108
36
32
/
/
movdqa
%
xmm5
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
102
15
111
195
/
/
movdqa
%
xmm3
%
xmm0
.
byte
102
15
111
202
/
/
movdqa
%
xmm2
%
xmm1
.
byte
102
15
111
212
/
/
movdqa
%
xmm4
%
xmm2
.
byte
102
15
111
223
/
/
movdqa
%
xmm7
%
xmm3
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
129
196
188
0
0
0
/
/
add
0xbc
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
144
/
/
nop
.
byte
24
0
/
/
sbb
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
41
0
/
/
sub
%
eax
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
88
/
/
pop
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
81
0
/
/
add
%
dl
0x0
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
74
/
/
dec
%
edx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_clamp_x_1_sse2_lowp
.
globl
_sk_clamp_x_1_sse2_lowp
FUNCTION
(
_sk_clamp_x_1_sse2_lowp
)
_sk_clamp_x_1_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
ed81
<
_sk_clamp_x_1_sse2_lowp
+
0xd
>
.
byte
88
/
/
pop
%
eax
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
15
87
255
/
/
xorps
%
xmm7
%
xmm7
.
byte
15
95
207
/
/
maxps
%
xmm7
%
xmm1
.
byte
15
95
199
/
/
maxps
%
xmm7
%
xmm0
.
byte
15
40
184
223
25
0
0
/
/
movaps
0x19df
(
%
eax
)
%
xmm7
.
byte
15
93
199
/
/
minps
%
xmm7
%
xmm0
.
byte
15
93
207
/
/
minps
%
xmm7
%
xmm1
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
120
4
/
/
lea
0x4
(
%
eax
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_repeat_x_1_sse2_lowp
.
globl
_sk_repeat_x_1_sse2_lowp
FUNCTION
(
_sk_repeat_x_1_sse2_lowp
)
_sk_repeat_x_1_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
232
0
0
0
0
/
/
call
edf0
<
_sk_repeat_x_1_sse2_lowp
+
0xd
>
.
byte
94
/
/
pop
%
esi
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
243
15
91
225
/
/
cvttps2dq
%
xmm1
%
xmm4
.
byte
243
15
91
232
/
/
cvttps2dq
%
xmm0
%
xmm5
.
byte
15
91
237
/
/
cvtdq2ps
%
xmm5
%
xmm5
.
byte
15
91
228
/
/
cvtdq2ps
%
xmm4
%
xmm4
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
194
244
1
/
/
cmpltps
%
xmm4
%
xmm6
.
byte
15
40
190
112
25
0
0
/
/
movaps
0x1970
(
%
esi
)
%
xmm7
.
byte
15
84
247
/
/
andps
%
xmm7
%
xmm6
.
byte
15
92
230
/
/
subps
%
xmm6
%
xmm4
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
84
247
/
/
andps
%
xmm7
%
xmm6
.
byte
15
92
238
/
/
subps
%
xmm6
%
xmm5
.
byte
15
40
117
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm6
.
byte
15
92
197
/
/
subps
%
xmm5
%
xmm0
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
92
204
/
/
subps
%
xmm4
%
xmm1
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
204
/
/
maxps
%
xmm4
%
xmm1
.
byte
15
95
196
/
/
maxps
%
xmm4
%
xmm0
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
93
199
/
/
minps
%
xmm7
%
xmm0
.
byte
15
93
207
/
/
minps
%
xmm7
%
xmm1
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
116
36
16
/
/
movaps
%
xmm6
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_mirror_x_1_sse2_lowp
.
globl
_sk_mirror_x_1_sse2_lowp
FUNCTION
(
_sk_mirror_x_1_sse2_lowp
)
_sk_mirror_x_1_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
15
41
93
232
/
/
movaps
%
xmm3
-
0x18
(
%
ebp
)
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
232
0
0
0
0
/
/
call
ee9a
<
_sk_mirror_x_1_sse2_lowp
+
0x1a
>
.
byte
88
/
/
pop
%
eax
.
byte
15
40
160
54
25
0
0
/
/
movaps
0x1936
(
%
eax
)
%
xmm4
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
40
176
182
24
0
0
/
/
movaps
0x18b6
(
%
eax
)
%
xmm6
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
89
254
/
/
mulps
%
xmm6
%
xmm7
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
243
15
91
238
/
/
cvttps2dq
%
xmm6
%
xmm5
.
byte
15
91
197
/
/
cvtdq2ps
%
xmm5
%
xmm0
.
byte
15
194
240
1
/
/
cmpltps
%
xmm0
%
xmm6
.
byte
15
40
168
198
24
0
0
/
/
movaps
0x18c6
(
%
eax
)
%
xmm5
.
byte
15
84
245
/
/
andps
%
xmm5
%
xmm6
.
byte
15
92
198
/
/
subps
%
xmm6
%
xmm0
.
byte
243
15
91
247
/
/
cvttps2dq
%
xmm7
%
xmm6
.
byte
15
91
246
/
/
cvtdq2ps
%
xmm6
%
xmm6
.
byte
15
194
254
1
/
/
cmpltps
%
xmm6
%
xmm7
.
byte
15
84
253
/
/
andps
%
xmm5
%
xmm7
.
byte
15
92
247
/
/
subps
%
xmm7
%
xmm6
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
88
192
/
/
addps
%
xmm0
%
xmm0
.
byte
15
92
200
/
/
subps
%
xmm0
%
xmm1
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
88
246
/
/
addps
%
xmm6
%
xmm6
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
15
88
212
/
/
addps
%
xmm4
%
xmm2
.
byte
15
88
204
/
/
addps
%
xmm4
%
xmm1
.
byte
15
40
160
6
30
0
0
/
/
movaps
0x1e06
(
%
eax
)
%
xmm4
.
byte
15
84
204
/
/
andps
%
xmm4
%
xmm1
.
byte
15
84
212
/
/
andps
%
xmm4
%
xmm2
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
95
212
/
/
maxps
%
xmm4
%
xmm2
.
byte
15
95
204
/
/
maxps
%
xmm4
%
xmm1
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
15
93
205
/
/
minps
%
xmm5
%
xmm1
.
byte
15
93
213
/
/
minps
%
xmm5
%
xmm2
.
byte
15
40
109
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm5
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
120
4
/
/
lea
0x4
(
%
eax
)
%
edi
.
byte
15
41
108
36
64
/
/
movaps
%
xmm5
0x40
(
%
esp
)
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
193
/
/
movaps
%
xmm1
%
xmm0
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
15
40
211
/
/
movaps
%
xmm3
%
xmm2
.
byte
15
40
93
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm3
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
131
196
96
/
/
add
0x60
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_decal_x_sse2_lowp
.
globl
_sk_decal_x_sse2_lowp
FUNCTION
(
_sk_decal_x_sse2_lowp
)
_sk_decal_x_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
194
233
2
/
/
cmpleps
%
xmm1
%
xmm5
.
byte
242
15
112
237
232
/
/
pshuflw
0xe8
%
xmm5
%
xmm5
.
byte
243
15
112
237
232
/
/
pshufhw
0xe8
%
xmm5
%
xmm5
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
15
194
224
2
/
/
cmpleps
%
xmm0
%
xmm4
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
243
15
112
228
232
/
/
pshufhw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
108
229
/
/
punpcklqdq
%
xmm5
%
xmm4
.
byte
243
15
16
111
64
/
/
movss
0x40
(
%
edi
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
194
253
1
/
/
cmpltps
%
xmm5
%
xmm7
.
byte
15
40
109
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm5
.
byte
242
15
112
246
232
/
/
pshuflw
0xe8
%
xmm6
%
xmm6
.
byte
243
15
112
246
232
/
/
pshufhw
0xe8
%
xmm6
%
xmm6
.
byte
102
15
112
246
232
/
/
pshufd
0xe8
%
xmm6
%
xmm6
.
byte
242
15
112
255
232
/
/
pshuflw
0xe8
%
xmm7
%
xmm7
.
byte
243
15
112
255
232
/
/
pshufhw
0xe8
%
xmm7
%
xmm7
.
byte
102
15
112
255
232
/
/
pshufd
0xe8
%
xmm7
%
xmm7
.
byte
102
15
108
254
/
/
punpcklqdq
%
xmm6
%
xmm7
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
102
15
113
244
15
/
/
psllw
0xf
%
xmm4
.
byte
102
15
113
228
15
/
/
psraw
0xf
%
xmm4
.
byte
102
15
113
247
15
/
/
psllw
0xf
%
xmm7
.
byte
102
15
113
231
15
/
/
psraw
0xf
%
xmm7
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
243
15
127
63
/
/
movdqu
%
xmm7
(
%
edi
)
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
141
121
8
/
/
lea
0x8
(
%
ecx
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
41
108
36
16
/
/
movaps
%
xmm5
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_decal_y_sse2_lowp
.
globl
_sk_decal_y_sse2_lowp
FUNCTION
(
_sk_decal_y_sse2_lowp
)
_sk_decal_y_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
15
194
235
2
/
/
cmpleps
%
xmm3
%
xmm5
.
byte
242
15
112
237
232
/
/
pshuflw
0xe8
%
xmm5
%
xmm5
.
byte
243
15
112
237
232
/
/
pshufhw
0xe8
%
xmm5
%
xmm5
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
15
194
226
2
/
/
cmpleps
%
xmm2
%
xmm4
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
243
15
112
228
232
/
/
pshufhw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
108
229
/
/
punpcklqdq
%
xmm5
%
xmm4
.
byte
243
15
16
111
68
/
/
movss
0x44
(
%
edi
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
243
/
/
movaps
%
xmm3
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
194
253
1
/
/
cmpltps
%
xmm5
%
xmm7
.
byte
15
40
109
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm5
.
byte
242
15
112
246
232
/
/
pshuflw
0xe8
%
xmm6
%
xmm6
.
byte
243
15
112
246
232
/
/
pshufhw
0xe8
%
xmm6
%
xmm6
.
byte
102
15
112
246
232
/
/
pshufd
0xe8
%
xmm6
%
xmm6
.
byte
242
15
112
255
232
/
/
pshuflw
0xe8
%
xmm7
%
xmm7
.
byte
243
15
112
255
232
/
/
pshufhw
0xe8
%
xmm7
%
xmm7
.
byte
102
15
112
255
232
/
/
pshufd
0xe8
%
xmm7
%
xmm7
.
byte
102
15
108
254
/
/
punpcklqdq
%
xmm6
%
xmm7
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
102
15
113
244
15
/
/
psllw
0xf
%
xmm4
.
byte
102
15
113
228
15
/
/
psraw
0xf
%
xmm4
.
byte
102
15
113
247
15
/
/
psllw
0xf
%
xmm7
.
byte
102
15
113
231
15
/
/
psraw
0xf
%
xmm7
.
byte
102
15
219
252
/
/
pand
%
xmm4
%
xmm7
.
byte
15
40
101
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm4
.
byte
243
15
127
63
/
/
movdqu
%
xmm7
(
%
edi
)
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
141
121
8
/
/
lea
0x8
(
%
ecx
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
100
36
48
/
/
movaps
%
xmm4
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
41
108
36
16
/
/
movaps
%
xmm5
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_decal_x_and_y_sse2_lowp
.
globl
_sk_decal_x_and_y_sse2_lowp
FUNCTION
(
_sk_decal_x_and_y_sse2_lowp
)
_sk_decal_x_and_y_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
139
77
12
/
/
mov
0xc
(
%
ebp
)
%
ecx
.
byte
139
57
/
/
mov
(
%
ecx
)
%
edi
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
194
225
2
/
/
cmpleps
%
xmm1
%
xmm4
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
243
15
112
228
232
/
/
pshufhw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
236
232
/
/
pshufd
0xe8
%
xmm4
%
xmm5
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
15
194
224
2
/
/
cmpleps
%
xmm0
%
xmm4
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
243
15
112
228
232
/
/
pshufhw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
108
229
/
/
punpcklqdq
%
xmm5
%
xmm4
.
byte
243
15
16
111
64
/
/
movss
0x40
(
%
edi
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
15
194
245
1
/
/
cmpltps
%
xmm5
%
xmm6
.
byte
15
40
248
/
/
movaps
%
xmm0
%
xmm7
.
byte
15
194
253
1
/
/
cmpltps
%
xmm5
%
xmm7
.
byte
15
87
237
/
/
xorps
%
xmm5
%
xmm5
.
byte
102
15
113
244
15
/
/
psllw
0xf
%
xmm4
.
byte
102
15
113
228
15
/
/
psraw
0xf
%
xmm4
.
byte
242
15
112
246
232
/
/
pshuflw
0xe8
%
xmm6
%
xmm6
.
byte
243
15
112
246
232
/
/
pshufhw
0xe8
%
xmm6
%
xmm6
.
byte
102
15
112
246
232
/
/
pshufd
0xe8
%
xmm6
%
xmm6
.
byte
242
15
112
255
232
/
/
pshuflw
0xe8
%
xmm7
%
xmm7
.
byte
243
15
112
255
232
/
/
pshufhw
0xe8
%
xmm7
%
xmm7
.
byte
102
15
112
255
232
/
/
pshufd
0xe8
%
xmm7
%
xmm7
.
byte
102
15
108
254
/
/
punpcklqdq
%
xmm6
%
xmm7
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
15
194
243
2
/
/
cmpleps
%
xmm3
%
xmm6
.
byte
242
15
112
246
232
/
/
pshuflw
0xe8
%
xmm6
%
xmm6
.
byte
243
15
112
246
232
/
/
pshufhw
0xe8
%
xmm6
%
xmm6
.
byte
102
15
112
246
232
/
/
pshufd
0xe8
%
xmm6
%
xmm6
.
byte
15
194
234
2
/
/
cmpleps
%
xmm2
%
xmm5
.
byte
242
15
112
237
232
/
/
pshuflw
0xe8
%
xmm5
%
xmm5
.
byte
243
15
112
237
232
/
/
pshufhw
0xe8
%
xmm5
%
xmm5
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
102
15
108
238
/
/
punpcklqdq
%
xmm6
%
xmm5
.
byte
243
15
16
119
68
/
/
movss
0x44
(
%
edi
)
%
xmm6
.
byte
102
15
113
247
15
/
/
psllw
0xf
%
xmm7
.
byte
102
15
113
231
15
/
/
psraw
0xf
%
xmm7
.
byte
102
15
113
245
15
/
/
psllw
0xf
%
xmm5
.
byte
102
15
113
229
15
/
/
psraw
0xf
%
xmm5
.
byte
102
15
219
236
/
/
pand
%
xmm4
%
xmm5
.
byte
102
15
219
239
/
/
pand
%
xmm7
%
xmm5
.
byte
15
198
246
0
/
/
shufps
0x0
%
xmm6
%
xmm6
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
194
230
1
/
/
cmpltps
%
xmm6
%
xmm4
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
194
254
1
/
/
cmpltps
%
xmm6
%
xmm7
.
byte
15
40
117
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm6
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
243
15
112
228
232
/
/
pshufhw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
242
15
112
255
232
/
/
pshuflw
0xe8
%
xmm7
%
xmm7
.
byte
243
15
112
255
232
/
/
pshufhw
0xe8
%
xmm7
%
xmm7
.
byte
102
15
112
255
232
/
/
pshufd
0xe8
%
xmm7
%
xmm7
.
byte
102
15
108
252
/
/
punpcklqdq
%
xmm4
%
xmm7
.
byte
15
40
101
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm4
.
byte
102
15
113
247
15
/
/
psllw
0xf
%
xmm7
.
byte
102
15
113
231
15
/
/
psraw
0xf
%
xmm7
.
byte
102
15
219
253
/
/
pand
%
xmm5
%
xmm7
.
byte
15
40
109
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm5
.
byte
243
15
127
63
/
/
movdqu
%
xmm7
(
%
edi
)
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
141
121
8
/
/
lea
0x8
(
%
ecx
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
108
36
48
/
/
movaps
%
xmm5
0x30
(
%
esp
)
.
byte
15
41
100
36
32
/
/
movaps
%
xmm4
0x20
(
%
esp
)
.
byte
15
41
116
36
16
/
/
movaps
%
xmm6
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_check_decal_mask_sse2_lowp
.
globl
_sk_check_decal_mask_sse2_lowp
FUNCTION
(
_sk_check_decal_mask_sse2_lowp
)
_sk_check_decal_mask_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
40
109
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm5
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
139
62
/
/
mov
(
%
esi
)
%
edi
.
byte
15
16
63
/
/
movups
(
%
edi
)
%
xmm7
.
byte
15
84
199
/
/
andps
%
xmm7
%
xmm0
.
byte
15
84
207
/
/
andps
%
xmm7
%
xmm1
.
byte
15
84
215
/
/
andps
%
xmm7
%
xmm2
.
byte
15
84
223
/
/
andps
%
xmm7
%
xmm3
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
141
126
8
/
/
lea
0x8
(
%
esi
)
%
edi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
41
108
36
32
/
/
movaps
%
xmm5
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
255
86
4
/
/
call
*
0x4
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_gradient_sse2_lowp
.
globl
_sk_gradient_sse2_lowp
FUNCTION
(
_sk_gradient_sse2_lowp
)
_sk_gradient_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
108
1
0
0
/
/
sub
0x16c
%
esp
.
byte
232
0
0
0
0
/
/
call
f2f5
<
_sk_gradient_sse2_lowp
+
0x11
>
.
byte
88
/
/
pop
%
eax
.
byte
137
69
164
/
/
mov
%
eax
-
0x5c
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
0
/
/
mov
(
%
eax
)
%
eax
.
byte
137
198
/
/
mov
%
eax
%
esi
.
byte
139
0
/
/
mov
(
%
eax
)
%
eax
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
131
248
2
/
/
cmp
0x2
%
eax
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
114
48
/
/
jb
f33f
<
_sk_gradient_sse2_lowp
+
0x5b
>
.
byte
139
78
36
/
/
mov
0x24
(
%
esi
)
%
ecx
.
byte
72
/
/
dec
%
eax
.
byte
131
193
4
/
/
add
0x4
%
ecx
.
byte
102
15
239
255
/
/
pxor
%
xmm7
%
xmm7
.
byte
102
15
239
237
/
/
pxor
%
xmm5
%
xmm5
.
byte
243
15
16
17
/
/
movss
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
194
216
2
/
/
cmpleps
%
xmm0
%
xmm3
.
byte
102
15
250
251
/
/
psubd
%
xmm3
%
xmm7
.
byte
15
194
209
2
/
/
cmpleps
%
xmm1
%
xmm2
.
byte
102
15
250
234
/
/
psubd
%
xmm2
%
xmm5
.
byte
131
193
4
/
/
add
0x4
%
ecx
.
byte
72
/
/
dec
%
eax
.
byte
117
223
/
/
jne
f31e
<
_sk_gradient_sse2_lowp
+
0x3a
>
.
byte
102
15
112
215
229
/
/
pshufd
0xe5
%
xmm7
%
xmm2
.
byte
102
15
112
223
78
/
/
pshufd
0x4e
%
xmm7
%
xmm3
.
byte
15
41
141
104
255
255
255
/
/
movaps
%
xmm1
-
0x98
(
%
ebp
)
.
byte
15
41
133
248
254
255
255
/
/
movaps
%
xmm0
-
0x108
(
%
ebp
)
.
byte
102
15
112
229
78
/
/
pshufd
0x4e
%
xmm5
%
xmm4
.
byte
102
15
126
225
/
/
movd
%
xmm4
%
ecx
.
byte
137
77
136
/
/
mov
%
ecx
-
0x78
(
%
ebp
)
.
byte
102
15
112
229
231
/
/
pshufd
0xe7
%
xmm5
%
xmm4
.
byte
102
15
126
226
/
/
movd
%
xmm4
%
edx
.
byte
139
70
4
/
/
mov
0x4
(
%
esi
)
%
eax
.
byte
243
15
16
36
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm4
.
byte
137
85
232
/
/
mov
%
edx
-
0x18
(
%
ebp
)
.
byte
243
15
16
52
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm6
.
byte
15
20
244
/
/
unpcklps
%
xmm4
%
xmm6
.
byte
102
15
112
229
229
/
/
pshufd
0xe5
%
xmm5
%
xmm4
.
byte
102
15
126
233
/
/
movd
%
xmm5
%
ecx
.
byte
137
77
168
/
/
mov
%
ecx
-
0x58
(
%
ebp
)
.
byte
243
15
16
4
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm0
.
byte
102
15
126
225
/
/
movd
%
xmm4
%
ecx
.
byte
137
77
184
/
/
mov
%
ecx
-
0x48
(
%
ebp
)
.
byte
243
15
16
12
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm1
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
102
15
112
207
231
/
/
pshufd
0xe7
%
xmm7
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
137
77
240
/
/
mov
%
ecx
-
0x10
(
%
ebp
)
.
byte
243
15
16
12
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm1
.
byte
102
15
126
217
/
/
movd
%
xmm3
%
ecx
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
137
207
/
/
mov
%
ecx
%
edi
.
byte
137
125
200
/
/
mov
%
edi
-
0x38
(
%
ebp
)
.
byte
102
15
126
249
/
/
movd
%
xmm7
%
ecx
.
byte
137
77
236
/
/
mov
%
ecx
-
0x14
(
%
ebp
)
.
byte
243
15
16
36
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm4
.
byte
137
117
228
/
/
mov
%
esi
-
0x1c
(
%
ebp
)
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
137
203
/
/
mov
%
ecx
%
ebx
.
byte
137
93
224
/
/
mov
%
ebx
-
0x20
(
%
ebp
)
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
133
120
255
255
255
/
/
movapd
%
xmm0
-
0x88
(
%
ebp
)
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
15
20
226
/
/
unpcklps
%
xmm2
%
xmm4
.
byte
102
15
20
227
/
/
unpcklpd
%
xmm3
%
xmm4
.
byte
139
70
8
/
/
mov
0x8
(
%
esi
)
%
eax
.
byte
243
15
16
12
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm1
.
byte
139
117
136
/
/
mov
-
0x78
(
%
ebp
)
%
esi
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
139
77
168
/
/
mov
-
0x58
(
%
ebp
)
%
ecx
.
byte
243
15
16
4
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm0
.
byte
139
85
184
/
/
mov
-
0x48
(
%
ebp
)
%
edx
.
byte
243
15
16
12
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm1
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
102
15
20
195
/
/
unpcklpd
%
xmm3
%
xmm0
.
byte
102
15
41
133
8
255
255
255
/
/
movapd
%
xmm0
-
0xf8
(
%
ebp
)
.
byte
139
85
240
/
/
mov
-
0x10
(
%
ebp
)
%
edx
.
byte
243
15
16
12
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm1
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
139
125
236
/
/
mov
-
0x14
(
%
ebp
)
%
edi
.
byte
243
15
16
44
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm5
.
byte
243
15
16
12
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm1
.
byte
15
20
233
/
/
unpcklps
%
xmm1
%
xmm5
.
byte
102
15
20
235
/
/
unpcklpd
%
xmm3
%
xmm5
.
byte
139
93
228
/
/
mov
-
0x1c
(
%
ebp
)
%
ebx
.
byte
139
67
12
/
/
mov
0xc
(
%
ebx
)
%
eax
.
byte
139
125
232
/
/
mov
-
0x18
(
%
ebp
)
%
edi
.
byte
243
15
16
12
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm1
.
byte
243
15
16
52
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm6
.
byte
15
20
241
/
/
unpcklps
%
xmm1
%
xmm6
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
139
77
184
/
/
mov
-
0x48
(
%
ebp
)
%
ecx
.
byte
243
15
16
12
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm1
.
byte
15
20
217
/
/
unpcklps
%
xmm1
%
xmm3
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
102
15
41
157
232
254
255
255
/
/
movapd
%
xmm3
-
0x118
(
%
ebp
)
.
byte
243
15
16
12
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm1
.
byte
139
85
200
/
/
mov
-
0x38
(
%
ebp
)
%
edx
.
byte
243
15
16
52
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm6
.
byte
15
20
241
/
/
unpcklps
%
xmm1
%
xmm6
.
byte
139
117
236
/
/
mov
-
0x14
(
%
ebp
)
%
esi
.
byte
243
15
16
4
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm0
.
byte
139
125
224
/
/
mov
-
0x20
(
%
ebp
)
%
edi
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
15
20
199
/
/
unpcklps
%
xmm7
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
133
72
255
255
255
/
/
movapd
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
139
67
16
/
/
mov
0x10
(
%
ebx
)
%
eax
.
byte
139
93
232
/
/
mov
-
0x18
(
%
ebp
)
%
ebx
.
byte
243
15
16
52
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm6
.
byte
139
125
136
/
/
mov
-
0x78
(
%
ebp
)
%
edi
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
139
125
168
/
/
mov
-
0x58
(
%
ebp
)
%
edi
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
243
15
16
52
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm6
.
byte
15
20
222
/
/
unpcklps
%
xmm6
%
xmm3
.
byte
102
15
20
223
/
/
unpcklpd
%
xmm7
%
xmm3
.
byte
102
15
41
157
216
254
255
255
/
/
movapd
%
xmm3
-
0x128
(
%
ebp
)
.
byte
139
77
240
/
/
mov
-
0x10
(
%
ebp
)
%
ecx
.
byte
243
15
16
52
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm6
.
byte
243
15
16
60
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm7
.
byte
15
20
254
/
/
unpcklps
%
xmm6
%
xmm7
.
byte
243
15
16
52
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm6
.
byte
139
125
224
/
/
mov
-
0x20
(
%
ebp
)
%
edi
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
102
15
20
247
/
/
unpcklpd
%
xmm7
%
xmm6
.
byte
139
85
228
/
/
mov
-
0x1c
(
%
ebp
)
%
edx
.
byte
139
66
20
/
/
mov
0x14
(
%
edx
)
%
eax
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
139
77
200
/
/
mov
-
0x38
(
%
ebp
)
%
ecx
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
243
15
16
12
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm1
.
byte
137
249
/
/
mov
%
edi
%
ecx
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
137
206
/
/
mov
%
ecx
%
esi
.
byte
15
20
203
/
/
unpcklps
%
xmm3
%
xmm1
.
byte
102
15
20
207
/
/
unpcklpd
%
xmm7
%
xmm1
.
byte
243
15
16
28
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm3
.
byte
139
77
136
/
/
mov
-
0x78
(
%
ebp
)
%
ecx
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
139
125
168
/
/
mov
-
0x58
(
%
ebp
)
%
edi
.
byte
243
15
16
20
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm2
.
byte
139
125
184
/
/
mov
-
0x48
(
%
ebp
)
%
edi
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
102
15
20
215
/
/
unpcklpd
%
xmm7
%
xmm2
.
byte
139
66
24
/
/
mov
0x18
(
%
edx
)
%
eax
.
byte
139
85
240
/
/
mov
-
0x10
(
%
ebp
)
%
edx
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
139
93
200
/
/
mov
-
0x38
(
%
ebp
)
%
ebx
.
byte
243
15
16
60
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
139
93
236
/
/
mov
-
0x14
(
%
ebp
)
%
ebx
.
byte
243
15
16
4
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm0
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm0
.
byte
102
15
41
133
40
255
255
255
/
/
movapd
%
xmm0
-
0xd8
(
%
ebp
)
.
byte
139
117
232
/
/
mov
-
0x18
(
%
ebp
)
%
esi
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
139
77
168
/
/
mov
-
0x58
(
%
ebp
)
%
ecx
.
byte
243
15
16
4
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm0
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm0
.
byte
102
15
41
133
88
255
255
255
/
/
movapd
%
xmm0
-
0xa8
(
%
ebp
)
.
byte
139
69
228
/
/
mov
-
0x1c
(
%
ebp
)
%
eax
.
byte
139
64
28
/
/
mov
0x1c
(
%
eax
)
%
eax
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
139
77
200
/
/
mov
-
0x38
(
%
ebp
)
%
ecx
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
243
15
16
4
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm0
.
byte
139
77
224
/
/
mov
-
0x20
(
%
ebp
)
%
ecx
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm0
.
byte
102
15
41
133
24
255
255
255
/
/
movapd
%
xmm0
-
0xe8
(
%
ebp
)
.
byte
139
117
232
/
/
mov
-
0x18
(
%
ebp
)
%
esi
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
139
125
136
/
/
mov
-
0x78
(
%
ebp
)
%
edi
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
139
93
168
/
/
mov
-
0x58
(
%
ebp
)
%
ebx
.
byte
243
15
16
4
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm0
.
byte
139
85
184
/
/
mov
-
0x48
(
%
ebp
)
%
edx
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm0
.
byte
102
15
41
133
56
255
255
255
/
/
movapd
%
xmm0
-
0xc8
(
%
ebp
)
.
byte
139
69
228
/
/
mov
-
0x1c
(
%
ebp
)
%
eax
.
byte
139
64
32
/
/
mov
0x20
(
%
eax
)
%
eax
.
byte
139
85
240
/
/
mov
-
0x10
(
%
ebp
)
%
edx
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
139
85
200
/
/
mov
-
0x38
(
%
ebp
)
%
edx
.
byte
243
15
16
60
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
139
85
236
/
/
mov
-
0x14
(
%
ebp
)
%
edx
.
byte
243
15
16
4
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm0
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm0
.
byte
102
15
41
69
200
/
/
movapd
%
xmm0
-
0x38
(
%
ebp
)
.
byte
243
15
16
28
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm3
.
byte
243
15
16
60
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm7
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
243
15
16
4
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm0
.
byte
139
77
184
/
/
mov
-
0x48
(
%
ebp
)
%
ecx
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
15
20
195
/
/
unpcklps
%
xmm3
%
xmm0
.
byte
102
15
20
199
/
/
unpcklpd
%
xmm7
%
xmm0
.
byte
102
15
41
69
184
/
/
movapd
%
xmm0
-
0x48
(
%
ebp
)
.
byte
15
40
189
248
254
255
255
/
/
movaps
-
0x108
(
%
ebp
)
%
xmm7
.
byte
15
89
231
/
/
mulps
%
xmm7
%
xmm4
.
byte
15
88
225
/
/
addps
%
xmm1
%
xmm4
.
byte
15
40
157
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm3
.
byte
15
40
133
120
255
255
255
/
/
movaps
-
0x88
(
%
ebp
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
139
69
164
/
/
mov
-
0x5c
(
%
ebp
)
%
eax
.
byte
15
40
136
59
21
0
0
/
/
movaps
0x153b
(
%
eax
)
%
xmm1
.
byte
15
89
225
/
/
mulps
%
xmm1
%
xmm4
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
144
91
20
0
0
/
/
movaps
0x145b
(
%
eax
)
%
xmm2
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
41
133
120
255
255
255
/
/
movaps
%
xmm0
-
0x88
(
%
ebp
)
.
byte
15
88
226
/
/
addps
%
xmm2
%
xmm4
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
224
232
/
/
pshufd
0xe8
%
xmm0
%
xmm4
.
byte
243
15
91
133
120
255
255
255
/
/
cvttps2dq
-
0x88
(
%
ebp
)
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
108
224
/
/
punpcklqdq
%
xmm0
%
xmm4
.
byte
102
15
127
101
168
/
/
movdqa
%
xmm4
-
0x58
(
%
ebp
)
.
byte
15
89
239
/
/
mulps
%
xmm7
%
xmm5
.
byte
15
88
173
40
255
255
255
/
/
addps
-
0xd8
(
%
ebp
)
%
xmm5
.
byte
15
40
133
8
255
255
255
/
/
movaps
-
0xf8
(
%
ebp
)
%
xmm0
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
133
88
255
255
255
/
/
addps
-
0xa8
(
%
ebp
)
%
xmm0
.
byte
15
89
233
/
/
mulps
%
xmm1
%
xmm5
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
40
217
/
/
movaps
%
xmm1
%
xmm3
.
byte
15
88
194
/
/
addps
%
xmm2
%
xmm0
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
88
234
/
/
addps
%
xmm2
%
xmm5
.
byte
15
40
202
/
/
movaps
%
xmm2
%
xmm1
.
byte
243
15
91
197
/
/
cvttps2dq
%
xmm5
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
208
232
/
/
pshufd
0xe8
%
xmm0
%
xmm2
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
108
208
/
/
punpcklqdq
%
xmm0
%
xmm2
.
byte
102
15
127
85
136
/
/
movdqa
%
xmm2
-
0x78
(
%
ebp
)
.
byte
15
40
149
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm2
.
byte
15
89
215
/
/
mulps
%
xmm7
%
xmm2
.
byte
15
88
149
24
255
255
255
/
/
addps
-
0xe8
(
%
ebp
)
%
xmm2
.
byte
15
40
133
232
254
255
255
/
/
movaps
-
0x118
(
%
ebp
)
%
xmm0
.
byte
15
40
173
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm5
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
133
56
255
255
255
/
/
addps
-
0xc8
(
%
ebp
)
%
xmm0
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
88
209
/
/
addps
%
xmm1
%
xmm2
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
208
232
/
/
pshufd
0xe8
%
xmm0
%
xmm2
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
108
208
/
/
punpcklqdq
%
xmm0
%
xmm2
.
byte
15
89
247
/
/
mulps
%
xmm7
%
xmm6
.
byte
15
88
117
200
/
/
addps
-
0x38
(
%
ebp
)
%
xmm6
.
byte
15
40
133
216
254
255
255
/
/
movaps
-
0x128
(
%
ebp
)
%
xmm0
.
byte
15
89
197
/
/
mulps
%
xmm5
%
xmm0
.
byte
15
88
69
184
/
/
addps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
193
/
/
addps
%
xmm1
%
xmm0
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
15
88
241
/
/
addps
%
xmm1
%
xmm6
.
byte
243
15
91
198
/
/
cvttps2dq
%
xmm6
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
216
232
/
/
pshufd
0xe8
%
xmm0
%
xmm3
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
102
15
108
216
/
/
punpcklqdq
%
xmm0
%
xmm3
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
137
193
/
/
mov
%
eax
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
15
40
69
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
64
/
/
movaps
%
xmm0
0x40
(
%
esp
)
.
byte
15
40
69
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
40
69
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
68
36
4
/
/
mov
%
eax
0x4
(
%
esp
)
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
15
40
77
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm1
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
129
196
108
1
0
0
/
/
add
0x16c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_evenly_spaced_gradient_sse2_lowp
.
globl
_sk_evenly_spaced_gradient_sse2_lowp
FUNCTION
(
_sk_evenly_spaced_gradient_sse2_lowp
)
_sk_evenly_spaced_gradient_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
92
1
0
0
/
/
sub
0x15c
%
esp
.
byte
15
41
141
232
254
255
255
/
/
movaps
%
xmm1
-
0x118
(
%
ebp
)
.
byte
15
41
69
136
/
/
movaps
%
xmm0
-
0x78
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
f867
<
_sk_evenly_spaced_gradient_sse2_lowp
+
0x1c
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
48
/
/
mov
(
%
eax
)
%
esi
.
byte
139
14
/
/
mov
(
%
esi
)
%
ecx
.
byte
139
70
4
/
/
mov
0x4
(
%
esi
)
%
eax
.
byte
137
243
/
/
mov
%
esi
%
ebx
.
byte
73
/
/
dec
%
ecx
.
byte
102
15
110
209
/
/
movd
%
ecx
%
xmm2
.
byte
102
15
112
210
0
/
/
pshufd
0x0
%
xmm2
%
xmm2
.
byte
137
85
164
/
/
mov
%
edx
-
0x5c
(
%
ebp
)
.
byte
102
15
111
154
169
20
0
0
/
/
movdqa
0x14a9
(
%
edx
)
%
xmm3
.
byte
102
15
219
218
/
/
pand
%
xmm2
%
xmm3
.
byte
102
15
235
154
41
18
0
0
/
/
por
0x1229
(
%
edx
)
%
xmm3
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
15
235
146
185
20
0
0
/
/
por
0x14b9
(
%
edx
)
%
xmm2
.
byte
15
88
146
201
20
0
0
/
/
addps
0x14c9
(
%
edx
)
%
xmm2
.
byte
15
88
211
/
/
addps
%
xmm3
%
xmm2
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
15
89
218
/
/
mulps
%
xmm2
%
xmm3
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
243
15
91
194
/
/
cvttps2dq
%
xmm2
%
xmm0
.
byte
243
15
91
203
/
/
cvttps2dq
%
xmm3
%
xmm1
.
byte
102
15
112
209
229
/
/
pshufd
0xe5
%
xmm1
%
xmm2
.
byte
102
15
112
217
78
/
/
pshufd
0x4e
%
xmm1
%
xmm3
.
byte
102
15
112
225
231
/
/
pshufd
0xe7
%
xmm1
%
xmm4
.
byte
102
15
126
230
/
/
movd
%
xmm4
%
esi
.
byte
102
15
112
232
229
/
/
pshufd
0xe5
%
xmm0
%
xmm5
.
byte
243
15
16
36
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm4
.
byte
137
117
224
/
/
mov
%
esi
-
0x20
(
%
ebp
)
.
byte
102
15
126
217
/
/
movd
%
xmm3
%
ecx
.
byte
137
77
200
/
/
mov
%
ecx
-
0x38
(
%
ebp
)
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
15
20
220
/
/
unpcklps
%
xmm4
%
xmm3
.
byte
102
15
112
240
78
/
/
pshufd
0x4e
%
xmm0
%
xmm6
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
243
15
16
60
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm7
.
byte
137
202
/
/
mov
%
ecx
%
edx
.
byte
137
85
184
/
/
mov
%
edx
-
0x48
(
%
ebp
)
.
byte
102
15
126
209
/
/
movd
%
xmm2
%
ecx
.
byte
137
77
228
/
/
mov
%
ecx
-
0x1c
(
%
ebp
)
.
byte
243
15
16
12
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm1
.
byte
15
20
249
/
/
unpcklps
%
xmm1
%
xmm7
.
byte
102
15
112
200
231
/
/
pshufd
0xe7
%
xmm0
%
xmm1
.
byte
102
15
126
201
/
/
movd
%
xmm1
%
ecx
.
byte
137
77
240
/
/
mov
%
ecx
-
0x10
(
%
ebp
)
.
byte
243
15
16
12
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm1
.
byte
102
15
126
241
/
/
movd
%
xmm6
%
ecx
.
byte
137
77
168
/
/
mov
%
ecx
-
0x58
(
%
ebp
)
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
102
15
126
193
/
/
movd
%
xmm0
%
ecx
.
byte
243
15
16
36
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm4
.
byte
137
207
/
/
mov
%
ecx
%
edi
.
byte
137
125
232
/
/
mov
%
edi
-
0x18
(
%
ebp
)
.
byte
102
15
126
233
/
/
movd
%
xmm5
%
ecx
.
byte
137
77
236
/
/
mov
%
ecx
-
0x14
(
%
ebp
)
.
byte
243
15
16
44
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm5
.
byte
102
15
20
251
/
/
unpcklpd
%
xmm3
%
xmm7
.
byte
102
15
41
189
24
255
255
255
/
/
movapd
%
xmm7
-
0xe8
(
%
ebp
)
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
15
20
229
/
/
unpcklps
%
xmm5
%
xmm4
.
byte
102
15
20
226
/
/
unpcklpd
%
xmm2
%
xmm4
.
byte
137
217
/
/
mov
%
ebx
%
ecx
.
byte
137
77
220
/
/
mov
%
ecx
-
0x24
(
%
ebp
)
.
byte
139
65
8
/
/
mov
0x8
(
%
ecx
)
%
eax
.
byte
243
15
16
12
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm1
.
byte
139
117
200
/
/
mov
-
0x38
(
%
ebp
)
%
esi
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
4
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm0
.
byte
139
93
228
/
/
mov
-
0x1c
(
%
ebp
)
%
ebx
.
byte
243
15
16
12
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm1
.
byte
15
20
193
/
/
unpcklps
%
xmm1
%
xmm0
.
byte
102
15
20
194
/
/
unpcklpd
%
xmm2
%
xmm0
.
byte
102
15
41
133
72
255
255
255
/
/
movapd
%
xmm0
-
0xb8
(
%
ebp
)
.
byte
139
85
240
/
/
mov
-
0x10
(
%
ebp
)
%
edx
.
byte
243
15
16
12
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm1
.
byte
139
117
168
/
/
mov
-
0x58
(
%
ebp
)
%
esi
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
15
20
209
/
/
unpcklps
%
xmm1
%
xmm2
.
byte
243
15
16
12
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm1
.
byte
139
125
236
/
/
mov
-
0x14
(
%
ebp
)
%
edi
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
203
/
/
unpcklps
%
xmm3
%
xmm1
.
byte
102
15
20
202
/
/
unpcklpd
%
xmm2
%
xmm1
.
byte
139
65
12
/
/
mov
0xc
(
%
ecx
)
%
eax
.
byte
139
85
224
/
/
mov
-
0x20
(
%
ebp
)
%
edx
.
byte
243
15
16
20
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm2
.
byte
139
77
200
/
/
mov
-
0x38
(
%
ebp
)
%
ecx
.
byte
243
15
16
28
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm3
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
139
77
184
/
/
mov
-
0x48
(
%
ebp
)
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
243
15
16
52
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm6
.
byte
15
20
214
/
/
unpcklps
%
xmm6
%
xmm2
.
byte
102
15
20
211
/
/
unpcklpd
%
xmm3
%
xmm2
.
byte
102
15
41
149
8
255
255
255
/
/
movapd
%
xmm2
-
0xf8
(
%
ebp
)
.
byte
139
93
240
/
/
mov
-
0x10
(
%
ebp
)
%
ebx
.
byte
243
15
16
28
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm3
.
byte
243
15
16
52
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
139
117
232
/
/
mov
-
0x18
(
%
ebp
)
%
esi
.
byte
243
15
16
60
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm7
.
byte
243
15
16
28
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm3
.
byte
15
20
251
/
/
unpcklps
%
xmm3
%
xmm7
.
byte
102
15
20
254
/
/
unpcklpd
%
xmm6
%
xmm7
.
byte
139
117
220
/
/
mov
-
0x24
(
%
ebp
)
%
esi
.
byte
139
70
16
/
/
mov
0x10
(
%
esi
)
%
eax
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
139
85
200
/
/
mov
-
0x38
(
%
ebp
)
%
edx
.
byte
243
15
16
52
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
139
85
184
/
/
mov
-
0x48
(
%
ebp
)
%
edx
.
byte
243
15
16
20
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm2
.
byte
139
85
228
/
/
mov
-
0x1c
(
%
ebp
)
%
edx
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
15
20
211
/
/
unpcklps
%
xmm3
%
xmm2
.
byte
102
15
20
214
/
/
unpcklpd
%
xmm6
%
xmm2
.
byte
102
15
41
149
248
254
255
255
/
/
movapd
%
xmm2
-
0x108
(
%
ebp
)
.
byte
243
15
16
28
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm3
.
byte
139
125
168
/
/
mov
-
0x58
(
%
ebp
)
%
edi
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
15
20
243
/
/
unpcklps
%
xmm3
%
xmm6
.
byte
139
85
232
/
/
mov
-
0x18
(
%
ebp
)
%
edx
.
byte
243
15
16
4
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm0
.
byte
139
77
236
/
/
mov
-
0x14
(
%
ebp
)
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
15
20
194
/
/
unpcklps
%
xmm2
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
133
104
255
255
255
/
/
movapd
%
xmm0
-
0x98
(
%
ebp
)
.
byte
139
70
20
/
/
mov
0x14
(
%
esi
)
%
eax
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
243
15
16
28
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm3
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
15
20
218
/
/
unpcklps
%
xmm2
%
xmm3
.
byte
102
15
20
222
/
/
unpcklpd
%
xmm6
%
xmm3
.
byte
139
93
224
/
/
mov
-
0x20
(
%
ebp
)
%
ebx
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
139
125
200
/
/
mov
-
0x38
(
%
ebp
)
%
edi
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
139
77
184
/
/
mov
-
0x48
(
%
ebp
)
%
ecx
.
byte
243
15
16
44
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm5
.
byte
139
77
228
/
/
mov
-
0x1c
(
%
ebp
)
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
15
20
234
/
/
unpcklps
%
xmm2
%
xmm5
.
byte
102
15
20
238
/
/
unpcklpd
%
xmm6
%
xmm5
.
byte
139
69
220
/
/
mov
-
0x24
(
%
ebp
)
%
eax
.
byte
139
64
24
/
/
mov
0x18
(
%
eax
)
%
eax
.
byte
139
117
240
/
/
mov
-
0x10
(
%
ebp
)
%
esi
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
139
117
168
/
/
mov
-
0x58
(
%
ebp
)
%
esi
.
byte
243
15
16
52
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
243
15
16
4
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm0
.
byte
139
85
236
/
/
mov
-
0x14
(
%
ebp
)
%
edx
.
byte
243
15
16
20
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm2
.
byte
15
20
194
/
/
unpcklps
%
xmm2
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
133
56
255
255
255
/
/
movapd
%
xmm0
-
0xc8
(
%
ebp
)
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
139
117
184
/
/
mov
-
0x48
(
%
ebp
)
%
esi
.
byte
243
15
16
4
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm0
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
15
20
194
/
/
unpcklps
%
xmm2
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
133
120
255
255
255
/
/
movapd
%
xmm0
-
0x88
(
%
ebp
)
.
byte
139
125
220
/
/
mov
-
0x24
(
%
ebp
)
%
edi
.
byte
139
71
28
/
/
mov
0x1c
(
%
edi
)
%
eax
.
byte
139
93
240
/
/
mov
-
0x10
(
%
ebp
)
%
ebx
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
139
77
168
/
/
mov
-
0x58
(
%
ebp
)
%
ecx
.
byte
243
15
16
52
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
139
77
232
/
/
mov
-
0x18
(
%
ebp
)
%
ecx
.
byte
243
15
16
4
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm0
.
byte
243
15
16
20
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm2
.
byte
15
20
194
/
/
unpcklps
%
xmm2
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
133
40
255
255
255
/
/
movapd
%
xmm0
-
0xd8
(
%
ebp
)
.
byte
139
77
224
/
/
mov
-
0x20
(
%
ebp
)
%
ecx
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
139
85
200
/
/
mov
-
0x38
(
%
ebp
)
%
edx
.
byte
243
15
16
52
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
243
15
16
4
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm0
.
byte
139
117
228
/
/
mov
-
0x1c
(
%
ebp
)
%
esi
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
15
20
194
/
/
unpcklps
%
xmm2
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
133
88
255
255
255
/
/
movapd
%
xmm0
-
0xa8
(
%
ebp
)
.
byte
139
71
32
/
/
mov
0x20
(
%
edi
)
%
eax
.
byte
243
15
16
20
152
/
/
movss
(
%
eax
%
ebx
4
)
%
xmm2
.
byte
139
125
168
/
/
mov
-
0x58
(
%
ebp
)
%
edi
.
byte
243
15
16
52
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
139
125
232
/
/
mov
-
0x18
(
%
ebp
)
%
edi
.
byte
243
15
16
4
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm0
.
byte
139
125
236
/
/
mov
-
0x14
(
%
ebp
)
%
edi
.
byte
243
15
16
20
184
/
/
movss
(
%
eax
%
edi
4
)
%
xmm2
.
byte
15
20
194
/
/
unpcklps
%
xmm2
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
69
168
/
/
movapd
%
xmm0
-
0x58
(
%
ebp
)
.
byte
243
15
16
20
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm2
.
byte
243
15
16
52
144
/
/
movss
(
%
eax
%
edx
4
)
%
xmm6
.
byte
15
20
242
/
/
unpcklps
%
xmm2
%
xmm6
.
byte
139
77
184
/
/
mov
-
0x48
(
%
ebp
)
%
ecx
.
byte
243
15
16
4
136
/
/
movss
(
%
eax
%
ecx
4
)
%
xmm0
.
byte
243
15
16
20
176
/
/
movss
(
%
eax
%
esi
4
)
%
xmm2
.
byte
15
20
194
/
/
unpcklps
%
xmm2
%
xmm0
.
byte
102
15
20
198
/
/
unpcklpd
%
xmm6
%
xmm0
.
byte
102
15
41
69
200
/
/
movapd
%
xmm0
-
0x38
(
%
ebp
)
.
byte
15
40
181
232
254
255
255
/
/
movaps
-
0x118
(
%
ebp
)
%
xmm6
.
byte
15
89
230
/
/
mulps
%
xmm6
%
xmm4
.
byte
15
88
227
/
/
addps
%
xmm3
%
xmm4
.
byte
15
40
133
24
255
255
255
/
/
movaps
-
0xe8
(
%
ebp
)
%
xmm0
.
byte
15
89
69
136
/
/
mulps
-
0x78
(
%
ebp
)
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
139
69
164
/
/
mov
-
0x5c
(
%
ebp
)
%
eax
.
byte
15
40
152
201
15
0
0
/
/
movaps
0xfc9
(
%
eax
)
%
xmm3
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
40
168
233
14
0
0
/
/
movaps
0xee9
(
%
eax
)
%
xmm5
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
243
15
91
196
/
/
cvttps2dq
%
xmm4
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
108
208
/
/
punpcklqdq
%
xmm0
%
xmm2
.
byte
102
15
127
85
184
/
/
movdqa
%
xmm2
-
0x48
(
%
ebp
)
.
byte
15
89
206
/
/
mulps
%
xmm6
%
xmm1
.
byte
15
40
230
/
/
movaps
%
xmm6
%
xmm4
.
byte
15
88
141
56
255
255
255
/
/
addps
-
0xc8
(
%
ebp
)
%
xmm1
.
byte
15
40
85
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm2
.
byte
15
40
133
72
255
255
255
/
/
movaps
-
0xb8
(
%
ebp
)
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
88
133
120
255
255
255
/
/
addps
-
0x88
(
%
ebp
)
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
40
240
/
/
movaps
%
xmm0
%
xmm6
.
byte
15
88
205
/
/
addps
%
xmm5
%
xmm1
.
byte
243
15
91
193
/
/
cvttps2dq
%
xmm1
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
243
15
91
206
/
/
cvttps2dq
%
xmm6
%
xmm1
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
241
232
/
/
pshufd
0xe8
%
xmm1
%
xmm6
.
byte
102
15
108
240
/
/
punpcklqdq
%
xmm0
%
xmm6
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
88
189
40
255
255
255
/
/
addps
-
0xd8
(
%
ebp
)
%
xmm7
.
byte
15
40
133
8
255
255
255
/
/
movaps
-
0xf8
(
%
ebp
)
%
xmm0
.
byte
15
89
194
/
/
mulps
%
xmm2
%
xmm0
.
byte
15
88
133
88
255
255
255
/
/
addps
-
0xa8
(
%
ebp
)
%
xmm0
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
89
195
/
/
mulps
%
xmm3
%
xmm0
.
byte
15
88
197
/
/
addps
%
xmm5
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
88
253
/
/
addps
%
xmm5
%
xmm7
.
byte
243
15
91
199
/
/
cvttps2dq
%
xmm7
%
xmm0
.
byte
242
15
112
192
232
/
/
pshuflw
0xe8
%
xmm0
%
xmm0
.
byte
243
15
112
192
232
/
/
pshufhw
0xe8
%
xmm0
%
xmm0
.
byte
102
15
112
192
232
/
/
pshufd
0xe8
%
xmm0
%
xmm0
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
209
232
/
/
pshufd
0xe8
%
xmm1
%
xmm2
.
byte
102
15
108
208
/
/
punpcklqdq
%
xmm0
%
xmm2
.
byte
15
40
189
104
255
255
255
/
/
movaps
-
0x98
(
%
ebp
)
%
xmm7
.
byte
15
89
252
/
/
mulps
%
xmm4
%
xmm7
.
byte
15
88
125
168
/
/
addps
-
0x58
(
%
ebp
)
%
xmm7
.
byte
15
40
165
248
254
255
255
/
/
movaps
-
0x108
(
%
ebp
)
%
xmm4
.
byte
15
89
101
136
/
/
mulps
-
0x78
(
%
ebp
)
%
xmm4
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
88
101
200
/
/
addps
-
0x38
(
%
ebp
)
%
xmm4
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
40
77
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm1
.
byte
15
88
229
/
/
addps
%
xmm5
%
xmm4
.
byte
15
88
253
/
/
addps
%
xmm5
%
xmm7
.
byte
15
40
69
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm0
.
byte
243
15
91
223
/
/
cvttps2dq
%
xmm7
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
251
232
/
/
pshufd
0xe8
%
xmm3
%
xmm7
.
byte
243
15
91
220
/
/
cvttps2dq
%
xmm4
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
219
232
/
/
pshufd
0xe8
%
xmm3
%
xmm3
.
byte
102
15
108
223
/
/
punpcklqdq
%
xmm7
%
xmm3
.
byte
15
40
125
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm7
.
byte
139
125
12
/
/
mov
0xc
(
%
ebp
)
%
edi
.
byte
141
119
8
/
/
lea
0x8
(
%
edi
)
%
esi
.
byte
15
41
124
36
64
/
/
movaps
%
xmm7
0x40
(
%
esp
)
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
41
76
36
32
/
/
movaps
%
xmm1
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
116
36
4
/
/
mov
%
esi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
102
15
111
206
/
/
movdqa
%
xmm6
%
xmm1
.
byte
255
87
4
/
/
call
*
0x4
(
%
edi
)
.
byte
129
196
92
1
0
0
/
/
add
0x15c
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
.
globl
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
FUNCTION
(
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
)
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
96
/
/
sub
0x60
%
esp
.
byte
15
40
241
/
/
movaps
%
xmm1
%
xmm6
.
byte
232
0
0
0
0
/
/
call
fda6
<
_sk_evenly_spaced_2_stop_gradient_sse2_lowp
+
0x10
>
.
byte
90
/
/
pop
%
edx
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
243
15
16
33
/
/
movss
(
%
ecx
)
%
xmm4
.
byte
243
15
16
81
4
/
/
movss
0x4
(
%
ecx
)
%
xmm2
.
byte
15
198
228
0
/
/
shufps
0x0
%
xmm4
%
xmm4
.
byte
243
15
16
89
16
/
/
movss
0x10
(
%
ecx
)
%
xmm3
.
byte
15
198
219
0
/
/
shufps
0x0
%
xmm3
%
xmm3
.
byte
15
40
238
/
/
movaps
%
xmm6
%
xmm5
.
byte
15
89
236
/
/
mulps
%
xmm4
%
xmm5
.
byte
15
89
224
/
/
mulps
%
xmm0
%
xmm4
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
88
227
/
/
addps
%
xmm3
%
xmm4
.
byte
15
88
235
/
/
addps
%
xmm3
%
xmm5
.
byte
15
40
154
138
10
0
0
/
/
movaps
0xa8a
(
%
edx
)
%
xmm3
.
byte
15
89
235
/
/
mulps
%
xmm3
%
xmm5
.
byte
15
89
227
/
/
mulps
%
xmm3
%
xmm4
.
byte
15
40
130
170
9
0
0
/
/
movaps
0x9aa
(
%
edx
)
%
xmm0
.
byte
15
88
224
/
/
addps
%
xmm0
%
xmm4
.
byte
15
88
232
/
/
addps
%
xmm0
%
xmm5
.
byte
243
15
91
237
/
/
cvttps2dq
%
xmm5
%
xmm5
.
byte
242
15
112
237
232
/
/
pshuflw
0xe8
%
xmm5
%
xmm5
.
byte
243
15
112
237
232
/
/
pshufhw
0xe8
%
xmm5
%
xmm5
.
byte
102
15
112
237
232
/
/
pshufd
0xe8
%
xmm5
%
xmm5
.
byte
243
15
91
228
/
/
cvttps2dq
%
xmm4
%
xmm4
.
byte
242
15
112
228
232
/
/
pshuflw
0xe8
%
xmm4
%
xmm4
.
byte
243
15
112
228
232
/
/
pshufhw
0xe8
%
xmm4
%
xmm4
.
byte
102
15
112
228
232
/
/
pshufd
0xe8
%
xmm4
%
xmm4
.
byte
102
15
108
229
/
/
punpcklqdq
%
xmm5
%
xmm4
.
byte
102
15
127
101
232
/
/
movdqa
%
xmm4
-
0x18
(
%
ebp
)
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
105
20
/
/
movss
0x14
(
%
ecx
)
%
xmm5
.
byte
15
198
237
0
/
/
shufps
0x0
%
xmm5
%
xmm5
.
byte
15
40
254
/
/
movaps
%
xmm6
%
xmm7
.
byte
15
89
250
/
/
mulps
%
xmm2
%
xmm7
.
byte
15
89
209
/
/
mulps
%
xmm1
%
xmm2
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
88
213
/
/
addps
%
xmm5
%
xmm2
.
byte
15
88
253
/
/
addps
%
xmm5
%
xmm7
.
byte
15
89
251
/
/
mulps
%
xmm3
%
xmm7
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
243
15
91
239
/
/
cvttps2dq
%
xmm7
%
xmm5
.
byte
242
15
112
237
232
/
/
pshuflw
0xe8
%
xmm5
%
xmm5
.
byte
243
15
112
237
232
/
/
pshufhw
0xe8
%
xmm5
%
xmm5
.
byte
102
15
112
253
232
/
/
pshufd
0xe8
%
xmm5
%
xmm7
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
234
232
/
/
pshufd
0xe8
%
xmm2
%
xmm5
.
byte
102
15
108
239
/
/
punpcklqdq
%
xmm7
%
xmm5
.
byte
243
15
16
81
8
/
/
movss
0x8
(
%
ecx
)
%
xmm2
.
byte
15
198
210
0
/
/
shufps
0x0
%
xmm2
%
xmm2
.
byte
243
15
16
121
24
/
/
movss
0x18
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
40
206
/
/
movaps
%
xmm6
%
xmm1
.
byte
15
89
202
/
/
mulps
%
xmm2
%
xmm1
.
byte
15
89
212
/
/
mulps
%
xmm4
%
xmm2
.
byte
15
88
215
/
/
addps
%
xmm7
%
xmm2
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
139
125
20
/
/
mov
0x14
(
%
ebp
)
%
edi
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
89
211
/
/
mulps
%
xmm3
%
xmm2
.
byte
15
88
208
/
/
addps
%
xmm0
%
xmm2
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
201
232
/
/
pshufd
0xe8
%
xmm1
%
xmm1
.
byte
243
15
91
210
/
/
cvttps2dq
%
xmm2
%
xmm2
.
byte
242
15
112
210
232
/
/
pshuflw
0xe8
%
xmm2
%
xmm2
.
byte
243
15
112
210
232
/
/
pshufhw
0xe8
%
xmm2
%
xmm2
.
byte
102
15
112
210
232
/
/
pshufd
0xe8
%
xmm2
%
xmm2
.
byte
102
15
108
209
/
/
punpcklqdq
%
xmm1
%
xmm2
.
byte
243
15
16
73
12
/
/
movss
0xc
(
%
ecx
)
%
xmm1
.
byte
15
198
201
0
/
/
shufps
0x0
%
xmm1
%
xmm1
.
byte
15
89
241
/
/
mulps
%
xmm1
%
xmm6
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
243
15
16
121
28
/
/
movss
0x1c
(
%
ecx
)
%
xmm7
.
byte
15
198
255
0
/
/
shufps
0x0
%
xmm7
%
xmm7
.
byte
15
88
207
/
/
addps
%
xmm7
%
xmm1
.
byte
15
88
247
/
/
addps
%
xmm7
%
xmm6
.
byte
15
40
101
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm4
.
byte
15
89
243
/
/
mulps
%
xmm3
%
xmm6
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
40
125
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm7
.
byte
15
88
200
/
/
addps
%
xmm0
%
xmm1
.
byte
15
88
240
/
/
addps
%
xmm0
%
xmm6
.
byte
15
40
69
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm0
.
byte
243
15
91
222
/
/
cvttps2dq
%
xmm6
%
xmm3
.
byte
242
15
112
219
232
/
/
pshuflw
0xe8
%
xmm3
%
xmm3
.
byte
243
15
112
219
232
/
/
pshufhw
0xe8
%
xmm3
%
xmm3
.
byte
102
15
112
243
232
/
/
pshufd
0xe8
%
xmm3
%
xmm6
.
byte
243
15
91
201
/
/
cvttps2dq
%
xmm1
%
xmm1
.
byte
242
15
112
201
232
/
/
pshuflw
0xe8
%
xmm1
%
xmm1
.
byte
243
15
112
201
232
/
/
pshufhw
0xe8
%
xmm1
%
xmm1
.
byte
102
15
112
217
232
/
/
pshufd
0xe8
%
xmm1
%
xmm3
.
byte
102
15
108
222
/
/
punpcklqdq
%
xmm6
%
xmm3
.
byte
15
40
77
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm1
.
byte
141
72
8
/
/
lea
0x8
(
%
eax
)
%
ecx
.
byte
15
41
76
36
64
/
/
movaps
%
xmm1
0x40
(
%
esp
)
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
41
124
36
32
/
/
movaps
%
xmm7
0x20
(
%
esp
)
.
byte
15
41
100
36
16
/
/
movaps
%
xmm4
0x10
(
%
esp
)
.
byte
137
124
36
12
/
/
mov
%
edi
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
76
36
4
/
/
mov
%
ecx
0x4
(
%
esp
)
.
byte
137
20
36
/
/
mov
%
edx
(
%
esp
)
.
byte
15
40
69
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
102
15
111
205
/
/
movdqa
%
xmm5
%
xmm1
.
byte
255
80
4
/
/
call
*
0x4
(
%
eax
)
.
byte
131
196
96
/
/
add
0x60
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_unit_angle_sse2_lowp
.
globl
_sk_xy_to_unit_angle_sse2_lowp
FUNCTION
(
_sk_xy_to_unit_angle_sse2_lowp
)
_sk_xy_to_unit_angle_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
160
0
0
0
/
/
sub
0xa0
%
esp
.
byte
15
41
93
168
/
/
movaps
%
xmm3
-
0x58
(
%
ebp
)
.
byte
15
41
85
232
/
/
movaps
%
xmm2
-
0x18
(
%
ebp
)
.
byte
15
41
77
184
/
/
movaps
%
xmm1
-
0x48
(
%
ebp
)
.
byte
15
40
208
/
/
movaps
%
xmm0
%
xmm2
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
232
0
0
0
0
/
/
call
ff91
<
_sk_xy_to_unit_angle_sse2_lowp
+
0x23
>
.
byte
88
/
/
pop
%
eax
.
byte
15
40
128
15
13
0
0
/
/
movaps
0xd0f
(
%
eax
)
%
xmm0
.
byte
15
40
225
/
/
movaps
%
xmm1
%
xmm4
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
40
203
/
/
movaps
%
xmm3
%
xmm1
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
15
40
236
/
/
movaps
%
xmm4
%
xmm5
.
byte
15
194
233
1
/
/
cmpltps
%
xmm1
%
xmm5
.
byte
15
40
245
/
/
movaps
%
xmm5
%
xmm6
.
byte
15
85
241
/
/
andnps
%
xmm1
%
xmm6
.
byte
15
40
221
/
/
movaps
%
xmm5
%
xmm3
.
byte
15
85
220
/
/
andnps
%
xmm4
%
xmm3
.
byte
15
41
93
216
/
/
movaps
%
xmm3
-
0x28
(
%
ebp
)
.
byte
15
84
229
/
/
andps
%
xmm5
%
xmm4
.
byte
15
86
230
/
/
orps
%
xmm6
%
xmm4
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
84
248
/
/
andps
%
xmm0
%
xmm7
.
byte
15
84
69
232
/
/
andps
-
0x18
(
%
ebp
)
%
xmm0
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
15
194
208
1
/
/
cmpltps
%
xmm0
%
xmm2
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
15
85
240
/
/
andnps
%
xmm0
%
xmm6
.
byte
15
40
218
/
/
movaps
%
xmm2
%
xmm3
.
byte
15
85
223
/
/
andnps
%
xmm7
%
xmm3
.
byte
15
84
250
/
/
andps
%
xmm2
%
xmm7
.
byte
15
86
254
/
/
orps
%
xmm6
%
xmm7
.
byte
15
84
205
/
/
andps
%
xmm5
%
xmm1
.
byte
15
86
77
216
/
/
orps
-
0x28
(
%
ebp
)
%
xmm1
.
byte
15
94
225
/
/
divps
%
xmm1
%
xmm4
.
byte
15
84
194
/
/
andps
%
xmm2
%
xmm0
.
byte
15
86
195
/
/
orps
%
xmm3
%
xmm0
.
byte
15
94
248
/
/
divps
%
xmm0
%
xmm7
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
15
89
219
/
/
mulps
%
xmm3
%
xmm3
.
byte
15
40
247
/
/
movaps
%
xmm7
%
xmm6
.
byte
15
89
246
/
/
mulps
%
xmm6
%
xmm6
.
byte
15
40
136
175
13
0
0
/
/
movaps
0xdaf
(
%
eax
)
%
xmm1
.
byte
15
40
198
/
/
movaps
%
xmm6
%
xmm0
.
byte
15
89
193
/
/
mulps
%
xmm1
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
136
191
13
0
0
/
/
addps
0xdbf
(
%
eax
)
%
xmm1
.
byte
15
88
128
191
13
0
0
/
/
addps
0xdbf
(
%
eax
)
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
88
136
207
13
0
0
/
/
addps
0xdcf
(
%
eax
)
%
xmm1
.
byte
15
88
128
207
13
0
0
/
/
addps
0xdcf
(
%
eax
)
%
xmm0
.
byte
15
89
198
/
/
mulps
%
xmm6
%
xmm0
.
byte
15
89
203
/
/
mulps
%
xmm3
%
xmm1
.
byte
15
40
152
223
13
0
0
/
/
movaps
0xddf
(
%
eax
)
%
xmm3
.
byte
15
88
203
/
/
addps
%
xmm3
%
xmm1
.
byte
15
88
195
/
/
addps
%
xmm3
%
xmm0
.
byte
15
89
199
/
/
mulps
%
xmm7
%
xmm0
.
byte
15
89
204
/
/
mulps
%
xmm4
%
xmm1
.
byte
15
40
152
239
13
0
0
/
/
movaps
0xdef
(
%
eax
)
%
xmm3
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
15
92
216
/
/
subps
%
xmm0
%
xmm3
.
byte
15
84
218
/
/
andps
%
xmm2
%
xmm3
.
byte
15
85
208
/
/
andnps
%
xmm0
%
xmm2
.
byte
15
86
211
/
/
orps
%
xmm3
%
xmm2
.
byte
15
92
225
/
/
subps
%
xmm1
%
xmm4
.
byte
15
84
229
/
/
andps
%
xmm5
%
xmm4
.
byte
15
85
233
/
/
andnps
%
xmm1
%
xmm5
.
byte
15
86
236
/
/
orps
%
xmm4
%
xmm5
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
15
40
93
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm3
.
byte
15
194
220
1
/
/
cmpltps
%
xmm4
%
xmm3
.
byte
15
40
128
191
7
0
0
/
/
movaps
0x7bf
(
%
eax
)
%
xmm0
.
byte
15
40
200
/
/
movaps
%
xmm0
%
xmm1
.
byte
15
92
194
/
/
subps
%
xmm2
%
xmm0
.
byte
15
84
195
/
/
andps
%
xmm3
%
xmm0
.
byte
15
85
218
/
/
andnps
%
xmm2
%
xmm3
.
byte
139
77
8
/
/
mov
0x8
(
%
ebp
)
%
ecx
.
byte
139
85
16
/
/
mov
0x10
(
%
ebp
)
%
edx
.
byte
139
117
20
/
/
mov
0x14
(
%
ebp
)
%
esi
.
byte
15
40
85
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm2
.
byte
15
194
212
1
/
/
cmpltps
%
xmm4
%
xmm2
.
byte
15
92
205
/
/
subps
%
xmm5
%
xmm1
.
byte
15
86
216
/
/
orps
%
xmm0
%
xmm3
.
byte
15
84
202
/
/
andps
%
xmm2
%
xmm1
.
byte
15
85
213
/
/
andnps
%
xmm5
%
xmm2
.
byte
15
86
209
/
/
orps
%
xmm1
%
xmm2
.
byte
15
40
242
/
/
movaps
%
xmm2
%
xmm6
.
byte
15
40
125
232
/
/
movaps
-
0x18
(
%
ebp
)
%
xmm7
.
byte
15
40
199
/
/
movaps
%
xmm7
%
xmm0
.
byte
15
194
196
1
/
/
cmpltps
%
xmm4
%
xmm0
.
byte
15
40
136
207
7
0
0
/
/
movaps
0x7cf
(
%
eax
)
%
xmm1
.
byte
15
40
209
/
/
movaps
%
xmm1
%
xmm2
.
byte
15
92
203
/
/
subps
%
xmm3
%
xmm1
.
byte
15
84
200
/
/
andps
%
xmm0
%
xmm1
.
byte
15
85
195
/
/
andnps
%
xmm3
%
xmm0
.
byte
15
86
193
/
/
orps
%
xmm1
%
xmm0
.
byte
15
40
93
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm3
.
byte
15
40
235
/
/
movaps
%
xmm3
%
xmm5
.
byte
15
194
236
1
/
/
cmpltps
%
xmm4
%
xmm5
.
byte
15
92
214
/
/
subps
%
xmm6
%
xmm2
.
byte
15
84
213
/
/
andps
%
xmm5
%
xmm2
.
byte
15
85
238
/
/
andnps
%
xmm6
%
xmm5
.
byte
15
40
117
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm6
.
byte
15
86
234
/
/
orps
%
xmm2
%
xmm5
.
byte
15
40
205
/
/
movaps
%
xmm5
%
xmm1
.
byte
15
194
204
7
/
/
cmpordps
%
xmm4
%
xmm1
.
byte
15
194
224
7
/
/
cmpordps
%
xmm0
%
xmm4
.
byte
15
84
224
/
/
andps
%
xmm0
%
xmm4
.
byte
15
40
69
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm0
.
byte
15
84
205
/
/
andps
%
xmm5
%
xmm1
.
byte
15
40
85
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm2
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
141
120
4
/
/
lea
0x4
(
%
eax
)
%
edi
.
byte
15
41
84
36
64
/
/
movaps
%
xmm2
0x40
(
%
esp
)
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
41
116
36
32
/
/
movaps
%
xmm6
0x20
(
%
esp
)
.
byte
15
40
69
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
137
116
36
12
/
/
mov
%
esi
0xc
(
%
esp
)
.
byte
137
84
36
8
/
/
mov
%
edx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
12
36
/
/
mov
%
ecx
(
%
esp
)
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
15
40
215
/
/
movaps
%
xmm7
%
xmm2
.
byte
255
16
/
/
call
*
(
%
eax
)
.
byte
129
196
160
0
0
0
/
/
add
0xa0
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_xy_to_radius_sse2_lowp
.
globl
_sk_xy_to_radius_sse2_lowp
FUNCTION
(
_sk_xy_to_radius_sse2_lowp
)
_sk_xy_to_radius_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
131
236
80
/
/
sub
0x50
%
esp
.
byte
15
40
227
/
/
movaps
%
xmm3
%
xmm4
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
139
77
16
/
/
mov
0x10
(
%
ebp
)
%
ecx
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
15
40
117
56
/
/
movaps
0x38
(
%
ebp
)
%
xmm6
.
byte
15
89
192
/
/
mulps
%
xmm0
%
xmm0
.
byte
15
89
201
/
/
mulps
%
xmm1
%
xmm1
.
byte
15
40
250
/
/
movaps
%
xmm2
%
xmm7
.
byte
15
89
255
/
/
mulps
%
xmm7
%
xmm7
.
byte
15
88
248
/
/
addps
%
xmm0
%
xmm7
.
byte
15
89
219
/
/
mulps
%
xmm3
%
xmm3
.
byte
15
88
217
/
/
addps
%
xmm1
%
xmm3
.
byte
15
40
109
72
/
/
movaps
0x48
(
%
ebp
)
%
xmm5
.
byte
139
117
12
/
/
mov
0xc
(
%
ebp
)
%
esi
.
byte
15
81
199
/
/
sqrtps
%
xmm7
%
xmm0
.
byte
15
81
203
/
/
sqrtps
%
xmm3
%
xmm1
.
byte
141
126
4
/
/
lea
0x4
(
%
esi
)
%
edi
.
byte
15
41
108
36
64
/
/
movaps
%
xmm5
0x40
(
%
esp
)
.
byte
15
41
116
36
48
/
/
movaps
%
xmm6
0x30
(
%
esp
)
.
byte
15
40
93
40
/
/
movaps
0x28
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
32
/
/
movaps
%
xmm3
0x20
(
%
esp
)
.
byte
15
40
93
24
/
/
movaps
0x18
(
%
ebp
)
%
xmm3
.
byte
15
41
92
36
16
/
/
movaps
%
xmm3
0x10
(
%
esp
)
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
76
36
8
/
/
mov
%
ecx
0x8
(
%
esp
)
.
byte
137
124
36
4
/
/
mov
%
edi
0x4
(
%
esp
)
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
220
/
/
movaps
%
xmm4
%
xmm3
.
byte
255
22
/
/
call
*
(
%
esi
)
.
byte
131
196
80
/
/
add
0x50
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
HIDDEN
_sk_srcover_rgba_8888_sse2_lowp
.
globl
_sk_srcover_rgba_8888_sse2_lowp
FUNCTION
(
_sk_srcover_rgba_8888_sse2_lowp
)
_sk_srcover_rgba_8888_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
172
0
0
0
/
/
sub
0xac
%
esp
.
byte
102
15
111
227
/
/
movdqa
%
xmm3
%
xmm4
.
byte
15
41
85
200
/
/
movaps
%
xmm2
-
0x38
(
%
ebp
)
.
byte
15
41
77
216
/
/
movaps
%
xmm1
-
0x28
(
%
ebp
)
.
byte
102
15
111
248
/
/
movdqa
%
xmm0
%
xmm7
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
121
4
/
/
mov
0x4
(
%
ecx
)
%
edi
.
byte
15
175
125
20
/
/
imul
0x14
(
%
ebp
)
%
edi
.
byte
193
231
2
/
/
shl
0x2
%
edi
.
byte
3
57
/
/
add
(
%
ecx
)
%
edi
.
byte
137
211
/
/
mov
%
edx
%
ebx
.
byte
128
227
7
/
/
and
0x7
%
bl
.
byte
254
203
/
/
dec
%
bl
.
byte
128
251
6
/
/
cmp
0x6
%
bl
.
byte
232
0
0
0
0
/
/
call
101f9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x3f
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
15
182
211
/
/
movzbl
%
bl
%
edx
.
byte
119
29
/
/
ja
1021f
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x65
>
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
139
132
145
75
2
0
0
/
/
mov
0x24b
(
%
ecx
%
edx
4
)
%
eax
.
byte
1
200
/
/
add
%
ecx
%
eax
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
255
224
/
/
jmp
*
%
eax
.
byte
243
15
16
52
183
/
/
movss
(
%
edi
%
esi
4
)
%
xmm6
.
byte
15
87
219
/
/
xorps
%
xmm3
%
xmm3
.
byte
235
73
/
/
jmp
10268
<
_sk_srcover_rgba_8888_sse2_lowp
+
0xae
>
.
byte
102
15
16
52
183
/
/
movupd
(
%
edi
%
esi
4
)
%
xmm6
.
byte
15
16
92
183
16
/
/
movups
0x10
(
%
edi
%
esi
4
)
%
xmm3
.
byte
235
61
/
/
jmp
10268
<
_sk_srcover_rgba_8888_sse2_lowp
+
0xae
>
.
byte
102
15
110
68
183
8
/
/
movd
0x8
(
%
edi
%
esi
4
)
%
xmm0
.
byte
102
15
112
240
69
/
/
pshufd
0x45
%
xmm0
%
xmm6
.
byte
102
15
18
52
183
/
/
movlpd
(
%
edi
%
esi
4
)
%
xmm6
.
byte
235
43
/
/
jmp
10268
<
_sk_srcover_rgba_8888_sse2_lowp
+
0xae
>
.
byte
102
15
110
68
183
24
/
/
movd
0x18
(
%
edi
%
esi
4
)
%
xmm0
.
byte
102
15
112
216
69
/
/
pshufd
0x45
%
xmm0
%
xmm3
.
byte
243
15
16
68
183
20
/
/
movss
0x14
(
%
edi
%
esi
4
)
%
xmm0
.
byte
15
198
195
0
/
/
shufps
0x0
%
xmm3
%
xmm0
.
byte
15
198
195
226
/
/
shufps
0xe2
%
xmm3
%
xmm0
.
byte
15
40
216
/
/
movaps
%
xmm0
%
xmm3
.
byte
243
15
16
68
183
16
/
/
movss
0x10
(
%
edi
%
esi
4
)
%
xmm0
.
byte
243
15
16
216
/
/
movss
%
xmm0
%
xmm3
.
byte
102
15
16
52
183
/
/
movupd
(
%
edi
%
esi
4
)
%
xmm6
.
byte
102
15
40
206
/
/
movapd
%
xmm6
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
15
40
195
/
/
movaps
%
xmm3
%
xmm0
.
byte
102
15
40
214
/
/
movapd
%
xmm6
%
xmm2
.
byte
102
15
114
214
24
/
/
psrld
0x18
%
xmm6
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
15
114
211
24
/
/
psrld
0x18
%
xmm3
.
byte
102
15
114
243
16
/
/
pslld
0x10
%
xmm3
.
byte
102
15
114
227
16
/
/
psrad
0x10
%
xmm3
.
byte
102
15
107
243
/
/
packssdw
%
xmm3
%
xmm6
.
byte
102
15
111
153
103
12
0
0
/
/
movdqa
0xc67
(
%
ecx
)
%
xmm3
.
byte
102
15
114
208
16
/
/
psrld
0x10
%
xmm0
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
208
/
/
packssdw
%
xmm0
%
xmm2
.
byte
102
15
111
193
/
/
movdqa
%
xmm1
%
xmm0
.
byte
102
15
219
195
/
/
pand
%
xmm3
%
xmm0
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
249
236
/
/
psubw
%
xmm4
%
xmm5
.
byte
102
15
127
69
184
/
/
movdqa
%
xmm0
-
0x48
(
%
ebp
)
.
byte
102
15
213
197
/
/
pmullw
%
xmm5
%
xmm0
.
byte
102
15
253
195
/
/
paddw
%
xmm3
%
xmm0
.
byte
102
15
113
208
8
/
/
psrlw
0x8
%
xmm0
.
byte
102
15
253
199
/
/
paddw
%
xmm7
%
xmm0
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
127
77
152
/
/
movdqa
%
xmm1
-
0x68
(
%
ebp
)
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
15
253
203
/
/
paddw
%
xmm3
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
253
77
216
/
/
paddw
-
0x28
(
%
ebp
)
%
xmm1
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
102
15
127
85
216
/
/
movdqa
%
xmm2
-
0x28
(
%
ebp
)
.
byte
102
15
111
250
/
/
movdqa
%
xmm2
%
xmm7
.
byte
102
15
213
253
/
/
pmullw
%
xmm5
%
xmm7
.
byte
102
15
253
251
/
/
paddw
%
xmm3
%
xmm7
.
byte
102
15
113
215
8
/
/
psrlw
0x8
%
xmm7
.
byte
102
15
253
125
200
/
/
paddw
-
0x38
(
%
ebp
)
%
xmm7
.
byte
102
15
127
117
200
/
/
movdqa
%
xmm6
-
0x38
(
%
ebp
)
.
byte
102
15
213
238
/
/
pmullw
%
xmm6
%
xmm5
.
byte
102
15
253
235
/
/
paddw
%
xmm3
%
xmm5
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
253
236
/
/
paddw
%
xmm4
%
xmm5
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
113
246
8
/
/
psllw
0x8
%
xmm6
.
byte
102
15
127
69
168
/
/
movdqa
%
xmm0
-
0x58
(
%
ebp
)
.
byte
102
15
235
240
/
/
por
%
xmm0
%
xmm6
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
111
214
/
/
movdqa
%
xmm6
%
xmm2
.
byte
102
15
97
211
/
/
punpcklwd
%
xmm3
%
xmm2
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
113
244
8
/
/
psllw
0x8
%
xmm4
.
byte
102
15
235
231
/
/
por
%
xmm7
%
xmm4
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
97
196
/
/
punpcklwd
%
xmm4
%
xmm0
.
byte
102
15
235
194
/
/
por
%
xmm2
%
xmm0
.
byte
102
15
105
243
/
/
punpckhwd
%
xmm3
%
xmm6
.
byte
102
15
105
220
/
/
punpckhwd
%
xmm4
%
xmm3
.
byte
102
15
235
222
/
/
por
%
xmm6
%
xmm3
.
byte
128
251
6
/
/
cmp
0x6
%
bl
.
byte
119
16
/
/
ja
103a9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x1ef
>
.
byte
3
140
145
103
2
0
0
/
/
add
0x267
(
%
ecx
%
edx
4
)
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
102
15
126
4
183
/
/
movd
%
xmm0
(
%
edi
%
esi
4
)
.
byte
235
64
/
/
jmp
103e9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x22f
>
.
byte
243
15
127
4
183
/
/
movdqu
%
xmm0
(
%
edi
%
esi
4
)
.
byte
243
15
127
92
183
16
/
/
movdqu
%
xmm3
0x10
(
%
edi
%
esi
4
)
.
byte
235
51
/
/
jmp
103e9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x22f
>
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
15
126
84
183
8
/
/
movd
%
xmm2
0x8
(
%
edi
%
esi
4
)
.
byte
102
15
214
4
183
/
/
movq
%
xmm0
(
%
edi
%
esi
4
)
.
byte
235
33
/
/
jmp
103e9
<
_sk_srcover_rgba_8888_sse2_lowp
+
0x22f
>
.
byte
102
15
112
211
78
/
/
pshufd
0x4e
%
xmm3
%
xmm2
.
byte
102
15
126
84
183
24
/
/
movd
%
xmm2
0x18
(
%
edi
%
esi
4
)
.
byte
102
15
112
211
229
/
/
pshufd
0xe5
%
xmm3
%
xmm2
.
byte
102
15
126
84
183
20
/
/
movd
%
xmm2
0x14
(
%
edi
%
esi
4
)
.
byte
102
15
126
92
183
16
/
/
movd
%
xmm3
0x10
(
%
edi
%
esi
4
)
.
byte
243
15
127
4
183
/
/
movdqu
%
xmm0
(
%
edi
%
esi
4
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
137
193
/
/
mov
%
eax
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
15
40
69
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
64
/
/
movaps
%
xmm0
0x40
(
%
esp
)
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
40
69
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
68
36
4
/
/
mov
%
eax
0x4
(
%
esp
)
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
102
15
111
215
/
/
movdqa
%
xmm7
%
xmm2
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
129
196
172
0
0
0
/
/
add
0xac
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
28
0
/
/
sbb
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
61
0
0
0
50
/
/
cmp
0x32000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
106
0
/
/
add
%
ch
0x0
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
96
/
/
pusha
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
79
0
/
/
add
%
cl
0x0
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
169
1
0
0
200
/
/
add
%
ch
-
0x37ffffff
(
%
ecx
)
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
189
1
0
0
235
/
/
add
%
bh
-
0x14ffffff
(
%
ebp
)
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
229
/
/
add
%
ah
%
ch
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
218
/
/
add
%
bl
%
dl
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
207
/
/
add
%
cl
%
bh
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
HIDDEN
_sk_srcover_bgra_8888_sse2_lowp
.
globl
_sk_srcover_bgra_8888_sse2_lowp
FUNCTION
(
_sk_srcover_bgra_8888_sse2_lowp
)
_sk_srcover_bgra_8888_sse2_lowp
:
.
byte
85
/
/
push
%
ebp
.
byte
137
229
/
/
mov
%
esp
%
ebp
.
byte
83
/
/
push
%
ebx
.
byte
87
/
/
push
%
edi
.
byte
86
/
/
push
%
esi
.
byte
129
236
188
0
0
0
/
/
sub
0xbc
%
esp
.
byte
102
15
111
251
/
/
movdqa
%
xmm3
%
xmm7
.
byte
15
41
85
184
/
/
movaps
%
xmm2
-
0x48
(
%
ebp
)
.
byte
15
41
77
200
/
/
movaps
%
xmm1
-
0x38
(
%
ebp
)
.
byte
102
15
127
69
216
/
/
movdqa
%
xmm0
-
0x28
(
%
ebp
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
139
85
8
/
/
mov
0x8
(
%
ebp
)
%
edx
.
byte
139
8
/
/
mov
(
%
eax
)
%
ecx
.
byte
139
121
4
/
/
mov
0x4
(
%
ecx
)
%
edi
.
byte
15
175
125
20
/
/
imul
0x14
(
%
ebp
)
%
edi
.
byte
193
231
2
/
/
shl
0x2
%
edi
.
byte
3
57
/
/
add
(
%
ecx
)
%
edi
.
byte
137
211
/
/
mov
%
edx
%
ebx
.
byte
128
227
7
/
/
and
0x7
%
bl
.
byte
254
203
/
/
dec
%
bl
.
byte
128
251
6
/
/
cmp
0x6
%
bl
.
byte
232
0
0
0
0
/
/
call
104bc
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x40
>
.
byte
89
/
/
pop
%
ecx
.
byte
139
117
16
/
/
mov
0x10
(
%
ebp
)
%
esi
.
byte
15
182
211
/
/
movzbl
%
bl
%
edx
.
byte
119
29
/
/
ja
104e2
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x66
>
.
byte
102
15
239
228
/
/
pxor
%
xmm4
%
xmm4
.
byte
139
132
145
80
2
0
0
/
/
mov
0x250
(
%
ecx
%
edx
4
)
%
eax
.
byte
1
200
/
/
add
%
ecx
%
eax
.
byte
102
15
239
246
/
/
pxor
%
xmm6
%
xmm6
.
byte
255
224
/
/
jmp
*
%
eax
.
byte
243
15
16
52
183
/
/
movss
(
%
edi
%
esi
4
)
%
xmm6
.
byte
15
87
228
/
/
xorps
%
xmm4
%
xmm4
.
byte
235
73
/
/
jmp
1052b
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xaf
>
.
byte
102
15
16
52
183
/
/
movupd
(
%
edi
%
esi
4
)
%
xmm6
.
byte
15
16
100
183
16
/
/
movups
0x10
(
%
edi
%
esi
4
)
%
xmm4
.
byte
235
61
/
/
jmp
1052b
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xaf
>
.
byte
102
15
110
68
183
8
/
/
movd
0x8
(
%
edi
%
esi
4
)
%
xmm0
.
byte
102
15
112
240
69
/
/
pshufd
0x45
%
xmm0
%
xmm6
.
byte
102
15
18
52
183
/
/
movlpd
(
%
edi
%
esi
4
)
%
xmm6
.
byte
235
43
/
/
jmp
1052b
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xaf
>
.
byte
102
15
110
68
183
24
/
/
movd
0x18
(
%
edi
%
esi
4
)
%
xmm0
.
byte
102
15
112
224
69
/
/
pshufd
0x45
%
xmm0
%
xmm4
.
byte
243
15
16
68
183
20
/
/
movss
0x14
(
%
edi
%
esi
4
)
%
xmm0
.
byte
15
198
196
0
/
/
shufps
0x0
%
xmm4
%
xmm0
.
byte
15
198
196
226
/
/
shufps
0xe2
%
xmm4
%
xmm0
.
byte
15
40
224
/
/
movaps
%
xmm0
%
xmm4
.
byte
243
15
16
68
183
16
/
/
movss
0x10
(
%
edi
%
esi
4
)
%
xmm0
.
byte
243
15
16
224
/
/
movss
%
xmm0
%
xmm4
.
byte
102
15
16
52
183
/
/
movupd
(
%
edi
%
esi
4
)
%
xmm6
.
byte
102
15
40
206
/
/
movapd
%
xmm6
%
xmm1
.
byte
102
15
114
241
16
/
/
pslld
0x10
%
xmm1
.
byte
102
15
114
225
16
/
/
psrad
0x10
%
xmm1
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
200
/
/
packssdw
%
xmm0
%
xmm1
.
byte
15
40
196
/
/
movaps
%
xmm4
%
xmm0
.
byte
102
15
40
214
/
/
movapd
%
xmm6
%
xmm2
.
byte
102
15
114
214
24
/
/
psrld
0x18
%
xmm6
.
byte
102
15
114
246
16
/
/
pslld
0x10
%
xmm6
.
byte
102
15
114
230
16
/
/
psrad
0x10
%
xmm6
.
byte
102
15
114
212
24
/
/
psrld
0x18
%
xmm4
.
byte
102
15
114
244
16
/
/
pslld
0x10
%
xmm4
.
byte
102
15
114
228
16
/
/
psrad
0x10
%
xmm4
.
byte
102
15
107
244
/
/
packssdw
%
xmm4
%
xmm6
.
byte
102
15
111
153
164
9
0
0
/
/
movdqa
0x9a4
(
%
ecx
)
%
xmm3
.
byte
102
15
114
208
16
/
/
psrld
0x10
%
xmm0
.
byte
102
15
114
210
16
/
/
psrld
0x10
%
xmm2
.
byte
102
15
114
242
16
/
/
pslld
0x10
%
xmm2
.
byte
102
15
114
226
16
/
/
psrad
0x10
%
xmm2
.
byte
102
15
114
240
16
/
/
pslld
0x10
%
xmm0
.
byte
102
15
114
224
16
/
/
psrad
0x10
%
xmm0
.
byte
102
15
107
208
/
/
packssdw
%
xmm0
%
xmm2
.
byte
102
15
219
211
/
/
pand
%
xmm3
%
xmm2
.
byte
102
15
111
235
/
/
movdqa
%
xmm3
%
xmm5
.
byte
102
15
249
239
/
/
psubw
%
xmm7
%
xmm5
.
byte
102
15
127
85
136
/
/
movdqa
%
xmm2
-
0x78
(
%
ebp
)
.
byte
102
15
213
213
/
/
pmullw
%
xmm5
%
xmm2
.
byte
102
15
253
211
/
/
paddw
%
xmm3
%
xmm2
.
byte
102
15
113
210
8
/
/
psrlw
0x8
%
xmm2
.
byte
102
15
253
85
216
/
/
paddw
-
0x28
(
%
ebp
)
%
xmm2
.
byte
102
15
111
225
/
/
movdqa
%
xmm1
%
xmm4
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
127
77
216
/
/
movdqa
%
xmm1
-
0x28
(
%
ebp
)
.
byte
102
15
213
205
/
/
pmullw
%
xmm5
%
xmm1
.
byte
102
15
253
203
/
/
paddw
%
xmm3
%
xmm1
.
byte
102
15
113
209
8
/
/
psrlw
0x8
%
xmm1
.
byte
102
15
253
77
200
/
/
paddw
-
0x38
(
%
ebp
)
%
xmm1
.
byte
102
15
219
227
/
/
pand
%
xmm3
%
xmm4
.
byte
102
15
127
101
168
/
/
movdqa
%
xmm4
-
0x58
(
%
ebp
)
.
byte
102
15
213
229
/
/
pmullw
%
xmm5
%
xmm4
.
byte
102
15
253
227
/
/
paddw
%
xmm3
%
xmm4
.
byte
102
15
113
212
8
/
/
psrlw
0x8
%
xmm4
.
byte
102
15
253
101
184
/
/
paddw
-
0x48
(
%
ebp
)
%
xmm4
.
byte
102
15
127
117
184
/
/
movdqa
%
xmm6
-
0x48
(
%
ebp
)
.
byte
102
15
213
238
/
/
pmullw
%
xmm6
%
xmm5
.
byte
102
15
253
235
/
/
paddw
%
xmm3
%
xmm5
.
byte
102
15
113
213
8
/
/
psrlw
0x8
%
xmm5
.
byte
102
15
253
239
/
/
paddw
%
xmm7
%
xmm5
.
byte
102
15
111
241
/
/
movdqa
%
xmm1
%
xmm6
.
byte
102
15
113
246
8
/
/
psllw
0x8
%
xmm6
.
byte
102
15
127
101
152
/
/
movdqa
%
xmm4
-
0x68
(
%
ebp
)
.
byte
102
15
235
244
/
/
por
%
xmm4
%
xmm6
.
byte
102
15
239
219
/
/
pxor
%
xmm3
%
xmm3
.
byte
102
15
111
254
/
/
movdqa
%
xmm6
%
xmm7
.
byte
102
15
97
251
/
/
punpcklwd
%
xmm3
%
xmm7
.
byte
102
15
111
229
/
/
movdqa
%
xmm5
%
xmm4
.
byte
102
15
113
244
8
/
/
psllw
0x8
%
xmm4
.
byte
102
15
127
85
200
/
/
movdqa
%
xmm2
-
0x38
(
%
ebp
)
.
byte
102
15
235
226
/
/
por
%
xmm2
%
xmm4
.
byte
102
15
239
192
/
/
pxor
%
xmm0
%
xmm0
.
byte
102
15
97
196
/
/
punpcklwd
%
xmm4
%
xmm0
.
byte
102
15
235
199
/
/
por
%
xmm7
%
xmm0
.
byte
102
15
105
243
/
/
punpckhwd
%
xmm3
%
xmm6
.
byte
102
15
105
220
/
/
punpckhwd
%
xmm4
%
xmm3
.
byte
102
15
235
222
/
/
por
%
xmm6
%
xmm3
.
byte
128
251
6
/
/
cmp
0x6
%
bl
.
byte
119
16
/
/
ja
1066e
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x1f2
>
.
byte
3
140
145
108
2
0
0
/
/
add
0x26c
(
%
ecx
%
edx
4
)
%
ecx
.
byte
255
225
/
/
jmp
*
%
ecx
.
byte
102
15
126
4
183
/
/
movd
%
xmm0
(
%
edi
%
esi
4
)
.
byte
235
64
/
/
jmp
106ae
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x232
>
.
byte
243
15
127
4
183
/
/
movdqu
%
xmm0
(
%
edi
%
esi
4
)
.
byte
243
15
127
92
183
16
/
/
movdqu
%
xmm3
0x10
(
%
edi
%
esi
4
)
.
byte
235
51
/
/
jmp
106ae
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x232
>
.
byte
102
15
112
208
78
/
/
pshufd
0x4e
%
xmm0
%
xmm2
.
byte
102
15
126
84
183
8
/
/
movd
%
xmm2
0x8
(
%
edi
%
esi
4
)
.
byte
102
15
214
4
183
/
/
movq
%
xmm0
(
%
edi
%
esi
4
)
.
byte
235
33
/
/
jmp
106ae
<
_sk_srcover_bgra_8888_sse2_lowp
+
0x232
>
.
byte
102
15
112
211
78
/
/
pshufd
0x4e
%
xmm3
%
xmm2
.
byte
102
15
126
84
183
24
/
/
movd
%
xmm2
0x18
(
%
edi
%
esi
4
)
.
byte
102
15
112
211
229
/
/
pshufd
0xe5
%
xmm3
%
xmm2
.
byte
102
15
126
84
183
20
/
/
movd
%
xmm2
0x14
(
%
edi
%
esi
4
)
.
byte
102
15
126
92
183
16
/
/
movd
%
xmm3
0x10
(
%
edi
%
esi
4
)
.
byte
243
15
127
4
183
/
/
movdqu
%
xmm0
(
%
edi
%
esi
4
)
.
byte
139
69
12
/
/
mov
0xc
(
%
ebp
)
%
eax
.
byte
137
193
/
/
mov
%
eax
%
ecx
.
byte
141
65
8
/
/
lea
0x8
(
%
ecx
)
%
eax
.
byte
15
40
69
184
/
/
movaps
-
0x48
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
64
/
/
movaps
%
xmm0
0x40
(
%
esp
)
.
byte
15
40
69
168
/
/
movaps
-
0x58
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
48
/
/
movaps
%
xmm0
0x30
(
%
esp
)
.
byte
15
40
69
216
/
/
movaps
-
0x28
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
32
/
/
movaps
%
xmm0
0x20
(
%
esp
)
.
byte
15
40
69
136
/
/
movaps
-
0x78
(
%
ebp
)
%
xmm0
.
byte
15
41
68
36
16
/
/
movaps
%
xmm0
0x10
(
%
esp
)
.
byte
139
85
20
/
/
mov
0x14
(
%
ebp
)
%
edx
.
byte
137
84
36
12
/
/
mov
%
edx
0xc
(
%
esp
)
.
byte
137
116
36
8
/
/
mov
%
esi
0x8
(
%
esp
)
.
byte
137
68
36
4
/
/
mov
%
eax
0x4
(
%
esp
)
.
byte
139
69
8
/
/
mov
0x8
(
%
ebp
)
%
eax
.
byte
137
4
36
/
/
mov
%
eax
(
%
esp
)
.
byte
15
40
69
200
/
/
movaps
-
0x38
(
%
ebp
)
%
xmm0
.
byte
15
40
85
152
/
/
movaps
-
0x68
(
%
ebp
)
%
xmm2
.
byte
102
15
111
221
/
/
movdqa
%
xmm5
%
xmm3
.
byte
255
81
4
/
/
call
*
0x4
(
%
ecx
)
.
byte
129
196
188
0
0
0
/
/
add
0xbc
%
esp
.
byte
94
/
/
pop
%
esi
.
byte
95
/
/
pop
%
edi
.
byte
91
/
/
pop
%
ebx
.
byte
93
/
/
pop
%
ebp
.
byte
195
/
/
ret
.
byte
15
31
0
/
/
nopl
(
%
eax
)
.
byte
28
0
/
/
sbb
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
61
0
0
0
50
/
/
cmp
0x32000000
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
106
0
/
/
add
%
ch
0x0
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
96
/
/
pusha
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
79
0
/
/
add
%
cl
0x0
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
171
1
0
0
202
/
/
add
%
ch
-
0x35ffffff
(
%
ebx
)
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
191
1
0
0
237
/
/
add
%
bh
-
0x12ffffff
(
%
edi
)
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
231
/
/
add
%
ah
%
bh
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
220
/
/
add
%
bl
%
ah
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
209
/
/
add
%
dl
%
cl
.
byte
1
0
/
/
add
%
eax
(
%
eax
)
.
byte
0
/
/
.
byte
0x0
BALIGN16
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
63
0
/
/
cmpb
0x0
(
%
edi
)
.
byte
0
128
63
0
0
128
/
/
add
%
al
-
0x7fffffc1
(
%
eax
)
.
byte
63
/
/
aas
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
63
0
/
/
cmpb
0x0
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
1
/
/
add
%
al
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
2
/
/
add
%
al
(
%
edx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
3
/
/
add
%
al
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
1
/
/
add
%
al
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
1
/
/
add
%
al
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
1
/
/
add
%
al
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
1
/
/
add
%
al
(
%
ecx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
4
0
/
/
add
%
al
(
%
eax
%
eax
1
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
2
0
/
/
add
(
%
eax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
2
0
/
/
add
(
%
eax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
2
0
/
/
add
(
%
eax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
2
0
/
/
add
(
%
eax
)
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
eax
%
eax
1
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
eax
%
eax
1
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
eax
%
eax
1
)
.
byte
128
60
0
0
/
/
cmpb
0x0
(
%
eax
%
eax
1
)
.
byte
252
/
/
cld
.
byte
190
0
0
252
190
/
/
mov
0xbefc0000
%
esi
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
252
/
/
cld
.
byte
190
0
0
252
190
/
/
mov
0xbefc0000
%
esi
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
191
0
0
128
191
0
/
/
cmpb
0x0
-
0x40800000
(
%
edi
)
.
byte
0
128
191
0
0
128
/
/
add
%
al
-
0x7fffff41
(
%
eax
)
.
byte
191
0
0
224
64
/
/
mov
0x40e00000
%
edi
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
224
64
/
/
loopne
10828
<
.
literal16
+
0xd8
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
224
64
/
/
loopne
1082c
<
.
literal16
+
0xdc
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
224
64
/
/
loopne
10830
<
.
literal16
+
0xe0
>
.
byte
154
153
153
62
154
153
153
/
/
lcall
0x9999
0x9a3e9999
.
byte
62
154
153
153
62
154
153
153
/
/
ds
lcall
0x9999
0x9a3e9999
.
byte
62
61
10
23
63
61
/
/
ds
cmp
0x3d3f170a
%
eax
.
byte
10
23
/
/
or
(
%
edi
)
%
dl
.
byte
63
/
/
aas
.
byte
61
10
23
63
61
/
/
cmp
0x3d3f170a
%
eax
.
byte
10
23
/
/
or
(
%
edi
)
%
dl
.
byte
63
/
/
aas
.
byte
174
/
/
scas
%
es
:
(
%
edi
)
%
al
.
byte
71
/
/
inc
%
edi
.
byte
225
61
/
/
loope
10851
<
.
literal16
+
0x101
>
.
byte
174
/
/
scas
%
es
:
(
%
edi
)
%
al
.
byte
71
/
/
inc
%
edi
.
byte
225
61
/
/
loope
10855
<
.
literal16
+
0x105
>
.
byte
174
/
/
scas
%
es
:
(
%
edi
)
%
al
.
byte
71
/
/
inc
%
edi
.
byte
225
61
/
/
loope
10859
<
.
literal16
+
0x109
>
.
byte
174
/
/
scas
%
es
:
(
%
edi
)
%
al
.
byte
71
/
/
inc
%
edi
.
byte
225
61
/
/
loope
1085d
<
.
literal16
+
0x10d
>
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
127
67
/
/
jg
10877
<
.
literal16
+
0x127
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
127
67
/
/
jg
1087b
<
.
literal16
+
0x12b
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
127
67
/
/
jg
1087f
<
.
literal16
+
0x12f
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
127
67
/
/
jg
10883
<
.
literal16
+
0x133
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
127
0
0
/
/
cmpb
0x0
0x0
(
%
edi
)
.
byte
128
127
0
0
/
/
cmpb
0x0
0x0
(
%
edi
)
.
byte
128
127
0
0
/
/
cmpb
0x0
0x0
(
%
edi
)
.
byte
128
127
145
131
/
/
cmpb
0x83
-
0x6f
(
%
edi
)
.
byte
158
/
/
sahf
.
byte
61
145
131
158
61
/
/
cmp
0x3d9e8391
%
eax
.
byte
145
/
/
xchg
%
eax
%
ecx
.
byte
131
158
61
145
131
158
61
/
/
sbbl
0x3d
-
0x617c6ec3
(
%
esi
)
.
byte
92
/
/
pop
%
esp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
edi
)
%
bh
.
byte
92
/
/
pop
%
esp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
edi
)
%
bh
.
byte
92
/
/
pop
%
esp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
edi
)
%
bh
.
byte
92
/
/
pop
%
esp
.
byte
143
/
/
(
bad
)
.
byte
50
63
/
/
xor
(
%
edi
)
%
bh
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
ebx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
ebx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
ebx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
35
59
/
/
and
(
%
ebx
)
%
edi
.
byte
174
/
/
scas
%
es
:
(
%
edi
)
%
al
.
byte
71
/
/
inc
%
edi
.
byte
97
/
/
popa
.
byte
61
174
71
97
61
/
/
cmp
0x3d6147ae
%
eax
.
byte
174
/
/
scas
%
es
:
(
%
edi
)
%
al
.
byte
71
/
/
inc
%
edi
.
byte
97
/
/
popa
.
byte
61
174
71
97
61
/
/
cmp
0x3d6147ae
%
eax
.
byte
82
/
/
push
%
edx
.
byte
184
78
65
82
184
/
/
mov
0xb852414e
%
eax
.
byte
78
/
/
dec
%
esi
.
byte
65
/
/
inc
%
ecx
.
byte
82
/
/
push
%
edx
.
byte
184
78
65
82
184
/
/
mov
0xb852414e
%
eax
.
byte
78
/
/
dec
%
esi
.
byte
65
/
/
inc
%
ecx
.
byte
57
215
/
/
cmp
%
edx
%
edi
.
byte
32
187
57
215
32
187
/
/
and
%
bh
-
0x44df28c7
(
%
ebx
)
.
byte
57
215
/
/
cmp
%
edx
%
edi
.
byte
32
187
57
215
32
187
/
/
and
%
bh
-
0x44df28c7
(
%
ebx
)
.
byte
186
159
98
60
186
/
/
mov
0xba3c629f
%
edx
.
byte
159
/
/
lahf
.
byte
98
60
186
/
/
bound
%
edi
(
%
edx
%
edi
4
)
.
byte
159
/
/
lahf
.
byte
98
60
186
/
/
bound
%
edi
(
%
edx
%
edi
4
)
.
byte
159
/
/
lahf
.
byte
98
60
109
165
144
63
109
/
/
bound
%
edi
0x6d3f90a5
(
%
ebp
2
)
.
byte
165
/
/
movsl
%
ds
:
(
%
esi
)
%
es
:
(
%
edi
)
.
byte
144
/
/
nop
.
byte
63
/
/
aas
.
byte
109
/
/
insl
(
%
dx
)
%
es
:
(
%
edi
)
.
byte
165
/
/
movsl
%
ds
:
(
%
esi
)
%
es
:
(
%
edi
)
.
byte
144
/
/
nop
.
byte
63
/
/
aas
.
byte
109
/
/
insl
(
%
dx
)
%
es
:
(
%
edi
)
.
byte
165
/
/
movsl
%
ds
:
(
%
esi
)
%
es
:
(
%
edi
)
.
byte
144
/
/
nop
.
byte
63
/
/
aas
.
byte
252
/
/
cld
.
byte
191
16
62
252
191
/
/
mov
0xbffc3e10
%
edi
.
byte
16
62
/
/
adc
%
bh
(
%
esi
)
.
byte
252
/
/
cld
.
byte
191
16
62
252
191
/
/
mov
0xbffc3e10
%
edi
.
byte
16
62
/
/
adc
%
bh
(
%
esi
)
.
byte
168
177
/
/
test
0xb1
%
al
.
byte
152
/
/
cwtl
.
byte
59
168
177
152
59
168
/
/
cmp
-
0x57c4674f
(
%
eax
)
%
ebp
.
byte
177
152
/
/
mov
0x98
%
cl
.
byte
59
168
177
152
59
0
/
/
cmp
0x3b98b1
(
%
eax
)
%
ebp
.
byte
0
192
/
/
add
%
al
%
al
.
byte
64
/
/
inc
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
192
64
0
0
/
/
rolb
0x0
0x0
(
%
eax
)
.
byte
192
64
0
0
/
/
rolb
0x0
0x0
(
%
eax
)
.
byte
192
64
0
0
/
/
rolb
0x0
0x0
(
%
eax
)
.
byte
0
64
0
/
/
add
%
al
0x0
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
64
/
/
inc
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
64
0
/
/
add
%
al
0x0
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
64
/
/
inc
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
64
0
0
/
/
addb
0x0
0x0
(
%
eax
)
.
byte
128
64
0
0
/
/
addb
0x0
0x0
(
%
eax
)
.
byte
128
64
0
0
/
/
addb
0x0
0x0
(
%
eax
)
.
byte
128
64
171
170
/
/
addb
0xaa
-
0x55
(
%
eax
)
.
byte
42
62
/
/
sub
(
%
esi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
42
62
/
/
sub
(
%
esi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
42
62
/
/
sub
(
%
esi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
42
62
/
/
sub
(
%
esi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
62
171
/
/
ds
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
42
63
/
/
sub
(
%
edi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
42
63
/
/
sub
(
%
edi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
42
63
/
/
sub
(
%
edi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
42
63
/
/
sub
(
%
edi
)
%
bh
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
190
171
170
170
190
/
/
mov
0xbeaaaaab
%
esi
.
byte
171
/
/
stos
%
eax
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
170
/
/
stos
%
al
%
es
:
(
%
edi
)
.
byte
190
171
170
170
190
/
/
mov
0xbeaaaaab
%
esi
.
byte
129
128
128
59
129
128
128
59
129
128
/
/
addl
0x80813b80
-
0x7f7ec480
(
%
eax
)
.
byte
128
59
129
/
/
cmpb
0x81
(
%
ebx
)
.
byte
128
128
59
0
248
0
0
/
/
addb
0x0
0xf8003b
(
%
eax
)
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
8
33
/
/
or
%
ah
(
%
ecx
)
.
byte
132
55
/
/
test
%
dh
(
%
edi
)
.
byte
8
33
/
/
or
%
ah
(
%
ecx
)
.
byte
132
55
/
/
test
%
dh
(
%
edi
)
.
byte
8
33
/
/
or
%
ah
(
%
ecx
)
.
byte
132
55
/
/
test
%
dh
(
%
edi
)
.
byte
8
33
/
/
or
%
ah
(
%
ecx
)
.
byte
132
55
/
/
test
%
dh
(
%
edi
)
.
byte
224
7
/
/
loopne
10999
<
.
literal16
+
0x249
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
224
7
/
/
loopne
1099d
<
.
literal16
+
0x24d
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
224
7
/
/
loopne
109a1
<
.
literal16
+
0x251
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
224
7
/
/
loopne
109a5
<
.
literal16
+
0x255
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
33
8
/
/
and
%
ecx
(
%
eax
)
.
byte
2
58
/
/
add
(
%
edx
)
%
bh
.
byte
33
8
/
/
and
%
ecx
(
%
eax
)
.
byte
2
58
/
/
add
(
%
edx
)
%
bh
.
byte
33
8
/
/
and
%
ecx
(
%
eax
)
.
byte
2
58
/
/
add
(
%
edx
)
%
bh
.
byte
33
8
/
/
and
%
ecx
(
%
eax
)
.
byte
2
58
/
/
add
(
%
edx
)
%
bh
.
byte
31
/
/
pop
%
ds
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
8
/
/
add
%
cl
(
%
eax
)
.
byte
33
4
61
8
33
4
61
/
/
and
%
eax
0x3d042108
(
%
edi
1
)
.
byte
8
33
/
/
or
%
ah
(
%
ecx
)
.
byte
4
61
/
/
add
0x3d
%
al
.
byte
8
33
/
/
or
%
ah
(
%
ecx
)
.
byte
4
61
/
/
add
0x3d
%
al
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
0
128
/
/
addb
0x80
(
%
eax
)
.
byte
55
/
/
aaa
.
byte
128
0
128
/
/
addb
0x80
(
%
eax
)
.
byte
55
/
/
aaa
.
byte
128
0
128
/
/
addb
0x80
(
%
eax
)
.
byte
55
/
/
aaa
.
byte
128
0
128
/
/
addb
0x80
(
%
eax
)
.
byte
55
/
/
aaa
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
52
0
/
/
add
%
dh
(
%
eax
%
eax
1
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
0
/
/
xor
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
52
255
/
/
xor
0xff
%
al
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
10a04
<
.
literal16
+
0x2b4
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
10a08
<
.
literal16
+
0x2b8
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
10a0c
<
.
literal16
+
0x2bc
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
10a10
<
.
literal16
+
0x2c0
>
.
byte
119
115
/
/
ja
10a85
<
.
literal16
+
0x335
>
.
byte
248
/
/
clc
.
byte
194
119
115
/
/
ret
0x7377
.
byte
248
/
/
clc
.
byte
194
119
115
/
/
ret
0x7377
.
byte
248
/
/
clc
.
byte
194
119
115
/
/
ret
0x7377
.
byte
248
/
/
clc
.
byte
194
117
191
/
/
ret
0xbf75
.
byte
191
63
117
191
191
/
/
mov
0xbfbf753f
%
edi
.
byte
63
/
/
aas
.
byte
117
191
/
/
jne
109e9
<
.
literal16
+
0x299
>
.
byte
191
63
117
191
191
/
/
mov
0xbfbf753f
%
edi
.
byte
63
/
/
aas
.
byte
249
/
/
stc
.
byte
68
/
/
inc
%
esp
.
byte
180
62
/
/
mov
0x3e
%
ah
.
byte
249
/
/
stc
.
byte
68
/
/
inc
%
esp
.
byte
180
62
/
/
mov
0x3e
%
ah
.
byte
249
/
/
stc
.
byte
68
/
/
inc
%
esp
.
byte
180
62
/
/
mov
0x3e
%
ah
.
byte
249
/
/
stc
.
byte
68
/
/
inc
%
esp
.
byte
180
62
/
/
mov
0x3e
%
ah
.
byte
163
233
220
63
163
/
/
mov
%
eax
0xa33fdce9
.
byte
233
220
63
163
233
/
/
jmp
e9a44a26
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xe9a345aa
>
.
byte
220
63
/
/
fdivrl
(
%
edi
)
.
byte
163
233
220
63
81
/
/
mov
%
eax
0x513fdce9
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
/
/
inc
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
/
/
inc
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
/
/
inc
%
edx
.
byte
81
/
/
push
%
ecx
.
byte
140
242
/
/
mov
%
?
%
edx
.
byte
66
/
/
inc
%
edx
.
byte
141
188
190
63
141
188
190
/
/
lea
-
0x414372c1
(
%
esi
%
edi
4
)
%
edi
.
byte
63
/
/
aas
.
byte
141
188
190
63
141
188
190
/
/
lea
-
0x414372c1
(
%
esi
%
edi
4
)
%
edi
.
byte
63
/
/
aas
.
byte
248
/
/
clc
.
byte
245
/
/
cmc
.
byte
154
64
248
245
154
64
248
/
/
lcall
0xf840
0x9af5f840
.
byte
245
/
/
cmc
.
byte
154
64
248
245
154
64
254
/
/
lcall
0xfe40
0x9af5f840
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
/
/
inc
%
ecx
.
byte
254
/
/
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
/
/
inc
%
ecx
.
byte
254
/
/
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
/
/
inc
%
ecx
.
byte
254
/
/
(
bad
)
.
byte
210
221
/
/
rcr
%
cl
%
ch
.
byte
65
/
/
inc
%
ecx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
75
0
/
/
add
%
cl
0x0
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
75
/
/
dec
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
75
0
/
/
add
%
cl
0x0
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
75
/
/
dec
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
200
66
0
0
/
/
enter
0x42
0x0
.
byte
200
66
0
0
/
/
enter
0x42
0x0
.
byte
200
66
0
0
/
/
enter
0x42
0x0
.
byte
200
66
0
0
/
/
enter
0x42
0x0
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
195
/
/
add
%
al
%
bl
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
65
0
0
/
/
addb
0x0
0x0
(
%
ecx
)
.
byte
128
65
0
0
/
/
addb
0x0
0x0
(
%
ecx
)
.
byte
128
65
0
0
/
/
addb
0x0
0x0
(
%
ecx
)
.
byte
128
65
203
61
/
/
addb
0x3d
-
0x35
(
%
ecx
)
.
byte
13
60
203
61
13
/
/
or
0xd3dcb3c
%
eax
.
byte
60
203
/
/
cmp
0xcb
%
al
.
byte
61
13
60
203
61
/
/
cmp
0x3dcb3c0d
%
eax
.
byte
13
60
111
18
3
/
/
or
0x3126f3c
%
eax
.
byte
59
111
18
/
/
cmp
0x12
(
%
edi
)
%
ebp
.
byte
3
59
/
/
add
(
%
ebx
)
%
edi
.
byte
111
/
/
outsl
%
ds
:
(
%
esi
)
(
%
dx
)
.
byte
18
3
/
/
adc
(
%
ebx
)
%
al
.
byte
59
111
18
/
/
cmp
0x12
(
%
edi
)
%
ebp
.
byte
3
59
/
/
add
(
%
ebx
)
%
edi
.
byte
10
215
/
/
or
%
bh
%
dl
.
byte
163
59
10
215
163
/
/
mov
%
eax
0xa3d70a3b
.
byte
59
10
/
/
cmp
(
%
edx
)
%
ecx
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
163
59
10
215
163
/
/
mov
%
eax
0xa3d70a3b
.
byte
59
194
/
/
cmp
%
edx
%
eax
.
byte
24
17
/
/
sbb
%
dl
(
%
ecx
)
.
byte
60
194
/
/
cmp
0xc2
%
al
.
byte
24
17
/
/
sbb
%
dl
(
%
ecx
)
.
byte
60
194
/
/
cmp
0xc2
%
al
.
byte
24
17
/
/
sbb
%
dl
(
%
ecx
)
.
byte
60
194
/
/
cmp
0xc2
%
al
.
byte
24
17
/
/
sbb
%
dl
(
%
ecx
)
.
byte
60
203
/
/
cmp
0xcb
%
al
.
byte
61
13
190
203
61
/
/
cmp
0x3dcbbe0d
%
eax
.
byte
13
190
203
61
13
/
/
or
0xd3dcbbe
%
eax
.
byte
190
203
61
13
190
/
/
mov
0xbe0d3dcb
%
esi
.
byte
80
/
/
push
%
eax
.
byte
128
3
62
/
/
addb
0x3e
(
%
ebx
)
.
byte
80
/
/
push
%
eax
.
byte
128
3
62
/
/
addb
0x3e
(
%
ebx
)
.
byte
80
/
/
push
%
eax
.
byte
128
3
62
/
/
addb
0x3e
(
%
ebx
)
.
byte
80
/
/
push
%
eax
.
byte
128
3
62
/
/
addb
0x3e
(
%
ebx
)
.
byte
31
/
/
pop
%
ds
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
118
63
/
/
jbe
10b73
<
.
literal16
+
0x423
>
.
byte
31
/
/
pop
%
ds
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
118
63
/
/
jbe
10b77
<
.
literal16
+
0x427
>
.
byte
31
/
/
pop
%
ds
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
118
63
/
/
jbe
10b7b
<
.
literal16
+
0x42b
>
.
byte
31
/
/
pop
%
ds
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
118
63
/
/
jbe
10b7f
<
.
literal16
+
0x42f
>
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
eax
)
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
eax
)
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
eax
)
.
byte
246
64
83
63
/
/
testb
0x3f
0x53
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
248
/
/
clc
.
byte
65
/
/
inc
%
ecx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
248
/
/
clc
.
byte
65
/
/
inc
%
ecx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
248
/
/
clc
.
byte
65
/
/
inc
%
ecx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
248
/
/
clc
.
byte
65
/
/
inc
%
ecx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
124
66
/
/
jl
10bb6
<
.
literal16
+
0x466
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
124
66
/
/
jl
10bba
<
.
literal16
+
0x46a
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
124
66
/
/
jl
10bbe
<
.
literal16
+
0x46e
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
124
66
/
/
jl
10bc2
<
.
literal16
+
0x472
>
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
137
136
136
55
137
136
/
/
mov
%
ecx
-
0x7776c878
(
%
eax
)
.
byte
136
55
/
/
mov
%
dh
(
%
edi
)
.
byte
137
136
136
55
137
136
/
/
mov
%
ecx
-
0x7776c878
(
%
eax
)
.
byte
136
55
/
/
mov
%
dh
(
%
edi
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
137
136
136
57
137
136
/
/
mov
%
ecx
-
0x7776c678
(
%
eax
)
.
byte
136
57
/
/
mov
%
bh
(
%
ecx
)
.
byte
137
136
136
57
137
136
/
/
mov
%
ecx
-
0x7776c678
(
%
eax
)
.
byte
136
57
/
/
mov
%
bh
(
%
ecx
)
.
byte
240
0
0
/
/
lock
add
%
al
(
%
eax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
137
136
136
59
137
/
/
add
%
cl
-
0x76c47778
(
%
ecx
)
.
byte
136
136
59
137
136
136
/
/
mov
%
cl
-
0x777776c5
(
%
eax
)
.
byte
59
137
136
136
59
15
/
/
cmp
0xf3b8888
(
%
ecx
)
%
ecx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
137
136
136
61
137
/
/
add
%
cl
-
0x76c27778
(
%
ecx
)
.
byte
136
136
61
137
136
136
/
/
mov
%
cl
-
0x777776c3
(
%
eax
)
.
byte
61
137
136
136
61
/
/
cmp
0x3d888889
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
112
65
/
/
jo
10c45
<
.
literal16
+
0x4f5
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
112
65
/
/
jo
10c49
<
.
literal16
+
0x4f9
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
112
65
/
/
jo
10c4d
<
.
literal16
+
0x4fd
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
112
65
/
/
jo
10c51
<
.
literal16
+
0x501
>
.
byte
255
3
/
/
incl
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
3
/
/
incl
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
3
/
/
incl
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
3
/
/
incl
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
8
32
/
/
or
%
ah
(
%
eax
)
.
byte
128
58
8
/
/
cmpb
0x8
(
%
edx
)
.
byte
32
128
58
8
32
128
/
/
and
%
al
-
0x7fdff7c6
(
%
eax
)
.
byte
58
8
/
/
cmp
(
%
eax
)
%
cl
.
byte
32
128
58
0
192
127
/
/
and
%
al
0x7fc0003a
(
%
eax
)
.
byte
68
/
/
inc
%
esp
.
byte
0
192
/
/
add
%
al
%
al
.
byte
127
68
/
/
jg
10c7c
<
.
literal16
+
0x52c
>
.
byte
0
192
/
/
add
%
al
%
al
.
byte
127
68
/
/
jg
10c80
<
.
literal16
+
0x530
>
.
byte
0
192
/
/
add
%
al
%
al
.
byte
127
68
/
/
jg
10c84
<
.
literal16
+
0x534
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
64
/
/
inc
%
eax
.
byte
64
/
/
inc
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
64
/
/
inc
%
eax
.
byte
64
/
/
inc
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
64
/
/
inc
%
eax
.
byte
64
/
/
inc
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
64
/
/
inc
%
eax
.
byte
64
/
/
inc
%
eax
.
byte
0
128
0
0
0
128
/
/
add
%
al
-
0x80000000
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
128
0
0
0
128
/
/
add
%
al
-
0x80000000
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
10c63
<
.
literal16
+
0x513
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
0
/
/
jg
10c67
<
.
literal16
+
0x517
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
0
/
/
jg
10c6b
<
.
literal16
+
0x51b
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
0
/
/
jg
10c6f
<
.
literal16
+
0x51f
>
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
4
0
/
/
add
0x0
%
al
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
56
0
/
/
cmp
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
56
0
/
/
cmp
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
56
0
/
/
cmp
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
56
0
/
/
cmp
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
0
0
/
/
addb
0x0
(
%
eax
)
.
byte
0
128
0
0
0
128
/
/
add
%
al
-
0x80000000
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
128
255
255
255
127
/
/
add
%
al
0x7fffffff
(
%
eax
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
255
/
/
jg
10ca8
<
.
literal16
+
0x558
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
255
/
/
jg
10cac
<
.
literal16
+
0x55c
>
.
byte
255
/
/
(
bad
)
.
byte
255
/
/
(
bad
)
.
byte
127
0
/
/
jg
10cb1
<
.
literal16
+
0x561
>
.
byte
0
128
56
0
0
128
/
/
add
%
al
-
0x7fffffc8
(
%
eax
)
.
byte
56
0
/
/
cmp
%
al
(
%
eax
)
.
byte
0
128
56
0
0
128
/
/
add
%
al
-
0x7fffffc8
(
%
eax
)
.
byte
56
0
/
/
cmp
%
al
(
%
eax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
255
/
/
(
bad
)
.
byte
127
71
/
/
jg
10d1b
<
.
literal16
+
0x5cb
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
71
/
/
jg
10d1f
<
.
literal16
+
0x5cf
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
71
/
/
jg
10d23
<
.
literal16
+
0x5d3
>
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
127
71
/
/
jg
10d27
<
.
literal16
+
0x5d7
>
.
byte
208
/
/
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
208
/
/
ds
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
208
/
/
ds
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
208
/
/
ds
(
bad
)
.
byte
179
89
/
/
mov
0x59
%
bl
.
byte
62
89
/
/
ds
pop
%
ecx
.
byte
23
/
/
pop
%
ss
.
byte
55
/
/
aaa
.
byte
63
/
/
aas
.
byte
89
/
/
pop
%
ecx
.
byte
23
/
/
pop
%
ss
.
byte
55
/
/
aaa
.
byte
63
/
/
aas
.
byte
89
/
/
pop
%
ecx
.
byte
23
/
/
pop
%
ss
.
byte
55
/
/
aaa
.
byte
63
/
/
aas
.
byte
89
/
/
pop
%
ecx
.
byte
23
/
/
pop
%
ss
.
byte
55
/
/
aaa
.
byte
63
/
/
aas
.
byte
152
/
/
cwtl
.
byte
221
147
61
152
221
147
/
/
fstl
-
0x6c2267c3
(
%
ebx
)
.
byte
61
152
221
147
61
/
/
cmp
0x3d93dd98
%
eax
.
byte
152
/
/
cwtl
.
byte
221
147
61
255
255
0
/
/
fstl
0xffff3d
(
%
ebx
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
255
0
/
/
incl
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
83
/
/
push
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
83
0
/
/
add
%
dl
0x0
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
83
/
/
push
%
ebx
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
83
128
/
/
add
%
dl
-
0x80
(
%
ebx
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
211
128
0
0
211
128
/
/
roll
%
cl
-
0x7f2d0000
(
%
eax
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
211
128
0
0
211
111
/
/
roll
%
cl
0x6fd30000
(
%
eax
)
.
byte
43
231
/
/
sub
%
edi
%
esp
.
byte
187
111
43
231
187
/
/
mov
0xbbe72b6f
%
ebx
.
byte
111
/
/
outsl
%
ds
:
(
%
esi
)
(
%
dx
)
.
byte
43
231
/
/
sub
%
edi
%
esp
.
byte
187
111
43
231
187
/
/
mov
0xbbe72b6f
%
ebx
.
byte
159
/
/
lahf
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
202
60
159
/
/
lret
0x9f3c
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
202
60
159
/
/
lret
0x9f3c
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
202
60
159
/
/
lret
0x9f3c
.
byte
215
/
/
xlat
%
ds
:
(
%
ebx
)
.
byte
202
60
212
/
/
lret
0xd43c
.
byte
100
84
/
/
fs
push
%
esp
.
byte
189
212
100
84
189
/
/
mov
0xbd5464d4
%
ebp
.
byte
212
100
/
/
aam
0x64
.
byte
84
/
/
push
%
esp
.
byte
189
212
100
84
189
/
/
mov
0xbd5464d4
%
ebp
.
byte
169
240
34
62
169
/
/
test
0xa93e22f0
%
eax
.
byte
240
34
62
/
/
lock
and
(
%
esi
)
%
bh
.
byte
169
240
34
62
169
/
/
test
0xa93e22f0
%
eax
.
byte
240
34
62
/
/
lock
and
(
%
esi
)
%
bh
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
62
0
/
/
cmpb
0x0
(
%
esi
)
.
byte
0
128
62
0
0
128
/
/
add
%
al
-
0x7fffffc2
(
%
eax
)
.
byte
62
0
0
/
/
add
%
al
%
ds
:
(
%
eax
)
.
byte
128
62
0
/
/
cmpb
0x0
(
%
esi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
191
0
0
0
191
/
/
mov
0xbf000000
%
edi
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
191
0
0
0
191
/
/
add
%
bh
-
0x41000000
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
192
191
0
0
192
191
0
/
/
sarb
0x0
-
0x40400000
(
%
edi
)
.
byte
0
192
/
/
add
%
al
%
al
.
byte
191
0
0
192
191
/
/
mov
0xbfc00000
%
edi
.
byte
114
28
/
/
jb
10dce
<
.
literal16
+
0x67e
>
.
byte
199
/
/
(
bad
)
.
byte
62
114
28
/
/
jb
pt
10dd2
<
.
literal16
+
0x682
>
.
byte
199
/
/
(
bad
)
.
byte
62
114
28
/
/
jb
pt
10dd6
<
.
literal16
+
0x686
>
.
byte
199
/
/
(
bad
)
.
byte
62
114
28
/
/
jb
pt
10dda
<
.
literal16
+
0x68a
>
.
byte
199
/
/
(
bad
)
.
byte
62
85
/
/
ds
push
%
ebp
.
byte
85
/
/
push
%
ebp
.
byte
149
/
/
xchg
%
eax
%
ebp
.
byte
191
85
85
149
191
/
/
mov
0xbf955555
%
edi
.
byte
85
/
/
push
%
ebp
.
byte
85
/
/
push
%
ebp
.
byte
149
/
/
xchg
%
eax
%
ebp
.
byte
191
85
85
149
191
/
/
mov
0xbf955555
%
edi
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
192
63
0
/
/
sarb
0x0
(
%
edi
)
.
byte
0
192
/
/
add
%
al
%
al
.
byte
63
/
/
aas
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
192
63
0
/
/
sarb
0x0
(
%
edi
)
.
byte
0
192
/
/
add
%
al
%
al
.
byte
63
/
/
aas
.
byte
57
142
99
61
57
142
/
/
cmp
%
ecx
-
0x71c6c29d
(
%
esi
)
.
byte
99
61
57
142
99
61
/
/
arpl
%
di
0x3d638e39
.
byte
57
142
99
61
114
249
/
/
cmp
%
ecx
-
0x68dc29d
(
%
esi
)
.
byte
127
63
/
/
jg
10e33
<
.
literal16
+
0x6e3
>
.
byte
114
249
/
/
jb
10def
<
.
literal16
+
0x69f
>
.
byte
127
63
/
/
jg
10e37
<
.
literal16
+
0x6e7
>
.
byte
114
249
/
/
jb
10df3
<
.
literal16
+
0x6a3
>
.
byte
127
63
/
/
jg
10e3b
<
.
literal16
+
0x6eb
>
.
byte
114
249
/
/
jb
10df7
<
.
literal16
+
0x6a7
>
.
byte
127
63
/
/
jg
10e3f
<
.
literal16
+
0x6ef
>
.
byte
3
0
/
/
add
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
3
0
/
/
add
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
3
0
/
/
add
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
3
0
/
/
add
(
%
eax
)
%
eax
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
45
16
17
192
45
/
/
sub
0x2dc01110
%
eax
.
byte
16
17
/
/
adc
%
dl
(
%
ecx
)
.
byte
192
45
16
17
192
45
16
/
/
shrb
0x10
0x2dc01110
.
byte
17
192
/
/
adc
%
eax
%
eax
.
byte
18
120
57
/
/
adc
0x39
(
%
eax
)
%
bh
.
byte
64
/
/
inc
%
eax
.
byte
18
120
57
/
/
adc
0x39
(
%
eax
)
%
bh
.
byte
64
/
/
inc
%
eax
.
byte
18
120
57
/
/
adc
0x39
(
%
eax
)
%
bh
.
byte
64
/
/
inc
%
eax
.
byte
18
120
57
/
/
adc
0x39
(
%
eax
)
%
bh
.
byte
64
/
/
inc
%
eax
.
byte
32
148
90
62
32
148
90
/
/
and
%
dl
0x5a94203e
(
%
edx
%
ebx
2
)
.
byte
62
32
148
90
62
32
148
90
/
/
and
%
dl
%
ds
:
0x5a94203e
(
%
edx
%
ebx
2
)
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
push
%
ds
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
push
%
ds
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
push
%
ds
.
byte
62
4
157
/
/
ds
add
0x9d
%
al
.
byte
30
/
/
push
%
ds
.
byte
62
0
24
/
/
add
%
bl
%
ds
:
(
%
eax
)
.
byte
161
57
0
24
161
/
/
mov
0xa1180039
%
eax
.
byte
57
0
/
/
cmp
%
eax
(
%
eax
)
.
byte
24
161
57
0
24
161
/
/
sbb
%
ah
-
0x5ee7ffc7
(
%
ecx
)
.
byte
57
255
/
/
cmp
%
edi
%
edi
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
128
0
128
/
/
addb
0x80
(
%
eax
)
.
byte
0
128
0
128
0
128
/
/
add
%
al
-
0x7fff8000
(
%
eax
)
.
byte
0
128
0
128
0
128
/
/
add
%
al
-
0x7fff8000
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
0
/
/
incb
(
%
eax
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
254
1
/
/
incb
(
%
ecx
)
.
byte
248
/
/
clc
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
31
/
/
add
%
bl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
248
/
/
clc
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
0
248
/
/
add
%
bh
%
al
.
byte
224
255
/
/
loopne
10ee1
<
.
literal16
+
0x791
>
.
byte
224
255
/
/
loopne
10ee3
<
.
literal16
+
0x793
>
.
byte
224
255
/
/
loopne
10ee5
<
.
literal16
+
0x795
>
.
byte
224
255
/
/
loopne
10ee7
<
.
literal16
+
0x797
>
.
byte
224
255
/
/
loopne
10ee9
<
.
literal16
+
0x799
>
.
byte
224
255
/
/
loopne
10eeb
<
.
literal16
+
0x79b
>
.
byte
224
255
/
/
loopne
10eed
<
.
literal16
+
0x79d
>
.
byte
224
255
/
/
loopne
10eef
<
.
literal16
+
0x79f
>
.
byte
15
0
15
/
/
str
(
%
edi
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
15
/
/
add
%
cl
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
240
0
240
/
/
lock
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
240
/
/
add
%
dh
%
al
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
0
255
/
/
add
%
bh
%
bh
.
byte
240
255
240
/
/
lock
push
%
eax
.
byte
255
240
/
/
push
%
eax
.
byte
255
240
/
/
push
%
eax
.
byte
255
240
/
/
push
%
eax
.
byte
255
240
/
/
push
%
eax
.
byte
255
240
/
/
push
%
eax
.
byte
255
240
/
/
push
%
eax
.
byte
255
54
/
/
pushl
(
%
esi
)
.
byte
0
54
/
/
add
%
dh
(
%
esi
)
.
byte
0
54
/
/
add
%
dh
(
%
esi
)
.
byte
0
54
/
/
add
%
dh
(
%
esi
)
.
byte
0
54
/
/
add
%
dh
(
%
esi
)
.
byte
0
54
/
/
add
%
dh
(
%
esi
)
.
byte
0
54
/
/
add
%
dh
(
%
esi
)
.
byte
0
54
/
/
add
%
dh
(
%
esi
)
.
byte
0
183
0
183
0
183
/
/
add
%
dh
-
0x48ff4900
(
%
edi
)
.
byte
0
183
0
183
0
183
/
/
add
%
dh
-
0x48ff4900
(
%
edi
)
.
byte
0
183
0
183
0
19
/
/
add
%
dh
0x1300b700
(
%
edi
)
.
byte
0
19
/
/
add
%
dl
(
%
ebx
)
.
byte
0
19
/
/
add
%
dl
(
%
ebx
)
.
byte
0
19
/
/
add
%
dl
(
%
ebx
)
.
byte
0
19
/
/
add
%
dl
(
%
ebx
)
.
byte
0
19
/
/
add
%
dl
(
%
ebx
)
.
byte
0
19
/
/
add
%
dl
(
%
ebx
)
.
byte
0
19
/
/
add
%
dl
(
%
ebx
)
.
byte
0
/
/
.
byte
0x0
BALIGN4
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
63
/
/
add
%
bh
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
0
191
0
0
128
63
/
/
add
%
bh
0x3f800000
(
%
edi
)
.
byte
0
0
/
/
add
%
al
(
%
eax
)
.
byte
127
67
/
/
jg
10fb3
<
_sk_srcover_bgra_8888_sse2_lowp
+
0xb37
>
#
endif
