/
*
-
*
-
Mode
:
C
+
+
;
tab
-
width
:
8
;
indent
-
tabs
-
mode
:
nil
;
c
-
basic
-
offset
:
2
-
*
-
*
vim
:
set
ts
=
8
sts
=
2
et
sw
=
2
tw
=
80
:
*
This
Source
Code
Form
is
subject
to
the
terms
of
the
Mozilla
Public
*
License
v
.
2
.
0
.
If
a
copy
of
the
MPL
was
not
distributed
with
this
*
file
You
can
obtain
one
at
http
:
/
/
mozilla
.
org
/
MPL
/
2
.
0
/
.
*
/
#
include
"
gc
/
BufferAllocator
-
inl
.
h
"
#
include
"
mozilla
/
PodOperations
.
h
"
#
include
"
mozilla
/
ScopeExit
.
h
"
#
ifdef
XP_DARWIN
#
include
<
mach
/
mach_init
.
h
>
#
include
<
mach
/
vm_map
.
h
>
#
endif
#
include
"
gc
/
BufferAllocatorInternals
.
h
"
#
include
"
gc
/
GCInternals
.
h
"
#
include
"
gc
/
GCLock
.
h
"
#
include
"
gc
/
PublicIterators
.
h
"
#
include
"
gc
/
Zone
.
h
"
#
include
"
js
/
HeapAPI
.
h
"
#
include
"
util
/
Poison
.
h
"
#
include
"
gc
/
Heap
-
inl
.
h
"
#
include
"
gc
/
Marking
-
inl
.
h
"
using
namespace
js
;
using
namespace
js
:
:
gc
;
namespace
js
:
:
gc
{
static
constexpr
size_t
MinFreeRegionSize
=
1
<
<
BufferAllocator
:
:
MinSizeClassShift
;
static
constexpr
size_t
SmallRegionShift
=
14
;
/
/
16
KB
static
constexpr
size_t
SmallRegionSize
=
1
<
<
SmallRegionShift
;
static
constexpr
uintptr_t
SmallRegionMask
=
SmallRegionSize
-
1
;
static_assert
(
SmallRegionSize
>
=
MinMediumAllocSize
)
;
static_assert
(
SmallRegionSize
<
=
MaxMediumAllocSize
)
;
struct
alignas
(
CellAlignBytes
)
LargeBuffer
:
public
SlimLinkedListElement
<
LargeBuffer
>
{
void
*
alloc
;
size_t
bytes
;
bool
isNurseryOwned
;
bool
allocatedDuringCollection
=
false
;
#
ifdef
DEBUG
uint32_t
checkValue
=
LargeBufferCheckValue
;
#
endif
LargeBuffer
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
)
:
alloc
(
alloc
)
bytes
(
bytes
)
isNurseryOwned
(
nurseryOwned
)
{
MOZ_ASSERT
(
(
bytes
%
ChunkSize
)
=
=
0
)
;
}
inline
void
check
(
)
const
;
#
ifdef
DEBUG
inline
Zone
*
zone
(
)
;
inline
Zone
*
zoneFromAnyThread
(
)
;
#
endif
void
*
data
(
)
{
return
alloc
;
}
size_t
allocBytes
(
)
const
{
return
bytes
;
}
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
}
;
inline
void
LargeBuffer
:
:
check
(
)
const
{
MOZ_ASSERT
(
checkValue
=
=
LargeBufferCheckValue
)
;
}
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
GCRuntime
*
gc
)
:
LockGuard
(
gc
-
>
bufferAllocatorLock
)
{
}
BufferAllocator
:
:
AutoLock
:
:
AutoLock
(
BufferAllocator
*
allocator
)
:
LockGuard
(
allocator
-
>
lock
(
)
)
{
}
/
/
Describes
a
free
region
in
a
buffer
chunk
.
This
structure
is
stored
at
the
/
/
end
of
the
region
.
/
/
/
/
Medium
allocations
are
made
in
FreeRegions
in
increasing
address
order
.
The
/
/
final
allocation
will
contain
the
now
empty
and
unused
FreeRegion
structure
.
/
/
FreeRegions
are
stored
in
buckets
based
on
their
size
in
FreeLists
.
Each
/
/
bucket
is
a
linked
list
of
FreeRegions
.
struct
BufferAllocator
:
:
FreeRegion
:
public
SlimLinkedListElement
<
BufferAllocator
:
:
FreeRegion
>
{
uintptr_t
startAddr
;
bool
hasDecommittedPages
;
#
ifdef
DEBUG
uint32_t
checkValue
=
FreeRegionCheckValue
;
#
endif
explicit
FreeRegion
(
uintptr_t
startAddr
bool
decommitted
=
false
)
:
startAddr
(
startAddr
)
hasDecommittedPages
(
decommitted
)
{
}
static
FreeRegion
*
fromEndOffset
(
BufferChunk
*
chunk
uintptr_t
endOffset
)
{
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
return
fromEndAddr
(
uintptr_t
(
chunk
)
+
endOffset
)
;
}
static
FreeRegion
*
fromEndOffset
(
SmallBufferRegion
*
region
uintptr_t
endOffset
)
{
MOZ_ASSERT
(
endOffset
<
=
SmallRegionSize
)
;
return
fromEndAddr
(
uintptr_t
(
region
)
+
endOffset
)
;
}
static
FreeRegion
*
fromEndAddr
(
uintptr_t
endAddr
)
{
MOZ_ASSERT
(
endAddr
%
SmallAllocGranularity
=
=
0
)
;
auto
*
region
=
reinterpret_cast
<
FreeRegion
*
>
(
endAddr
-
sizeof
(
FreeRegion
)
)
;
region
-
>
check
(
)
;
return
region
;
}
void
check
(
)
const
{
MOZ_ASSERT
(
checkValue
=
=
FreeRegionCheckValue
)
;
}
uintptr_t
getEnd
(
)
const
{
return
uintptr_t
(
this
+
1
)
;
}
size_t
size
(
)
const
{
return
getEnd
(
)
-
startAddr
;
}
}
;
/
/
Iterator
that
yields
the
indexes
of
set
bits
in
a
mozilla
:
:
BitSet
.
template
<
size_t
N
>
class
BitSetIter
{
const
mozilla
:
:
BitSet
<
N
>
&
bitset
;
size_t
bit
=
0
;
public
:
explicit
BitSetIter
(
const
mozilla
:
:
BitSet
<
N
>
&
bitset
)
:
bitset
(
bitset
)
{
MOZ_ASSERT
(
!
done
(
)
)
;
if
(
!
bitset
[
bit
]
)
{
next
(
)
;
}
}
bool
done
(
)
const
{
MOZ_ASSERT
(
bit
<
=
N
|
|
bit
=
=
SIZE_MAX
)
;
return
bit
>
=
N
;
}
void
next
(
)
{
MOZ_ASSERT
(
!
done
(
)
)
;
bit
+
+
;
if
(
bit
!
=
N
)
{
bit
=
bitset
.
FindNext
(
bit
)
;
}
}
size_t
get
(
)
const
{
MOZ_ASSERT
(
!
done
(
)
)
;
return
bit
;
}
operator
size_t
(
)
const
{
return
get
(
)
;
}
}
;
/
/
Iterator
that
yields
offsets
and
pointers
into
a
block
of
memory
/
/
corresponding
to
the
bits
set
in
a
BitSet
.
template
<
typename
BitmapIter
size_t
Granularity
typename
T
=
void
>
class
BitmapToBlockIter
:
public
BitmapIter
{
uintptr_t
baseAddr
;
public
:
template
<
typename
S
>
BitmapToBlockIter
(
void
*
base
S
&
&
arg
)
:
BitmapIter
(
std
:
:
forward
<
S
>
(
arg
)
)
baseAddr
(
uintptr_t
(
base
)
)
{
}
size_t
getOffset
(
)
const
{
return
BitmapIter
:
:
get
(
)
*
Granularity
;
}
T
*
get
(
)
const
{
return
reinterpret_cast
<
T
*
>
(
baseAddr
+
getOffset
(
)
)
;
}
operator
T
*
(
)
const
{
return
get
(
)
;
}
}
;
/
/
A
chunk
containing
medium
buffer
allocations
for
a
single
zone
.
Unlike
/
/
ArenaChunk
allocations
from
different
zones
do
not
share
chunks
.
struct
BufferChunk
:
public
ChunkBase
public
SlimLinkedListElement
<
BufferChunk
>
{
#
ifdef
DEBUG
MainThreadOrGCTaskData
<
Zone
*
>
zone
;
#
endif
MainThreadOrGCTaskData
<
bool
>
allocatedDuringCollection
;
MainThreadData
<
bool
>
hasNurseryOwnedAllocs
;
MainThreadOrGCTaskData
<
bool
>
hasNurseryOwnedAllocsAfterSweep
;
static
constexpr
size_t
MaxAllocsPerChunk
=
ChunkSize
/
MediumAllocGranularity
;
/
/
Mark
bitmap
:
one
bit
minimum
per
allocation
no
gray
bits
.
MainThreadOrGCTaskData
<
AtomicBitmap
<
MaxAllocsPerChunk
>
>
markBits
;
/
/
Allocation
start
and
end
bitmaps
:
these
have
a
bit
set
corresponding
to
the
/
/
start
of
the
allocation
and
to
the
byte
after
the
end
of
allocation
(
except
/
/
for
the
end
of
the
chunk
)
.
using
PerAllocBitmap
=
mozilla
:
:
BitSet
<
MaxAllocsPerChunk
>
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
allocStartBitmap
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
allocEndBitmap
;
/
/
A
bitmap
indicating
whether
an
allocation
is
owned
by
a
nursery
or
a
/
/
tenured
GC
thing
.
MainThreadOrGCTaskData
<
PerAllocBitmap
>
nurseryOwnedBitmap
;
static
constexpr
size_t
PagesPerChunk
=
ChunkSize
/
PageSize
;
using
PerPageBitmap
=
mozilla
:
:
BitSet
<
PagesPerChunk
uint32_t
>
;
MainThreadOrGCTaskData
<
PerPageBitmap
>
decommittedPages
;
static
constexpr
size_t
SmallRegionsPerChunk
=
ChunkSize
/
SmallRegionSize
;
using
SmallRegionBitmap
=
AtomicBitmap
<
SmallRegionsPerChunk
>
;
MainThreadOrGCTaskData
<
SmallRegionBitmap
>
smallRegionBitmap
;
/
/
Free
regions
in
this
chunk
.
When
a
chunk
is
swept
its
free
regions
are
/
/
stored
here
.
When
the
chunk
is
being
used
for
allocation
these
are
moved
to
/
/
BufferAllocator
:
:
freeLists
.
|
ownsFreeLists
|
indicates
whether
this
is
in
/
/
use
.
MainThreadOrGCTaskData
<
BufferAllocator
:
:
FreeLists
>
freeLists
;
MainThreadOrGCTaskData
<
bool
>
ownsFreeLists
;
using
AllocIter
=
BitmapToBlockIter
<
BitSetIter
<
MaxAllocsPerChunk
>
MediumAllocGranularity
>
;
AllocIter
allocIter
(
)
{
return
{
this
allocStartBitmap
.
ref
(
)
}
;
}
using
SmallRegionIter
=
BitmapToBlockIter
<
SmallRegionBitmap
:
:
Iter
SmallRegionSize
SmallBufferRegion
>
;
SmallRegionIter
smallRegionIter
(
)
{
return
{
this
smallRegionBitmap
.
ref
(
)
}
;
}
static
const
BufferChunk
*
from
(
const
void
*
alloc
)
{
return
from
(
const_cast
<
void
*
>
(
alloc
)
)
;
}
static
BufferChunk
*
from
(
void
*
alloc
)
{
ChunkBase
*
chunk
=
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
kind
=
=
ChunkKind
:
:
Buffers
)
;
return
static_cast
<
BufferChunk
*
>
(
chunk
)
;
}
explicit
BufferChunk
(
Zone
*
zone
)
;
~
BufferChunk
(
)
;
void
setAllocated
(
void
*
alloc
size_t
bytes
bool
allocated
)
;
bool
isAllocated
(
const
void
*
alloc
)
const
;
bool
isAllocated
(
uintptr_t
offset
)
const
;
void
updateEndOffset
(
void
*
alloc
size_t
oldBytes
size_t
newBytes
)
;
void
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
;
bool
isNurseryOwned
(
const
void
*
alloc
)
const
;
void
setSmallBufferRegion
(
void
*
alloc
bool
smallAlloc
)
;
bool
isSmallBufferRegion
(
const
void
*
alloc
)
const
;
size_t
allocBytes
(
const
void
*
alloc
)
const
;
bool
setMarked
(
void
*
alloc
)
;
void
setUnmarked
(
void
*
alloc
)
;
bool
isMarked
(
const
void
*
alloc
)
const
;
/
/
Find
next
/
previous
allocations
from
|
offset
|
.
Return
ChunkSize
on
failure
.
size_t
findNextAllocated
(
uintptr_t
offset
)
const
;
size_t
findPrevAllocated
(
uintptr_t
offset
)
const
;
/
/
Find
next
/
previous
free
region
from
a
start
/
end
address
.
using
FreeRegion
=
BufferAllocator
:
:
FreeRegion
;
FreeRegion
*
findFollowingFreeRegion
(
uintptr_t
startAddr
)
;
FreeRegion
*
findPrecedingFreeRegion
(
uintptr_t
endAddr
)
;
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
private
:
template
<
size_t
Divisor
=
MinMediumAllocSize
size_t
Align
=
Divisor
>
size_t
ptrToIndex
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
(
uintptr_t
(
alloc
)
&
~
ChunkMask
)
=
=
uintptr_t
(
this
)
)
;
uintptr_t
offset
=
uintptr_t
(
alloc
)
&
ChunkMask
;
return
offsetToIndex
<
Divisor
Align
>
(
offset
)
;
}
template
<
size_t
Divisor
=
MinMediumAllocSize
size_t
Align
=
Divisor
>
static
size_t
offsetToIndex
(
uintptr_t
offset
)
{
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
offset
%
Align
=
=
0
)
;
return
offset
/
Divisor
;
}
const
void
*
ptrFromOffset
(
uintptr_t
offset
)
const
{
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
offset
%
MediumAllocGranularity
=
=
0
)
;
return
reinterpret_cast
<
void
*
>
(
uintptr_t
(
this
)
+
offset
)
;
}
size_t
findEndBit
(
size_t
startIndex
)
const
{
MOZ_ASSERT
(
startIndex
<
MaxAllocsPerChunk
)
;
if
(
startIndex
+
1
=
=
MaxAllocsPerChunk
)
{
return
MaxAllocsPerChunk
;
}
size_t
endIndex
=
allocEndBitmap
.
ref
(
)
.
FindNext
(
startIndex
+
1
)
;
if
(
endIndex
=
=
SIZE_MAX
)
{
return
MaxAllocsPerChunk
;
}
return
endIndex
;
}
#
ifdef
DEBUG
static
bool
isValidOffset
(
uintptr_t
offset
)
;
#
endif
}
;
constexpr
size_t
FirstMediumAllocOffset
=
RoundUp
(
sizeof
(
BufferChunk
)
MediumAllocGranularity
)
;
static_assert
(
FirstMediumAllocOffset
+
MaxMediumAllocSize
<
=
ChunkSize
)
;
#
ifdef
DEBUG
/
*
static
*
/
bool
BufferChunk
:
:
isValidOffset
(
uintptr_t
offset
)
{
return
offset
>
=
FirstMediumAllocOffset
&
&
offset
<
ChunkSize
;
}
#
endif
/
/
A
sub
-
region
backed
by
a
medium
allocation
which
contains
small
buffer
/
/
allocations
.
struct
SmallBufferRegion
{
static
constexpr
size_t
MaxAllocsPerRegion
=
SmallRegionSize
/
SmallAllocGranularity
;
/
/
Mark
bitmap
:
one
bit
minimum
per
allocation
no
gray
bits
.
MainThreadOrGCTaskData
<
AtomicBitmap
<
MaxAllocsPerRegion
>
>
markBits
;
using
PerAllocBitmap
=
mozilla
:
:
BitSet
<
MaxAllocsPerRegion
>
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
allocStartBitmap
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
allocEndBitmap
;
MainThreadOrGCTaskData
<
PerAllocBitmap
>
nurseryOwnedBitmap
;
MainThreadOrGCTaskData
<
bool
>
hasNurseryOwnedAllocs_
;
using
AllocIter
=
BitmapToBlockIter
<
BitSetIter
<
MaxAllocsPerRegion
>
SmallAllocGranularity
>
;
AllocIter
allocIter
(
)
{
return
{
this
allocStartBitmap
.
ref
(
)
}
;
}
static
SmallBufferRegion
*
from
(
void
*
alloc
)
{
uintptr_t
addr
=
uintptr_t
(
alloc
)
&
~
SmallRegionMask
;
auto
*
region
=
reinterpret_cast
<
SmallBufferRegion
*
>
(
addr
)
;
#
ifdef
DEBUG
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
region
)
)
;
MOZ_ASSERT
(
chunk
-
>
isSmallBufferRegion
(
region
)
)
;
#
endif
return
region
;
}
SmallBufferRegion
(
)
;
const
void
*
ptrFromOffset
(
uintptr_t
offset
)
const
;
void
setAllocated
(
void
*
alloc
size_t
bytes
bool
allocated
)
;
bool
isAllocated
(
const
void
*
alloc
)
const
;
bool
isAllocated
(
uintptr_t
offset
)
const
;
void
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
;
bool
isNurseryOwned
(
const
void
*
alloc
)
const
;
void
setHasNurseryOwnedAllocs
(
bool
value
)
;
bool
hasNurseryOwnedAllocs
(
)
const
;
size_t
allocBytes
(
const
void
*
alloc
)
const
;
bool
setMarked
(
void
*
alloc
)
;
void
setUnmarked
(
void
*
alloc
)
;
bool
isMarked
(
const
void
*
alloc
)
const
;
size_t
findNextAllocated
(
uintptr_t
offset
)
const
;
size_t
findPrevAllocated
(
uintptr_t
offset
)
const
;
bool
isPointerWithinAllocation
(
void
*
ptr
)
const
;
private
:
size_t
ptrToIndex
(
const
void
*
alloc
)
const
;
size_t
offsetToIndex
(
uintptr_t
offset
)
const
;
size_t
findEndBit
(
size_t
startIndex
)
const
{
MOZ_ASSERT
(
startIndex
<
MaxAllocsPerRegion
)
;
if
(
startIndex
+
1
=
=
MaxAllocsPerRegion
)
{
return
MaxAllocsPerRegion
;
}
size_t
endIndex
=
allocEndBitmap
.
ref
(
)
.
FindNext
(
startIndex
+
1
)
;
if
(
endIndex
=
=
SIZE_MAX
)
{
return
MaxAllocsPerRegion
;
}
return
endIndex
;
}
}
;
static
constexpr
size_t
FirstSmallAllocOffset
=
RoundUp
(
sizeof
(
SmallBufferRegion
)
SmallAllocGranularity
)
;
static_assert
(
FirstSmallAllocOffset
<
SmallRegionSize
)
;
static
void
CheckHighBitsOfPointer
(
void
*
ptr
)
{
#
ifdef
JS_64BIT
/
/
We
require
bit
48
and
higher
be
clear
.
MOZ_DIAGNOSTIC_ASSERT
(
(
uintptr_t
(
ptr
)
>
>
47
)
=
=
0
)
;
#
endif
}
BufferAllocator
:
:
FreeLists
:
:
FreeLists
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
}
BufferAllocator
:
:
FreeLists
&
BufferAllocator
:
:
FreeLists
:
:
operator
=
(
FreeLists
&
&
other
)
{
MOZ_ASSERT
(
this
!
=
&
other
)
;
assertEmpty
(
)
;
std
:
:
swap
(
lists
other
.
lists
)
;
std
:
:
swap
(
available
other
.
available
)
;
other
.
assertEmpty
(
)
;
return
*
this
;
}
size_t
BufferAllocator
:
:
FreeLists
:
:
getFirstAvailableSizeClass
(
size_t
minSizeClass
size_t
maxSizeClass
)
const
{
MOZ_ASSERT
(
maxSizeClass
<
=
MaxMediumAllocClass
)
;
size_t
result
=
available
.
FindNext
(
minSizeClass
)
;
MOZ_ASSERT
(
result
>
=
minSizeClass
)
;
MOZ_ASSERT_IF
(
result
!
=
SIZE_MAX
!
lists
[
result
]
.
isEmpty
(
)
)
;
if
(
result
>
maxSizeClass
)
{
return
SIZE_MAX
;
}
return
result
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
FreeLists
:
:
getFirstRegion
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
!
lists
[
sizeClass
]
.
isEmpty
(
)
)
;
return
lists
[
sizeClass
]
.
getFirst
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushFront
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
pushFront
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
pushBack
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
pushBack
(
region
)
;
available
[
sizeClass
]
=
true
;
}
void
BufferAllocator
:
:
FreeLists
:
:
append
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
append
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
prepend
(
FreeLists
&
&
other
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
if
(
!
other
.
lists
[
i
]
.
isEmpty
(
)
)
{
lists
[
i
]
.
prepend
(
std
:
:
move
(
other
.
lists
[
i
]
)
)
;
available
[
i
]
=
true
;
}
}
other
.
available
.
ResetAll
(
)
;
other
.
assertEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
remove
(
size_t
sizeClass
FreeRegion
*
region
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
lists
[
sizeClass
]
.
remove
(
region
)
;
available
[
sizeClass
]
=
!
lists
[
sizeClass
]
.
isEmpty
(
)
;
}
void
BufferAllocator
:
:
FreeLists
:
:
clear
(
)
{
for
(
auto
&
freeList
:
lists
)
{
new
(
&
freeList
)
FreeList
;
/
/
clear
(
)
is
less
efficient
.
}
available
.
ResetAll
(
)
;
}
template
<
typename
Pred
>
void
BufferAllocator
:
:
FreeLists
:
:
eraseIf
(
Pred
&
&
pred
)
{
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
FreeList
&
freeList
=
lists
[
i
]
;
freeList
.
eraseIf
(
std
:
:
forward
<
Pred
>
(
pred
)
)
;
available
[
i
]
=
!
freeList
.
isEmpty
(
)
;
}
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertEmpty
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
MOZ_ASSERT
(
lists
[
i
]
.
isEmpty
(
)
)
;
}
MOZ_ASSERT
(
available
.
IsEmpty
(
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
assertContains
(
size_t
sizeClass
FreeRegion
*
region
)
const
{
#
ifdef
DEBUG
MOZ_ASSERT
(
available
[
sizeClass
]
)
;
MOZ_ASSERT
(
lists
[
sizeClass
]
.
contains
(
region
)
)
;
#
endif
}
inline
void
BufferAllocator
:
:
FreeLists
:
:
checkAvailable
(
)
const
{
#
ifdef
DEBUG
for
(
size_t
i
=
0
;
i
<
AllocSizeClasses
;
i
+
+
)
{
MOZ_ASSERT
(
available
[
i
]
=
=
!
lists
[
i
]
.
isEmpty
(
)
)
;
}
#
endif
}
}
/
/
namespace
js
:
:
gc
MOZ_ALWAYS_INLINE
void
PoisonAlloc
(
void
*
alloc
uint8_t
value
size_t
bytes
MemCheckKind
kind
)
{
#
ifndef
EARLY_BETA_OR_EARLIER
/
/
Limit
poisoning
in
release
builds
.
bytes
=
std
:
:
min
(
bytes
size_t
(
256
)
)
;
#
endif
AlwaysPoison
(
alloc
value
bytes
kind
)
;
}
BufferChunk
:
:
BufferChunk
(
Zone
*
zone
)
:
ChunkBase
(
zone
-
>
runtimeFromMainThread
(
)
ChunkKind
:
:
Buffers
)
{
#
ifdef
DEBUG
this
-
>
zone
=
zone
;
MOZ_ASSERT
(
allocStartBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
allocEndBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
)
;
#
endif
}
BufferChunk
:
:
~
BufferChunk
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
allocStartBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
allocEndBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
#
endif
}
void
BufferChunk
:
:
setAllocated
(
void
*
alloc
size_t
bytes
bool
allocated
)
{
size_t
startBit
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
bytes
%
MediumAllocGranularity
=
=
0
)
;
size_t
endBit
=
startBit
+
bytes
/
MediumAllocGranularity
;
MOZ_ASSERT
(
endBit
<
=
MaxAllocsPerChunk
)
;
MOZ_ASSERT
(
allocStartBitmap
.
ref
(
)
[
startBit
]
!
=
allocated
)
;
MOZ_ASSERT_IF
(
endBit
!
=
MaxAllocsPerChunk
allocStartBitmap
.
ref
(
)
[
startBit
]
=
=
allocEndBitmap
.
ref
(
)
[
endBit
]
)
;
MOZ_ASSERT_IF
(
startBit
+
1
<
MaxAllocsPerChunk
allocStartBitmap
.
ref
(
)
.
FindNext
(
startBit
+
1
)
>
=
endBit
)
;
MOZ_ASSERT
(
findEndBit
(
startBit
)
>
=
endBit
)
;
allocStartBitmap
.
ref
(
)
[
startBit
]
=
allocated
;
if
(
endBit
!
=
MaxAllocsPerChunk
)
{
allocEndBitmap
.
ref
(
)
[
endBit
]
=
allocated
;
}
}
bool
BufferChunk
:
:
isAllocated
(
const
void
*
alloc
)
const
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
allocStartBitmap
.
ref
(
)
[
bit
]
;
}
bool
BufferChunk
:
:
isAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
return
allocStartBitmap
.
ref
(
)
[
bit
]
;
}
void
BufferChunk
:
:
updateEndOffset
(
void
*
alloc
size_t
oldBytes
size_t
newBytes
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
MOZ_ASSERT
(
oldBytes
%
MediumAllocGranularity
=
=
0
)
;
MOZ_ASSERT
(
newBytes
%
MediumAllocGranularity
=
=
0
)
;
size_t
startBit
=
ptrToIndex
(
alloc
)
;
size_t
oldEndBit
=
startBit
+
oldBytes
/
MediumAllocGranularity
;
MOZ_ASSERT
(
oldEndBit
<
=
MaxAllocsPerChunk
)
;
if
(
oldEndBit
!
=
MaxAllocsPerChunk
)
{
MOZ_ASSERT
(
allocEndBitmap
.
ref
(
)
[
oldEndBit
]
)
;
allocEndBitmap
.
ref
(
)
[
oldEndBit
]
=
false
;
}
size_t
newEndBit
=
startBit
+
newBytes
/
MediumAllocGranularity
;
MOZ_ASSERT
(
newEndBit
<
=
MaxAllocsPerChunk
)
;
MOZ_ASSERT_IF
(
startBit
+
1
<
MaxAllocsPerChunk
allocStartBitmap
.
ref
(
)
.
FindNext
(
startBit
+
1
)
>
=
newEndBit
)
;
MOZ_ASSERT
(
findEndBit
(
startBit
)
>
=
newEndBit
)
;
if
(
newEndBit
!
=
MaxAllocsPerChunk
)
{
allocEndBitmap
.
ref
(
)
[
newEndBit
]
=
true
;
}
}
size_t
BufferChunk
:
:
findNextAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
next
=
allocStartBitmap
.
ref
(
)
.
FindNext
(
bit
)
;
if
(
next
=
=
SIZE_MAX
)
{
return
ChunkSize
;
}
return
next
*
MediumAllocGranularity
;
}
size_t
BufferChunk
:
:
findPrevAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
prev
=
allocStartBitmap
.
ref
(
)
.
FindPrev
(
bit
)
;
if
(
prev
=
=
SIZE_MAX
)
{
return
ChunkSize
;
}
return
prev
*
MediumAllocGranularity
;
}
void
BufferChunk
:
:
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
=
nurseryOwned
;
}
bool
BufferChunk
:
:
isNurseryOwned
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
;
}
void
BufferChunk
:
:
setSmallBufferRegion
(
void
*
alloc
bool
smallAlloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
<
SmallRegionSize
SmallRegionSize
>
(
alloc
)
;
smallRegionBitmap
.
ref
(
)
.
setBit
(
bit
smallAlloc
)
;
}
bool
BufferChunk
:
:
isSmallBufferRegion
(
const
void
*
alloc
)
const
{
/
/
Allow
any
valid
small
alloc
pointer
within
the
region
.
size_t
bit
=
ptrToIndex
<
SmallRegionSize
SmallAllocGranularity
>
(
alloc
)
;
return
smallRegionBitmap
.
ref
(
)
.
getBit
(
bit
)
;
}
size_t
BufferChunk
:
:
allocBytes
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
startBit
=
ptrToIndex
(
alloc
)
;
size_t
endBit
=
findEndBit
(
startBit
)
;
MOZ_ASSERT
(
endBit
>
startBit
)
;
MOZ_ASSERT
(
endBit
<
=
MaxAllocsPerChunk
)
;
return
(
endBit
-
startBit
)
*
MediumAllocGranularity
;
}
bool
BufferChunk
:
:
setMarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
/
/
This
is
thread
safe
but
can
return
false
positives
if
another
thread
also
/
/
marked
the
same
allocation
at
the
same
time
;
if
(
markBits
.
ref
(
)
.
getBit
(
bit
)
)
{
return
false
;
}
markBits
.
ref
(
)
.
setBit
(
bit
true
)
;
return
true
;
}
void
BufferChunk
:
:
setUnmarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
markBits
.
ref
(
)
.
setBit
(
bit
false
)
;
}
bool
BufferChunk
:
:
isMarked
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
markBits
.
ref
(
)
.
getBit
(
bit
)
;
}
SmallBufferRegion
:
:
SmallBufferRegion
(
)
{
#
ifdef
DEBUG
MOZ_ASSERT
(
allocStartBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
allocEndBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
MOZ_ASSERT
(
nurseryOwnedBitmap
.
ref
(
)
.
IsEmpty
(
)
)
;
#
endif
}
uintptr_t
SmallBufferRegion
:
:
ptrToIndex
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
(
uintptr_t
(
alloc
)
&
~
SmallRegionMask
)
=
=
uintptr_t
(
this
)
)
;
uintptr_t
offset
=
uintptr_t
(
alloc
)
&
SmallRegionMask
;
return
offsetToIndex
(
offset
)
;
}
uintptr_t
SmallBufferRegion
:
:
offsetToIndex
(
uintptr_t
offset
)
const
{
MOZ_ASSERT
(
offset
>
=
FirstSmallAllocOffset
)
;
MOZ_ASSERT
(
offset
<
SmallRegionSize
)
;
MOZ_ASSERT
(
offset
%
SmallAllocGranularity
=
=
0
)
;
return
offset
/
SmallAllocGranularity
;
}
const
void
*
SmallBufferRegion
:
:
ptrFromOffset
(
uintptr_t
offset
)
const
{
MOZ_ASSERT
(
offset
>
=
FirstSmallAllocOffset
)
;
MOZ_ASSERT
(
offset
<
SmallRegionSize
)
;
MOZ_ASSERT
(
offset
%
SmallAllocGranularity
=
=
0
)
;
return
reinterpret_cast
<
void
*
>
(
uintptr_t
(
this
)
+
offset
)
;
}
void
SmallBufferRegion
:
:
setAllocated
(
void
*
alloc
size_t
bytes
bool
allocated
)
{
size_t
startBit
=
ptrToIndex
(
alloc
)
;
MOZ_ASSERT
(
bytes
%
SmallAllocGranularity
=
=
0
)
;
size_t
endBit
=
startBit
+
bytes
/
SmallAllocGranularity
;
MOZ_ASSERT
(
endBit
<
=
MaxAllocsPerRegion
)
;
MOZ_ASSERT
(
allocStartBitmap
.
ref
(
)
[
startBit
]
!
=
allocated
)
;
MOZ_ASSERT_IF
(
endBit
!
=
MaxAllocsPerRegion
allocStartBitmap
.
ref
(
)
[
startBit
]
=
=
allocEndBitmap
.
ref
(
)
[
endBit
]
)
;
MOZ_ASSERT_IF
(
startBit
+
1
<
MaxAllocsPerRegion
allocStartBitmap
.
ref
(
)
.
FindNext
(
startBit
+
1
)
>
=
endBit
)
;
MOZ_ASSERT
(
findEndBit
(
startBit
)
>
=
endBit
)
;
allocStartBitmap
.
ref
(
)
[
startBit
]
=
allocated
;
if
(
endBit
!
=
MaxAllocsPerRegion
)
{
allocEndBitmap
.
ref
(
)
[
endBit
]
=
allocated
;
}
}
bool
SmallBufferRegion
:
:
isAllocated
(
const
void
*
alloc
)
const
{
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
allocStartBitmap
.
ref
(
)
[
bit
]
;
}
bool
SmallBufferRegion
:
:
isAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
return
allocStartBitmap
.
ref
(
)
[
bit
]
;
}
void
SmallBufferRegion
:
:
setNurseryOwned
(
void
*
alloc
bool
nurseryOwned
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
=
nurseryOwned
;
}
bool
SmallBufferRegion
:
:
isNurseryOwned
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
nurseryOwnedBitmap
.
ref
(
)
[
bit
]
;
}
void
SmallBufferRegion
:
:
setHasNurseryOwnedAllocs
(
bool
value
)
{
hasNurseryOwnedAllocs_
=
value
;
}
bool
SmallBufferRegion
:
:
hasNurseryOwnedAllocs
(
)
const
{
return
hasNurseryOwnedAllocs_
.
ref
(
)
;
}
size_t
SmallBufferRegion
:
:
allocBytes
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
startBit
=
ptrToIndex
(
alloc
)
;
size_t
endBit
=
findEndBit
(
startBit
)
;
MOZ_ASSERT
(
endBit
>
=
startBit
)
;
MOZ_ASSERT
(
endBit
<
=
MaxAllocsPerRegion
)
;
return
(
endBit
-
startBit
)
*
SmallAllocGranularity
;
}
bool
SmallBufferRegion
:
:
setMarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
/
/
This
is
thread
safe
but
can
return
false
positives
if
another
thread
also
/
/
marked
the
same
allocation
at
the
same
time
;
if
(
markBits
.
ref
(
)
.
getBit
(
bit
)
)
{
return
false
;
}
markBits
.
ref
(
)
.
setBit
(
bit
true
)
;
return
true
;
}
void
SmallBufferRegion
:
:
setUnmarked
(
void
*
alloc
)
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
markBits
.
ref
(
)
.
setBit
(
bit
false
)
;
}
bool
SmallBufferRegion
:
:
isMarked
(
const
void
*
alloc
)
const
{
MOZ_ASSERT
(
isAllocated
(
alloc
)
)
;
size_t
bit
=
ptrToIndex
(
alloc
)
;
return
markBits
.
ref
(
)
.
getBit
(
bit
)
;
}
size_t
SmallBufferRegion
:
:
findNextAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
next
=
allocStartBitmap
.
ref
(
)
.
FindNext
(
bit
)
;
if
(
next
=
=
SIZE_MAX
)
{
return
SmallRegionSize
;
}
return
next
*
SmallAllocGranularity
;
}
size_t
SmallBufferRegion
:
:
findPrevAllocated
(
uintptr_t
offset
)
const
{
size_t
bit
=
offsetToIndex
(
offset
)
;
size_t
prev
=
allocStartBitmap
.
ref
(
)
.
FindPrev
(
bit
)
;
if
(
prev
=
=
SIZE_MAX
)
{
return
SmallRegionSize
;
}
return
prev
*
SmallAllocGranularity
;
}
BufferAllocator
:
:
BufferAllocator
(
Zone
*
zone
)
:
zone
(
zone
)
sweptMixedChunks
(
lock
(
)
)
sweptTenuredChunks
(
lock
(
)
)
sweptNurseryFreeLists
(
lock
(
)
)
sweptTenuredFreeLists
(
lock
(
)
)
sweptLargeTenuredAllocs
(
lock
(
)
)
minorState
(
State
:
:
NotCollecting
)
majorState
(
State
:
:
NotCollecting
)
minorSweepingFinished
(
lock
(
)
)
majorSweepingFinished
(
lock
(
)
)
{
}
BufferAllocator
:
:
~
BufferAllocator
(
)
{
#
ifdef
DEBUG
checkGCStateNotInUse
(
)
;
MOZ_ASSERT
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
}
bool
BufferAllocator
:
:
isEmpty
(
)
const
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
|
|
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
return
mixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
&
&
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
;
}
Mutex
&
BufferAllocator
:
:
lock
(
)
const
{
return
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
.
bufferAllocatorLock
;
}
void
*
BufferAllocator
:
:
alloc
(
size_t
bytes
bool
nurseryOwned
)
{
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
return
allocLarge
(
bytes
nurseryOwned
false
)
;
}
if
(
IsSmallAllocSize
(
bytes
)
)
{
return
allocSmall
(
bytes
nurseryOwned
false
)
;
}
return
allocMedium
(
bytes
nurseryOwned
false
)
;
}
void
*
BufferAllocator
:
:
allocInGC
(
size_t
bytes
bool
nurseryOwned
)
{
/
/
Currently
this
is
used
during
tenuring
only
.
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
void
*
result
;
if
(
IsLargeAllocSize
(
bytes
)
)
{
result
=
allocLarge
(
bytes
nurseryOwned
true
)
;
}
else
if
(
IsSmallAllocSize
(
bytes
)
)
{
result
=
allocSmall
(
bytes
nurseryOwned
true
)
;
}
else
{
result
=
allocMedium
(
bytes
nurseryOwned
true
)
;
}
if
(
!
result
)
{
return
nullptr
;
}
/
/
Barrier
to
mark
nursery
-
owned
allocations
that
happen
during
collection
.
We
/
/
don
'
t
need
to
do
this
for
tenured
-
owned
allocations
because
we
don
'
t
sweep
/
/
tenured
-
owned
allocations
that
happened
after
the
start
of
a
major
/
/
collection
.
if
(
nurseryOwned
)
{
markNurseryOwnedAlloc
(
result
false
)
;
}
return
result
;
}
#
ifdef
DEBUG
inline
Zone
*
LargeBuffer
:
:
zone
(
)
{
Zone
*
zone
=
zoneFromAnyThread
(
)
;
MOZ_ASSERT
(
CurrentThreadCanAccessZone
(
zone
)
)
;
return
zone
;
}
inline
Zone
*
LargeBuffer
:
:
zoneFromAnyThread
(
)
{
return
BufferChunk
:
:
from
(
this
)
-
>
zone
;
}
#
endif
#
ifdef
XP_DARWIN
static
inline
void
VirtualCopyPages
(
void
*
dst
const
void
*
src
size_t
bytes
)
{
MOZ_ASSERT
(
(
uintptr_t
(
dst
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
(
uintptr_t
(
src
)
&
PageMask
)
=
=
0
)
;
MOZ_ASSERT
(
bytes
>
=
ChunkSize
)
;
kern_return_t
r
=
vm_copy
(
mach_task_self
(
)
vm_address_t
(
src
)
vm_size_t
(
bytes
)
vm_address_t
(
dst
)
)
;
if
(
r
!
=
KERN_SUCCESS
)
{
MOZ_CRASH
(
"
vm_copy
(
)
failed
"
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
realloc
(
void
*
alloc
size_t
oldBytes
size_t
newBytes
bool
nurseryOwned
)
{
/
/
Reallocate
a
buffer
.
This
has
the
same
semantics
as
standard
libarary
/
/
realloc
:
if
|
ptr
|
is
null
it
creates
a
new
allocation
and
if
it
fails
it
/
/
returns
|
nullptr
|
and
the
original
|
ptr
|
is
still
valid
.
if
(
!
alloc
)
{
MOZ_ASSERT
(
oldBytes
=
=
0
)
;
return
this
-
>
alloc
(
newBytes
nurseryOwned
)
;
}
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
=
=
nurseryOwned
)
;
MOZ_ASSERT_IF
(
zone
-
>
isGCMarkingOrSweeping
(
)
majorState
=
=
State
:
:
Marking
)
;
oldBytes
=
GetGoodAllocSize
(
oldBytes
)
;
newBytes
=
GetGoodAllocSize
(
newBytes
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
MOZ_ASSERT
(
oldBytes
=
=
buffer
-
>
allocBytes
(
)
)
;
/
/
We
can
shrink
large
allocations
(
on
some
platforms
)
.
if
(
newBytes
<
buffer
-
>
allocBytes
(
)
&
&
IsLargeAllocSize
(
newBytes
)
)
{
if
(
shrinkLarge
(
buffer
newBytes
)
)
{
return
alloc
;
}
}
}
else
if
(
IsMediumAlloc
(
alloc
)
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
/
/
We
can
'
t
grow
/
shrink
in
chunks
that
are
being
swept
.
if
(
!
isSweepingChunkAfterMerge
(
chunk
)
)
{
MOZ_ASSERT
(
oldBytes
=
=
chunk
-
>
allocBytes
(
alloc
)
)
;
/
/
We
can
grow
or
shrink
medium
allocations
.
if
(
newBytes
<
oldBytes
&
&
!
IsSmallAllocSize
(
newBytes
)
)
{
if
(
shrinkMedium
(
alloc
oldBytes
newBytes
)
)
{
return
alloc
;
}
}
if
(
newBytes
>
oldBytes
&
&
!
IsLargeAllocSize
(
newBytes
)
)
{
if
(
growMedium
(
alloc
oldBytes
newBytes
)
)
{
return
alloc
;
}
}
}
}
else
{
/
/
TODO
:
Grow
and
shrink
small
allocations
.
MOZ_ASSERT
(
oldBytes
=
=
SmallBufferRegion
:
:
from
(
alloc
)
-
>
allocBytes
(
alloc
)
)
;
}
if
(
newBytes
=
=
oldBytes
)
{
return
alloc
;
}
void
*
newAlloc
=
this
-
>
alloc
(
newBytes
nurseryOwned
)
;
if
(
!
newAlloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
free
(
alloc
)
;
}
)
;
size_t
bytesToCopy
=
std
:
:
min
(
newBytes
oldBytes
)
;
#
ifdef
XP_DARWIN
if
(
bytesToCopy
>
=
ChunkSize
)
{
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
MOZ_ASSERT
(
IsLargeAlloc
(
newAlloc
)
)
;
VirtualCopyPages
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
#
endif
memcpy
(
newAlloc
alloc
bytesToCopy
)
;
return
newAlloc
;
}
void
BufferAllocator
:
:
free
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
freeLarge
(
alloc
)
;
return
;
}
if
(
IsMediumAlloc
(
alloc
)
)
{
freeMedium
(
alloc
)
;
return
;
}
/
/
Can
'
t
free
small
allocations
.
}
/
*
static
*
/
bool
BufferAllocator
:
:
IsBufferAlloc
(
void
*
alloc
)
{
/
/
Precondition
:
|
alloc
|
is
a
pointer
to
a
buffer
allocation
a
GC
thing
or
a
/
/
direct
nursery
allocation
returned
by
Nursery
:
:
allocateBuffer
.
if
(
IsLargeAlloc
(
alloc
)
)
{
return
true
;
}
ChunkBase
*
chunk
=
detail
:
:
GetGCAddressChunkBase
(
alloc
)
;
return
chunk
-
>
getKind
(
)
=
=
ChunkKind
:
:
Buffers
;
}
size_t
BufferAllocator
:
:
getAllocSize
(
void
*
alloc
)
{
/
/
Access
to
allocEndBitmap
can
race
if
the
allocation
is
in
a
chunk
that
is
/
/
currently
being
swept
.
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
!
=
State
:
:
Sweeping
)
;
MOZ_ASSERT
(
majorState
!
=
State
:
:
Sweeping
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
allocBytes
(
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
allocBytes
(
alloc
)
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
allocBytes
(
alloc
)
;
}
bool
BufferAllocator
:
:
isNurseryOwned
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
buffer
-
>
isNurseryOwned
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
isNurseryOwned
(
alloc
)
;
}
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isNurseryOwned
(
alloc
)
;
}
void
BufferAllocator
:
:
markNurseryOwnedAlloc
(
void
*
alloc
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
isNurseryOwned
(
alloc
)
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
markLargeNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
return
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
markSmallNurseryOwnedBuffer
(
alloc
ownerWasTenured
)
;
return
;
}
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
markMediumNurseryOwnedBuffer
(
alloc
ownerWasTenured
)
;
}
void
BufferAllocator
:
:
markSmallNurseryOwnedBuffer
(
void
*
alloc
bool
ownerWasTenured
)
{
#
ifdef
DEBUG
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
#
endif
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
MOZ_ASSERT
(
region
-
>
isNurseryOwned
(
alloc
)
)
;
if
(
ownerWasTenured
)
{
region
-
>
setNurseryOwned
(
alloc
false
)
;
/
/
If
all
nursery
owned
allocations
in
the
region
were
tenured
then
/
/
chunk
-
>
isNurseryOwned
(
region
)
will
now
be
stale
.
It
will
be
updated
when
/
/
the
region
is
swept
.
return
;
}
region
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
markMediumNurseryOwnedBuffer
(
void
*
alloc
bool
ownerWasTenured
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
MOZ_ASSERT
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
;
if
(
ownerWasTenured
)
{
/
/
Change
the
allocation
to
a
tenured
owned
one
.
This
prevents
sweeping
in
a
/
/
minor
collection
.
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
size_t
size
=
chunk
-
>
allocBytes
(
alloc
)
;
updateHeapSize
(
size
false
false
)
;
return
;
}
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
markLargeNurseryOwnedBuffer
(
LargeBuffer
*
buffer
bool
ownerWasTenured
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
/
/
The
buffer
metadata
is
held
in
a
small
buffer
.
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
if
(
region
-
>
isNurseryOwned
(
buffer
)
)
{
markSmallNurseryOwnedBuffer
(
buffer
ownerWasTenured
)
;
}
largeNurseryAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
if
(
ownerWasTenured
)
{
buffer
-
>
isNurseryOwned
=
false
;
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
size_t
usableSize
=
buffer
-
>
allocBytes
(
)
;
updateHeapSize
(
usableSize
false
false
)
;
return
;
}
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
bool
BufferAllocator
:
:
isMarkedBlack
(
void
*
alloc
)
{
if
(
IsLargeAlloc
(
alloc
)
)
{
/
/
The
buffer
metadata
is
held
in
a
small
buffer
.
alloc
=
lookupLargeBuffer
(
alloc
)
;
}
else
if
(
!
IsSmallAlloc
(
alloc
)
)
{
MOZ_ASSERT
(
IsMediumAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isMarked
(
alloc
)
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
return
region
-
>
isMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
traceEdge
(
JSTracer
*
trc
Cell
*
owner
void
*
*
bufferp
const
char
*
name
)
{
/
/
Buffers
are
conceptually
part
of
the
owning
cell
and
are
not
reported
to
/
/
the
tracer
.
/
/
TODO
:
This
should
be
unified
with
the
rest
of
the
tracing
system
.
MOZ_ASSERT
(
owner
)
;
MOZ_ASSERT
(
bufferp
)
;
void
*
buffer
=
*
bufferp
;
MOZ_ASSERT
(
buffer
)
;
if
(
!
IsLargeAlloc
(
buffer
)
&
&
js
:
:
gc
:
:
detail
:
:
GetGCAddressChunkBase
(
buffer
)
-
>
isNurseryChunk
(
)
)
{
/
/
JSObject
slots
and
elements
can
be
allocated
in
the
nursery
and
this
is
/
/
handled
separately
.
return
;
}
MOZ_ASSERT
(
IsBufferAlloc
(
buffer
)
)
;
if
(
IsLargeAlloc
(
buffer
)
)
{
traceLargeAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
if
(
IsSmallAlloc
(
buffer
)
)
{
traceSmallAlloc
(
trc
owner
bufferp
name
)
;
return
;
}
traceMediumAlloc
(
trc
owner
bufferp
name
)
;
}
void
BufferAllocator
:
:
traceSmallAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
region
-
>
isNurseryOwned
(
alloc
)
)
{
markSmallNurseryOwnedBuffer
(
alloc
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
region
-
>
isNurseryOwned
(
alloc
)
)
{
markSmallTenuredAlloc
(
alloc
)
;
}
return
;
}
}
void
BufferAllocator
:
:
traceMediumAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumNurseryOwnedBuffer
(
alloc
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
markMediumTenuredAlloc
(
alloc
)
;
}
return
;
}
}
void
BufferAllocator
:
:
traceLargeAlloc
(
JSTracer
*
trc
Cell
*
owner
void
*
*
allocp
const
char
*
name
)
{
void
*
alloc
=
*
allocp
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
/
/
Trace
small
buffer
that
holds
large
buffer
metadata
.
traceSmallAlloc
(
trc
owner
reinterpret_cast
<
void
*
*
>
(
&
buffer
)
"
LargeBuffer
"
)
;
if
(
trc
-
>
isTenuringTracer
(
)
)
{
if
(
isNurseryOwned
(
alloc
)
)
{
markLargeNurseryOwnedBuffer
(
buffer
owner
-
>
isTenured
(
)
)
;
}
return
;
}
if
(
trc
-
>
isMarkingTracer
(
)
)
{
if
(
!
isNurseryOwned
(
alloc
)
)
{
markLargeTenuredBuffer
(
buffer
)
;
}
return
;
}
}
bool
BufferAllocator
:
:
markTenuredAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
alloc
)
;
MOZ_ASSERT
(
!
isNurseryOwned
(
alloc
)
)
;
if
(
IsLargeAlloc
(
alloc
)
)
{
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
)
;
return
markLargeTenuredBuffer
(
buffer
)
;
}
if
(
IsSmallAlloc
(
alloc
)
)
{
return
markSmallTenuredAlloc
(
alloc
)
;
}
return
markMediumTenuredAlloc
(
alloc
)
;
}
bool
BufferAllocator
:
:
markSmallTenuredAlloc
(
void
*
alloc
)
{
auto
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
chunk
-
>
allocatedDuringCollection
)
{
/
/
Will
not
be
swept
already
counted
as
marked
.
return
false
;
}
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
isAllocated
(
alloc
)
)
;
return
region
-
>
setMarked
(
alloc
)
;
}
bool
BufferAllocator
:
:
markMediumTenuredAlloc
(
void
*
alloc
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
isAllocated
(
alloc
)
)
;
if
(
chunk
-
>
allocatedDuringCollection
)
{
/
/
Will
not
be
swept
already
counted
as
marked
.
return
false
;
}
return
chunk
-
>
setMarked
(
alloc
)
;
}
void
BufferAllocator
:
:
startMinorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
if
(
majorState
=
=
State
:
:
NotCollecting
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
;
if
(
gc
-
>
hasZealMode
(
ZealMode
:
:
CheckHeapBeforeMinorGC
)
)
{
/
/
This
is
too
expensive
to
run
on
every
minor
GC
.
checkGCStateNotInUse
(
lock
)
;
}
}
#
endif
/
/
Large
allocations
that
are
marked
when
tracing
the
nursery
will
be
moved
/
/
back
to
the
main
list
.
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
std
:
:
swap
(
largeNurseryAllocs
.
ref
(
)
largeNurseryAllocsToSweep
.
ref
(
)
)
;
minorState
=
State
:
:
Marking
;
}
bool
BufferAllocator
:
:
startMinorSweeping
(
)
{
/
/
Called
during
minor
GC
.
Operates
on
the
active
allocs
/
chunks
lists
.
The
'
to
/
/
sweep
'
lists
do
not
contain
nursery
owned
allocations
.
#
ifdef
DEBUG
MOZ_ASSERT
(
minorState
=
=
State
:
:
Marking
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
for
(
LargeBuffer
*
buffer
:
largeNurseryAllocsToSweep
.
ref
(
)
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
)
;
}
#
endif
/
/
Check
whether
there
are
any
medium
chunks
containing
nursery
owned
/
/
allocations
that
need
to
be
swept
.
if
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
&
&
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
/
/
Nothing
more
to
do
.
Don
'
t
transition
to
sweeping
state
.
minorState
=
State
:
:
NotCollecting
;
return
false
;
}
/
/
TODO
:
There
are
more
efficient
ways
to
remove
the
free
regions
in
nursery
/
/
chunks
from
the
free
lists
but
all
require
some
more
bookkeeping
.
I
don
'
t
/
/
know
how
much
difference
such
a
change
would
make
.
/
/
/
/
Some
possibilities
are
:
/
/
-
maintain
a
separate
list
of
free
regions
in
each
chunk
and
use
that
to
/
/
remove
those
regions
in
nursery
chunks
/
/
-
have
separate
free
lists
for
nursery
/
tenured
chunks
/
/
-
keep
free
regions
at
different
ends
of
the
free
list
depending
on
chunk
/
/
kind
freeLists
.
ref
(
)
.
eraseIf
(
[
]
(
FreeRegion
*
region
)
{
return
BufferChunk
:
:
from
(
region
)
-
>
hasNurseryOwnedAllocs
;
}
)
;
mixedChunksToSweep
.
ref
(
)
=
std
:
:
move
(
mixedChunks
.
ref
(
)
)
;
minorState
=
State
:
:
Sweeping
;
return
true
;
}
struct
LargeAllocToFree
{
size_t
bytes
;
LargeAllocToFree
*
next
=
nullptr
;
explicit
LargeAllocToFree
(
size_t
bytes
)
:
bytes
(
bytes
)
{
}
}
;
static
void
PushLargeAllocToFree
(
LargeAllocToFree
*
*
listHead
LargeBuffer
*
buffer
)
{
auto
*
alloc
=
new
(
buffer
-
>
data
(
)
)
LargeAllocToFree
(
buffer
-
>
bytes
)
;
alloc
-
>
next
=
*
listHead
;
*
listHead
=
alloc
;
}
static
void
FreeLargeAllocs
(
LargeAllocToFree
*
listHead
)
{
while
(
listHead
)
{
LargeAllocToFree
*
alloc
=
listHead
;
LargeAllocToFree
*
next
=
alloc
-
>
next
;
UnmapPages
(
alloc
alloc
-
>
bytes
)
;
listHead
=
next
;
}
}
void
BufferAllocator
:
:
sweepForMinorCollection
(
)
{
/
/
Called
on
a
background
thread
.
MOZ_ASSERT
(
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
}
/
/
Bug
1961749
:
Freeing
large
buffers
can
be
slow
so
it
might
be
worth
/
/
splitting
sweeping
into
two
phases
so
that
all
zones
get
their
medium
/
/
buffers
swept
and
made
available
for
allocation
before
any
large
buffers
/
/
are
freed
.
/
/
Freeing
large
buffers
may
be
slow
so
leave
that
till
the
end
.
However
/
/
large
buffer
metadata
is
stored
in
small
buffers
so
form
a
list
of
large
/
/
buffers
to
free
before
sweeping
small
buffers
.
LargeAllocToFree
*
largeAllocsToFree
=
nullptr
;
while
(
!
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeNurseryAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
PushLargeAllocToFree
(
&
largeAllocsToFree
buffer
)
;
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unregisterLarge
(
buffer
true
lock
)
;
}
while
(
!
mixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
mixedChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
FreeLists
sweptFreeLists
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
SweepNursery
false
sweptFreeLists
)
)
{
{
AutoLock
lock
(
this
)
;
sweptMixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
if
(
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
)
{
sweptNurseryFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
else
{
sweptTenuredFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
}
/
/
Signal
to
the
main
thread
that
swept
data
is
available
by
setting
this
/
/
relaxed
atomic
flag
.
hasMinorSweepDataToMerge
=
true
;
}
}
/
/
Unmap
large
buffers
.
FreeLargeAllocs
(
largeAllocsToFree
)
;
/
/
Signal
to
main
thread
to
update
minorState
.
{
AutoLock
lock
(
this
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
minorSweepingFinished
=
true
;
hasMinorSweepDataToMerge
=
true
;
}
}
void
BufferAllocator
:
:
startMajorCollection
(
MaybeLock
&
lock
)
{
maybeMergeSweptData
(
lock
)
;
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
checkGCStateNotInUse
(
lock
)
;
/
/
Everything
is
tenured
since
we
just
evicted
the
nursery
or
will
be
by
the
/
/
time
minor
sweeping
finishes
.
MOZ_ASSERT
(
mixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
tenuredChunksToSweep
.
ref
(
)
=
std
:
:
move
(
tenuredChunks
.
ref
(
)
)
;
largeTenuredAllocsToSweep
.
ref
(
)
=
std
:
:
move
(
largeTenuredAllocs
.
ref
(
)
)
;
/
/
Clear
the
active
free
lists
to
prevent
further
allocation
in
chunks
that
/
/
will
be
swept
.
freeLists
.
ref
(
)
.
clear
(
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
/
/
Ensure
swept
nursery
chunks
are
moved
to
the
tenuredChunks
lists
in
/
/
mergeSweptData
.
majorStartedWhileMinorSweeping
=
true
;
}
#
ifdef
DEBUG
MOZ_ASSERT
(
tenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
freeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
largeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
#
endif
majorState
=
State
:
:
Marking
;
}
void
BufferAllocator
:
:
startMajorSweeping
(
MaybeLock
&
lock
)
{
/
/
Called
when
a
zone
transitions
from
marking
to
sweeping
.
#
ifdef
DEBUG
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
zone
-
>
isGCFinished
(
)
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
.
refNoCheck
(
)
)
;
#
endif
maybeMergeSweptData
(
lock
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
majorState
=
State
:
:
Sweeping
;
}
void
BufferAllocator
:
:
sweepForMajorCollection
(
bool
shouldDecommit
)
{
/
/
Called
on
a
background
thread
.
MOZ_ASSERT
(
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
)
;
/
/
Sweep
large
allocs
first
since
they
rely
on
the
mark
bits
of
their
/
/
corresponding
LargeBuffer
structures
which
are
stored
small
buffers
.
/
/
/
/
It
'
s
tempting
to
try
and
optimize
this
by
moving
the
allocations
between
/
/
lists
when
they
are
marked
in
the
same
way
as
for
nursery
sweeping
.
This
/
/
would
require
synchronizing
the
list
modification
when
marking
in
parallel
/
/
so
is
probably
not
worth
it
.
LargeAllocList
sweptLargeAllocs
;
LargeAllocToFree
*
largeAllocsToFree
=
nullptr
;
while
(
!
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
LargeBuffer
*
buffer
=
largeTenuredAllocsToSweep
.
ref
(
)
.
popFirst
(
)
;
if
(
isLargeTenuredMarked
(
buffer
)
)
{
sweptLargeAllocs
.
pushBack
(
buffer
)
;
}
else
{
PushLargeAllocToFree
(
&
largeAllocsToFree
buffer
)
;
MaybeLock
lock
(
std
:
:
in_place
this
)
;
unregisterLarge
(
buffer
true
lock
)
;
}
}
while
(
!
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
tenuredChunksToSweep
.
ref
(
)
.
popFirst
(
)
;
FreeLists
sweptFreeLists
;
if
(
sweepChunk
(
chunk
SweepKind
:
:
SweepTenured
shouldDecommit
sweptFreeLists
)
)
{
{
AutoLock
lock
(
this
)
;
sweptTenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
sweptTenuredFreeLists
.
ref
(
)
.
append
(
std
:
:
move
(
sweptFreeLists
)
)
;
}
/
/
Signal
to
the
main
thread
that
swept
data
is
available
by
setting
this
/
/
relaxed
atomic
flag
.
hasMinorSweepDataToMerge
=
true
;
}
}
/
/
Unmap
large
buffers
.
/
/
/
/
Bug
1961749
:
This
could
possibly
run
after
signalling
sweeping
is
finished
/
/
or
concurrently
with
other
sweeping
.
FreeLargeAllocs
(
largeAllocsToFree
)
;
AutoLock
lock
(
this
)
;
sweptLargeTenuredAllocs
.
ref
(
)
=
std
:
:
move
(
sweptLargeAllocs
)
;
/
/
Signal
to
main
thread
to
update
majorState
.
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
majorSweepingFinished
=
true
;
}
static
void
ClearAllocatedDuringCollection
(
SlimLinkedList
<
BufferChunk
>
&
list
)
{
for
(
auto
*
buffer
:
list
)
{
buffer
-
>
allocatedDuringCollection
=
false
;
}
}
static
void
ClearAllocatedDuringCollection
(
SlimLinkedList
<
LargeBuffer
>
&
list
)
{
for
(
auto
*
element
:
list
)
{
element
-
>
allocatedDuringCollection
=
false
;
}
}
void
BufferAllocator
:
:
finishMajorCollection
(
const
AutoLock
&
lock
)
{
/
/
This
can
be
called
in
any
state
:
/
/
/
/
-
NotCollecting
:
after
major
sweeping
has
finished
and
the
state
has
been
/
/
reset
to
NotCollecting
in
mergeSweptData
.
/
/
/
/
-
Marking
:
if
collection
was
aborted
and
startMajorSweeping
was
not
/
/
called
.
/
/
/
/
-
Sweeping
:
if
sweeping
has
finished
and
mergeSweptData
has
not
been
/
/
called
yet
.
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
majorSweepingFinished
)
;
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
lock
)
;
}
if
(
majorState
=
=
State
:
:
Marking
)
{
abortMajorSweeping
(
lock
)
;
}
#
ifdef
DEBUG
checkGCStateNotInUse
(
lock
)
;
#
endif
}
void
BufferAllocator
:
:
abortMajorSweeping
(
const
AutoLock
&
lock
)
{
/
/
We
have
aborted
collection
without
sweeping
this
zone
.
Restore
or
rebuild
/
/
the
original
state
.
MOZ_ASSERT
(
majorState
=
=
State
:
:
Marking
)
;
MOZ_ASSERT
(
sweptTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
clearAllocatedDuringCollectionState
(
lock
)
;
/
/
Rebuild
free
lists
for
chunks
we
didn
'
t
end
up
sweeping
.
for
(
BufferChunk
*
chunk
:
tenuredChunksToSweep
.
ref
(
)
)
{
MOZ_ALWAYS_TRUE
(
sweepChunk
(
chunk
SweepKind
:
:
RebuildFreeLists
false
freeLists
.
ref
(
)
)
)
;
}
tenuredChunks
.
ref
(
)
.
prepend
(
std
:
:
move
(
tenuredChunksToSweep
.
ref
(
)
)
)
;
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
largeTenuredAllocsToSweep
.
ref
(
)
)
)
;
majorState
=
State
:
:
NotCollecting
;
}
void
BufferAllocator
:
:
clearAllocatedDuringCollectionState
(
const
AutoLock
&
lock
)
{
ClearAllocatedDuringCollection
(
mixedChunks
.
ref
(
)
)
;
ClearAllocatedDuringCollection
(
tenuredChunks
.
ref
(
)
)
;
/
/
This
flag
is
not
set
for
large
nursery
-
owned
allocations
.
ClearAllocatedDuringCollection
(
largeTenuredAllocs
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
mergeSweptData
(
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
)
{
AutoLock
lock
(
this
)
;
mergeSweptData
(
lock
)
;
}
void
BufferAllocator
:
:
maybeMergeSweptData
(
MaybeLock
&
lock
)
{
if
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
{
if
(
lock
.
isNothing
(
)
)
{
lock
.
emplace
(
this
)
;
}
mergeSweptData
(
lock
.
ref
(
)
)
;
}
}
void
BufferAllocator
:
:
mergeSweptData
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
|
|
majorState
=
=
State
:
:
Sweeping
)
;
if
(
majorSweepingFinished
)
{
clearAllocatedDuringCollectionState
(
lock
)
;
if
(
minorState
=
=
State
:
:
Sweeping
)
{
majorFinishedWhileMinorSweeping
=
true
;
}
}
/
/
Merge
swept
chunks
that
previously
contained
nursery
owned
allocations
.
If
/
/
semispace
nursery
collection
is
in
use
then
these
chunks
may
contain
both
/
/
nursery
and
tenured
-
owned
allocations
otherwise
all
allocations
will
be
/
/
tenured
-
owned
.
while
(
!
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
{
BufferChunk
*
chunk
=
sweptMixedChunks
.
ref
(
)
.
popLast
(
)
;
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
;
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
NotCollecting
&
&
!
majorFinishedWhileMinorSweeping
!
chunk
-
>
allocatedDuringCollection
)
;
if
(
majorFinishedWhileMinorSweeping
)
{
chunk
-
>
allocatedDuringCollection
=
false
;
}
if
(
chunk
-
>
hasNurseryOwnedAllocs
)
{
mixedChunks
.
ref
(
)
.
pushFront
(
chunk
)
;
}
else
if
(
majorStartedWhileMinorSweeping
)
{
tenuredChunksToSweep
.
ref
(
)
.
pushFront
(
chunk
)
;
}
else
{
tenuredChunks
.
ref
(
)
.
pushFront
(
chunk
)
;
}
}
/
/
Merge
swept
chunks
that
did
not
contain
nursery
owned
allocations
.
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
sweptTenuredChunks
.
ref
(
)
)
{
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
;
MOZ_ASSERT
(
!
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
)
;
MOZ_ASSERT
(
!
chunk
-
>
allocatedDuringCollection
)
;
}
#
endif
tenuredChunks
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptTenuredChunks
.
ref
(
)
)
)
;
freeLists
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptNurseryFreeLists
.
ref
(
)
)
)
;
if
(
!
majorStartedWhileMinorSweeping
)
{
freeLists
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptTenuredFreeLists
.
ref
(
)
)
)
;
}
else
{
sweptTenuredFreeLists
.
ref
(
)
.
clear
(
)
;
}
largeTenuredAllocs
.
ref
(
)
.
prepend
(
std
:
:
move
(
sweptLargeTenuredAllocs
.
ref
(
)
)
)
;
hasMinorSweepDataToMerge
=
false
;
if
(
minorSweepingFinished
)
{
MOZ_ASSERT
(
minorState
=
=
State
:
:
Sweeping
)
;
minorState
=
State
:
:
NotCollecting
;
minorSweepingFinished
=
false
;
majorStartedWhileMinorSweeping
=
false
;
majorFinishedWhileMinorSweeping
=
false
;
#
ifdef
DEBUG
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
true
)
;
}
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
verifyChunk
(
chunk
false
)
;
}
#
endif
}
if
(
majorSweepingFinished
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
Sweeping
)
;
majorState
=
State
:
:
NotCollecting
;
majorSweepingFinished
=
false
;
MOZ_ASSERT
(
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
}
}
void
BufferAllocator
:
:
clearMarkStateAfterBarrierVerification
(
)
{
MOZ_ASSERT
(
!
zone
-
>
wasGCStarted
(
)
)
;
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
for
(
auto
*
chunks
:
{
&
mixedChunks
.
ref
(
)
&
tenuredChunks
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
chunk
-
>
markBits
.
ref
(
)
.
clear
(
)
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
region
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
}
}
}
bool
BufferAllocator
:
:
isPointerWithinBuffer
(
void
*
ptr
)
{
maybeMergeSweptData
(
)
;
for
(
const
auto
*
chunks
:
{
&
mixedChunks
.
ref
(
)
&
tenuredChunks
.
ref
(
)
}
)
{
for
(
auto
*
chunk
:
*
chunks
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
if
(
majorState
=
=
State
:
:
Marking
)
{
for
(
auto
*
chunk
:
tenuredChunksToSweep
.
ref
(
)
)
{
if
(
chunk
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
/
/
Note
we
cannot
safely
access
data
that
is
being
swept
on
another
thread
.
for
(
const
auto
*
allocs
:
{
&
largeNurseryAllocs
.
ref
(
)
&
largeTenuredAllocs
.
ref
(
)
}
)
{
for
(
auto
*
alloc
:
*
allocs
)
{
if
(
alloc
-
>
isPointerWithinAllocation
(
ptr
)
)
{
return
true
;
}
}
}
return
false
;
}
bool
BufferChunk
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
uintptr_t
offset
=
uintptr_t
(
ptr
)
-
uintptr_t
(
this
)
;
if
(
offset
>
=
ChunkSize
|
|
offset
<
FirstMediumAllocOffset
)
{
return
false
;
}
if
(
smallRegionBitmap
.
ref
(
)
.
getBit
(
offset
/
SmallRegionSize
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
ptr
)
;
return
region
-
>
isPointerWithinAllocation
(
ptr
)
;
}
uintptr_t
allocOffset
=
findPrevAllocated
(
RoundDown
(
offset
MinMediumAllocSize
)
)
;
MOZ_ASSERT
(
allocOffset
<
=
ChunkSize
)
;
if
(
allocOffset
=
=
ChunkSize
)
{
return
false
;
}
const
void
*
alloc
=
ptrFromOffset
(
allocOffset
)
;
size_t
size
=
allocBytes
(
alloc
)
;
return
offset
<
allocOffset
+
size
;
}
bool
SmallBufferRegion
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
uintptr_t
offset
=
uintptr_t
(
ptr
)
-
uintptr_t
(
this
)
;
MOZ_ASSERT
(
offset
<
SmallRegionSize
)
;
uintptr_t
allocOffset
=
findPrevAllocated
(
RoundDown
(
offset
SmallAllocGranularity
)
)
;
MOZ_ASSERT
(
allocOffset
<
=
SmallRegionSize
)
;
if
(
allocOffset
=
=
SmallRegionSize
)
{
return
false
;
}
const
void
*
alloc
=
ptrFromOffset
(
allocOffset
)
;
size_t
size
=
allocBytes
(
alloc
)
;
return
offset
<
allocOffset
+
size
;
}
bool
LargeBuffer
:
:
isPointerWithinAllocation
(
void
*
ptr
)
const
{
return
uintptr_t
(
ptr
)
-
uintptr_t
(
alloc
)
<
bytes
;
}
#
ifdef
DEBUG
void
BufferAllocator
:
:
checkGCStateNotInUse
(
)
{
maybeMergeSweptData
(
)
;
AutoLock
lock
(
this
)
;
/
/
Some
fields
are
protected
by
this
lock
.
checkGCStateNotInUse
(
lock
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
MaybeLock
&
maybeLock
)
{
if
(
maybeLock
.
isNothing
(
)
)
{
/
/
Some
fields
are
protected
by
this
lock
.
maybeLock
.
emplace
(
this
)
;
}
checkGCStateNotInUse
(
maybeLock
.
ref
(
)
)
;
}
void
BufferAllocator
:
:
checkGCStateNotInUse
(
const
AutoLock
&
lock
)
{
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
bool
isNurserySweeping
=
minorState
=
=
State
:
:
Sweeping
;
checkChunkListGCStateNotInUse
(
mixedChunks
.
ref
(
)
true
false
)
;
checkChunkListGCStateNotInUse
(
tenuredChunks
.
ref
(
)
false
false
)
;
if
(
isNurserySweeping
)
{
checkChunkListGCStateNotInUse
(
sweptMixedChunks
.
ref
(
)
true
majorFinishedWhileMinorSweeping
)
;
checkChunkListGCStateNotInUse
(
sweptTenuredChunks
.
ref
(
)
false
false
)
;
}
else
{
MOZ_ASSERT
(
mixedChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
largeNurseryAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptMixedChunks
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptTenuredChunks
.
ref
(
)
.
isEmpty
(
)
)
;
sweptNurseryFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
sweptTenuredFreeLists
.
ref
(
)
.
assertEmpty
(
)
;
MOZ_ASSERT
(
!
majorStartedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
majorFinishedWhileMinorSweeping
)
;
MOZ_ASSERT
(
!
hasMinorSweepDataToMerge
)
;
MOZ_ASSERT
(
!
minorSweepingFinished
)
;
MOZ_ASSERT
(
!
majorSweepingFinished
)
;
}
MOZ_ASSERT
(
tenuredChunksToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
checkAllocListGCStateNotInUse
(
largeNurseryAllocs
.
ref
(
)
true
)
;
checkAllocListGCStateNotInUse
(
largeTenuredAllocs
.
ref
(
)
false
)
;
MOZ_ASSERT
(
largeTenuredAllocsToSweep
.
ref
(
)
.
isEmpty
(
)
)
;
MOZ_ASSERT
(
sweptLargeTenuredAllocs
.
ref
(
)
.
isEmpty
(
)
)
;
}
void
BufferAllocator
:
:
checkChunkListGCStateNotInUse
(
BufferChunkList
&
chunks
bool
hasNurseryOwnedAllocs
bool
allowAllocatedDuringCollection
)
{
for
(
BufferChunk
*
chunk
:
chunks
)
{
checkChunkGCStateNotInUse
(
chunk
allowAllocatedDuringCollection
)
;
verifyChunk
(
chunk
hasNurseryOwnedAllocs
)
;
}
}
void
BufferAllocator
:
:
checkChunkGCStateNotInUse
(
BufferChunk
*
chunk
bool
allowAllocatedDuringCollection
)
{
MOZ_ASSERT_IF
(
!
allowAllocatedDuringCollection
!
chunk
-
>
allocatedDuringCollection
)
;
MOZ_ASSERT
(
chunk
-
>
markBits
.
ref
(
)
.
isEmpty
(
)
)
;
}
void
BufferAllocator
:
:
verifyChunk
(
BufferChunk
*
chunk
bool
hasNurseryOwnedAllocs
)
{
MOZ_ASSERT
(
chunk
-
>
hasNurseryOwnedAllocs
=
=
hasNurseryOwnedAllocs
)
;
static
constexpr
size_t
StepBytes
=
MediumAllocGranularity
;
size_t
freeOffset
=
FirstMediumAllocOffset
;
size_t
freeListsFreeRegionCount
=
0
;
if
(
chunk
-
>
ownsFreeLists
)
{
for
(
const
FreeList
&
list
:
chunk
-
>
freeLists
.
ref
(
)
)
{
for
(
const
FreeRegion
*
region
:
list
)
{
MOZ_ASSERT
(
BufferChunk
:
:
from
(
region
)
=
=
chunk
)
;
freeListsFreeRegionCount
+
+
;
}
}
}
else
{
MOZ_ASSERT
(
chunk
-
>
freeLists
.
ref
(
)
.
isEmpty
(
)
)
;
}
size_t
chunkFreeRegionCount
=
0
;
for
(
auto
iter
=
chunk
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
/
/
Check
any
free
region
preceding
this
allocation
.
size_t
offset
=
iter
.
getOffset
(
)
;
MOZ_ASSERT
(
offset
>
=
FirstMediumAllocOffset
)
;
if
(
offset
>
freeOffset
)
{
verifyFreeRegion
(
chunk
offset
offset
-
freeOffset
chunkFreeRegionCount
)
;
}
/
/
Check
this
allocation
.
void
*
alloc
=
iter
.
get
(
)
;
MOZ_ASSERT_IF
(
chunk
-
>
isNurseryOwned
(
alloc
)
hasNurseryOwnedAllocs
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
endOffset
=
offset
+
bytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
for
(
size_t
i
=
offset
+
StepBytes
;
i
<
endOffset
;
i
+
=
StepBytes
)
{
MOZ_ASSERT
(
!
chunk
-
>
isAllocated
(
i
)
)
;
}
if
(
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
{
auto
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
MOZ_ASSERT_IF
(
region
-
>
hasNurseryOwnedAllocs
(
)
hasNurseryOwnedAllocs
)
;
verifySmallBufferRegion
(
region
chunkFreeRegionCount
)
;
}
freeOffset
=
endOffset
;
}
/
/
Check
any
free
region
following
the
last
allocation
.
if
(
freeOffset
<
ChunkSize
)
{
verifyFreeRegion
(
chunk
ChunkSize
ChunkSize
-
freeOffset
chunkFreeRegionCount
)
;
}
MOZ_ASSERT_IF
(
chunk
-
>
ownsFreeLists
freeListsFreeRegionCount
=
=
chunkFreeRegionCount
)
;
}
void
BufferAllocator
:
:
verifyFreeRegion
(
BufferChunk
*
chunk
uintptr_t
endOffset
size_t
expectedSize
size_t
&
freeRegionCount
)
{
MOZ_ASSERT
(
expectedSize
>
=
MinFreeRegionSize
)
;
auto
*
freeRegion
=
FreeRegion
:
:
fromEndOffset
(
chunk
endOffset
)
;
MOZ_ASSERT
(
freeRegion
-
>
isInList
(
)
)
;
MOZ_ASSERT
(
freeRegion
-
>
size
(
)
=
=
expectedSize
)
;
freeRegionCount
+
+
;
}
void
BufferAllocator
:
:
verifySmallBufferRegion
(
SmallBufferRegion
*
region
size_t
&
freeRegionCount
)
{
bool
foundNurseryOwnedAllocs
=
false
;
static
constexpr
size_t
StepBytes
=
SmallAllocGranularity
;
size_t
freeOffset
=
FirstSmallAllocOffset
;
for
(
auto
iter
=
region
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
/
/
Check
any
free
region
preceding
this
allocation
.
size_t
offset
=
iter
.
getOffset
(
)
;
MOZ_ASSERT
(
offset
>
=
FirstSmallAllocOffset
)
;
if
(
offset
>
freeOffset
)
{
verifyFreeRegion
(
region
offset
offset
-
freeOffset
freeRegionCount
)
;
}
/
/
Check
this
allocation
.
void
*
alloc
=
iter
.
get
(
)
;
MOZ_ASSERT_IF
(
region
-
>
isNurseryOwned
(
alloc
)
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
size_t
bytes
=
region
-
>
allocBytes
(
alloc
)
;
uintptr_t
endOffset
=
offset
+
bytes
;
MOZ_ASSERT
(
endOffset
<
=
SmallRegionSize
)
;
for
(
size_t
i
=
offset
+
StepBytes
;
i
<
endOffset
;
i
+
=
StepBytes
)
{
MOZ_ASSERT
(
!
region
-
>
isAllocated
(
i
)
)
;
}
if
(
region
-
>
isNurseryOwned
(
alloc
)
)
{
foundNurseryOwnedAllocs
=
true
;
}
freeOffset
=
endOffset
;
}
MOZ_ASSERT
(
foundNurseryOwnedAllocs
=
=
region
-
>
hasNurseryOwnedAllocs
(
)
)
;
/
/
Check
any
free
region
following
the
last
allocation
.
if
(
freeOffset
<
SmallRegionSize
)
{
verifyFreeRegion
(
region
SmallRegionSize
SmallRegionSize
-
freeOffset
freeRegionCount
)
;
}
}
void
BufferAllocator
:
:
verifyFreeRegion
(
SmallBufferRegion
*
region
uintptr_t
endOffset
size_t
expectedSize
size_t
&
freeRegionCount
)
{
if
(
expectedSize
<
MinFreeRegionSize
)
{
return
;
}
auto
*
freeRegion
=
FreeRegion
:
:
fromEndOffset
(
region
endOffset
)
;
MOZ_ASSERT
(
freeRegion
-
>
isInList
(
)
)
;
MOZ_ASSERT
(
freeRegion
-
>
size
(
)
=
=
expectedSize
)
;
freeRegionCount
+
+
;
}
void
BufferAllocator
:
:
checkAllocListGCStateNotInUse
(
LargeAllocList
&
list
bool
isNurseryOwned
)
{
for
(
LargeBuffer
*
buffer
:
list
)
{
MOZ_ASSERT
(
buffer
-
>
isNurseryOwned
=
=
isNurseryOwned
)
;
MOZ_ASSERT_IF
(
!
isNurseryOwned
!
buffer
-
>
allocatedDuringCollection
)
;
}
}
#
endif
void
*
BufferAllocator
:
:
allocSmall
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
MOZ_ASSERT
(
IsSmallAllocSize
(
bytes
)
)
;
/
/
Round
up
to
next
available
size
.
bytes
=
RoundUp
(
std
:
:
max
(
bytes
MinSmallAllocSize
)
SmallAllocGranularity
)
;
MOZ_ASSERT
(
bytes
<
=
MaxSmallAllocSize
)
;
/
/
Get
size
class
from
|
bytes
|
.
size_t
sizeClass
=
SizeClassForSmallAlloc
(
bytes
)
;
void
*
alloc
=
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retrySmallAlloc
(
bytes
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
SmallBufferRegion
*
region
=
SmallBufferRegion
:
:
from
(
alloc
)
;
region
-
>
setAllocated
(
alloc
bytes
true
)
;
MOZ_ASSERT
(
region
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
MOZ_ASSERT
(
!
region
-
>
isNurseryOwned
(
alloc
)
)
;
region
-
>
setNurseryOwned
(
alloc
nurseryOwned
)
;
auto
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
if
(
nurseryOwned
&
&
!
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
region
-
>
setHasNurseryOwnedAllocs
(
true
)
;
if
(
!
chunk
-
>
hasNurseryOwnedAllocs
)
{
tenuredChunks
.
ref
(
)
.
remove
(
chunk
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
true
;
mixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
}
/
/
Heap
size
updates
are
done
for
the
small
buffer
region
as
a
whole
not
/
/
individual
allocations
within
it
.
MOZ_ASSERT
(
!
region
-
>
isMarked
(
alloc
)
)
;
MOZ_ASSERT
(
IsSmallAlloc
(
alloc
)
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retrySmallAlloc
(
size_t
bytes
size_t
sizeClass
bool
inGC
)
{
if
(
hasMinorSweepDataToMerge
)
{
/
/
Avoid
taking
the
lock
unless
we
know
there
is
data
to
merge
.
This
reduces
/
/
context
switches
.
mergeSweptData
(
)
;
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
if
(
ptr
)
{
return
ptr
;
}
}
if
(
!
allocNewSmallRegion
(
inGC
)
)
{
return
nullptr
;
}
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxSmallAllocClass
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
bool
BufferAllocator
:
:
allocNewSmallRegion
(
bool
inGC
)
{
void
*
ptr
=
allocMediumAligned
(
SmallRegionSize
inGC
)
;
if
(
!
ptr
)
{
return
false
;
}
auto
*
region
=
new
(
ptr
)
SmallBufferRegion
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
chunk
-
>
setSmallBufferRegion
(
region
true
)
;
uintptr_t
freeStart
=
uintptr_t
(
region
)
+
FirstSmallAllocOffset
;
uintptr_t
freeEnd
=
uintptr_t
(
region
)
+
SmallRegionSize
;
size_t
sizeClass
=
SizeClassForFreeRegion
(
freeEnd
-
freeStart
SizeKind
:
:
Small
)
;
ptr
=
reinterpret_cast
<
void
*
>
(
freeEnd
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
freeRegion
=
new
(
ptr
)
FreeRegion
(
freeStart
)
;
MOZ_ASSERT
(
freeRegion
-
>
getEnd
(
)
=
=
freeEnd
)
;
freeLists
.
ref
(
)
.
pushFront
(
sizeClass
freeRegion
)
;
return
true
;
}
/
*
static
*
/
bool
BufferAllocator
:
:
IsSmallAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
/
/
Test
for
large
buffers
before
calling
this
so
we
can
assume
|
alloc
|
is
/
/
inside
a
chunk
.
MOZ_ASSERT
(
!
IsLargeAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
chunk
-
>
isSmallBufferRegion
(
alloc
)
;
}
void
*
BufferAllocator
:
:
allocMedium
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
bytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
bytes
)
)
;
/
/
Round
up
to
next
allowed
size
.
bytes
=
RoundUp
(
bytes
MediumAllocGranularity
)
;
MOZ_ASSERT
(
bytes
<
=
MaxMediumAllocSize
)
;
/
/
Get
size
class
from
|
bytes
|
.
size_t
sizeClass
=
SizeClassForMediumAlloc
(
bytes
)
;
void
*
alloc
=
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retryMediumAlloc
(
bytes
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
setAllocated
(
alloc
bytes
nurseryOwned
inGC
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retryMediumAlloc
(
size_t
bytes
size_t
sizeClass
bool
inGC
)
{
if
(
hasMinorSweepDataToMerge
)
{
/
/
Avoid
taking
the
lock
unless
we
know
there
is
data
to
merge
.
This
reduces
/
/
context
switches
.
mergeSweptData
(
)
;
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
if
(
ptr
)
{
return
ptr
;
}
}
if
(
!
allocNewChunk
(
inGC
)
)
{
return
nullptr
;
}
void
*
ptr
=
bumpAlloc
(
bytes
sizeClass
MaxMediumAllocClass
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
/
/
Differentiate
between
small
and
medium
size
classes
.
Large
allocations
do
not
/
/
use
size
classes
.
static
bool
IsMediumSizeClass
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
BufferAllocator
:
:
AllocSizeClasses
)
;
return
sizeClass
>
=
MinMediumAllocClass
;
}
/
*
static
*
/
BufferAllocator
:
:
SizeKind
BufferAllocator
:
:
SizeClassKind
(
size_t
sizeClass
)
{
return
IsMediumSizeClass
(
sizeClass
)
?
SizeKind
:
:
Medium
:
SizeKind
:
:
Small
;
}
void
*
BufferAllocator
:
:
bumpAlloc
(
size_t
bytes
size_t
sizeClass
size_t
maxSizeClass
)
{
MOZ_ASSERT
(
SizeClassKind
(
sizeClass
)
=
=
SizeClassKind
(
maxSizeClass
)
)
;
freeLists
.
ref
(
)
.
checkAvailable
(
)
;
/
/
Find
smallest
suitable
size
class
that
has
free
regions
.
sizeClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
maxSizeClass
)
;
if
(
sizeClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
FreeRegion
*
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
sizeClass
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
>
=
bytes
)
;
void
*
ptr
=
allocFromRegion
(
region
bytes
sizeClass
)
;
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
sizeClass
)
;
return
ptr
;
}
#
ifdef
DEBUG
static
size_t
GranularityForSizeClass
(
size_t
sizeClass
)
{
return
IsMediumSizeClass
(
sizeClass
)
?
MediumAllocGranularity
:
SmallAllocGranularity
;
}
#
endif
/
/
DEBUG
void
*
BufferAllocator
:
:
allocFromRegion
(
FreeRegion
*
region
size_t
bytes
size_t
sizeClass
)
{
uintptr_t
start
=
region
-
>
startAddr
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
start
)
;
MOZ_ASSERT_IF
(
sizeClass
!
=
MaxMediumAllocClass
region
-
>
size
(
)
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT_IF
(
sizeClass
=
=
MaxMediumAllocClass
region
-
>
size
(
)
>
=
MaxMediumAllocSize
)
;
MOZ_ASSERT
(
start
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
/
/
Ensure
whole
region
is
commited
.
if
(
region
-
>
hasDecommittedPages
)
{
recommitRegion
(
region
)
;
}
/
/
Allocate
from
start
of
region
.
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
start
)
;
start
+
=
bytes
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
=
start
)
;
/
/
Update
region
start
.
region
-
>
startAddr
=
start
;
return
ptr
;
}
/
/
Allocate
a
region
of
size
|
bytes
|
aligned
to
|
bytes
|
.
The
maximum
size
is
/
/
limited
to
256KB
.
In
practice
this
is
only
ever
used
to
allocate
/
/
SmallBufferRegions
.
void
*
BufferAllocator
:
:
allocMediumAligned
(
size_t
bytes
bool
inGC
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxAlignedAllocSize
)
;
MOZ_ASSERT
(
mozilla
:
:
IsPowerOfTwo
(
bytes
)
)
;
/
/
Get
size
class
from
|
bytes
|
.
size_t
sizeClass
=
SizeClassForMediumAlloc
(
bytes
)
;
void
*
alloc
=
alignedAlloc
(
sizeClass
)
;
if
(
MOZ_UNLIKELY
(
!
alloc
)
)
{
alloc
=
retryAlignedAlloc
(
sizeClass
inGC
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
}
setAllocated
(
alloc
bytes
false
inGC
)
;
return
alloc
;
}
MOZ_NEVER_INLINE
void
*
BufferAllocator
:
:
retryAlignedAlloc
(
size_t
sizeClass
bool
inGC
)
{
if
(
hasMinorSweepDataToMerge
)
{
/
/
Avoid
taking
the
lock
unless
we
know
there
is
data
to
merge
.
This
reduces
/
/
context
switches
.
mergeSweptData
(
)
;
void
*
ptr
=
alignedAlloc
(
sizeClass
)
;
if
(
ptr
)
{
return
ptr
;
}
}
if
(
!
allocNewChunk
(
inGC
)
)
{
return
nullptr
;
}
void
*
ptr
=
alignedAlloc
(
sizeClass
)
;
MOZ_ASSERT
(
ptr
)
;
return
ptr
;
}
void
*
BufferAllocator
:
:
alignedAlloc
(
size_t
sizeClass
)
{
freeLists
.
ref
(
)
.
checkAvailable
(
)
;
/
/
Try
the
first
free
region
for
the
smallest
possible
size
class
.
This
will
/
/
fail
if
that
region
is
for
the
exact
size
class
requested
but
the
region
is
/
/
not
aligned
.
size_t
allocClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
MaxMediumAllocClass
)
;
MOZ_ASSERT
(
allocClass
>
=
sizeClass
)
;
if
(
allocClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
FreeRegion
*
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
allocClass
)
;
void
*
ptr
=
alignedAllocFromRegion
(
region
sizeClass
)
;
if
(
ptr
)
{
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
allocClass
)
;
return
ptr
;
}
/
/
If
we
couldn
'
t
allocate
an
aligned
region
try
a
larger
size
class
.
This
/
/
only
happens
if
we
selected
the
size
class
equal
to
the
requested
size
.
MOZ_ASSERT
(
allocClass
=
=
sizeClass
)
;
allocClass
=
freeLists
.
ref
(
)
.
getFirstAvailableSizeClass
(
sizeClass
+
1
MaxMediumAllocClass
)
;
if
(
allocClass
=
=
SIZE_MAX
)
{
return
nullptr
;
}
region
=
freeLists
.
ref
(
)
.
getFirstRegion
(
allocClass
)
;
ptr
=
alignedAllocFromRegion
(
region
sizeClass
)
;
MOZ_ASSERT
(
ptr
)
;
updateFreeListsAfterAlloc
(
&
freeLists
.
ref
(
)
region
allocClass
)
;
return
ptr
;
}
void
*
BufferAllocator
:
:
alignedAllocFromRegion
(
FreeRegion
*
region
size_t
sizeClass
)
{
/
/
Attempt
to
allocate
an
aligned
region
from
|
region
|
.
uintptr_t
start
=
region
-
>
startAddr
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
>
start
)
;
MOZ_ASSERT
(
region
-
>
size
(
)
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT
(
(
region
-
>
size
(
)
%
MinMediumAllocSize
)
=
=
0
)
;
size_t
bytes
=
SizeClassBytes
(
sizeClass
)
;
size_t
alignedStart
=
RoundUp
(
start
bytes
)
;
size_t
end
=
alignedStart
+
bytes
;
if
(
end
>
region
-
>
getEnd
(
)
)
{
return
nullptr
;
}
/
/
Align
the
start
of
the
region
creating
a
new
free
region
out
of
the
space
/
/
at
the
start
if
necessary
.
if
(
alignedStart
!
=
start
)
{
size_t
alignBytes
=
alignedStart
-
start
;
void
*
prefix
=
allocFromRegion
(
region
alignBytes
sizeClass
)
;
MOZ_ASSERT
(
uintptr_t
(
prefix
)
=
=
start
)
;
(
void
)
prefix
;
MOZ_ASSERT
(
!
region
-
>
hasDecommittedPages
)
;
addFreeRegion
(
&
freeLists
.
ref
(
)
start
alignBytes
SizeKind
:
:
Medium
false
ListPosition
:
:
Back
)
;
}
/
/
Now
the
start
is
aligned
we
can
use
the
normal
allocation
method
.
MOZ_ASSERT
(
region
-
>
startAddr
%
bytes
=
=
0
)
;
return
allocFromRegion
(
region
bytes
sizeClass
)
;
}
void
BufferAllocator
:
:
setAllocated
(
void
*
alloc
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
chunk
-
>
setAllocated
(
alloc
bytes
true
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
MOZ_ASSERT
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
;
chunk
-
>
setNurseryOwned
(
alloc
nurseryOwned
)
;
if
(
nurseryOwned
&
&
!
chunk
-
>
hasNurseryOwnedAllocs
)
{
tenuredChunks
.
ref
(
)
.
remove
(
chunk
)
;
chunk
-
>
hasNurseryOwnedAllocs
=
true
;
mixedChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
}
MOZ_ASSERT
(
!
chunk
-
>
isMarked
(
alloc
)
)
;
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
updateHeapSize
(
bytes
checkThresholds
false
)
;
}
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
}
void
BufferAllocator
:
:
updateFreeListsAfterAlloc
(
FreeLists
*
freeLists
FreeRegion
*
region
size_t
sizeClass
)
{
/
/
Updates
|
freeLists
|
after
an
allocation
from
|
region
|
which
is
currently
in
/
/
the
|
sizeClass
|
free
list
.
This
may
move
the
region
to
a
different
free
/
/
list
.
freeLists
-
>
assertContains
(
sizeClass
region
)
;
/
/
If
the
region
is
still
valid
for
further
allocations
of
this
size
class
/
/
then
there
'
s
nothing
to
do
.
size_t
classBytes
=
SizeClassBytes
(
sizeClass
)
;
size_t
newSize
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
newSize
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
if
(
newSize
>
=
classBytes
)
{
return
;
}
/
/
Remove
region
from
this
free
list
.
freeLists
-
>
remove
(
sizeClass
region
)
;
/
/
If
the
region
is
now
empty
then
we
'
re
done
.
if
(
newSize
=
=
0
)
{
return
;
}
/
/
Otherwise
region
is
now
too
small
.
Move
it
to
the
appropriate
bucket
for
/
/
its
reduced
size
if
possible
.
if
(
newSize
<
MinFreeRegionSize
)
{
/
/
We
can
'
t
record
a
region
this
small
.
The
free
space
will
not
be
reused
/
/
until
enough
adjacent
space
become
free
return
;
}
size_t
newSizeClass
=
SizeClassForFreeRegion
(
newSize
SizeClassKind
(
sizeClass
)
)
;
MOZ_ASSERT_IF
(
newSizeClass
!
=
MaxMediumAllocClass
newSize
>
=
SizeClassBytes
(
newSizeClass
)
)
;
MOZ_ASSERT
(
newSizeClass
<
=
sizeClass
)
;
MOZ_ASSERT_IF
(
newSizeClass
!
=
MaxMediumAllocClass
newSizeClass
<
sizeClass
)
;
MOZ_ASSERT
(
SizeClassKind
(
newSizeClass
)
=
=
SizeClassKind
(
sizeClass
)
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
void
BufferAllocator
:
:
recommitRegion
(
FreeRegion
*
region
)
{
MOZ_ASSERT
(
region
-
>
hasDecommittedPages
)
;
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
region
)
;
uintptr_t
startAddr
=
RoundUp
(
region
-
>
startAddr
PageSize
)
;
uintptr_t
endAddr
=
RoundDown
(
uintptr_t
(
region
)
PageSize
)
;
size_t
startPage
=
(
startAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
size_t
endPage
=
(
endAddr
-
uintptr_t
(
chunk
)
)
/
PageSize
;
/
/
If
the
start
of
the
region
does
not
lie
on
a
page
boundary
the
page
it
is
/
/
in
should
be
committed
as
it
must
either
contain
the
start
of
the
chunk
a
/
/
FreeRegion
or
an
allocation
.
MOZ_ASSERT_IF
(
(
region
-
>
startAddr
%
PageSize
)
!
=
0
!
chunk
-
>
decommittedPages
.
ref
(
)
[
startPage
-
1
]
)
;
/
/
The
end
of
the
region
should
be
committed
as
it
holds
FreeRegion
|
region
|
.
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
MarkPagesInUseSoft
(
reinterpret_cast
<
void
*
>
(
startAddr
)
endAddr
-
startAddr
)
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
false
;
}
region
-
>
hasDecommittedPages
=
false
;
}
static
inline
StallAndRetry
ShouldStallAndRetry
(
bool
inGC
)
{
return
inGC
?
StallAndRetry
:
:
Yes
:
StallAndRetry
:
:
No
;
}
bool
BufferAllocator
:
:
allocNewChunk
(
bool
inGC
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromMainThread
(
)
-
>
gc
;
AutoLockGCBgAlloc
lock
(
gc
)
;
ArenaChunk
*
baseChunk
=
gc
-
>
getOrAllocChunk
(
ShouldStallAndRetry
(
inGC
)
lock
)
;
if
(
!
baseChunk
)
{
return
false
;
}
CheckHighBitsOfPointer
(
baseChunk
)
;
/
/
Ensure
all
memory
is
initially
committed
.
if
(
!
baseChunk
-
>
decommittedPages
.
IsEmpty
(
)
)
{
MOZ_ASSERT
(
DecommitEnabled
(
)
)
;
MarkPagesInUseSoft
(
baseChunk
ChunkSize
)
;
}
/
/
Unpoison
past
the
ChunkBase
header
.
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
baseChunk
)
+
sizeof
(
ChunkBase
)
)
;
size_t
size
=
ChunkSize
-
sizeof
(
ChunkBase
)
;
SetMemCheckKind
(
ptr
size
MemCheckKind
:
:
MakeUndefined
)
;
BufferChunk
*
chunk
=
new
(
baseChunk
)
BufferChunk
(
zone
)
;
chunk
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
tenuredChunks
.
ref
(
)
.
pushBack
(
chunk
)
;
uintptr_t
freeStart
=
uintptr_t
(
chunk
)
+
FirstMediumAllocOffset
;
uintptr_t
freeEnd
=
uintptr_t
(
chunk
)
+
ChunkSize
;
size_t
sizeClass
=
SizeClassForFreeRegion
(
freeEnd
-
freeStart
SizeKind
:
:
Medium
)
;
MOZ_ASSERT
(
sizeClass
>
MaxSmallAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
=
MaxMediumAllocClass
)
;
ptr
=
reinterpret_cast
<
void
*
>
(
freeEnd
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
freeStart
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
freeEnd
)
;
freeLists
.
ref
(
)
.
pushFront
(
sizeClass
region
)
;
return
true
;
}
static
void
SetDeallocated
(
BufferChunk
*
chunk
void
*
alloc
size_t
bytes
)
{
MOZ_ASSERT
(
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
chunk
-
>
setNurseryOwned
(
alloc
false
)
;
chunk
-
>
setAllocated
(
alloc
bytes
false
)
;
}
static
void
SetDeallocated
(
SmallBufferRegion
*
region
void
*
alloc
size_t
bytes
)
{
MOZ_ASSERT
(
region
-
>
allocBytes
(
alloc
)
=
=
bytes
)
;
region
-
>
setNurseryOwned
(
alloc
false
)
;
region
-
>
setAllocated
(
alloc
bytes
false
)
;
}
bool
BufferAllocator
:
:
sweepChunk
(
BufferChunk
*
chunk
SweepKind
sweepKind
bool
shouldDecommit
FreeLists
&
freeLists
)
{
/
/
Find
all
regions
of
free
space
in
|
chunk
|
and
add
them
to
the
swept
free
/
/
lists
.
/
/
TODO
:
It
could
be
beneficialy
to
allocate
from
most
-
full
chunks
first
.
This
/
/
could
happen
by
sweeping
all
chunks
and
then
sorting
them
by
how
much
free
/
/
space
they
had
and
then
adding
their
free
regions
to
the
free
lists
in
that
/
/
order
.
if
(
sweepKind
=
=
SweepKind
:
:
RebuildFreeLists
)
{
chunk
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
bool
hasNurseryOwnedAllocs
=
false
;
size_t
freeStart
=
FirstMediumAllocOffset
;
bool
sweptAny
=
false
;
size_t
mallocHeapBytesFreed
=
0
;
/
/
First
sweep
any
small
buffer
regions
.
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
MOZ_ASSERT
(
!
chunk
-
>
isMarked
(
region
)
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
region
)
=
=
SmallRegionSize
)
;
if
(
!
sweepSmallBufferRegion
(
chunk
region
sweepKind
freeLists
)
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
SweepNursery
|
|
sweepKind
=
=
SweepKind
:
:
SweepTenured
)
;
chunk
-
>
setSmallBufferRegion
(
region
false
)
;
SetDeallocated
(
chunk
region
SmallRegionSize
)
;
PoisonAlloc
(
region
JS_SWEPT_TENURED_PATTERN
sizeof
(
SmallBufferRegion
)
MemCheckKind
:
:
MakeUndefined
)
;
mallocHeapBytesFreed
+
=
SmallRegionSize
;
sweptAny
=
true
;
}
else
if
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
hasNurseryOwnedAllocs
=
true
;
}
}
for
(
auto
iter
=
chunk
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
void
*
alloc
=
iter
.
get
(
)
;
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
uintptr_t
allocEnd
=
iter
.
getOffset
(
)
+
bytes
;
bool
nurseryOwned
=
chunk
-
>
isNurseryOwned
(
alloc
)
;
bool
canSweep
=
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
&
&
CanSweepAlloc
(
nurseryOwned
sweepKind
)
;
bool
shouldSweep
=
canSweep
&
&
!
chunk
-
>
isMarked
(
alloc
)
;
if
(
shouldSweep
)
{
/
/
Dead
.
Update
allocated
bitmap
metadata
and
heap
size
accounting
.
if
(
!
nurseryOwned
)
{
mallocHeapBytesFreed
+
=
bytes
;
}
SetDeallocated
(
chunk
alloc
bytes
)
;
PoisonAlloc
(
alloc
JS_SWEPT_TENURED_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
sweptAny
=
true
;
}
else
{
/
/
Alive
.
Add
any
free
space
before
this
allocation
.
uintptr_t
allocStart
=
iter
.
getOffset
(
)
;
if
(
freeStart
!
=
allocStart
)
{
addSweptRegion
(
chunk
freeStart
allocStart
shouldDecommit
!
sweptAny
freeLists
)
;
}
freeStart
=
allocEnd
;
if
(
canSweep
)
{
chunk
-
>
setUnmarked
(
alloc
)
;
}
if
(
nurseryOwned
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
SweepNursery
)
;
hasNurseryOwnedAllocs
=
true
;
}
}
}
if
(
mallocHeapBytesFreed
)
{
bool
inMajorGC
=
sweepKind
=
=
SweepKind
:
:
SweepTenured
;
zone
-
>
mallocHeapSize
.
removeBytes
(
mallocHeapBytesFreed
inMajorGC
)
;
}
if
(
freeStart
=
=
FirstMediumAllocOffset
&
&
sweepKind
!
=
SweepKind
:
:
RebuildFreeLists
)
{
/
/
Chunk
is
empty
.
Give
it
back
to
the
system
.
bool
allMemoryCommitted
=
chunk
-
>
decommittedPages
.
ref
(
)
.
IsEmpty
(
)
;
chunk
-
>
~
BufferChunk
(
)
;
ArenaChunk
*
tenuredChunk
=
ArenaChunk
:
:
init
(
chunk
gc
allMemoryCommitted
)
;
AutoLockGC
lock
(
gc
)
;
gc
-
>
recycleChunk
(
tenuredChunk
lock
)
;
return
false
;
}
/
/
Add
any
free
space
from
the
last
allocation
to
the
end
of
the
chunk
.
if
(
freeStart
!
=
ChunkSize
)
{
addSweptRegion
(
chunk
freeStart
ChunkSize
shouldDecommit
!
sweptAny
freeLists
)
;
}
chunk
-
>
hasNurseryOwnedAllocsAfterSweep
=
hasNurseryOwnedAllocs
;
return
true
;
}
/
*
static
*
/
bool
BufferAllocator
:
:
CanSweepAlloc
(
bool
nurseryOwned
BufferAllocator
:
:
SweepKind
sweepKind
)
{
static_assert
(
SweepKind
:
:
SweepNursery
=
=
SweepKind
(
uint8_t
(
true
)
)
)
;
static_assert
(
SweepKind
:
:
SweepTenured
=
=
SweepKind
(
uint8_t
(
false
)
)
)
;
SweepKind
requiredKind
=
SweepKind
(
uint8_t
(
nurseryOwned
)
)
;
return
sweepKind
=
=
requiredKind
;
}
void
BufferAllocator
:
:
addSweptRegion
(
BufferChunk
*
chunk
uintptr_t
freeStart
uintptr_t
freeEnd
bool
shouldDecommit
bool
expectUnchanged
FreeLists
&
freeLists
)
{
/
/
Add
the
region
from
|
freeStart
|
to
|
freeEnd
|
to
the
appropriate
swept
free
/
/
list
based
on
its
size
.
MOZ_ASSERT
(
freeStart
>
=
FirstMediumAllocOffset
)
;
MOZ_ASSERT
(
freeStart
<
freeEnd
)
;
MOZ_ASSERT
(
freeEnd
<
=
ChunkSize
)
;
MOZ_ASSERT
(
(
freeStart
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
(
freeEnd
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT_IF
(
shouldDecommit
DecommitEnabled
(
)
)
;
/
/
Decommit
pages
if
|
shouldDecommit
|
was
specified
but
leave
space
for
/
/
the
FreeRegion
structure
at
the
end
.
bool
anyDecommitted
=
false
;
uintptr_t
decommitStart
=
RoundUp
(
freeStart
PageSize
)
;
uintptr_t
decommitEnd
=
RoundDown
(
freeEnd
-
sizeof
(
FreeRegion
)
PageSize
)
;
size_t
endPage
=
decommitEnd
/
PageSize
;
if
(
shouldDecommit
&
&
decommitEnd
>
decommitStart
)
{
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
decommitStart
+
uintptr_t
(
chunk
)
)
;
MarkPagesUnusedSoft
(
ptr
decommitEnd
-
decommitStart
)
;
size_t
startPage
=
decommitStart
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
=
true
;
}
anyDecommitted
=
true
;
}
else
{
/
/
Check
for
any
previously
decommitted
pages
.
uintptr_t
startPage
=
RoundDown
(
freeStart
PageSize
)
/
PageSize
;
for
(
size_t
i
=
startPage
;
i
!
=
endPage
;
i
+
+
)
{
if
(
chunk
-
>
decommittedPages
.
ref
(
)
[
i
]
)
{
anyDecommitted
=
true
;
}
}
}
/
/
The
last
page
must
have
previously
been
either
a
live
allocation
or
a
/
/
FreeRegion
so
it
must
already
be
committed
.
MOZ_ASSERT
(
!
chunk
-
>
decommittedPages
.
ref
(
)
[
endPage
]
)
;
freeStart
+
=
uintptr_t
(
chunk
)
;
freeEnd
+
=
uintptr_t
(
chunk
)
;
size_t
bytes
=
freeEnd
-
freeStart
;
addFreeRegion
(
&
freeLists
freeStart
bytes
SizeKind
:
:
Medium
anyDecommitted
ListPosition
:
:
Back
expectUnchanged
)
;
}
bool
BufferAllocator
:
:
sweepSmallBufferRegion
(
BufferChunk
*
chunk
SmallBufferRegion
*
region
SweepKind
sweepKind
FreeLists
&
freeLists
)
{
bool
hasNurseryOwnedAllocs
=
false
;
if
(
sweepKind
=
=
SweepKind
:
:
RebuildFreeLists
)
{
region
-
>
markBits
.
ref
(
)
.
clear
(
)
;
}
size_t
freeStart
=
FirstSmallAllocOffset
;
bool
sweptAny
=
false
;
for
(
auto
iter
=
region
-
>
allocIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
void
*
alloc
=
iter
.
get
(
)
;
size_t
bytes
=
region
-
>
allocBytes
(
alloc
)
;
uintptr_t
allocEnd
=
iter
.
getOffset
(
)
+
bytes
;
bool
nurseryOwned
=
region
-
>
isNurseryOwned
(
alloc
)
;
bool
canSweep
=
CanSweepAlloc
(
nurseryOwned
sweepKind
)
;
bool
shouldSweep
=
canSweep
&
&
!
region
-
>
isMarked
(
alloc
)
;
if
(
shouldSweep
)
{
/
/
Dead
.
Update
allocated
bitmap
metadata
and
heap
size
accounting
.
SetDeallocated
(
region
alloc
bytes
)
;
PoisonAlloc
(
alloc
JS_SWEPT_TENURED_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
sweptAny
=
true
;
}
else
{
/
/
Alive
.
Add
any
free
space
before
this
allocation
.
uintptr_t
allocStart
=
iter
.
getOffset
(
)
;
if
(
freeStart
!
=
allocStart
)
{
addSweptRegion
(
region
freeStart
allocStart
!
sweptAny
freeLists
)
;
}
freeStart
=
allocEnd
;
if
(
canSweep
)
{
region
-
>
setUnmarked
(
alloc
)
;
}
if
(
nurseryOwned
)
{
MOZ_ASSERT
(
sweepKind
=
=
SweepKind
:
:
SweepNursery
)
;
hasNurseryOwnedAllocs
=
true
;
}
sweptAny
=
false
;
}
}
if
(
freeStart
=
=
FirstSmallAllocOffset
&
&
sweepKind
!
=
SweepKind
:
:
RebuildFreeLists
)
{
/
/
Region
is
empty
.
return
false
;
}
/
/
Add
any
free
space
from
the
last
allocation
to
the
end
of
the
chunk
.
if
(
freeStart
!
=
SmallRegionSize
)
{
addSweptRegion
(
region
freeStart
SmallRegionSize
!
sweptAny
freeLists
)
;
}
region
-
>
setHasNurseryOwnedAllocs
(
hasNurseryOwnedAllocs
)
;
return
true
;
}
void
BufferAllocator
:
:
addSweptRegion
(
SmallBufferRegion
*
region
uintptr_t
freeStart
uintptr_t
freeEnd
bool
expectUnchanged
FreeLists
&
freeLists
)
{
/
/
Add
the
region
from
|
freeStart
|
to
|
freeEnd
|
to
the
appropriate
swept
free
/
/
list
based
on
its
size
.
Unused
pages
in
small
buffer
regions
are
not
/
/
decommitted
.
MOZ_ASSERT
(
freeStart
>
=
FirstSmallAllocOffset
)
;
MOZ_ASSERT
(
freeStart
<
freeEnd
)
;
MOZ_ASSERT
(
freeEnd
<
=
SmallRegionSize
)
;
MOZ_ASSERT
(
freeStart
%
SmallAllocGranularity
=
=
0
)
;
MOZ_ASSERT
(
freeEnd
%
SmallAllocGranularity
=
=
0
)
;
freeStart
+
=
uintptr_t
(
region
)
;
freeEnd
+
=
uintptr_t
(
region
)
;
size_t
bytes
=
freeEnd
-
freeStart
;
addFreeRegion
(
&
freeLists
freeStart
bytes
SizeKind
:
:
Small
false
ListPosition
:
:
Back
expectUnchanged
)
;
}
void
BufferAllocator
:
:
freeMedium
(
void
*
alloc
)
{
/
/
Free
a
medium
sized
allocation
.
This
coalesces
the
free
space
with
any
/
/
neighboring
free
regions
.
Coalescing
is
necessary
for
resize
to
work
/
/
properly
.
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
if
(
isSweepingChunkAfterMerge
(
chunk
)
)
{
return
;
/
/
We
can
'
t
free
if
the
chunk
is
currently
being
swept
.
}
size_t
bytes
=
chunk
-
>
allocBytes
(
alloc
)
;
PoisonAlloc
(
alloc
JS_FREED_BUFFER_PATTERN
bytes
MemCheckKind
:
:
MakeUndefined
)
;
/
/
Update
heap
size
for
tenured
owned
allocations
.
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
zone
-
>
mallocHeapSize
.
removeBytes
(
bytes
updateRetained
)
;
}
/
/
TODO
:
Since
the
mark
bits
are
atomic
it
'
s
probably
OK
to
unmark
even
if
/
/
the
chunk
is
currently
being
swept
.
If
we
get
lucky
the
memory
will
be
/
/
freed
sooner
.
chunk
-
>
setUnmarked
(
alloc
)
;
/
/
Set
region
as
not
allocated
and
clear
metadata
.
SetDeallocated
(
chunk
alloc
bytes
)
;
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
uintptr_t
startAddr
=
uintptr_t
(
alloc
)
;
uintptr_t
endAddr
=
startAddr
+
bytes
;
/
/
First
check
whether
there
is
a
free
region
following
the
allocation
.
FreeRegion
*
region
;
uintptr_t
endOffset
=
endAddr
&
ChunkMask
;
if
(
endOffset
=
=
0
|
|
chunk
-
>
isAllocated
(
endOffset
)
)
{
/
/
The
allocation
abuts
the
end
of
the
chunk
or
another
allocation
.
Add
the
/
/
allocation
as
a
new
free
region
.
/
/
/
/
The
new
region
is
added
to
the
front
of
relevant
list
so
as
to
reuse
/
/
recently
freed
memory
preferentially
.
This
may
reduce
fragmentation
.
See
/
/
"
The
Memory
Fragmentation
Problem
:
Solved
?
"
by
Johnstone
et
al
.
region
=
addFreeRegion
(
freeLists
startAddr
bytes
SizeKind
:
:
Medium
false
ListPosition
:
:
Front
)
;
MOZ_ASSERT
(
region
)
;
/
/
Always
succeeds
for
medium
allocations
.
}
else
{
/
/
There
is
a
free
region
following
this
allocation
.
Expand
the
existing
/
/
region
down
to
cover
the
newly
freed
space
.
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
updateFreeRegionStart
(
freeLists
region
startAddr
SizeKind
:
:
Medium
)
;
}
/
/
Next
check
for
any
preceding
free
region
and
coalesce
.
FreeRegion
*
precRegion
=
chunk
-
>
findPrecedingFreeRegion
(
startAddr
)
;
if
(
precRegion
)
{
if
(
freeLists
)
{
size_t
sizeClass
=
SizeClassForFreeRegion
(
precRegion
-
>
size
(
)
SizeKind
:
:
Medium
)
;
freeLists
-
>
remove
(
sizeClass
precRegion
)
;
}
updateFreeRegionStart
(
freeLists
region
precRegion
-
>
startAddr
SizeKind
:
:
Medium
)
;
if
(
precRegion
-
>
hasDecommittedPages
)
{
region
-
>
hasDecommittedPages
=
true
;
}
}
}
bool
BufferAllocator
:
:
isSweepingChunkAfterMerge
(
BufferChunk
*
chunk
)
{
if
(
chunk
-
>
hasNurseryOwnedAllocs
&
&
hasMinorSweepDataToMerge
)
{
/
/
Only
merge
if
this
chunk
has
nursery
allocations
and
we
know
there
is
/
/
data
to
merge
.
mergeSweptData
(
)
;
}
return
isSweepingChunk
(
chunk
)
;
}
bool
BufferAllocator
:
:
isSweepingChunk
(
BufferChunk
*
chunk
)
const
{
return
(
minorState
=
=
State
:
:
Sweeping
&
&
chunk
-
>
hasNurseryOwnedAllocs
)
|
|
(
majorState
=
=
State
:
:
Sweeping
&
&
!
chunk
-
>
allocatedDuringCollection
)
;
}
BufferAllocator
:
:
FreeRegion
*
BufferAllocator
:
:
addFreeRegion
(
FreeLists
*
freeLists
uintptr_t
start
size_t
bytes
SizeKind
kind
bool
anyDecommitted
ListPosition
position
bool
expectUnchanged
/
*
=
false
*
/
)
{
static_assert
(
sizeof
(
FreeRegion
)
<
=
MinFreeRegionSize
)
;
if
(
bytes
<
MinFreeRegionSize
)
{
/
/
We
can
'
t
record
a
region
this
small
.
The
free
space
will
not
be
reused
/
/
until
enough
adjacent
space
become
free
.
return
nullptr
;
}
size_t
sizeClass
=
SizeClassForFreeRegion
(
bytes
kind
)
;
MOZ_ASSERT_IF
(
sizeClass
!
=
MaxMediumAllocClass
bytes
>
=
SizeClassBytes
(
sizeClass
)
)
;
MOZ_ASSERT
(
start
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
MOZ_ASSERT
(
bytes
%
GranularityForSizeClass
(
sizeClass
)
=
=
0
)
;
uintptr_t
end
=
start
+
bytes
;
#
ifdef
DEBUG
if
(
expectUnchanged
)
{
/
/
We
didn
'
t
free
any
allocations
so
there
should
already
be
a
FreeRegion
/
/
from
|
start
|
to
|
end
|
.
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
end
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
start
)
;
}
#
endif
void
*
ptr
=
reinterpret_cast
<
void
*
>
(
end
-
sizeof
(
FreeRegion
)
)
;
FreeRegion
*
region
=
new
(
ptr
)
FreeRegion
(
start
anyDecommitted
)
;
MOZ_ASSERT
(
region
-
>
getEnd
(
)
=
=
end
)
;
if
(
freeLists
)
{
if
(
position
=
=
ListPosition
:
:
Front
)
{
freeLists
-
>
pushFront
(
sizeClass
region
)
;
}
else
{
freeLists
-
>
pushBack
(
sizeClass
region
)
;
}
}
return
region
;
}
void
BufferAllocator
:
:
updateFreeRegionStart
(
FreeLists
*
freeLists
FreeRegion
*
region
uintptr_t
newStart
SizeKind
kind
)
{
MOZ_ASSERT
(
(
newStart
&
~
ChunkMask
)
=
=
(
uintptr_t
(
region
)
&
~
ChunkMask
)
)
;
MOZ_ASSERT
(
region
-
>
startAddr
!
=
newStart
)
;
/
/
TODO
:
Support
realloc
for
small
regions
.
MOZ_ASSERT
(
kind
=
=
SizeKind
:
:
Medium
)
;
size_t
oldSize
=
region
-
>
size
(
)
;
region
-
>
startAddr
=
newStart
;
if
(
!
freeLists
)
{
return
;
}
size_t
currentSizeClass
=
SizeClassForFreeRegion
(
oldSize
kind
)
;
size_t
newSizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
kind
)
;
MOZ_ASSERT
(
SizeClassKind
(
newSizeClass
)
=
=
SizeClassKind
(
currentSizeClass
)
)
;
if
(
currentSizeClass
!
=
newSizeClass
)
{
freeLists
-
>
remove
(
currentSizeClass
region
)
;
freeLists
-
>
pushFront
(
newSizeClass
region
)
;
}
}
bool
BufferAllocator
:
:
growMedium
(
void
*
alloc
size_t
oldBytes
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
newBytes
>
oldBytes
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
!
isSweepingChunk
(
chunk
)
)
;
MOZ_ASSERT
(
oldBytes
=
=
chunk
-
>
allocBytes
(
alloc
)
)
;
uintptr_t
endOffset
=
(
uintptr_t
(
alloc
)
&
ChunkMask
)
+
oldBytes
;
MOZ_ASSERT
(
endOffset
<
=
ChunkSize
)
;
if
(
endOffset
=
=
ChunkSize
)
{
return
false
;
/
/
Can
'
t
extend
because
we
'
re
at
the
end
of
the
chunk
.
}
size_t
endAddr
=
uintptr_t
(
chunk
)
+
endOffset
;
if
(
chunk
-
>
isAllocated
(
endOffset
)
)
{
return
false
;
/
/
Can
'
t
extend
because
we
abut
another
allocation
.
}
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
endAddr
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
endAddr
)
;
size_t
extraBytes
=
newBytes
-
oldBytes
;
if
(
region
-
>
size
(
)
<
extraBytes
)
{
return
false
;
/
/
Can
'
t
extend
because
following
free
region
is
too
small
.
}
size_t
sizeClass
=
SizeClassForFreeRegion
(
region
-
>
size
(
)
SizeKind
:
:
Medium
)
;
allocFromRegion
(
region
extraBytes
sizeClass
)
;
/
/
If
the
allocation
is
in
a
chunk
where
we
'
ve
cleared
the
free
lists
before
/
/
sweeping
we
don
'
t
need
to
update
the
free
lists
.
if
(
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
)
{
updateFreeListsAfterAlloc
(
freeLists
region
sizeClass
)
;
}
chunk
-
>
updateEndOffset
(
alloc
oldBytes
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
updateHeapSize
(
extraBytes
true
updateRetained
)
;
}
return
true
;
}
bool
BufferAllocator
:
:
shrinkMedium
(
void
*
alloc
size_t
oldBytes
size_t
newBytes
)
{
MOZ_ASSERT
(
!
IsSmallAllocSize
(
newBytes
)
)
;
MOZ_ASSERT
(
!
IsLargeAllocSize
(
newBytes
)
)
;
newBytes
=
std
:
:
max
(
newBytes
MinMediumAllocSize
)
;
MOZ_ASSERT
(
newBytes
=
=
GetGoodAllocSize
(
newBytes
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
MOZ_ASSERT
(
chunk
-
>
zone
=
=
zone
)
;
MOZ_ASSERT
(
!
isSweepingChunk
(
chunk
)
)
;
MOZ_ASSERT
(
oldBytes
=
=
chunk
-
>
allocBytes
(
alloc
)
)
;
if
(
newBytes
=
=
oldBytes
)
{
/
/
Requested
size
is
the
same
after
adjusting
to
a
valid
medium
alloc
size
.
return
false
;
}
MOZ_ASSERT
(
newBytes
<
oldBytes
)
;
size_t
sizeChange
=
oldBytes
-
newBytes
;
/
/
Update
allocation
size
.
chunk
-
>
updateEndOffset
(
alloc
oldBytes
newBytes
)
;
MOZ_ASSERT
(
chunk
-
>
allocBytes
(
alloc
)
=
=
newBytes
)
;
if
(
!
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bool
updateRetained
=
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
;
zone
-
>
mallocHeapSize
.
removeBytes
(
sizeChange
updateRetained
)
;
}
uintptr_t
startOffset
=
uintptr_t
(
alloc
)
&
ChunkMask
;
uintptr_t
oldEndOffset
=
startOffset
+
oldBytes
;
uintptr_t
newEndOffset
=
startOffset
+
newBytes
;
MOZ_ASSERT
(
oldEndOffset
<
=
ChunkSize
)
;
/
/
Poison
freed
memory
.
uintptr_t
chunkAddr
=
uintptr_t
(
chunk
)
;
PoisonAlloc
(
reinterpret_cast
<
void
*
>
(
chunkAddr
+
newEndOffset
)
JS_SWEPT_TENURED_PATTERN
sizeChange
MemCheckKind
:
:
MakeUndefined
)
;
FreeLists
*
freeLists
=
getChunkFreeLists
(
chunk
)
;
/
/
If
we
abut
another
allocation
then
add
a
new
free
region
.
if
(
oldEndOffset
=
=
ChunkSize
|
|
chunk
-
>
isAllocated
(
oldEndOffset
)
)
{
uintptr_t
freeStart
=
chunkAddr
+
newEndOffset
;
addFreeRegion
(
freeLists
freeStart
sizeChange
SizeKind
:
:
Medium
false
ListPosition
:
:
Front
)
;
return
true
;
}
/
/
Otherwise
find
the
following
free
region
and
extend
it
down
.
FreeRegion
*
region
=
chunk
-
>
findFollowingFreeRegion
(
chunkAddr
+
oldEndOffset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
chunkAddr
+
oldEndOffset
)
;
updateFreeRegionStart
(
freeLists
region
chunkAddr
+
newEndOffset
SizeKind
:
:
Medium
)
;
return
true
;
}
BufferAllocator
:
:
FreeLists
*
BufferAllocator
:
:
getChunkFreeLists
(
BufferChunk
*
chunk
)
{
MOZ_ASSERT_IF
(
majorState
=
=
State
:
:
Sweeping
chunk
-
>
allocatedDuringCollection
)
;
if
(
majorState
=
=
State
:
:
Marking
&
&
!
chunk
-
>
allocatedDuringCollection
)
{
/
/
The
chunk
has
been
queued
for
sweeping
and
the
free
lists
cleared
.
return
nullptr
;
}
return
&
freeLists
.
ref
(
)
;
}
BufferAllocator
:
:
FreeRegion
*
BufferChunk
:
:
findFollowingFreeRegion
(
uintptr_t
startAddr
)
{
/
/
Find
the
free
region
that
starts
at
|
startAddr
|
which
is
not
allocated
and
/
/
not
at
the
end
of
the
chunk
.
Always
returns
a
region
.
uintptr_t
offset
=
uintptr_t
(
startAddr
)
&
ChunkMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
MediumAllocGranularity
)
=
=
0
)
;
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
/
/
Already
marked
as
not
allocated
.
offset
=
findNextAllocated
(
offset
)
;
MOZ_ASSERT
(
offset
<
=
ChunkSize
)
;
auto
*
region
=
FreeRegion
:
:
fromEndOffset
(
this
offset
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
startAddr
)
;
return
region
;
}
BufferAllocator
:
:
FreeRegion
*
BufferChunk
:
:
findPrecedingFreeRegion
(
uintptr_t
endAddr
)
{
/
/
Find
the
free
region
if
any
that
ends
at
|
endAddr
|
which
may
be
/
/
allocated
or
at
the
start
of
the
chunk
.
uintptr_t
offset
=
uintptr_t
(
endAddr
)
&
ChunkMask
;
MOZ_ASSERT
(
isValidOffset
(
offset
)
)
;
MOZ_ASSERT
(
(
offset
%
MediumAllocGranularity
)
=
=
0
)
;
if
(
offset
=
=
FirstMediumAllocOffset
)
{
return
nullptr
;
/
/
Already
at
start
of
chunk
.
}
MOZ_ASSERT
(
!
isAllocated
(
offset
)
)
;
offset
=
findPrevAllocated
(
offset
)
;
if
(
offset
!
=
ChunkSize
)
{
/
/
Found
a
preceding
allocation
.
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
uintptr_t
(
alloc
)
+
bytes
<
=
endAddr
)
;
if
(
uintptr_t
(
alloc
)
+
bytes
=
=
endAddr
)
{
/
/
No
free
space
between
preceding
allocation
and
|
endAddr
|
.
return
nullptr
;
}
}
auto
*
region
=
FreeRegion
:
:
fromEndAddr
(
endAddr
)
;
#
ifdef
DEBUG
region
-
>
check
(
)
;
if
(
offset
!
=
ChunkSize
)
{
const
void
*
alloc
=
ptrFromOffset
(
offset
)
;
size_t
bytes
=
allocBytes
(
alloc
)
;
MOZ_ASSERT
(
region
-
>
startAddr
=
=
uintptr_t
(
alloc
)
+
bytes
)
;
}
else
{
MOZ_ASSERT
(
region
-
>
startAddr
=
=
uintptr_t
(
this
)
+
FirstMediumAllocOffset
)
;
}
#
endif
return
region
;
}
/
*
static
*
/
size_t
BufferAllocator
:
:
SizeClassForSmallAlloc
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinSmallAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxSmallAllocSize
)
;
size_t
log2Size
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
>
=
bytes
)
;
MOZ_ASSERT
(
MinSizeClassShift
=
=
mozilla
:
:
CeilingLog2
(
MinFreeRegionSize
)
)
;
if
(
log2Size
<
MinSizeClassShift
)
{
return
0
;
}
size_t
sizeClass
=
log2Size
-
MinSizeClassShift
;
MOZ_ASSERT
(
sizeClass
<
=
MaxSmallAllocClass
)
;
return
sizeClass
;
}
/
*
static
*
/
size_t
BufferAllocator
:
:
SizeClassForMediumAlloc
(
size_t
bytes
)
{
MOZ_ASSERT
(
bytes
>
=
MinMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
<
=
MaxMediumAllocSize
)
;
size_t
log2Size
=
mozilla
:
:
CeilingLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
>
=
bytes
)
;
MOZ_ASSERT
(
log2Size
>
=
MinMediumAllocShift
)
;
size_t
sizeClass
=
log2Size
-
MinMediumAllocShift
+
MinMediumAllocClass
;
MOZ_ASSERT
(
sizeClass
>
=
MinMediumAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
return
sizeClass
;
}
/
*
static
*
/
size_t
BufferAllocator
:
:
SizeClassForFreeRegion
(
size_t
bytes
SizeKind
kind
)
{
MOZ_ASSERT
(
bytes
>
=
MinFreeRegionSize
)
;
MOZ_ASSERT
(
bytes
<
ChunkSize
)
;
if
(
kind
=
=
SizeKind
:
:
Medium
&
&
bytes
>
=
MaxMediumAllocSize
)
{
/
/
Free
regions
large
enough
for
MaxMediumAllocSize
don
'
t
have
to
have
/
/
enough
space
for
that
size
rounded
up
to
the
next
power
of
two
as
is
the
/
/
case
for
smaller
regions
.
return
MaxMediumAllocClass
;
}
size_t
log2Size
=
mozilla
:
:
FloorLog2
(
bytes
)
;
MOZ_ASSERT
(
(
size_t
(
1
)
<
<
log2Size
)
<
=
bytes
)
;
MOZ_ASSERT
(
log2Size
>
=
MinSizeClassShift
)
;
size_t
sizeClass
=
std
:
:
min
(
log2Size
-
MinSizeClassShift
AllocSizeClasses
-
1
)
;
if
(
kind
=
=
SizeKind
:
:
Small
)
{
return
std
:
:
min
(
sizeClass
MaxSmallAllocClass
)
;
}
sizeClass
+
+
;
/
/
Medium
size
classes
start
after
small
ones
.
MOZ_ASSERT
(
sizeClass
>
=
MinMediumAllocClass
)
;
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
return
sizeClass
;
}
/
*
static
*
/
inline
size_t
BufferAllocator
:
:
SizeClassBytes
(
size_t
sizeClass
)
{
MOZ_ASSERT
(
sizeClass
<
AllocSizeClasses
)
;
/
/
The
first
medium
size
class
is
the
same
size
as
the
last
small
size
class
.
if
(
sizeClass
>
=
MinMediumAllocClass
)
{
sizeClass
-
-
;
}
return
1
<
<
(
sizeClass
+
MinSizeClassShift
)
;
}
/
*
static
*
/
bool
BufferAllocator
:
:
IsMediumAlloc
(
void
*
alloc
)
{
MOZ_ASSERT
(
IsBufferAlloc
(
alloc
)
)
;
/
/
Test
for
large
buffers
before
calling
this
so
we
can
assume
|
alloc
|
is
/
/
inside
a
chunk
.
MOZ_ASSERT
(
!
IsLargeAlloc
(
alloc
)
)
;
BufferChunk
*
chunk
=
BufferChunk
:
:
from
(
alloc
)
;
return
!
chunk
-
>
isSmallBufferRegion
(
alloc
)
;
}
bool
BufferAllocator
:
:
needLockToAccessBufferMap
(
)
const
{
MOZ_ASSERT
(
CurrentThreadCanAccessZone
(
zone
)
|
|
CurrentThreadIsPerformingGC
(
)
)
;
return
minorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
|
|
majorState
.
refNoCheck
(
)
=
=
State
:
:
Sweeping
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
)
{
MaybeLock
lock
;
return
lookupLargeBuffer
(
alloc
lock
)
;
}
LargeBuffer
*
BufferAllocator
:
:
lookupLargeBuffer
(
void
*
alloc
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
lock
.
isNothing
(
)
)
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
auto
ptr
=
largeAllocMap
.
ref
(
)
.
readonlyThreadsafeLookup
(
alloc
)
;
MOZ_ASSERT
(
ptr
)
;
LargeBuffer
*
buffer
=
ptr
-
>
value
(
)
;
MOZ_ASSERT
(
buffer
-
>
data
(
)
=
=
alloc
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
return
buffer
;
}
void
*
BufferAllocator
:
:
allocLarge
(
size_t
bytes
bool
nurseryOwned
bool
inGC
)
{
bytes
=
RoundUp
(
bytes
ChunkSize
)
;
MOZ_ASSERT
(
bytes
>
MaxMediumAllocSize
)
;
MOZ_ASSERT
(
bytes
>
=
bytes
)
;
/
/
Allocate
a
small
buffer
the
size
of
a
LargeBuffer
to
hold
the
metadata
.
static_assert
(
sizeof
(
LargeBuffer
)
<
=
MaxSmallAllocSize
)
;
void
*
bufferPtr
=
allocSmall
(
sizeof
(
LargeBuffer
)
nurseryOwned
inGC
)
;
if
(
!
bufferPtr
)
{
return
nullptr
;
}
/
/
Large
allocations
are
aligned
to
the
chunk
size
even
if
they
are
smaller
/
/
than
a
chunk
.
This
allows
us
to
tell
large
buffer
allocations
apart
by
/
/
looking
at
the
pointer
alignment
.
void
*
alloc
=
MapAlignedPages
(
bytes
ChunkSize
ShouldStallAndRetry
(
inGC
)
)
;
if
(
!
alloc
)
{
return
nullptr
;
}
auto
freeGuard
=
mozilla
:
:
MakeScopeExit
(
[
&
]
(
)
{
UnmapPages
(
alloc
bytes
)
;
}
)
;
CheckHighBitsOfPointer
(
alloc
)
;
auto
*
buffer
=
new
(
bufferPtr
)
LargeBuffer
(
alloc
bytes
nurseryOwned
)
;
{
MaybeLock
lock
;
if
(
needLockToAccessBufferMap
(
)
)
{
lock
.
emplace
(
this
)
;
}
if
(
!
largeAllocMap
.
ref
(
)
.
putNew
(
alloc
buffer
)
)
{
return
nullptr
;
}
}
freeGuard
.
release
(
)
;
if
(
nurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
else
{
buffer
-
>
allocatedDuringCollection
=
majorState
!
=
State
:
:
NotCollecting
;
largeTenuredAllocs
.
ref
(
)
.
pushBack
(
buffer
)
;
}
/
/
Update
memory
accounting
and
trigger
an
incremental
slice
if
needed
.
if
(
!
nurseryOwned
)
{
bool
checkThresholds
=
!
inGC
;
updateHeapSize
(
bytes
checkThresholds
false
)
;
}
MOZ_ASSERT
(
IsLargeAlloc
(
alloc
)
)
;
return
alloc
;
}
void
BufferAllocator
:
:
updateHeapSize
(
size_t
bytes
bool
checkThresholds
bool
updateRetainedSize
)
{
/
/
Update
memory
accounting
and
trigger
an
incremental
slice
if
needed
.
/
/
TODO
:
This
will
eventually
be
attributed
to
gcHeapSize
.
zone
-
>
mallocHeapSize
.
addBytes
(
bytes
updateRetainedSize
)
;
if
(
checkThresholds
)
{
GCRuntime
*
gc
=
&
zone
-
>
runtimeFromAnyThread
(
)
-
>
gc
;
gc
-
>
maybeTriggerGCAfterMalloc
(
zone
)
;
}
}
/
*
static
*
/
bool
BufferAllocator
:
:
IsLargeAlloc
(
void
*
alloc
)
{
return
(
uintptr_t
(
alloc
)
&
ChunkMask
)
=
=
0
;
}
bool
BufferAllocator
:
:
markLargeTenuredBuffer
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
if
(
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
}
/
/
Bug
1961755
:
This
method
can
return
false
positives
.
A
fully
atomic
version
/
/
would
be
preferable
in
this
case
.
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
return
region
-
>
setMarked
(
buffer
)
;
}
bool
BufferAllocator
:
:
isLargeTenuredMarked
(
LargeBuffer
*
buffer
)
{
MOZ_ASSERT
(
!
buffer
-
>
isNurseryOwned
)
;
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
auto
*
region
=
SmallBufferRegion
:
:
from
(
buffer
)
;
return
region
-
>
isMarked
(
buffer
)
;
}
void
BufferAllocator
:
:
freeLarge
(
void
*
alloc
)
{
MaybeLock
lock
;
LargeBuffer
*
buffer
=
lookupLargeBuffer
(
alloc
lock
)
;
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
DebugOnlyPoison
(
alloc
JS_FREED_BUFFER_PATTERN
buffer
-
>
allocBytes
(
)
MemCheckKind
:
:
MakeUndefined
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
;
/
/
Large
allocations
are
currently
being
swept
.
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
if
(
buffer
-
>
isNurseryOwned
)
{
largeNurseryAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
else
if
(
majorState
=
=
State
:
:
Marking
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
largeTenuredAllocsToSweep
.
ref
(
)
.
remove
(
buffer
)
;
}
else
{
largeTenuredAllocs
.
ref
(
)
.
remove
(
buffer
)
;
}
unmapLarge
(
buffer
false
lock
)
;
}
bool
BufferAllocator
:
:
shrinkLarge
(
LargeBuffer
*
buffer
size_t
newBytes
)
{
MOZ_ASSERT
(
IsLargeAllocSize
(
newBytes
)
)
;
#
ifdef
XP_WIN
/
/
Can
'
t
unmap
part
of
a
region
mapped
with
VirtualAlloc
on
Windows
.
/
/
/
/
It
is
possible
to
decommit
the
physical
pages
so
we
could
do
that
and
/
/
track
virtual
size
as
well
as
committed
size
.
This
would
also
allow
us
to
/
/
grow
the
allocation
again
if
necessary
.
return
false
;
#
else
MOZ_ASSERT
(
buffer
-
>
zone
(
)
=
=
zone
)
;
if
(
!
buffer
-
>
isNurseryOwned
&
&
majorState
=
=
State
:
:
Sweeping
&
&
!
buffer
-
>
allocatedDuringCollection
)
{
return
false
;
/
/
Large
allocations
are
currently
being
swept
.
}
MOZ_ASSERT
(
buffer
-
>
isInList
(
)
)
;
newBytes
=
RoundUp
(
newBytes
ChunkSize
)
;
size_t
oldBytes
=
buffer
-
>
bytes
;
MOZ_ASSERT
(
oldBytes
>
newBytes
)
;
size_t
shrinkBytes
=
oldBytes
-
newBytes
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
shrinkBytes
false
)
;
}
buffer
-
>
bytes
=
newBytes
;
void
*
endPtr
=
reinterpret_cast
<
void
*
>
(
uintptr_t
(
buffer
-
>
data
(
)
)
+
newBytes
)
;
UnmapPages
(
endPtr
shrinkBytes
)
;
return
true
;
#
endif
}
void
BufferAllocator
:
:
unmapLarge
(
LargeBuffer
*
buffer
bool
isSweeping
MaybeLock
&
lock
)
{
unregisterLarge
(
buffer
isSweeping
lock
)
;
UnmapPages
(
buffer
-
>
data
(
)
buffer
-
>
bytes
)
;
}
void
BufferAllocator
:
:
unregisterLarge
(
LargeBuffer
*
buffer
bool
isSweeping
MaybeLock
&
lock
)
{
MOZ_ASSERT
(
buffer
-
>
zoneFromAnyThread
(
)
=
=
zone
)
;
MOZ_ASSERT
(
!
buffer
-
>
isInList
(
)
)
;
MOZ_ASSERT_IF
(
isSweeping
|
|
needLockToAccessBufferMap
(
)
lock
.
isSome
(
)
)
;
#
ifdef
DEBUG
auto
ptr
=
largeAllocMap
.
ref
(
)
.
lookup
(
buffer
-
>
data
(
)
)
;
MOZ_ASSERT
(
ptr
&
&
ptr
-
>
value
(
)
=
=
buffer
)
;
#
endif
largeAllocMap
.
ref
(
)
.
remove
(
buffer
-
>
data
(
)
)
;
/
/
Drop
the
lock
now
we
'
ve
updated
the
map
.
lock
.
reset
(
)
;
if
(
!
buffer
-
>
isNurseryOwned
)
{
zone
-
>
mallocHeapSize
.
removeBytes
(
buffer
-
>
bytes
isSweeping
)
;
}
}
#
include
"
js
/
Printer
.
h
"
#
include
"
util
/
GetPidProvider
.
h
"
static
const
char
*
const
BufferAllocatorStatsPrefix
=
"
BufAllc
:
"
;
#
define
FOR_EACH_BUFFER_STATS_FIELD
(
_
)
\
_
(
"
PID
"
7
"
%
7zu
"
pid
)
\
_
(
"
Runtime
"
14
"
0x
%
12p
"
runtime
)
\
_
(
"
Timestamp
"
10
"
%
10
.
6f
"
timestamp
.
ToSeconds
(
)
)
\
_
(
"
Reason
"
20
"
%
-
20
.
20s
"
reason
)
\
_
(
"
"
2
"
%
2s
"
"
"
)
\
_
(
"
TotalKB
"
8
"
%
8zu
"
totalBytes
/
1024
)
\
_
(
"
UsedKB
"
8
"
%
8zu
"
stats
.
usedBytes
/
1024
)
\
_
(
"
FreeKB
"
8
"
%
8zu
"
stats
.
freeBytes
/
1024
)
\
_
(
"
Zs
"
3
"
%
3zu
"
zoneCount
)
\
_
(
"
"
7
"
%
7s
"
"
"
)
\
_
(
"
MixSRs
"
6
"
%
6zu
"
stats
.
mixedSmallRegions
)
\
_
(
"
TnrSRs
"
6
"
%
6zu
"
stats
.
tenuredSmallRegions
)
\
_
(
"
MixCs
"
6
"
%
6zu
"
stats
.
mixedChunks
)
\
_
(
"
TnrCs
"
6
"
%
6zu
"
stats
.
tenuredChunks
)
\
_
(
"
FreeRs
"
6
"
%
6zu
"
stats
.
freeRegions
)
\
_
(
"
LNurAs
"
6
"
%
6zu
"
stats
.
largeNurseryAllocs
)
\
_
(
"
LTnrAs
"
6
"
%
6zu
"
stats
.
largeTenuredAllocs
)
/
*
static
*
/
void
BufferAllocator
:
:
printStatsHeader
(
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
#
define
PRINT_METADATA_NAME
(
name
width
_1
_2
)
\
sprinter
.
printf
(
"
%
-
*
s
"
width
name
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_METADATA_NAME
)
#
undef
PRINT_METADATA_NAME
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
/
*
static
*
/
void
BufferAllocator
:
:
printStats
(
GCRuntime
*
gc
mozilla
:
:
TimeStamp
creationTime
bool
isMajorGC
FILE
*
file
)
{
Sprinter
sprinter
;
if
(
!
sprinter
.
init
(
)
)
{
return
;
}
sprinter
.
put
(
BufferAllocatorStatsPrefix
)
;
size_t
pid
=
getpid
(
)
;
JSRuntime
*
runtime
=
gc
-
>
rt
;
mozilla
:
:
TimeDuration
timestamp
=
mozilla
:
:
TimeStamp
:
:
Now
(
)
-
creationTime
;
const
char
*
reason
=
isMajorGC
?
"
post
major
slice
"
:
"
pre
minor
GC
"
;
size_t
zoneCount
=
0
;
Stats
stats
;
for
(
AllZonesIter
zone
(
gc
)
;
!
zone
.
done
(
)
;
zone
.
next
(
)
)
{
zoneCount
+
+
;
zone
-
>
bufferAllocator
.
getStats
(
stats
)
;
}
size_t
totalBytes
=
stats
.
usedBytes
+
stats
.
freeBytes
+
stats
.
adminBytes
;
#
define
PRINT_FIELD_VALUE
(
_1
_2
format
value
)
\
sprinter
.
printf
(
"
"
format
value
)
;
FOR_EACH_BUFFER_STATS_FIELD
(
PRINT_FIELD_VALUE
)
#
undef
PRINT_FIELD_VALUE
sprinter
.
put
(
"
\
n
"
)
;
JS
:
:
UniqueChars
str
=
sprinter
.
release
(
)
;
if
(
!
str
)
{
return
;
}
fputs
(
str
.
get
(
)
file
)
;
}
size_t
BufferAllocator
:
:
getSizeOfNurseryBuffers
(
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
size_t
bytes
=
0
;
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
for
(
auto
alloc
=
chunk
-
>
allocIter
(
)
;
!
alloc
.
done
(
)
;
alloc
.
next
(
)
)
{
if
(
chunk
-
>
isNurseryOwned
(
alloc
)
)
{
bytes
+
=
chunk
-
>
allocBytes
(
alloc
)
;
}
}
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
bytes
+
=
buffer
-
>
allocBytes
(
)
;
}
return
bytes
;
}
void
BufferAllocator
:
:
addSizeOfExcludingThis
(
size_t
*
usedBytesOut
size_t
*
freeBytesOut
size_t
*
adminBytesOut
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
MOZ_ASSERT
(
majorState
=
=
State
:
:
NotCollecting
)
;
Stats
stats
;
getStats
(
stats
)
;
*
usedBytesOut
+
=
stats
.
usedBytes
;
*
freeBytesOut
+
=
stats
.
freeBytes
;
*
adminBytesOut
+
=
stats
.
adminBytes
;
}
static
void
GetChunkStats
(
BufferChunk
*
chunk
BufferAllocator
:
:
Stats
&
stats
)
{
stats
.
usedBytes
+
=
ChunkSize
-
FirstMediumAllocOffset
;
stats
.
adminBytes
+
=
FirstMediumAllocOffset
;
for
(
auto
iter
=
chunk
-
>
smallRegionIter
(
)
;
!
iter
.
done
(
)
;
iter
.
next
(
)
)
{
SmallBufferRegion
*
region
=
iter
.
get
(
)
;
if
(
region
-
>
hasNurseryOwnedAllocs
(
)
)
{
stats
.
mixedSmallRegions
+
+
;
}
else
{
stats
.
tenuredSmallRegions
+
+
;
}
stats
.
adminBytes
+
=
FirstSmallAllocOffset
;
}
}
void
BufferAllocator
:
:
getStats
(
Stats
&
stats
)
{
maybeMergeSweptData
(
)
;
MOZ_ASSERT
(
minorState
=
=
State
:
:
NotCollecting
)
;
for
(
BufferChunk
*
chunk
:
mixedChunks
.
ref
(
)
)
{
stats
.
mixedChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
BufferChunk
*
chunk
:
tenuredChunks
.
ref
(
)
)
{
(
void
)
chunk
;
stats
.
tenuredChunks
+
+
;
GetChunkStats
(
chunk
stats
)
;
}
for
(
const
LargeBuffer
*
buffer
:
largeNurseryAllocs
.
ref
(
)
)
{
stats
.
largeNurseryAllocs
+
+
;
stats
.
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
stats
.
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
const
LargeBuffer
*
buffer
:
largeTenuredAllocs
.
ref
(
)
)
{
stats
.
largeTenuredAllocs
+
+
;
stats
.
usedBytes
+
=
buffer
-
>
allocBytes
(
)
;
stats
.
adminBytes
+
=
sizeof
(
LargeBuffer
)
;
}
for
(
const
FreeList
&
freeList
:
freeLists
.
ref
(
)
)
{
for
(
const
FreeRegion
*
region
:
freeList
)
{
stats
.
freeRegions
+
+
;
size_t
size
=
region
-
>
size
(
)
;
MOZ_ASSERT
(
stats
.
usedBytes
>
=
size
)
;
stats
.
usedBytes
-
=
size
;
stats
.
freeBytes
+
=
size
;
}
}
}
